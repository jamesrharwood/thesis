# Methods

## Qualitative approach and research paradigm
<!-- Qualitative approach (e.g., ethnography, grounded theory, case study, phenomenology, narrative research) and guiding theory if appropriate; identifying the research paradigm (e.g., postpositivist, constructivist/ interpretivist) is also recommended; rationale** -->
We took a postpositivist, active research approach, whereby we expected all participants to have some negative responses to the intervention with only partial overlap with each other, and we acknowledged that our backgrounds and bias would affect how we interpreted those responses.

## Researcher characteristics and reflexivity
<!-- Researchers’ characteristics that may influence the research, including personal attributes, qualifications/experience, relationship with participants, assumptions, and/or presuppositions; potential or actual interaction between researchers’ characteristics and the research questions, approach, methods, results, and/or transferability -->

## Context
<!-- Setting/site and salient contextual factors; rationale** -->
Interviews took place online. Participants interacted with the website from within their own home or office, using their own computer. This was convenient and also meant we may detect problems with the website rendering on different browsers, screen sizes, and internet connections.

## Sampling strategy
<!-- How and why research participants, documents, or events were selected; criteria for deciding when no further sampling was necessary (e.g., sampling saturation); rationale** -->
We used purposive sampling, seeking participants that were all (#TODO: doing qualitative research?) but varied in their experience (years doing research), workplace (academia, industry, clinician etc.), english proficiency, geographic location, and stage of writing (drafting vs editing). We chose these (#TODO dimensions) as our previous work identified the barriers encountered differed between these groups. We were prepared to do additional sampling should we discover other dimensions of importance during data collection.

We used information power to guide our initial sample size. We considered our aim to narrow and sample to be specific but with adequate variation. Our intervention was based on behaviour change theory, we expected our methods to provide ample opportunities for deep discussion. For these reasons, we felt confident recruiting and initial sample of 10 participants. We decided to stop sampling when a) we had not identified any additional dimensions of importance and b) #ASK: once 3 participants in a row elicited fewer than 3 additional problem areas.

<!-- # DECIDE: renumeration -->

## Ethical issues pertaining to human subjects
<!-- Documentation of approval by an appropriate ethics review board and participant consent, or explanation for lack thereof; other confidentiality and data security issues -->
The study was approved by the Medical Sciences Interdivisional Research Ethics Committee at Oxford University. Participants gave informed consent via a web-form.

## Data collection methods
<!-- Types of data collected; details of data collection procedures including (as appropriate) start and stop dates of data collection and analysis, iterative process, triangulation of sources/methods, and modification of procedures in response to evolving study findings; rationale** -->

### Demographics questionnaire

After giving informed consent, participants provided their experience, place of work, english proficiency, and country of residence using an online form.

### 5 second test

Participants were invited to an initial interview conducted over Zoom. The lead researcher shared their screen and displayed the website homepage. Without warning, the page was closed after 5 seconds after which we used semi structured interviews to assess participants immediate understanding of and feeling towards the home page. Questions included:

"What do you think the website is about?"

If the participant mentioned the term "reporting guidelines", we asked "What do you think reporting guidelines are?" and "What tasks do you think you can use reporting guidelines for?"

"How would you describe the design of the site?"

"What would make you want to learn more about the website?"

"What impact do you expect the website to have on your job as a researcher?"

### User protocols / think aloud tasks

#### 1. Finding guidance and resources

We gave participants descriptions of research and asked them to identify the most relevant reporting guideline from the website. We did not tell them how to go about this - participants could use direct links on the homepage, the search bar, or could follow the "wizard" questionnaire. We did this task 3 times. The first time, participants were asked to find a guideline for animal research. This was the easiest guideline to find as the description was displayed prominently on the home page. Second, we asked participants to find guidance for reporting cohort studies. This was a little harder as it required participants to read and understand descriptions of study designs to distinguish between different, related, epidemiology guidelines. Finally, we asked participants to find what guidance they would to report (# DECIDE: what impossible task?). This was a difficult question as there is not perfect reporting guideline for this kind of article. Instead, participants had to (# FIXME: complete task).

We also presented tasks that involved finding tools. We asked participants to imagine they had been asked to submit a "completed checklist" by a journal, requiring them to find the guidance, then find and use the associated checklist. We asked participants what they expected "to do lists" and "templates" to be, and when they might be used.

#### 2. Finding information within a guideline

We then asked participants to find information within the #DECIDE: guideline. We asked participants how they would report #FIXME, why it is important  to describe #FIXME, and what they should write if they hadn't done #FIXME. These questions required participants to find items #FIXME respectively, and to locate content within collapsible boxes.

### Plus - minus test

At the end of the first interview we gave participants a task to complete in their own time over the coming weeks. We provided them with the methods items of the #FIXME guideline in a #DECIDE Word file. Participants were to read the methods items of the #FIXME guidelines in their own time and highlight sentences that elicited positive or negative responses and to mark them with a "+" or "-" symbol. They could add notes if they wanted to.

### Writing evaluation (Performance test)

If participants were actively writing an article we asked them to use the guidelines to write their methods section. If they had something already written, we asked them to complete a reporting checklist. We asked them to do this in their own time, within two weeks. Once complete, participants sent their work to JH via email who then checked their reporting against the the #FIXME guidelines, noting which items had been reported fully and which hadn't.

### Retrospective interview

Participants then attended a follow up interview two weeks later where JH asked open questions to explore the reasons behind participants + and - marks, and reasons for neglecting any items in their writing sample. The +/- test aimed to pick up non-specific responses, which would include parts of the text where the participant found difficult to understand. The writing check, however, would also reveal parts of the guidance that the participant had unknowingly _mis_understood.

Finally, we asked the participants again for their opinions on the website and guidance and how it could be improved.

<!-- #DECIDE: Do we really need people to go away and write, then come back? Recruiting people that are actively writing may bias our sample as we may attract participants that are looking for help/feedback on their writing. If we were to ditch this step, we could ask participants to do the +/- test live, in the session, and then interview them immediately. A middle ground could be to give them an option: Participants could expect to attend 2 interviews, but then if it turns out they aren't writing anything actively, we could cancel the second interview and do the +/- there and then. Downside of this is that we can't offer manuscript review as an incentive. But downside of offering manuscript review as an incentive is that it may bias our sample (to people looking for help) and also may give the game away. Perhaps we could counter act that by recruiting people that we already know don't like guidelines? But would they do the writing task?-->

## Data collection instruments and technologies
<!-- Description of instruments (e.g., interview guides, questionnaires) and devices (e.g., audio recorders) used for data collection; if/how the instrument(s) changed over the course of the study -->
Interviews were conducted over Zoom, using its in-built video and audio recording. We created interview schedules (#REF) and piloted them amongst students in the department. The version of the website tested can be viewed at (#REF).

## Units of study
<!-- Number and relevant characteristics of participants, documents, or events included in the study; level of participation (could be reported in results) --> #FIXME move to Results?

## Data processing
<!-- Methods for processing data prior to and during analysis, including transcription, data entry, data management and security, verification of data integrity, data coding, and anonymization/de-identification of excerpts -->
We used Trint to automatically transcribe audio recordings, and then manually double checked the transcripts and added context from the videos, interview notes, +/- annotations and writing sample. We imported transcripts into NVivo (#REF), creating cases for participants and intervention components.

## Data analysis
<!-- Process by which inferences, themes, etc., were identified and developed, including the researchers involved in data analysis; usually references a specific paradigm or approach; rationale** -->
We coded positive and negative experiences and grouped them by intervention component. We did this because we expected its output - negative and positive experiences grouped by intervention component - to be easier to act upon than if we grouped experiences by method.

<!-- # DECIDE: count similar problems? -->

## Techniques to enhance trustworthiness
<!-- Techniques to enhance trustworthiness and credibility of data analysis (e.g., member checking, audit trail, triangulation); rationale** -->
* Double checking of coding
* Member checking
* Triangulation? Read the paper Charlotte recommended. Perhaps it is about mixed method studies? Perhaps that means I could count errors / number of people that did a task successfully? https://www.bmj.com/bmj/section-pdf/186156?path=/bmj/341/7783/Research_Methods_Reporting.full.pdf
