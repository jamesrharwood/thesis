#### What more could be done to improve guideline adherence? 

Reading these studies raised many questions in my mind. I outline some of them rhetorically in the following paragraphs. 

Some articles end with rallying cries for heavy editorial enforcement, like "major improvements need active enforcement" @glasziouReducingWasteIncomplete2014. But would the heavy enforcement policy used in Pandis' study be feasible for other journals? 

It would be tempting to look at Pandis' results and conclude that heavy editorial enforcement is the best option. The dentistry journal in this study was small; only 23 manuscripts over 2 years were subjected to this treatment which took "from 30 to 60 minutes" per manuscript and the authors admit the benefits should be "considered in the light of the additional time requirement and need for greater editorial input during the peer review process". Perhaps this explains why few journals have adopted this policy in the decade since. But might there might other reasons? Perhaps editors are not sufficiently familiar with trial design or CONSORT content. Would it be feasible to extend the policy to cover more reporting guidelines? Would editors have the necessarily resources and expertise to treat all study designs in this way? Do editors see a benefit? Are they concerned about frustrating authors with multiple rounds of review? And even if the approach were feasible, it still did not result in complete adherence.

Other articles have called for lighter forms of enforcement. "We need to promote more active implementation, such as submission of the checklist with the manuscript." wrote Dechartres @dechartresEvolutionPoorReporting2017, and "it is not sufficient for journals to simply recommend the use of STREGA to authors in the authorsâ€™ instructions; instead, journals should require submission of the STREGA checklist together with the manuscript" wrote Nedovic @nedovicEvaluationEndorsementSTrengthening2016". But the PLOS One and BMJ Open studies found little effect of checklist completion on reporting quality @struthersGoodReportsDevelopingWebsite2021  @hairRandomisedControlledTrial2019. Peer reviews focussing on reporting might help @coboEffectUsingReporting2011.

Is enforcement the only answer? When reading these articles I noted some included incidental findings that hinted at other ways to improve reporting guidelines. In the PLOS One study, reviewers assessing adherence did not fully understand the ARRIVE guidelines themselves, and so did not always agree. The study authors wrote this "suggests the current guideline may require clearer dissemination". In the WebCONSORT study, @hopewellImpactWebbasedTool2016 Hopewell et al. made some guesses for why their intervention failed. 39% of manuscripts were excluded because editors had incorrectly identified them as randomised trials when they were not, and a quarter of authors selected inappropriate extensions. Perhaps the custom combined checklists were too long? Perhaps reporting items were not adequately explained? Perhaps intervening at revision stage was too late? But because they did not collect feedback from authors nor editors during their study, these were stabs in the dark.

In our BMJ Open study we surveyed authors that completed checklists. Many reported finding the checklist too long, confusing, irrelevant. <!--In retrospect, I am proud that we attempted to collect feedback from authors as none of the other studies I've mentioned in this chapter did.--> However, because we used multiple choice question with a (small) box for a free text answer, perhaps authors faced other problems that we missed. And we made no attempt to collect feedback from authors that _did not_ complete a checklist, or from editorial staff. These findings hint at opportunities beyond enforcement. Perhaps we could make guidelines easier to understand, find, and use. But how, exactly?

Other researchers seem to have had similar thoughts. Because outcome assessors rarely agreed whether a study adhered correctly to ARRIVE, Hair et al. @hairRandomisedControlledTrial2019  suggested refining the guideline's "content" and "perceived clarity". They also suggest reducing the number of items to make the guideline quicker to use. When revising ARRIVE, its developers sought feedback from authors on its clarity, and they indicated which items should be prioritised. 

Hopewell et al. created WebCONSORT because they worried combining CONSORT with its extensions may be "cumbersome and difficult" without providing evidence for this claim. I am also guilty of building an intervention to fix a problem without evidence. In 2018 I worked with EQUATOR as a freelance developer to create GoodReports.org, a website where authors could fill out checklists on-line. We made this website to address three problems we believed, anecdotally, to exist. First, some guideline checklists were in unusable formats (e.g. PDFs that could not be filled). Secondly, some guidelines were being paywalls. And thirdly, we worried that some authors might struggle to identify which guideline to use. And so we made a website where checklists were fillable, all guidance was open, and a questionnaire could help authors find the right guideline. Instinctively, I tried to make the website look nice and be easy to use. But those decisions were just default developer behaviour, and I not with any kind of behaviour change in mind. We kept the checklist text and guidance text exactly the same as it is in publications. 

The innovations I describe int he previous section - WebCONSORT, GoodReports, ARRIVE's prioritization - felt to me like stabs in the dark. They each address one or more barriers, but were based more on anecdote than evidence, and none of them took a systematic approach to identifying and addressing barriers. 

I wanted to find a thorough approach to identifying and addressing barriers. 