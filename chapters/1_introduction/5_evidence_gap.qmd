## Evidence gap: What more could be done to improve guideline adherence? 

Some of the articles I have cited end with rallying cries like "major improvements need active enforcement" @glasziouReducingWasteIncomplete2014. It would be tempting to look at Pandis' results as support for heavy editorial enforcement being the best option, but this approach may not generalise to other journals and other guidelines. The dentistry journal in this study was small. Only 23 manuscripts underwent this treatment over 2 years, and despite giving "30 to 60 minutes" of editorial attention to each manuscript, not all completely adhered to CONSORT. The study authors admit the benefits should be "considered in the light of the additional time requirement and need for greater editorial input during the peer review process". 

<!-- Might there be other reasons? Perhaps editors are not sufficiently familiar with trial design or CONSORT content?Would editors have the necessarily resources and expertise to expand the policy to cover other reporting guidelines? Do editors see a benefit? Are they concerned about frustrating authors with multiple rounds of review?  -->

Other articles have called for lighter forms of enforcement; "We need to promote more active implementation, such as submission of the checklist with the manuscript" wrote Dechartres @dechartresEvolutionPoorReporting2017, and "it is not sufficient for journals to simply recommend the use of STREGA to authors in the authorsâ€™ instructions; instead, journals should require submission of the STREGA checklist together with the manuscript" wrote Nedovic @nedovicEvaluationEndorsementSTrengthening2016. But the PLOS One @hairRandomisedControlledTrial2019 and BMJ Open studies @struthersGoodReportsDevelopingWebsite2021 found little effect of checklist completion on reporting quality. Additionally, the PLOS One study found that enforcing checklists, although less burdensome than the editorial enforcement described by Pandis, still came with costs to both editors and authors and significantly prolonged publication times. Peer reviews focussing on reporting might help @coboEffectUsingReporting2011 but is relatively unexplored as an option and may be unacceptable to reviewers.

These studies all focussed on different methods of enforcement. Some include incidental findings hinting at areas-for-improvement unfixable by enforcement alone. For example, because reviewers assessing adherence in the PLOS One study did not always agree or fully understand the guidance, the study authors suggested refining the guideline's "content" and "perceived clarity". In the WebCONSORT study, @hopewellImpactWebbasedTool2016 Hopewell et al. made some guesses for why their intervention failed. They had to exclude 39% of manuscripts because editors had incorrectly identified them as randomised trials, and a quarter of authors selected inappropriate extensions. As a solution, they suggest a tool to help authors and editors identify study types. The study authors also raised other hypotheses to explain why their intervention failed: perhaps the custom combined checklists were too long, unclear, or perhaps giving feedback during manuscript revision was too late. 

Both of these studies may have benefited from a qualitative component to understand why the interventions were not working. In our BMJ Open study we surveyed authors after they completed checklists. Many reported finding the checklist too long, confusing, or irrelevant. However, because we used a multiple choice question with a (small) box for a free text answer, and because we did not survey authors if they _did not_ complete a checklist, our results were fairly superficial. 

These incidental findings suggest authors may face challenges when trying to use reporting guidelines that require solutions beyond enforcement, and some groups have tried to address these challenges. Noting the outcome assessors' confusion in the PLOS One study, ARRIVE's developers took steps to refine its clarity when they revised it @perciedusertReportingAnimalResearch2020. They also decided to prioritise items to make the guidance quicker to apply. Hopewell et al. @hopewellImpactWebbasedTool2016 created WebCONSORT because they worried that combining CONSORT with its extensions may be "cumbersome and difficult" without providing evidence for this claim. In 2018 I worked with the UK EQUATOR Centre as a freelance developer to create GoodReports.org, a website where authors could answer a questionnaire to find the right guideline, and then complete the checklist online (previously, some reporting guidelines came with non-editable PDF checklists).

These innovation efforts shared limitations. None took steps to identify barriers thoroughly. By focussing on a few barriers they may have neglected others or introduced new ones. For example, in trying to make combining checklists easier, WebCONSORT may inadvertently made checklists longer, and increased the risk of authors selecting inappropriate guidance. Secondly, these studies did not systematically consider options to solve those barriers. For example, ARRIVE's development team decided to prioritize items as a way to make the guidance quicker to apply, but this is not the only solution. They could also have considered making guidance more concise, providing suggested wording or creating tools to speed up writing. Thirdly, although the studies describe their innovations, they do not always describe changes beyond the tool in question (e.g., changes to editorial practice) or how changes are expected to alter behaviour.

#STRETCH I could explain this better I think