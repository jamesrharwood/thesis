## The problem of poor reporting in health research

Facing an uncertain choice during treatment for multiple myeloma, epidemiologist Alessandro Liberati wrote "Why was I forced to make my decision knowing that information was somewhere but not available?" @liberatiUnfinishedTripUncertainties2004. When I started my DPhil governments may have been asking the same question. The world was in the grip of COVID-19 and decision makers were wading through a deluge of patchy research articles missing important information @ziemannPoorReportingQuality2022. In the years since, members of my family have had to make treatment decisions where the evidence is of "low certainty" because key details are missing from research articles.

A selfish silver-lining of these tumultuous years was that my family and friends finally understood the problem my thesis addresses: when medical researchers inadequately describe what they did, why they did it, and what they found, other people cannot understand, replicate, or use their work. Research costs huge amounts of time, money, and effort, and the written account is typically its sole legacy. When details are omitted they are lost. The remaining gaps are sources of doubt; are they accidental omissions? Oversights? Cover-ups? Whatever their source, the gaps fragment the full picture, and the potential value to patients drains away.

Early concern over reporting quality came from frustrated reviewers unable to find the data they needed within research reports. For example, in 1963, Glick @glickInadequaciesReportingClinical1963 found many reports of psychiatric therapies used ambiguous descriptions of treatment duration like "at least two months" or "from one to several months". These descriptions were so vague they were "unsuitable for comparative purposes". In the decades since, evidence syntheses have increasingly underpinned clinical guidance, and the Cochrane Collaboration and the National Institute for Health and Care Excellence (NICE) are two organisations conducting such syntheses. Their reviews influence clinical practice throughout the UK and globally, but are undermined when the underlying literature omits details. Dechartres et al. @dechartresEvolutionPoorReporting2017 looked at all Cochrane reviews published between 2011 and 2014. These reviews included over 20,000 randomised controlled trials. A third of these trials were so poorly reported, reviewers could not judge their potential for bias thereby limiting the confidence of conclusions. Some researchers fear that _unclear_ bias may actually be _high-but-hidden_ bias. For example, SavoviÄ‡ et al. @savovicAssociationRiskofBiasAssessments2018 found exaggerated intervention effect estimates in randomized trials with high or unclear (versus low) risk of bias judgements. Poor reporting, therefore, not only undermines the confidence of systematic reviews but potentially allows distortions to creep in to the evidence un-checked.

No similar data exists for NICE reviews, so instead I'll draw on personal experience of consulting NICE evidence when a relative was deciding between two cancer procedures. Both options had serious side effects, similar success rates, and neither had a clear-cut advantage given my relative's medical history. To respect my relative's privacy I won't cite the NICE review, but it included ten studies: seven case series, two non-randomised comparison studies, and one randomised trial. Two studies did not describe participants' gender, one did not describe ages, four did not describe their exclusion criteria, three did not describe side-effects fully. Consequently, NICE's recommendation was less certain than it could have been. Did the case series include patients similar to my relative? We couldn't tell. Did participants of equivalent age experience the side effects we feared? No idea. My family echoed Liberati's frustration: the missing information was probably collected at some point and we (as tax payers) may even have paid for some of that research, but because details were not written down they would not help us in our hour of need. In typical scientific objectivity, Dechartres wrote @dechartresEvolutionPoorReporting2017 "waste related to poor reporting could be completely avoided", but it's hard to remain so dispassionate when tossing and turning in the small hours of the morning, deliberating a life-changing decision, when details matter more than ever.

Two of the studies in that NICE review barely described the procedure in question, so had the surgeon wished to repeat it for my relative he wouldn't have been able to. This reveals another consequence of poor reporting: when research is poorly described, it cannot be repeated. Doctors and service providers need clear descriptions to replicate interventions @glasziouWhatMissingDescriptions2008. As Feinstein noted in 1974 @feinsteinClinicalBiostatisticsXXV1974, it is difficult enough for a clinician to understand the value of unfamiliar procedure, but "it is much more difficult when he is not told what that procedure was". For example, Davidson et al. @davidsonExerciseInterventionsLow2021 reviewed trials describing exercise interventions for chronic back pain and found authors often did not describe interventions sufficiently for other healthcare providers to copy them. Researchers also need clear descriptions to understand, appraise, and repeat each others' work For example, Carp @carpSecretLivesExperiments2012 described how a third of 241 brain imaging studies missed information necessary to interpret and repeat them, like the number of examinations, examination duration, and the resolution of images. 

As well as being wasteful, poor reporting breeds mistrust. The COVID pandemic saw a deluge of research, yet of 251 trial reports @kappTransparencyReportingCharacteristics2022, only 14% described harms adequately; fifty five trials (22%) did not report any information about harms, and most others (n=150, 60%) did not explain whether harms resulted in participants dropping out. Fear over possible side effects stopped many people from getting vaccines @pourrazaviCOVID19VaccineHesitancy2023, seeking medical treatment @yilmazHesitancyRegardingMedical, or wearing masks @taylorNegativeAttitudesFacemasks2021. By not reporting side effects, trial reports did nothing to quell those fears and, in some cases, may have stoked them further. Even years later, in April 2024, a British member of parliament describing himself as "double vaccinated and vaccine-harmed" @Covid19ResponseExcess2024 told the House of Commons that the "adverse effects of the[Covid-19] vaccine...are not currently known", before speaking of "gaslighting" and blaming the vaccine as causing "enormous harm". He repeatedly railed against an omission of information about harms &mdash; "crucial information was hidden from the regulator and the public", "The public are being denied that data... yet again, data is hidden with impunity" &mdash; before jumping to un-evidenced, exaggerated, or conspirational claims of harm and cover-ups. The absence of evidence seemingly left a void for fantasy and fear to fill. 

These are a mere handful of many studies documenting poor reporting in medical literature. A 2023 systematic review found 148 published between 2020-2022 alone @santoMethodsResultsStudies2023. All investigated reporting quality in different medical research disciplines, and *almost* all concluded reporting was sub-optimal. Hence, poor reporting is a long-standing problem, plagues all disciplines, devalues research, seeds mistrust in science, introduces avoidable doubt and patient distress, and derails the uptake of new knowledge into clinical practice.