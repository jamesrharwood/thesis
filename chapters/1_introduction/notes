# Botos

Of 2209 submissions included in the analysis, 1144 (51.8%) indicated that at least one reporting guideline was used. The STROBE guidelines were the most common (n = 531, 24.0%). Of the 2068 (93.6%) submissions that were rejected, 1105 (50.1%) indicated using reporting guidelines and 963 (43.6%) did not (mean [SD] scores of rejected vs not rejected, 0.53 [0.50] vs 0.49 [0.50], P = .47). Of the 1033 ratings for adherence to reporting guidelines, mean (SD) scores for not rejected vs rejected submissions were 3.2 (1.61) vs 2.9 (1.57) (P = .005), and mean (SD) scores for reporting guidelines use vs no use were 3.1 (1.48) vs 2.9 (1.70) (P = .01). Of the 1036 ratings for clarity of presentation, mean (SD) scores for not rejected vs rejected submissions were 3.6 (1.00) vs 3.1 (1.08) (P < .001), whereas mean (SD) scores for reporting guidelines use vs no use were 3.3 (1.04) vs 3.3 (1.10) (P = .64).


ICARUS study - ARRIVE
"There is considerable room for improvement, and this study shows that an editorial policy of making ARRIVE checklist completion “mandatory” without compliance checks has little or no impact"

"Journal endorsement of these guidelines appear to have improved reporting quality [15,18]; however, it is often unclear what actions journals take to promote adherence [17]and the extent of editorial involvement is likely to have an impact. Prior reports indicate that assessing compliance with reporting guidelines at the stage of peer review leads to a significant improvement of reporting quality [5]. Other approaches (e.g., actions on the part of funders or institutions) may also be beneficial, but a successful strategy is likely to be **multi-dimensional**. Further, the findings reported here and the limited agreement between outcome assessors both in this study and in the recent investigation of study quality following the introduction of a new editorial policy at Nature journals [12] suggests that an important part of guideline development should be refinement of the content, the number of items (with fewer generally being better), and the agreement between assessors. It may be that a more formal adoption of research improvement strategies, with an original focus on a smaller number of items judged by a stakeholder to be of greatest importance, will allow an incremental approach to enabling and measuring improvement"

"We need therefore a better understanding of the barriers to implementing quality checklists for animal experiments"

"Perhaps unsurprisingly, due to the additional time required for ARRIVE checklist requests, both the number of days manuscripts spent in the PLOS editorial office and the number of days from manuscript submission to AE assignment were found to be significantly longer in the intervention group."

"The editorial resource required to ensure that all accepted publications meet the requirements of the ARRIVE checklist is likely to be considerable, given that PLOS ONE is a high-volume publisher, with around 44,000 submissions per year"

"Another consideration is the perceived clarity of the checklist to authors and reviewers. Although reviewer agreement was generally high, a few questions were less well understood by our outcome assessors which suggests the current guidelines may require clearer dissemination among the research community"

# Kilicoglu



# Deschartres

"Although using reporting guidelines, such as the CONSORT checklist, is associated with more complete reporting,48 49 their implementation varies between journals44-52 with many journals having no policy or mentioning only the existence of the CONSORT statement in the instructions to authors.44-52 We need to promote more active implementation, such as submission of the checklist with the manuscript, as it has been associated with better reporting."

No experience collection - retrospective 
# Dejong

Nothing 

# Nedovic

"Therefore, we suggest that it is not sufficient for journals to simply recommend the use of STREGA to authors in the authors’ instructions; instead, journals should require submission of the STREGA checklist together with the manuscript"

No experience collection - retrospective 

# Howell

No conjecture on why they saw no effect or how to improve SQUIRE.

No attempt to collect data on experience. - retrospective

# Pouwels

"The recently implemented strategy of PLOS Medicine is an interesting solution [17]. Requiring authors to submit a checklist with sufficient text excerpted from the manuscript to explain how they accomplished all applicable items [17] may results in better adherence to the guideline."

Retrospective

# Bastuji-garin

No ideas



# BMJ Open

# Hopewell - WebCONSORT

Not based on any evidence. 

"However, proliferation of these extensions makes their application difficult for a specific trial as it involves combining items from the main CONSORT checklist with those from one or more extensions. This could be cumbersome and difficult to apply in practice and so CONSORT may have limited impact on the reporting quality of these trials. The objective of this study was to evaluate the impact of a simple web-based tool (called WebCONSORT, which incorporates the main CONSORT checklist and different CONSORT extensions) on the completeness of reporting of randomised trials published in biomedical journals. WebCONSORT allows authors to obtain a customised CONSORT checklist and flow diagram specific to their trial design and type of intervention. Our hypothesis was that this tool would allow optimal use of the CONSORT Statement and its extensions, thus leading to an improvement in the transparency of articles related to randomised trials."

Didn't collect any evidence about experience. 

Hypothesised reasons for failure:

* journals already endorsed CONSORT so maybe authors already believed they were complying (but low compliance doesn't support that)
* Customised checklists were very long. Could have been overwhelming. 
* No instructions on how to implement each item, so checklist info alone might not have been enough.
* Might have been too late. The intervention happened at revision stage. Might be useful to intervene earlier - CobWEB?

Did find evidence that editors struggle to identify study type. They had to exclude 39% of manuscripts because they were not RCTs, despite clear instructions given to editors and authors. 

Also found evidence that a quarter of authors selected inappropriate CONSORT extension or failed to select the correct one. 

There is a clear need for better education much earlier in the publication process for authors and journal editorial staff on when and how to implement CONSORT and, in particular, CONSORT-related extensions. It may be more effective to focus on a core set of CONSORT items with more detailed information on how to implement each item within the context of a specific trial.

# Baker - ARRIVE 

"In 2010, the Animal Research: Reporting of In Vivo Experiments (ARRIVE) guidelines were introduced to help improve reporting standards. They were published in PLOS Biology and endorsed by funding agencies and publishers and their journals, including PLOS, Nature research journals, and other top-tier journals. Yet our analysis of papers published in PLOS and Nature journals indicates that there has been very little improvement in reporting standards since then. This suggests that authors, referees, and editors generally are ignoring guidelines, and the editorial endorsement is yet to be effectively implemented."

They point out that different journals might have to use different enforcement strategies, comparing PLOS One - tens of thousands of submissions each year - with PLOS Medicine, which publishes relativey few comparative animal studies. They note that items could be enforced in different ways. Some form part of the ethical review process, others might require input from experts (e.g., statisticians). They suggest that journals could choose to enforce "A pragmatic approach might be to implement the most important aspects of the guidelines [3],[4], such as reporting the extent of blinding and randomisation".