# Discussion

## Description of findings

<!-- 
* short summary of the main findings
* include some interpretation of the data in the context of previous findings, experiences, theory, or a guiding paradigm or approach.
* The discussion provides authors an opportunity to:
  * elaborate on their findings in relation to their research question(s) and study purpose(s);
  * connect their findings to prior empirical work, theories, and/or frameworks; 
  * and discuss implications:
    * describe how findings contribute to or advance the field
    * Implications may include transferability, or specifying the appropriate scope for generalization of the findings beyond the study (e.g., to other settings, populations, time periods, circumstances).
-->

### Summary of results

Participants identified {{< var counts.ideas >}} ideas employing all intervention functions. There were ideas to consider before, during, and after creating guidance, ideas to consider when writing the guidance down into a guideline, ideas about tools to help guidance application, ideas about ongoing activities to support or promote guidance use, and ideas about refining guidance over time in response to feedback. Many of these ideas could be enacted by guideline developers, publishers, and EQUATOR, but participants also saw opportunities for ethics committees, funders, academics, registries, and syllabus writers; stakeholders who are typically less frequently considered.

### Relation to study purpose

My objective was to identify ideas of how to address factors that may influence adherence to reporting guidelines, and I believed that including perspectives from a range of stakeholders would lead to more ideas. This seems to be the case: before this study, I had thought of {{< var counts.jh_ideas >}} ideas, EQUATOR staff then extended this to {{< var counts.EQUATOR_ideas >}}, still far below the number finally identified. 

My second aim was to garner stakeholder buy-in. I had a fair response rate from guideline groups and of those that did respond the reception was positive, even if they were unable to take part. Multiple guideline developers volunteered their guideline to be a "guinneapig" and expressed support for my work. Most developers were accepting of the barriers I presented. Only one developer expressed scepticism that reporting guidelines were anything but perfect, and requested additional evidence of authors' negative experiences.

### Relations to previous findings
<!-- 
#ASK: some interpretation of the data in the context of previous findings, experiences, theory, or a guiding paradigm or approach?? Not sure about this -->
Where the barriers I identified previously offer explanations for why previous intervention studies (#REF) have had disappointing results, the ideas generated here provide hypotheses for how these interventions could be improved, and who could improve them.

### Benefits of the BCW

Using the intervention function and policy categories as a guide was an effective way to prompt discussion when participants ran out of ideas or repeatedly focussed on the same type of intervention that had already been documented.

The framework also helped guideline developers detach themselves from their creations and look at them objectively. This was a relief, as I had worried that developers may feel defensive or criticised. Keeping discussion focussed on guidelines _in general_ (not theirs in particular), having evidence for barriers, and including mixed perspectives within groups, also helped.

### Interpretations

Two questions divided participants more than others; whether reporting guidelines should be agnostic to design choices, and whether they should be viewed as rules (standards or requirements), or just a point in the right direction (guidance). The term 'guideline' is used ambiguously in scholarly publishing. NICE defines clinical guidelines as 'recommendations', which is in line with the Cambridge dictionary definition: "information intended to advise people on how something should be done". Thus guidelines "advise" or "recommend" but don't enforce or legislate. However, _journal author guidelines_ can be perceived as _rules_ that researchers have to adhere to if they want to publish, synonymous with _instructions_. Some participants were concerned that reporting guidelines may be perceived as rules, and that authors should be reassured that "guidelines are just that: guidelines!". Conversely, a few participants argued strongly that reporting guidelines should be seen as rules.

These divisions over whether guidelines are guides or rules, and whether they should include opinions about design, may reflect differences between the scope of reporting guidelines, and the perspective of developers. For example, some reporting guidelines cover specific types of study that are homogenous in design and approach (e.g. ontology and epistemology). In this instance, developers may feel comfortable making assumptions or recommendations about design choices, and may be more likely to consider guidelines as standards that can be enforced. In comparison, developers of guidelines with a broader scope may have purposefully tried to not make assumptions about design or approach in an attempt to accommodate studies that share a single feature (e.g. population or method) but vary in all others, such as their design or ontology.

The division could also reflect variation in developers' aims. Transparency may be the end goal for most, but others may consider reporting standards to be a stepping stone towards influencing (what they consider to be) good design.

<!-- #ASK: connect their findings to prior empirical work, theories, and/or frameworks -->

### Implications

#### Contributions to the field

This is the first time a Behaviour Change framework has been used to identify ways to encourage the application of reporting guidance. It is an important step in the journey towards an evidence based intervention underpinned with theory. It's also the first time that different guideline developers have come together with publishers and academics to consider reporting guidance dissemination as a system. 

These results will be of use to the reporting guideline community. Developers may find inspiration here when writing or revising guidance. At the time of writing, the EQUATOR Network was in the process of updating its advice for guideline developers, which I hope will be informed by these results. The ideas generated may also be of interest to publishers, funders, and Universities. And, of course, Within my DPhil, it is the groundwork for subsequent chapters.

#### Transferability

Although only a few reporting guideline development groups took part in this study, most ideas identified were abstract enough to generalise to most reporting guidance, which tend to be developed and distributed in similar ways; (e.g., all development groups will have to consider what guidance to create, its scope, how to communicate it clearly and how to disseminate it). Some ideas may even generalise to other interventions to encourage good research practices (e.g., to communicate personal benefits, not to patronize researchers). Some ideas may generalise to clinical guidelines, although those are better studied.

## Limitations, and future work

### Limitations

The study may have benefited from more diversity in participants' backgrounds and expertise. All participants were western and proficient in English. Although this is partly a limitation of who writes reporting guideline, we could have sought input from publishing houses that cater to non-western authors. We could have recruited participants from funders, ethics committees, or registries, or experts familiar with user experience of websites or written documents. Broadening the participant pool would certainly have lead to more ideas. For instance, with my background in creating software products, I noticed a few blind spots whilst analysing and writing up the results, which I noted in my reflection diary.

For example, participants did not consider writing guidance using in active voice, directed at the intended user, an accepted rule-of-thumb in marketing and used by NICE (#REF). Nor did they consider optimizing resources for search engines. This would make them easier to find, and could be a way to encourage earlier use. For example, EQUATOR's website ranks highly in Google searches for "STROBE checklist" but not "How to write an observational epidemiology study" (#REF).

My focus groups were smaller than I had planned.Some would consider these group sizes too small to be called focus groups, and may instead call them paired interviews, dyads, or triads ([#REF](https://www.aqr.org.uk/glossary/triad)). A hallmark of focus groups is that they use "group interaction to produce data" @l.morganFocusGroupsQualitative1997, and these interactions may include sharing experiences and challenging each other. 

However, I did not feel the small group sizes to be a limitation in this study for two reasons. Firstly, participants had deep understandings of the topic (evidenced by sessions overrunning and participants dwelling on a single topic) which meant that even pairs of participants had plenty to discuss, share, and debate. If I had condensed participants into, say, 3 groups of 5-6 participants, each participants would have had less time to speak and I anticipate that many ideas would have gone un-spoken. Secondly, because participants were co-editing a file and building upon the thoughts of previous groups, participants could react and respond to to participants from previous groups.  

### Future work

We purposefully did not seek input from authors as this study required input from experts familiar with reporting guideline dissemination. We will explore authors' perspectives in future work.

We also made no attempt to prioritize ideas. This prioritization should be done be each stakeholder individually as it is subjective (what is practical for one may be impractical for another). In my next chapter I describe how I worked with EQUATOR to prioritise ideas and turn them into intervention components. 
