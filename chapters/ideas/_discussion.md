# Discussion

## Summary of results

Before this study, I had thought of {{< var counts.ideas_JH >}} ideas to improve reporting guideline dissemination. EQUATOR staff then extended this to {{< var counts.ideas_EQUATOR >}}, and the participants in this study have extended this further to {{< var counts.ideas >}} ideas. Thus my main objective, to elicit new ideas, was met. I was buoyed by the support for the project. Most of the guideline developers were keen to be kept abreast of my work, and a few offered to work together on future studies. Only one developer expressed scepticism that reporting guidelines are anything but already perfect (see @sec-box-reflexivity-focus-group). 

I've tried to give each idea its own airtime in my results section, but here I will highlight a couple of areas of tension. Guideline developers felt differently about whether or not reporting guidelines should include design or procedural advice and would sometimes talk reporting and design issues interchangeably. There were different reasons for including design advice. Sometimes design was used to justify why the reporting item was important "You need to describe how you did X because studies that _don't_ do X are at risk of Y". Other times, design advice was included to be helpful or educational. Whereas all participants agreed that design advice is useful, some felt concerned that placing design advice within reporting guidance, or using examples of poor design to justify reporting items, has negative consequences. It makes guidance longer and risks shaming authors, making them less likely to report transparently and thereby undermining the true purpose of reporting guidance.

There was also disagreement over whether reporting guidelines should be viewed as rules (standards or requirements), or just a point in the right direction (guidance). The term 'guideline' is ambiguous in this way. NICE defines clinical guidelines as 'recommendations', which is in line with the Cambridge dictionary definition: "information intended to advise people on how something should be done". Thus guidelines "advise" or "recommend" but don't enforce or legislate. However, _journal author guidelines_ can be perceived as _rules_ that researchers have to adhere to if they want to publish, synonymous with _instructions_.

## Results in context

<!-- #Note: Are these in the right place? Or are these implications? -->

Where the barriers I identified previously offer explanations for why previous intervention studies (#REF) have had disappointing results, the ideas generated here provide hypotheses for how these interventions could be improved, and who could improve them. These results will be of use to the reporting guideline community. Developers may find inspiration here when writing or revising guidance. At the time of writing, the EQUATOR Network was in the process of updating its advice for guideline developers and was hoping to redesign their website. These results will be pertinent to both of these projects.

Thinking about transferability, the ideas here could be of interest to developers of other kinds of guidelines or guideline (e.g., clinical guidelines, critical appraisal tools like CASP and latitude #REF). More interestingly (to me at least), is the transferability of the _approach_. Although theories of behaviour change are often used in clinical projects seeking to improve adherence to clinical guidelines, the approach is not utilised by meta-researchers or people seeking to improve adherence to best practices in academia. At the time of writing, I am not aware of any such interventions that have used behaviour change theory. Without such a framework, brainstorming may get "stuck" on certain types of intervention (in my experience, regulation, education, and training) and neglect others. Researchers seeking to influence the behaviour of other behaviours could benefit from bringing in stakeholders and using a behaviour change framework to structure discussion. (see @sec-where-do-I-sit in my discussion chapter).

## Box: Reflexivity whilst running and analysing focus groups

* BCW seemed to help participants gain distance from their resources and look at them with fresh(er) eyes.
* Using the intervention function and policy categories as prompts was an effective way to prompt discussion or move things along when participants got stuck in a loop. 
* Overall I felt I improved as a facilitator. I quickly identified a few weaknesses - I may talk too long when nervous, and I had a habit of asking two questions in one. My training at Oxford Qualitative Courses prepared me well and I could feel myself improving.
* One participant in particular was a little challenging. They continued to refuse to accept that reporting guidelines may be difficult to use, and demanded empirical, quantitative, evidence to the fact.

## Strengths, limitations, and future work

contributions-to-the-field:

* First time RG dissemination has been thought of in this way.
* Brought together stakeholders
* First step towards a unified dissemination plan backed up by theory
* Will underpin future feasibility and evaluation studies

limitations:
  
* all western and native english speakers
* We purposefully didn't rate or rank ideas. The BCW suggests using APEASE criteria which stands for #TODO. Different stakeholders will rank ideas differently especially with regards to cost-effectiveness.
* Could have included funders, more journals, or other experts. Whilst analysing the data ideas, the software developer inside of me continued to come up with ideas. For example, when considering how to make resources findable nobody considered search engine optimization. Enriching websites with metadata so that search engines can index them properly is marketing 1-0-1, and including phrases like "how to write" or "how to describe" may help authors find researchers earlier in their research. SEO seems obvious to me as a developer, but was overlooked by participants who may not be as familiar with it.  

integration-with-prior-work:

implications:
