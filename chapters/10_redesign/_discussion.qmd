## Discussion

I have demonstrated how I have used a data-driven approach, guided by behaviour theory, to re-design how reporting guidance is disseminated. I have proposed {{< var counts.redesign.intervention-components >}} intervention components, addressing {{< var counts.redesign.barriers-targeted >}} barriers and employing {{< var counts.redesign.intervention-functions-used >}} intervention functions. By linking components with barriers and functions, I have justified my suggestions using evidence and described how they are theorized to work. I have then created a prototype website to demonstrate how these components could be realised.

Together, these changes amount to a complete redesign of two key parts of the existing system through which reporting guidelines are currently disseminated; the guidelines themselves, and the EQUATOR Network website which is visited by almost 1 million authors each year.

### When comparing current intervention

This reassessment required participants to take a step back and look at the current set-up with fresh eyes. We did this informally. Some participants shared long-standing frustrations with the website or guidelines. One participant shared designs she had created years ago for a redesigned EQUATOR website. Other times, after discussing a barrier or idea, we would go to the guidelines to see how things are  done currently.

So this comparison was ad-hoc, and I have included pieces of it in this chapter purely to provide context to the proposed changes. I sought out examples of a behaviour change technique being implemented, not being implemented, or being implemented poorly. I made generalisations about RGs using words like "some" or "few" to give an impression of how frequently RGs currently use a given BCT. These frequency descriptions are based on my own observation, and not on a formal audit.

I considered systematically auditing the content of EQUATOR Network website and popular guidelines to see which behaviour change techniques they employ and which of our ideas were already present. I decided against this  for two reasons. Firstly, with so many  ideas and so many guidelines, this would have taken time and I decided instead to prioritize building and testing a prototype. Secondly, this audit wouldn't have dramatically influenced the intervention components we designed, but would merely quantify how different my proposed intervention is to the current set-up. Who would be interested in quantifying this difference? Perhaps my thesis examiners, and perhaps the guideline development community. But quantifying differences wouldn't bring me any further towards helping authors or impacting reporting quality, like building a prototype would. Should the guideline development community need that evidence then this audit could be done in the future once the redesigned intervention has been refined (see next chapter) and finalised.

### Limitations & reflections on process

Using a framework and a systematic method helped participants (and I) to check our biases. Instead of relying on personal preference, we tried to ensure choices reflected the function we were trying to employ. For example, when choosing a background image, instead of asking "do you like this one?", the questions became "what feelings do you think this image conveys? Does it communicate simplicity?". Working as a group helped mitigate individual preferences and peculiarities. 

However, there is no avoiding the fact that many decisions required a degree of subjectivity and, as lead researcher, designer, and developer, often these decisions landed on my shoulders. I tried to mitigate this by involving EQUATOR members in the workshops and development process, prioritizing their ideas over my own, and providing many opportunities for feedback. But the result definitely has my “stamp”. If someone else had built it using the same table of intervention components then some things might be the same (like simplifying the user journey from 5 steps to 2, or the conventional layout of the home page) but other things would look very different (like the choice of wording and images).

Using a framework also helped participants to consider options that may not have otherwise come to mind. However, our imagination may have been constrained by what already exists. Although I encouraged blue-sky thinking, participants often focussed on tweaking what already exists instead of starting from a blank slate. If reporting guidelines didn't exist, how else might we have tackled poor reporting? If EQUATOR didn't exist, would we have a similar organisation to fill its place? How might that organisation be structured, governed, and what kind of legal entity might it be? If the publishing industry didn't exist, might we have imagined different ways of describing research that were more formulaic than free-form articles?

These imagination constraints may be a weakness, but they are also al practical. EQUATOR is in a privileged position that in that it is known and trusted by publishers, guideline developers, and many authors. Thousands of journals and authors already use reporting checklists. So whilst the changes proposed in this chapter (and the ideas proposed in the chapter before) may be criticised for not being radical enough, for an organisation (and a PhD student) with limited time and resources, it makes sense to improve a system that already has significant buy-in from the academic community, over and above destroying that system or trying to create a new one from scratch.

Our horizons may have also been limited by group-think. If I were to repeat the work, I would have included a small, diverse group of authors to take part in the design process. I would have invited representatives from the publishing industry, funding community, and people more familiar with designing digital behavioural interventions. Including these diverse, informed voices in the design process could have lead to more radical design choices. 

#TODO Some of this discussion was written before I split the workshops into their own chapter. Content might be duplicated.

I did, however, include guideline developers in the design process. When editing SRQR I made sure to include Bridget O'Reilly, SRQR's lead author, in every step. I explained my process and invited her feedback during and after editing. The experience was very positive. Bridget was supportive of what I was doing and liked the end result. But I acknowledge that other guideline developers may feel protective over their writing, and that many may not have the time nor funding to revise their guidance. I discuss these limitations further in my discussion chapter.

Editing SRQR revealed another limitation: gaps in item description. There was often no guidance of what to write if an item wasn't or couldn't be done. For instance, the target sample size item had no instruction of what to write if you didn't ever have a target in mind. Some items were missing any kind of justification of why the item was important and to whom. Bridget felt that filling these blanks would require time and input from SRQR's development team, and so I left these gaps unfilled for now.

### Future work

I anticipate similar gaps for other reporting guidelines, and would seek to work alongside guideline developers to fill them and I upload other popular reporting guidelines and edit items into a consistent structure using the same process as for SRQR. Further development work will be required before the new website can be made live. Some of this work are technical tasks that, although necessary, do not have behavioural impact. For example, I will need to integrate the new website as a subdomain of EQUATOR's existing one, and I will create automated tests that run before each deployment. However, other tasks _do_ appear on the list of intervention ideas, and will affect behaviour. For example, I intend to optimise each guideline page so that it ranks highly in search engines.

Beyond the intervention components presented here, the prioritization exercise identified other ideas that EQUATOR would like to implement, but that I chose not to act upon. For example, participants favoured developing training resources specific to individual reporting items, and creation of network of “reporting champions”, akin to the UKRN model. EQUATOR participants liked the idea of lobbying funders to require reporting guidelines be used for applications. The work in this chapter could be used to support funding applications to support these endeavours.

I mentioned earlier that one limitation of this study was that authors weren't included in the design process. In the next chapter, I explain how I have addressed this by piloting the website amongst authors.

#TODO perhaps here I could explain how this new site differs to GRs.