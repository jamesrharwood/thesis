## Introduction

The output of my focus groups was a long list of ideas. Because I had encouraged blue-sky thinking amongst a broad range of stakeholders, not all ideas were appropriate nor practical for EQUATOR to implement. For example, EQUATOR aren't able to offer financial rewards for good reporting. And although EQUATOR _could_ advocate for Universities to fire researchers that don't adhere to reporting guidelines, would staff support such a policy? Our first objective, therefore, was to determine which ideas we _could_ implement, which ones we _should_ implement, and _how_ we could turn them into intervention components. Of our favoured options, I then had to decide which ideas I could turn into intervention components within the context and confines of my PhD I then had to design and build the intervention.

In this chapter I explain how I a) worked with EQUATOR UK to prioritize intervention functions and policy categories in order to determine which ideas to consider b) used behavioural analysis to turn ideas into intervention components which I then c) developed into an intervention prototype.

## Methods and Results

### Objective 1: Prioritizing intervention functions and policy categories

#### Purpose

To identify which ideas EQUATOR and I should take forward.

#### Methods

The prioritization exercise occurred during the final three workshops described in chapter {{< var chapters.workshops >}}. Although 6 members of the UK EQUATOR Centre took part in the internal workshops, only 4 took part in the prioritisation exercise.

With over a hundred ideas, it was impractical to rank them individually. There were too many, and some were very similar, which made discussions at this level tedious and confusing. Additionally, because ideas often conflated intervention functions and policy categories, it was difficult to tease ideas apart - if we didn't like an idea, was it because we didn't like its function or its mode of delivery? For example, in the idea "journal instructions to authors should use reassuring authors", although EQUATOR members may have favoured using persuasive "reassuring" language, they may have felt that expending effort to lobby journals to change their author guidelines may be an expensive and impractical use of research budget.

Instead, I followed the recommendation of Michie et al, which advises to prioritize intervention functions and policy categories instead of the ideas themselves @michieBehaviourChangeWheel2011 (see chapter {{< var chapters.bcw >}} for an overview).  I used ideas as concrete examples of these abstract concepts, but I didn't ask members to rank ideas directly.

Michie et al @michieBehaviourChangeWheel2011 recommend using the APEASE criteria to prioritze options, which stands for Affordability, Practicability, Efficacy, Acceptability, Side-effects, and Equity. Participants considered the APEASE criteria for each intervention function on their own before discussing as a group and adding notes to a co-edited file. As per Michie et al's advice @michieBehaviourChangeWheel2011, I then asked them to decide which intervention function(s) they would prioritize, which they would consider but not prioritize, and which (if any) they would avoid. Participants then discussed their preferences as a group before reaching consensus. I then repeated the exercise to prioritize policy categories.

To decide which of the favoured options I could pursue within the constraints of my DPhil, I reflected on ideas on my own, discussed them with my supervisors, and discussed options with the workshop participants to reach a decision together.

#### Results

Participants saw Enablement, Education, Training, Persuasion, Modelling, and Environmental Restructuring as ranking favourably on all APEASE criteria. Of these, Enablement was viewed as a particularly low-hanging fruit that would likely be effective and welcomed by the research community.

Participants found the remaining intervention functions problematic. Incentivization and restriction (e.g. rewarding guideline adherence with funding or reduced article processing, or punishing non-adherence by withholding these benefits) were seen as inequitable because as long as guideline adherence is harder for some researchers than others, less-experienced, poorly resourced researchers would be at a disadvantage, as would those working in disciplines where reporting guidelines are poorly designed or harder to apply. Conversely, participants stated that enablement has potential to reduce existing inequalities. In addition to being inequitable, participants voiced that restriction or punishment would be unacceptable to researchers, as would coercion (the threat of punishment). Furthermore, threats without enforcement could become known as paper tigers, lose effectiveness, and erode trust.

Regarding Policy Categories, environmental planning was seen as the most affordable, effective, acceptable, safe, and equitable. When talking about the _environment_, EQUATOR was really talking about the _digital_ environment, within which EQUATOR has control over its own website but not websites or digital services run by others. Improving and extending the website was viewed as an affordable investment, that would be welcomed by authors (thus acceptable), and a practical way to increase the proportion of authors engaging with reporting guidelines that would increase equity without side effects.

Members also recognised communication as a favourable policy category that is already utilised. Communication channels included the website, mailing lists, social media, conferences, and publications. Whilst communication was rated as affordable, practicable, acceptable, safe, and equitable, its effectiveness may be limited. Although communication could promote awareness of reporting guidelines are available, what they are, and how they can be used, EQUATOR members noted that the efficacy of a communications campaign may be undermined if the campaign is directing authors to a website that is difficult to use, or guidelines that are difficult to access or understand. Thus communication on its own might not be sufficient, and members suggested that it should come after improving the digital environment.

Service provision was also favoured, as long as the service was financially sustainable. So too where guidelines, specifically guidance to help reporting guideline developers create and disseminate resources. Participants felt that Legislation, Regulation, and Fiscal Measures would not be acceptable to researchers and were not not practical options for EQUATOR to use.

I then had to decide which of the favoured policy categories to focus on as part of my PhD; (digital) environmental planning, communication, service provision, or guidelines. It was important that my choice was doable within my funding window, and I wanted it to have a lasting impact after my DPhil was finished.

Creating a new service felt unsustainable as it would likely stop once my funding ran out. Developing guidance for guideline developers could be useful and acceptable, but I didn't have enough time to organise a delphi process with guideline developers. I could have developed a communications campaign, but EQUATOR members felt this was something they could do independently and that it should be done after addressing other barriers.

Instead, refining and extending the existing EQUATOR website felt like the perfect choice for multiple reasons. Firstly, planning the digital environment was ranked most highly as a policy category. Secondly, the website can be used as a means of delivering many of the highly ranked intervention functions (enablement, education, persuasion, modelling). Thirdly, it spoke to my skills as a software developer and is something that the UK EQUATOR staff could not do on their own. And finally, these changes could be made within the time limit of my PhD and, importantly, the impact would be sustained after I finish.

### Objective 3: Define intervention content

#### Purpose

Having decided that I would focus on refining and extending the EQUATOR website, I then had to decide which intervention changes I could feasibly make. I wanted to define these changes in terms of the barriers and behavioural drivers each change was designed to target, and the intervention function the change is employing. Defining intervention content in this way is useful because it helps intervention developers to understand why the component has been added (or removed), how it is theorised to be working and, therefore, how its effectiveness may be tested.

#### Methods

For every idea generated from the workshops and focus groups, I labelled which barriers it was addressing, which behavioural drivers it was targeting, and which intervention functions it was employing to do so. This list was data driven, in that it was based upon the ideas and barriers generated from previous research (see chapters {{< var chapters.synthesis >}}, {{< var chapters.review >}}, {{< var chapters.web-audit >}}, {{< var chapters.journal-audit >}}, {{< var chapters.workshops >}} and {{< var chapters.focus-groups >}}). To give structure and context to this list, I grouped ideas according to the sub-behaviours they targeted: 1) engaging with guidance and 2) applying it (see section on identifying the target behaviour in chapter {{< var chapter.workshops >}}).

Once all ideas were coded, I selected ideas to implement by considering a) whether they could be incorporated into a web-based intervention, b) the priority of the intervention function (determined in objective 2), and c) whether I could feasibly deliver the idea within the time constraints of my DPhil.

#### Results

See table @tbl-int-plan for all {{< var counts.ideas >}} ideas, labelled with the barriers they address, the drivers they target, the intervention functions they use, and whether I could implement them.

### Objective 4: Building the intervention

#### Purpose

The result of objective 3 was a list of components that were abstract. “Reassuring language” or “design that communicates simplicity” could be realised in many different ways. To build a working prototype that could be piloted I had to make these real and turn them into a working prototype.

#### Methods and Results

##### Designing the intervention

I began by describing how each intervention component could be realised and how this compared to the existing system (see #tab-int-plan). In doing these comparisons, I looked at how the EQUATOR website is currently, and I made generalisation about how popular reporting guidelines are disseminated, and the content of their Example and Elaboration documents and checklists.

Designing was iterative and collaborative. I included the same members of EQUATOR UK that had participated in the workshops. We met 3 times between November 2022 and January 2023 to discuss intervention design.

We began by deciding which webpages required redesigning, and how webpages should be navigated. On the existing website, authors starting on the home page must visit up to 5 webpages to reach the full reporting guidance. Many authors leave at each step and so few reach the guidance. We redesigned this workflow to reduce this journey to 2 webpages - the EQUATOR home page, and a reporting guideline page containing the full guidance. These different website layouts are visualised in @fig-sankey-b4 and @fig-sankey-after.

Workshop participants then sketched ideas for how the home page and guidance pages could be laid out and where intervention components could be placed. Once participants had agreed on a layout, I created an alpha version of the new website and invited members to comment on it. These were webpages that could be viewed in a browser, but used dummy text and images. After another round of feedback I refined the alpha version, populated it with real text and images, and participants gave feedback again. The new pages can be viewed in @fig-home, @fig-rg-intro, and @fig-discussion, and can be compared with the old pages in @fig-home-b4 and @fig-db-b4.

My intention was to create guideline pages for a sample of the most frequently accessed guidelines so that the website felt real for pilot participants. However, many intervention components involved changing the wording and layout of the guidance itself. Editing multiple guidelines was neither feasible not necessary, as we only needed one edited guideline to pilot the new website.

I selected SRQR as my test guideline to edit because I was familiar with it, having used it when writing up my own research, and because I felt it would make a good guideline to test with (see next chapter for why). I got written permission from Bridget O'Brien, the lead developer or SRQR, and from the publisher. I kept Bridget up to date with my work and invited her feedback.

I began editing SRQR by pasting the text into Microsoft Word and rearranging content into categories: what to write, how/where to write it, what to write if the item wasn't/couldn't be done, why the item is important and to whom, examples. I edited sentences to speak directly to authors. E.g. "Describe X" instead of "Authors should describe X", and to use active voice. This shortened the text and made it clearer that the primary audience is authors.

For composite items I split the sub-items into bulleted lists. E.g.

> For each X, describe:
>
> * X
> * Y
> * Z
>

I rearranged conditional sub-items so that they read "If X, then describe Y", rather than "Describe Y if X". I moved definitions into the glossary and contextual information into notes. I edited the resulting text to join it back together. I edited the tone of voice to add reassuring language. An example of the redesigned guidance can be viewed in #sec-box-item.

After development, I double checked the intervention against the initial list of intervention components to ensure I had covered all of them. I consulted with EQUATOR members to verify that the components were realised as expected and invited another round of feedback.

##### System architecture

When considering architecture options I prioritized technology that could feasibly be maintained by EQUATOR staff or a future PhD student. I looked for tools that would be familiar to early career researchers. I considered DIY website builders (like Wix or Squarespace) but these services can be expensive. Most offer a 'drag and drop' building experience which, although easy to use, is a laborious way of uploading and formatting large amounts of content. Should EQUATOR want to change how an item is presented, they would have to manually edit each item for each reporting guideline. Additionally, our intended intervention changes required custom functionality that wasn't offered by these services (e.g. integration with a DOI minting service, glossary definitions, discussion boards).

Although coding languages like `html` or `javascript` are used by many software developers to create websites, few early career researchers are familiar with them. I decided on `markdown`, an incredibly simple language that can be learnt in a few minutes. It uses asterisks, underscores, and carets to make text **\*\*bold\*\***, _\_italic\__, or ^\^superscript\^^. Headings, URLS, and references are similarly easy, and can be simplified further by using one of many readily available editors that make writing markdown feel like writing a Microsoft Word document.

Many researchers write reproducible manuscripts in markdown using tools like RStudio or Quarto. Quarto can turn markdown into many different file formats including docx, pdf, and html (a website). Quarto documents can be further customised using programming languages commonly used in research, like Ruby or Python. Quarto requires no technical knowledge, is easy to learn, has great documentation, and is open source.

The website is served using Github Pages which is free, beginner friendly, configurable, and integrates (almost) seamlessly with Github's version control system which will already be familiar to many researchers. I wanted EQUATOR to have ultimate control over the website. I also wanted guideline developers to have selective access to edit their own guideline content but not to other guidelines. I have built the website such that each reporting guideline is stored in its own repository on Github, accessible only to its developers. These guideline repositories are then "pulled in" to the main EQUATOR repository, so EQUATOR can double check changes that developers make before allowing them to go live on the site.

The website source code can be viewed at {{< var website-github-repo-url >}} and the website can be viewed at {{< var website-url >}}.
