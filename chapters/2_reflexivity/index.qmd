---
title: "Reflexivity and Context"
format: 
    html:
        output-file: index.html
    docx:
        output-file: JH-chapter-reflexivity.docx
---

<!-- #CA - make this shorter and also add reflections at the end of each chapter. I don't need to justify how GR is different to what I built before. Just say that it was a stab in the dark, not grounded in evidence, no systematic approach. -->

In this chapter, I examine my own lack of innocence. I reflect on my experiences with reporting guidelines before starting this project, and the experiences of my supervisors, and how my prior held opinions may have influenced my research.

This reflection is important as "A researcher's background and position will affect what they choose to investigate, the angle of investigation, the methods judged most adequate for this purpose, the findings considered most appropriate, and the framing and communication of conclusions" @malterudQualitativeResearchStandards2001. More succinctly, "Despite the sterility of the instruments, we never come innocent to a research task" @cloughNarrativesFictionsEducational2002. 

I'll begin with a bit of background about myself. This thesis marks the ten year anniversary of my attempts to improve research reporting. My early interest came from reading about psychology's reproducibility crisis #REF, the cancer reproducibility project #REF, Ben Goldacre's Bad Pharma #REF, and Ioannidis' essay "Why Most Published Research Findings Are False" #REF. The compelling Lancet series on research waste @macleodBiomedicalResearchIncreasing2014 drilled home a magnitude of inefficiencies (or, what an optimist might call _areas for improvement_) in medical research. I was most stuck by the article on incomplete or unusable research reports @glasziouReducingWasteIncomplete2014, where a key takeaway was that "although reporting guidelines are important, major improvements need active enforcement" by editors or reviewers. I took Paul Glasziou's words as a call to action, and I set about creating tools to help editors enforce reporting guidelines. 

```
Tools I built:

* citation miner
* statistics calculators - like Michele's
* Manuscript checker
* Worked with EQUATOR as a freelance developer, made GoodReports. 
* Worked with EQUATOR on the BMJ project and felt dejected when the results showed no effect. Had something to prove. 

Through these projects I learnt to code.
Enjoyed creating stuff.
Stereotype of a developer sitting in the basement alone. Somewhat true. Not that exposed to users. 

Two main dangers: usability and product market fit (solving a problem that doesn't exist / creating a product nobody needs).
Usability is a bit easier to avoid, at least to a basic level. Heuristics / written material. It became something I tried to apply to everything I built. Something I was always aware of, although I'm no expert.

Product market fit is harder for me. Marketing websites advise you to do user interviews but as a software developer you lack the expertise to run interviews, money to pay participants, and motivation (happier behind a screen). Almost fell into that hole again - interviewed with a firm idea of the problem I wanted to solve. Kinda like a WebCONSORT on steroids. Custom guidelines (we now call franken guidelines)

Using the word _user_ because that's how developers talk. It's also how I talked when I started my PhD. It took some time for my supervisors to convince me to refer to people who use reporting guidelines as _authors_, _researchers_, _editors_ e.t.c. instead of _users_.
```

<!--
The first was software to automatically check whether manuscripts adhered to journal author guidelines. 
In 2014 I approached the UK EQUATOR Center and we collaborated on two projects. First, having identified that authors found it hard to work out which guideline to use, EQUATOR asked me to help make a questionnaire that would help authors to self "diagnose" their manuscript and find the correct guidance. Second, because some guidelines don't have checklists in usable formats, we made www.GoodReports.org where authors could read and complete reporting checklists online.

In 2018 we put these three things together; my manuscript checker guided authors through EQUATOR's questionnaire and, when appropriate, instructed them to complete checklist on GoodReports.org. We tested it in collaboration with BMJ Open by comparing manuscripts where the submitting author chose to use the tool with those who chose not to. We found that prompting authors to complete a reporting checklist was associated with more checklists being submitted. However, although authors did act on some of the automated feedback from my manuscript checker - such as fixing their abstract structure, adding missing declarations or legends - authors that completed a reporting checklist upon submission did _not_ add reporting guidance content. 

I was demoralized. After five years of work, I found no evidence that what we had built was helping authors adhere to reporting guidance. Gradually the disappointment made way for questions. Perhaps I didn't understand authors as much as I thought I did? Perhaps what I'd built was hard to use? Perhaps _guidelines_ were hard to use? Perhaps authors didn't want to change their work just as they are about to submit it? Perhaps when Glasziou wrote "improvements need active enforcement" he already knew that enforcing checklists alone wouldn't be sufficient, and that editors would actually have to enforce adherence by checking manuscripts thoroughly? But, according to the editors I'd spoken to, that would be incredibly impractical, so is enforcement really the only option? 

When this PhD position appeared, I saw it as an opportunity to continue this work and answer some of these questions. Hence, at the start of this project, I was firmly in agreement that poor reporting is a source of waste and that reporting guidelines can help reduce this waste. But I was sceptical that journals would ever be willing and able to check adherence manually, or that enforcing checklists is effective. I was keen to find other, easier, more effective strategies to promote guideline adherence. -->

My supervisors have their own backstories. Jen, Michael, and Gary are affiliated with the EQUATOR Network. Gary is Director of the UK EQUATOR Centre and an author of many reporting guidelines and a working-group member for many others. Michael studies reporting completeness, the robustness of reporting guidelines development methods, and the consolidation of different reporting guidelines for the reporting of studies of nutritional interventions. Jen is involved in many studies investigating reporting and reporting guidelines, and runs training on writing and reporting guidelines.

Charlotte joined my supervision team in my second year, once we realised that my thesis was taking a sharp and unexpected turn towards qualitative methods. Charlotte has a lot of experience in qualitative research and behaviour change theory, both from her own research and from leading the Oxford course on qualitative methods. However, she had never studied reporting guidelines before, nor any other meta-research phenomenon. Charlotte has published a suggested amendment to a reporting guideline that she uses in her own work @alburyGenderConsolidatedCriteria2021. Whereas Jen, Michael, and Gary can be described as reporting guideline advocates, Charlotte was a little more cool-headed. Although she felt that reporting guidelines are useful for writing up quantitative work, she found guidelines for qualitative research frustrating because they were developed from a positivist perspective, and so didn't fit a lot of qualitative work.

<!--
At the start of my project, before Charlotte joined, none of my supervisors, nor any other staff members at the UK EQUATOR center, had ever considered reporting guidelines as a behaviour change intervention. I _also_ had never heard of behaviour change theory, and I didn't come to my PhD knowing that was the direction I would take (in fact, when interviewing, I was adamant that I wanted to pursue a completely different idea. The fact that I had nicknamed it "frankenstein guidelines" was probably a good indication that the idea was best left alone). 

During my first year, I began to see reporting guidelines as more than just "a thing". I elaborate on this in chapter {{ var chapters.bcw }} but, briefly, I began to see the interconnected parts; the checklists, example and elaboration documents, guideline publications, journal author guidelines, publishing policies, the EQUATOR website. I saw how authors are influenced by editors and peer reviewers. I began to think as "writing" as a multi-step process, and I began to chart the steps that an author must go through to successfully adhere to a reporting guideline, all the way from learning about them from a journal website, through to finding the guidance, reading it, and acting on it. 

Sometimes I found it difficult to articulate my thinking and it took some time to work out where my research "fitted in". Coming from a software development world, I naturally reached for words that were familiar from marketing and user experience. Terms like "retention rate", "conversion rate", or "acquisition funnel". These terms weren't always a good fit, and I tried to find their scholarly counterparts. I had a "lightbulb moment" in a coffee shop in Crouch End when I read the MRC guidance on complex interventions @craigDevelopingEvaluatingComplex2021, which gave me a vocabulary for a lot of my thoughts.

To summarise, I came to this PhD after a few years of trying to combat poor reporting, and already convinced that reporting guidelines were useful, but sceptical that enforcement was the only way forward. With regards to behaviour change and qualitative methods, I was far more naive, as was my supervisory team until Charlotte joined. Hence the questions I chose to explore, the objectives I set, the approaches I chose, my interpretation of qualitative data, have inescapably been influenced by my previous endeavours, and by my initial "innocence" with regards to behaviour change and qualitative methods. The approach described in this thesis is uncharted territory for EQUATOR, and territory that I hope will prove fertile. -->

```
In summary:

I came to this PhD with 

* passion for improving research, deep respect for reporting guidelines and EQUATOR, and determination to *finally* make something that actually helps
* a software developer's vocabulary and mindset
* if someone isn't using what you've made properly, then that's the creator's responsibility to fix, not the user, and it's probably because you don't understand your user as well as you think. 
```

Having set the scene, I'll begin my thesis where I began my research: by trying to understand why authors don't adhere to reporting guidelines. In chapter {{< var chapters.introduction >}} I described the evidence that reporting guidelines have had little effect on reporting quality, but I wanted to know _why_. Chapters {{< var chapters.synthesis >}} - {{< var chapters.journal-audit >}} describe my mixed method approach to finding possible answers to this question. In the next chapter, I describe how my first step was to seek and synthesise all available qualitative data that explored this question. 