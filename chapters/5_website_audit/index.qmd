---
title: "A service evaluation of equator-network.org"
format: 
    html:
        output-file: index.html
    docx:
        output-file: JH-chapter-website-audit.docx
---

## Introduction

In chapter {{< var chapters.review >}} I found two studies that exploring authors' awareness of EQUATOR as an organisation [@fullerWhatAffectsAuthors2015; @girayAssessmentKnowledgeAwareness2020a]. Although some authors will know EQUATOR from their training programmes or publications, most will know EQUATOR from its website. Despite running on a shoestring budget and without any in-house expertise in software, design, or user experience, the website's traffic has increased from 100,000 users to almost 1 million over the last 10 years, attracting visitors from all around the world. For context, around 4.3 million authors published 2.4 million academic articles in 2014 @plumePublishPerishRise2014, and the STM association estimated that 11 million people working in research and development in 2018 @stmassociationSTMGlobalBrief. Whilst these numbers will have grown since, they suggest that EQUATOR's website traffic numbers are within a single order of magnitude of its potential audience size. 

Hence what started as a research-group's attempt to catalogue reporting guidelines has become a significant part of the academic eco-system. And yet, EQUATOR has never formally considered how successfully their website is performing, or how it could be improved. This process - assessing how effectively a website contributes towards an organisation's objectives - is generally referred to as a website service evaluation. There is no standard definition nor approach @allisonComprehensiveFrameworkEvaluate2019. Some evaluations use in-depth qualitative methods like interviews, card-sorting, and observed browsing or task completion. In chapter {{< var chapters.pilot >}} I describe how I use some of these techniques to refine the website, but at this stage, my objective was to perform a quick and broad evaluation, and so I decided to use automated tools to explore all user activity.

In this chapter I describe how I worked with EQUATOR to identify key metrics of success and how I explored those metrics using Google Analytics, Google Tag Manager, and an automated exit survey. In the discussion section, I infer how the website may need improving.

#Published article volume estimates are for research in general. I haven't found numbers specific to medicine yet.

#TODO: dashboard no longer works

## Methods 

Google Analytics @AnalyticsToolsSolutions is a web analytics service that helps website owners track and understand users' activity. Used by 85% of websites globally, it is the most popular web analytics service by far @UsageStatisticsMarket. EQUATOR has used Google Analytics to collect data since creating their website. Mostly they use the data to report high level impact metrics to funders (such as number of visitors) but they have never used it to evaluate their site in depth.

When evaluating a website, there are a huge number of metrics you can consider; Google Analytics collects over 50 by default [@GA4AutomaticallyCollected; @GA4PredefinedUser], and Google Tag Manager @WebMobileTag allows you to collect additional custom metrics (see @tbl-definitions for definitions of Google Analytics terms and metrics). With so many options, the first step of evaluation is to decide what metrics are most important for your website's objectives. 

I met with three members of the UK EQUATOR Centre in October 2021. When I asked what the main purposes of their website @EQUATORNetworkEnhancing was, I received many answers. EQUATOR has content aimed at authors, editors, peer reviewers, educators, librarians, and guideline developers. There are web pages promoting training courses, toolkits for writing and reviewing research, newsletters, and blogs highlighting work done by EQUATOR and guideline developers. 

However, at its core, the EQUATOR staff I spoke to agreed that the purpose of the website is to help the global research community learn about and access reporting guidelines. They want website visitors to access the guidance that is right for them and come back to the website whenever they need guidance.

To explore how far reality meets this vision, I used Google Analytics to answer the following questions:

1. How many people visit the website each year? 
2. Where (in the world) are visitors from, and how does this compare with the global distribution of researchers?
3. How often do visitors come back?
4. How do visitors get to the website?
5. Do visitors engage with the website?
6. What content do visitors view?
7. What reporting guideline database records do visitors view?
8. How many visitors continue to access guidance on a third party site?
9. How may visitors access publications vs. checklists?

#Could elaborate on these but my gut is to keep chapter short.

The last two questions could not be answered by Google Analytics' default configuration; it recorded which database records visitors looked at, but not how many people went on to view guidance on third party websites, or whether visitors were viewing checklists or full guidance. Therefore I used Google Tag Manager to create two custom metrics: one which counted when visitors downloaded a reporting checklist file, and another which counted when visitors accessed a third party website. 

#TODO name and reference technique 

EQUATOR staff felt it was important that website users were able to access the _right_ resource. Although Google Analytics could tell us _what_ pages visitors access, it can't tell us what they _needed_, or _why_. To explore this, I decided to use PopupSmart @PopupBuilderThat to build an exit survey (an online questionnaire that pops up when a user appears to be leaving the site, for instance, if their mouse moves towards the close button or URL bar).  I decided to use an open ended question in the hope of receiving richer responses. I didn't want to annoy users or block them from using the website, and I wanted to keep the survey short in the hope of maximizing repose rate, and so I decided to limit the survey to a single question. 

We decided against asking "What were you looking for today?" as that wouldn't tell us whether users had found what they needed. We considered "How easy was it to find what you were looking for?" but decided it was too closed, the word "easy" was too  subjective, and it assumed too much about the users' intent. Instead, given that the questionnaire would only appear as users prepared to leave the site, we decided to ask "Why are you leaving?". We intended to add narrower follow up questions in the future, depending on the responses to this initial, broad question. We hoped that the question was open enough to cater to users who had found what they needed, those that hadn't, and users that were leaving for any other reason. For example, one EQUATOR member joked "We might get answers like 'because it was not the right website I was looking for, I was looking for geography websites'...! haha" (spoiler alert: this wasn't far off the mark!).

{{< include _table_definitions.qmd >}}

## Results

### How many people visit the website? 

830 134 users visited equator-network.org in 2021 (see @tbl-users).

### Where (in the world) are visitors from, and how does this compare with the global distribution of researchers?

#DECIDE Could add a table of how many citable medical research documents come out of the top 10 countries

Over 800,000 users visited the EQUATOR website in 2021 (@tbl-users), 150,000 more than the previous year, thus continuing growth seen since attracting 20,000 visitors in 2008. A third of users were from the United States or United Kingdom. The geographic distribution of users didn't always align with global publishing trends. For example, Brazil accounted for the same proportion of users as the UK (7%) despite producing far fewer citable, medical documents @ScimagoJournalCountry. Conversely, China produces twice as many citable documents as the UK but accounts for only 5% of users. Two fifths of users had their browsers set to a language other than English (Table 4), most frequently Portuguese, Chinese, and Spanish. 

### How often do visitors come back?

Google Analytics classified almost all users as new (see @tbl-users), which means they had not visited the site within the previous 2 years (the default expiry limit for Google Analytics cookies). Most users only visited the site once within 2021. 

{{< include _table_users.qmd >}}

### How do visitors get to the website?

The source of half of the traffic was labelled as "unknown", meaning that Google Analytics had no way of knowing where that traffic came from (see @tbl-sessions). A third of users arrive at the site via a search engine. An eighth of traffic was explicitly labelled as referrals from other websites, most commonly Wiley and Elsevier journals and Manuscript Central. 

A third of sessions begin with users arriving on the home page (see @tbl-landing-pages). Most other sessions begin with authors arriving directly on a reporting guideline database record page. 

### Do visitors engage with the website?

Over half of sessions (53%) ended without the user interacting with the site at all (see @tbl-sessions). Google Analytics calls this behaviour bouncing. Bounce behaviour was similar regardless of which webpage user arrive on: 45% for the home page, >50% for most reporting guideline database record pages. Two thirds of sessions lasted less than 10 seconds, suggesting little interaction.

{{< include _table_sessions.qmd >}}

{{< include _table_landing_pages.qmd >}}

### What content do visitors view?

Reporting guideline database record pages were viewed in half of all sessions (see @tbl-sessions). The home page was viewed in a third of sessions. Only 7% of sessions included a view of any page listed within the `/library` directory. All other content categories, including pages within the `/toolkits` directory, were viewed in 1% of sessions or less. 

### What guideline database records do visitors view?

STROBE was the most viewed reporting guideline, with 231,207 unique page views in 2021. These numbers dropped rapidly: SQUIRE, the tenth most viewed reporting guideline record, was viewed ten times less frequently than STROBE. Only 65 reporting guideline pages were viewed in more than 0.1% of sessions. There are over 500 reporting guidelines indexed in EQUATOR's database, but hardly any are viewed regularly. Only 13 guideline database records were viewed in more than 1% of visits, all of which appear in the list of "reporting guidelines for main study types" featured on EQUATOR's home page and in a side bar on all reporting guideline sub-pages. 

{{< include _table_rgs.qmd >}}

### How many visitors continue to access guidance on a third party site?

Many users leave the guideline reporting guideline database page without accessing any resources. For example, the STROBE database record was viewed 110,000 times, but the checklist only downloaded 30,000 times (~1 in 3) and the link to the full guidance was only clicked 2,500 times (~1 in 44). Overall, only around 1 in 4 users end up clicking a link that will direct them to a checklist or guideline publication. 

### How may visitors access publications vs. checklists?

Of the ten most viewed reporting guideline database pages, all except COREQ and SRQR offer downloadable checklists. For STROBE, STARD, CONSORT, and SQUIRE, the checklist links are clicked more often than links to the full guidance (15:1, 8:1, 6:1, and 3:1 respectively). For PRISMA, CARE, TRIPOD, SPIRIT, link clicks to each resource are more similar (around 1:1).

### Survey data suggest some visitors may not understand what the website is about, and may not find it useful

Our survey received 33 responses in 2 weeks (see @tbl-survey-responses), after being viewed by 25,660 people, (a response rate of 0.1%).

A few indicated clearly that the user had found what they were looking for; "got what I was looking for!" wrote one user, "found what I needed!" wrote another. Other users hadn't been so successful, writing "Could not find what I needed", "I cannot find the guidance that I seek", "I don't see what I want", and "could not find any reporting guidelines for reporting guidelines (specifically, abstracts for reporting guidelines)". Some visitors voiced frustration with the website ("i did not under stand any thing", "The site is very complex", "Too big a mess").

Some users hinted at why they were on the website; e.g. "to get reporting guidelines", "a tool called standard for reporting qualitative research". One user wrote "word format would be more easy to fulfil", suggesting they had been looking for a checklist. Others clearly didn't understand what the EQUATOR website was about. Two authors seemed to be looking for requirements for specific journals: one wrote "format for paper submission to Hindawi", and another wrote "awful site.  I just want to know the requirements in terms of number of words and format for a submission and cannot seem to find this anywhere". Another author was looking for a "quality of life questionnaire", another for "scientific research". More cryptically, one visitor simply wrote "ALGERIA", another "Germany" (perhaps EQUATOR staff were correct to worry that their website could be mistaken as a geography resource).

Some users voiced frustration with the survey itself ("do something about your annoying pop up!"). Wary of annoying users and given the poor response rate I decided to take the survey down instead of modifying it.

{{< include _table_survey_responses.qmd >}}

## Discussion 

Summary:

A core objective of the EQUATOR Network website is to help the global research community to find and access reporting guidelines. Web analytics show that only 1 in 4 users who visited the website ended up clicking a link that would take them to a checklist or to a guideline's PubMed page. Fewer users would end up actually reaching the guideline, as users then have to navigate from PubMed, to the published article (which may be paywalled), and then find the guidance within the publication itself which can be hidden in a table or supplement. 

Overall, web analytics and survey responses suggest that engagement could be improved. Visitors stay for less than 10 seconds (63%) often not interacting with the site at all (53% bounce rate), almost none return, and some may not understand what the site is about or how to find what they need. 

There are many possible reasons for poor engagement. Perhaps the website's content or structure is too complex, perhaps the design puts people off. These reasons could be explored qualitatively using interviews or think aloud and this work should also include non-native english speakers; 40% of users had their browsers set to something other than English. Non-native english speakers may struggle to understand the website content. This could also explain why some countries are under-represented: authors may be less likely to be aware of, discover, or use, resources that are not in their own language. 

To help non-native english speakers, it may be appropriate to manually translate key parts of the website (like landing pages) and popular guidelines into some languages. But manual translations are expensive, difficult to update, and cannot be scaled. Automatic machine translation is more scalable, updates automatically, and inaccuracies can be refined with custom glossaries and language models. EQUATOR has since integration automatic translation of their website. They could also consider using search engine optimisation to reach non-english speaking authors. For example, they could create landing pages in Chinese or Spanish, or add foreign language words to reporting guideline meta-data so that search engines index them.

Search engine optimisation could also help EQUATOR reach more authors. EQUATOR could add metadata, optimization their website for mobile phones, and take advantage of Google Search's featured snippets and description features. EQUATOR should consider what keywords to optimise for, and consider that some keywords ay help catch authors at earlier stages of writing. Google Search Console (a Google product that allows website owners to view how their site performs in Google searches) shows that when users search for "STROBE guidelines," EQUATOR's site appears at the top of search results and 36% click this result. However, if a user searches for "how to write an epidemiological report," EQUATOR drops to 29^th^ place with a click rate of 0%. EQUATOR should ensure the site is optimized for naïve users at an early stage of writing who may not know guideline acronyms.

Increasing traffic from search engines would be useful, but so would understanding where current traffic comes from and, at the moment, half of the traffic comes from "unknown" sources. This is how traffic is labelled when Google has no information about where that traffic comes from. Although it's possible that some of this traffic comes from links within offline documents, it's far more likely that this traffic represents referral traffic from websites that are linking to EQUATOR using links that start with *http* instead of *https*. When a secure website (with an address that starts with *https*) links to a less secure one (whose address starts with *http*) address, no referral data gets sent. EQUATOR upgraded its website to use *https* years ago, but journals have continued to link to EQUATOR using the old, less secure, *http* address. 

EQUATOR could ask journals and submission systems to update how they link to EQUATOR's website. This would result in more correct referral data.This data would tell EQUATOR which journals are successfully recommending reporting guidelines and it would allow EQUATOR to infer visitors' intentions. For example, traffic from submission systems may signify authors who are in the very late stages of writing, and may be seeking a checklist.

Indeed, many authors _do_ appear to be accessing checklists over and above full guidance. This could be their intention (if I'm right that "unknown" traffic is actually coming from journal and submission systems), or it could be because EQUATOR places checklists at the very top of reporting guideline database record pages. Either way, guideline developers should be aware that, for many authors, checklists may be the primary way they interact with the guidance. Some authors may even think that the checklist _is_ the guidance, and never discover that full guidance exists (a possibility that first appeared in my thematic synthesis {{< var chapters.synthesis >}} and was confirmed, later, in my pilot {{< var chapters.pilot >}}). Hence guideline developers should ensure checklists link to the full guidance, and should include key information about the guideline like its aim, scope, and how it is intended to be used.

Some readers may point out that only a minority of the 500+ guidelines in EQUATOR's database have downloadable checklists. However, very few guidelines receive any meaningful traffic. Only 13 guideline database records were viewed in more than 1% of visits. The remaining guidelines form a long tail, with some guidelines receiving no traffic at all. Of course, some guidelines will naturally be accessed more often (most people would expect general guidance for systematic reviews to be accessed more than, say, guidance for eye-tracking studies in dentistry). But the severity of the skew suggests that authors may not be discovering guidance that is most appropriate for them. Alternatively, a cynic may interpret this skew as evidence that some reporting guidelines simply aren't useful. Possible improvements may be to make the search function easier to find and use, and to have more links between related guidelines along with clear instructions of when each reporting guideline should or should not be used. 

Similarly, besides the 13 popular reporting guidelines, the majority of content on EQUATOR's website (e.g. its toolkits, blog, and library network) is hardly ever accessed. EQUATOR could try to better understand who is using their website and why, and consider reorganising content to make it easier to find, or pruning dead-weight to simplify the website.

### Limitations

Google Analytics uses cookies to track users over time. If a user clears their cookies between visits or uses multiple devices or browsers, the user will appear as multiple users. Cookies expire after 2 years by default. The proportion of new vs. returning users is thus an overestimation but, nevertheless, is still high.

Ultimately, numbers can only tell you so much. Counting bounces is useful, but only qualitative research will explain *why* users bounce from the EQUATOR website with the frequency that they do.

The data presented here cannot be used to draw comparisons with other websites. For example, I was tempted to compare EQUATOR's bounce rate of 53% with eCommerce benchmarks (around 40%, from an analysis of 1068 european eCommerce websites @ecommercefoundationEcommerceBenchmarkRetail). But this comparison isn't useful. EQUATOR's website is unusual in that it is a free learning resource, not a shop, and its users are different in terms of who they are, what they are trying to do,  why they are doing it, and how they get to the website. Consequently it's not clear whether EQUATOR's bounce rate should ideally be lower than, higher than, or equivalent to eCommerce websites. Perhaps it could be useful to compare EQUATOR's website with similar resources but, as yet, I've not found any with publicly available analytics. 

### Conclusions

This chapter presents a first step in evaluating the EQUATOR Network's website. These data will help EQUATOR UK understand how the website could be improved in order to further help the global research community learn about and access reporting guidelines. If EQUATOR decides to make any changes, the data here will provide useful benchmarks against which to measure progress.

# References


