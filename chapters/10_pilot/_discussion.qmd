## Discussion

The purpose of this study was to identify deficiencies in a website for disseminating reporting guidance. I interviewed {{< var counts.pilot.participants >}} researchers and used multiple qualitative methods to identify {{< var counts.pilot.deficiencies >}} deficiencies. Most intervention components on the website's home page aim to communicate what reporting guidelines are, that they are best used early in writing, and how they will benefit the author. The results demonstrated most of these components to be somewhat successful, but not yet optimal. For example, some participants needed more than 5 seconds to realise the website was about resources to help them write. Participants often found later, longer content more useful than the short text at the top of the page. In seeking to balance brevity and clarity, perhaps I had been too mean with my word count. If "easier writing" is vague, "faster first drafts" might be concrete. If "complete reporting" isn't intuitive, perhaps "writing up research fully so that everyone can understand, repeat, apply, and synthesise your work" is. 

I had sought a similar balance between clarity and brevity when trying to organise the full SRQR guidance (35 pages in its original form) onto a single webpage, in a way that made it appear shorter and less intimidating. Again, the current design was somewhat successful. Participants liked the web features I had used to make the guidance more digestible, like expandable content, navigation menus, subheadings and consistent structure. However, some still felt the guidance looked too long, whilst others wanted to add content that would make it longer still; more examples, more information, more definitions, more signposts to other help. One solution may be to display reporting items on separate pages, as the ARRIVE developers have done on their website @ARRIVEGuidelines. Another may be to display a summary of the guidance at the very beginning. 

Many participants commented on the website's design. Whereas I had been somewhat successful in projecting simplicity, for some participants, this crossed the line to basic-ness, especially in the first iteration. Many intervention components use design as a way to persuade and communicate with authors: I wanted pictures to depict tools, benefits, and purpose; layout and colours convey a feeling of ease and openness. Sadly neither I nor my colleagues possess expert design skills. Images took a long to create and, unlike text, are hard to iterate. This is a pity, as design often seemed more salient to participants than text, and bad design misled participants and put them off. 

Design was also linked to another theme important to this study: credibility. For some participants, the website's basic design eroded its trustworthiness. I mitigated this partially in the second iteration (e.g., by including logos), but future design iterations would ideally include professional design input. 

### Influences

Credibility rests on more than just design. Participants also wanted assurance that the guidance (text) could be trusted, which necessitated understanding the relationship between EQUATOR, guidelines developers, the original guideline publications, and the content of associated resources. Understanding this relationship was one of six new influences participants mentioned that may affect whether they successfully adhere to reporting guidelines. In chapters {{< var chapters.synthesis >}} and {{< var chapters.review >}} I argued the need for more, in depth qualitative exploration of influences. Although I did not aim to solicit influences in this study, that I found novel influences incidentally suggests I have contributed towards filling that gap. Participants also mentioned eleven influences that I _had_ previously identified in my earlier work. Therefore, this study adds credibility to my previous findings whilst also building upon them. 

### Strengths

Finding novel influences is testament to the strengths of this study. I recruited authors with diverse backgrounds and writing experience. My methods solicited rich information. My thorough analysis used my intervention component table as a framework to draw as much information as possible for the data. In contrast, many studies I reviewed in chapters {{< var chapters.synthesis >}} and {{< var chapters.review >}} recruited homogenous samples, solicited thin description through surveys, and described their analysis techniques poorly. The few studies that elicited rich information focussed on content (e.g., PRISMA 2 @pageUpdatingGuidanceReporting2021) or application (SQUIRE 2 @daviesFindingsNovelApproach2016) of a reporting guideline but not the design or the website/publication hosting the guideline. By focussing on diverse recruitment, rich exploration of the guidance text and surrounding platform, and thorough analysis and reporting, I have strengthened my study and addressed limitations seen in others.

### Limitations 

However, other limitations remain. I will now discuss how 1) this study lacked contextual diversity and 2) not all intervention components were explored.

#### Context

My web audit (chapter {{< var chapters.web-audit >}}) found only half of EQUATOR's current visitors view the home page. Many arrive to the website directly on a reporting guideline page, often as a referral from a journal or a search engine. Because so many visitors never view the home page, many intervention components need to be placed on the home page _and_ the reporting guideline page. For instance, na√Øve visitors should be able to tell what reporting guidelines are whether they arrive on the home page or directly on a guideline page. Some participants noticed this duplication and a few suggested removing or minimising it. However, because all participants viewed the home page first, this study did not capture experiences representative of website visitors that never see the home page. Therefore, future studies should explore the experiences of participants viewing the guideline page without seeing the home page.

Many authors discover reporting guidelines as they are submitting to a journal, whereas authors in this study were not. Because authors described manuscript submission as an inconvenient moment to intervene (see chapter {{< var chapters.synthesis >}}), this may influence how authors experience the website. Once the website is live and journals are directing traffic to it, future studies can explore the experiences of authors using the website in contexts that are more true-to-life, as part of their journal submission journey. Similarly, if funders or ethics boards begin asking applicants to use reporting guidelines this context should be explored too.

#### Not all intervention components were explored equally:

Some intervention components received little to no discussion. The five second test, think aloud, plus minus test, and writing evaluation all examine _salient_ intervention components and will not elicit discussion of un-noticed components.

Sometimes this was useful and expected. For example, one component was to _remove_ patronizing language. That nobody spontaneously described the website as patronizing was a success. Similarly, another component was to use terms consistently. This component was only salient when it had not been applied properly, for instance, where I had used the terms "guidelines" and "reporting guidelines" interchangeably. For components like these, a good outcome is to go unmentioned. However, other components still deserve evaluation even though they are purposefully not salient. My semi structured interview questions addressed this limitation to some extent by asking participants directly about less salient features. 

Some components could not be fully explored until the website is further developed. For example, although participants recognised the search button, they could not explore the search functionality. Although participants said they liked the links to related guidelines, I could not explore participants' ability to find and select guidelines because the website only included SRQR. Once other guidelines are uploaded, future studies could use task based protocols @cravenTaskBasedApproach2007 to explore how participants find, compare, and select appropriate guidelines.

### Future studies 

Whereas the limitations above affected my success in reaching my objectives (identifying deficiencies), my objectives were themselves limited and further work is needed to develop the website into a fully functional resource. I will now discuss potential future studies, including 1) prioritising deficiencies; 2) further iterations to address deficiencies; 3) extending the website with other guidelines, checklists, templates, examples, and resources; 4) evaluating components that could not be evaluated in this study; 5) comparing authors' preference between the new and existing website and guidelines; 6) real world evaluations and 7) evaluating reporting guideline content.

#### Prioritising deficiencies

I made no attempt to prioritise deficiencies. Although some were more commonly raised than others, this was because of saliency and because of the methods I chose. For example, by choosing to use the 5 second test, I encouraged participants to focus on components featured at the top of the landing page. Similarly, my semi structured interview questions drew attention to particular components. Therefore, code frequencies should not dictate deficiencies' priority and I purposefully have not reported them. 

Instead, de Jong and Schellens @dejongDocumentEvaluationMethodology2000 suggest ranking deficiencies according to their likelihood and severity. Likelihood refers to the number of users that may be affected by the deficiency, and severity means the degree to which the deficiency will block the desired behavioural outcome. I made no attempt to estimate these factors systematically in this study. Instead, I judged them instinctively when deciding what I could feasibly change in the first iteration.

#### More iterations are needed to fix deficiencies

Once prioritised, the remaining deficiencies need addressing and it is my intention, funding permitting, to design and evaluate new iterations after my DPhil. Testing future iterations with an identical protocol would offer continuity. It may be more prudent, however, to adjust the study protocol to target particular components or contexts. 

#### Future evaluations are needed after extending the website

Future evaluations will also be required after the website is extended with more guidelines, search functionality, and with checklists, templates, and links to training and resources. Because different reporting guidelines cater to different research communities, and because these communities may have their own nuances and needs, future evaluations should recruit participants from these communities. For example, CARE may be more commonly used by clinical academics, and ARRIVE authors may come from the life sciences and medical sciences. One reason I chose SRQR was for its diverse user base. As the website grows and its audience expands, recruitment should diversify further.

Once checklists and templates are added, future evaluations should explore participants' experiences of using these resources with and without prior exposure to the website. Just as some authors will bypass the home page and land directly on a guideline page (see Context section within limitations), some authors may receive a checklist or template directly from a colleague or journal without first visiting the website. Therefore, these resources should be evaluated in isolation _and_ within the context of the website.

#### Evaluating components not explored in this study

Some components could not be explored in this study. Optimizing the website for search engines can only be assessed by an audit and by monitoring the website's rankings using a tool like Google's search console @GoogleSearchConsole once the website is live. Another component involved adding information to items to instruct authors what to do if a particular item was not, or could not be done. This item was more applicable to reporting guidelines for quantitative research, many of which make assumptions about design choices. SRQR is fairly agnostic to design choices, and I only added information to one item (item 5, regarding qualitative approach). In the writing evaluation, I asked participants to describe what part of their manuscript they were working on and I then recommended 2 or 3 reporting relevant reporting items. Item 5 was not relevant to any participants, and so no participants noticed nor commented on the component.

#### Comparing preferences

This study did not aim to explore whether participants preferred the revised reporting guideline and website over the existing ones. The few participants who made this comparison naturally all expressed preference for the redesign, but future studies could explore preferences in detail. Doing this qualitatively would reveal _reasons_ behind preferences. A larger survey could confirm whether authors prefer one version above another.

#### Real world evaluations

Once the new website and redesigned reporting guidelines are live, real world evaluations should continue to monitor, understand, and improve authors' experiences. This will include using google analytics to monitor how authors use the website, online surveys and other feedback channels, and opportunistic recruitment of authors engaged in their day-to-day work. 

Some important metrics include the proportion of authors that return to use the website, the proportion who access resources for drafting vs checking (I would hope to see more authors use the former), and the length of time authors engage with guidance. 

Because journals will probably continue to be an important dissemination channel, one possibility would be a mixed methods feasibility study, in collaboration with a journal. Such a study could combine google analytics data with author interviews and writing evaluations of manuscript submissions. 

#### Evaluating guideline content

This study did not attempt to evaluate the SRQR recommendations themselves, but rather the guidelines' presentation. I was interested in what participants thought of the structure, order, and layout of the guidelines, but not of its content. I was trying to look at the guideline on a macro level, and I was not interested in whether participants took issue with particular instructions. 

I hope that guideline developers will begin evaluating their content in more detail. They could make use of de Jong and Schellens' advice which, as I mentioned earlier, suggests a range of methods to explore the criteria needed for text to be effective @dejongReaderFocusedTextEvaluation1997.

### Conclusions 

This study aimed to identify deficiencies in a redesigned version of the SRQR guideline and EQUATOR Network home page. Intervention components were deficient if they could more successfully drive authors towards our target behaviour: to use reporting guidance as early as possible in their research pipeline. In identifying {{< var counts.pilot.deficiencies >}} deficiencies, I met my objective, but this success is a double-edged sword. This is the final research chapter of my thesis, and it would have been satisfying to conclude with "I've done it! The website is perfect!", but the results of this study prove otherwise. Unfortunately, the realities of iterative design and limited funding force me to end with unfinished business. Nevertheless, I have suggested further studies to continue and extend the work presented here. In the next chapter, I will discuss my thesis as a whole, directions for future work, and implications for guideline developers and other meta-researchers interested in changing the scholarly system.

### Reflections on this chapter

Writing the methods and results sections of this chapter were high and low points respectively. When writing the methods, I combined what I'd learnt from writing all previous chapters. As in chapter {{< var chapters.synthesis >}} I started by working through the reporting guideline item-by-item but this time I wrote bullet points instead of prose. I then drew on what I had learnt about structure and drafting when writing chapter {{< var chapters.defining-content >}} to reorganise these bullet points into a coherent linear narrative. I then topped and tailed paragraphs with topic and linking sentences, before sandwiching evidence and examples in between. I was happy with the result. The first draft scanned better than other chapters, and I felt confident I'd included guideline content.

The low came when writing my results section. I had intended to report results for each intervention component. If my thesis word count weren't so limited this is what I would have done. Other departments have higher limits to better accommodate qualitative research with lengthy results. I found it impossible to condense my results further without losing details and nuances I wanted to retain, and so instead I decided to move them to an appendix and to construct a narrative around some highlights. But in selecting highlights I was giving saliency and importance to a subset of results. Some readers may interpret this as an additional subjective layer of analysis and may criticise me for picking these highlights, but the alternative would have been to reduce my results so much they lost their richness. Stuck between two bad options I chose the former. I have tried to be clear that selecting highlights was not another stage of analysis, and I refer readers to my full results in the appendix. Yet I feel like my word limit has forced me to add another filter to my results that I never wanted to apply. 

When conducting these interviews there were moments I was aware of my emotional responses. Sometimes participants would validate my intentions, or confirm my hypotheses. For example, when one spoke about the challenges of incorporating writing guidance into their own practice, this resonated with my experience of writing my previous chapter. Other times participants criticised what I'd made or spoke against my expectations. I believe I dealt with these moments well, in part because of my experience in software development. When collecting feedback on something you've created, it is tempting to dwell on the positive and dismiss the negative. Over the years, I've learnt to pay most attention to the negative, the surprising, the friction felt when someone's experiences are not what you expected. When I first started working in development these moments could feel like a personal attack, uncomfortable or disappointing, but I've learnt that they are often the most valuable. Nowadays I tend to lean into those moments and I felt myself doing that in these interviews. When participants said something unexpected or critical, I would ask more questions to try to fully understand their point of view. 