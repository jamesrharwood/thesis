## Methods

My objective was to refine the website by identifying parts that may not be functioning as I had intended them to. 

```
Choosing a guideline

* Editing a guideline was time consuming
* Not possible nor necessary to do all, only needed one
* SRQR was good choice because general = large sampling pool, used by academics and non-academics, I was familiar with it
```

#TODO: ## Qualitative approach and research paradigm
<!-- Qualitative approach (e.g., ethnography, grounded theory, case study, phenomenology, narrative research) and guiding theory if appropriate; identifying the research paradigm (e.g., postpositivist, constructivist/ interpretivist) is also recommended; rationale** -->

#TODO: ## Researcher characteristics and reflexivity
<!-- Researchers’ characteristics that may influence the research, including personal attributes, qualifications/experience, relationship with participants, assumptions, and/or presuppositions; potential or actual interaction between researchers’ characteristics and the research questions, approach, methods, results, and/or transferability -->

### Sampling strategy
<!-- How and why research participants, documents, or events were selected; criteria for deciding when no further sampling was necessary (e.g., sampling saturation); rationale** -->
To be eligible, participants had to be actively engaged in qualitative research and be able to attend an online interview conducted in English. I used purposive sampling, seeking participants that were all doing qualitative research but varied in their experience (years doing research), workplace (academia, industry, clinician etc.), geographic location, and stage of writing (drafting vs editing). I chose these dimensions as my previous work identified that different groups of people encounter different barriers. I was prepared to do additional sampling should I discover other dimensions of importance during data collection. Participants were offered £50 as remuneration for their time as a bank transfer (or Amazon voucher for UK participants).

I used information power to guide my initial sample size (see chapter {{< var chapters.focus-groups >}} for an introduction to information power). I considered my aim to be narrow and sample to be specific but with adequate variation. My intervention and analysis was based on a behaviour change framework, and I expected my methods to provide ample opportunities for deep discussion. For these reasons, I felt confident that 10 participants would provide adequate information power whilst also being manageable within the time limit of my DPhil. 

I recruited participants by a) posting adverts on Twitter which were retweeted by the EQUATOR Network and AuthorAid @AuthorAIDHome (a network for researchers from low and middle income countries); b) emails forwarded by AuthorAid, the African Research Integrity Network @ARINHomePage and 101 Health Research @101HealthResearch (a publication service provider based in the Philippines); and c) a notification to authors using Penelope.ai @PenelopeAi, a free manuscript checking tool used by medical journals.


### Data collection methods
<!-- Types of data collected; details of data collection procedures including (as appropriate) start and stop dates of data collection and analysis, iterative process, triangulation of sources/methods, and modification of procedures in response to evolving study findings; rationale** -->
Defining intervention components by their target barriers and intervention functions lends itself nicely to designing a pilot study to gather feedback. Because I knew what each component was supposed to be _doing_ I could design an interview schedule with questions and tasks to specifically explore intervention functions. 

My interview schedule had 4 parts:
 
1. 5 second test
2. Think Aloud + Interview
3. Writing task (at home)
4. Writing review + interview

{{< include _table.qmd >}}

### 5 Second Test

GOT TO HERE

~For example, the purpose of the top of the home page is to communicate what reporting guidelines are and how they benefit authors. Instead of asking general questions (like "what do you think of...?"), I decided to ask more specific questions ("What do you think the website is about? How do you think it might influence your work?"). Furthermore, because the purpose was to communicate these things _quickly_, I decided to only give pilot participants 5s before asking these questions.~ Similarly, defining components in this way will facilitate future quantitative work to assess efficacy.

### Demographics questionnaire

After giving informed consent, participants provided their experience, place of work, english proficiency, and country of residence using an online form.

### 5 second test

Participants were invited to an initial interview conducted over Zoom. The lead researcher shared their screen and displayed the website homepage. Without warning, the page was closed after 5 seconds after which we used semi structured interviews to assess participants immediate understanding of and feeling towards the home page. Questions included:

"What do you think the website is about?"

If the participant mentioned the term "reporting guidelines", we asked "What do you think reporting guidelines are?" and "What tasks do you think you can use reporting guidelines for?"

"How would you describe the design of the site?"

"What would make you want to learn more about the website?"

"What impact do you expect the website to have on your job as a researcher?"

Wanted to know whether:

* the participants realised the website was about guidance to help you *write-up* (as opposed to design or conduct) research
* the ppt realised the guidance should be used during writing (as opposed to editing)
* the ppt realised the guidance should help *them* in some way (as opposed to other people), e.g., quicker, confidence. 

Testing the written content and design. 

### User protocols / think aloud tasks

#### 1. Finding guidance and resources

We gave participants descriptions of research and asked them to identify the most relevant reporting guideline from the website. We did not tell them how to go about this - participants could use direct links on the homepage, the search bar, or could follow the "wizard" questionnaire. We did this task 3 times. The first time, participants were asked to find a guideline for animal research. This was the easiest guideline to find as the description was displayed prominently on the home page. Second, we asked participants to find guidance for reporting cohort studies. This was a little harder as it required participants to read and understand descriptions of study designs to distinguish between different, related, epidemiology guidelines. Finally, we asked participants to find what guidance they would to report (# DECIDE: what impossible task?). This was a difficult question as there is not perfect reporting guideline for this kind of article. Instead, participants had to (# FIXME: complete task).

We also presented tasks that involved finding tools. We asked participants to imagine they had been asked to submit a "completed checklist" by a journal, requiring them to find the guidance, then find and use the associated checklist. We asked participants what they expected "to do lists" and "templates" to be, and when they might be used.

#### 2. Finding information within a guideline

We then asked participants to find information within the #DECIDE: guideline. We asked participants how they would report #FIXME, why it is important to describe #FIXME, and what they should write if they hadn't done #FIXME. These questions required participants to find items #FIXME respectively, and to locate content within collapsible boxes.

#### Plus - minus test

At the end of the first interview we gave participants a task to complete in their own time over the coming weeks. We provided them with the methods items of the #FIXME guideline in a #DECIDE Word file. Participants were to read the methods items of the #FIXME guidelines in their own time and highlight sentences that elicited positive or negative responses and to mark them with a "+" or "-" symbol. They could add notes if they wanted to.

#### Writing evaluation (Performance test)

If participants were actively writing an article we asked them to use the guidelines to write their methods section. If they had something already written, we asked them to complete a reporting checklist. We asked them to do this in their own time, within two weeks. Once complete, participants sent their work to JH via email who then checked their reporting against the the #FIXME guidelines, noting which items had been reported fully and which hadn't.

#### Retrospective interview

Participants then attended a follow up interview two weeks later where JH asked open questions to explore the reasons behind participants + and - marks, and reasons for neglecting any items in their writing sample. The +/- test aimed to pick up non-specific responses, which would include parts of the text where the participant found difficult to understand. The writing check, however, would also reveal parts of the guidance that the participant had unknowingly _mis_understood.

Finally, we asked the participants again for their opinions on the website and guidance and how it could be improved.

<!-- #DECIDE: Do we really need people to go away and write, then come back? Recruiting people that are actively writing may bias our sample as we may attract participants that are looking for help/feedback on their writing. If we were to ditch this step, we could ask participants to do the +/- test live, in the session, and then interview them immediately. A middle ground could be to give them an option: Participants could expect to attend 2 interviews, but then if it turns out they aren't writing anything actively, we could cancel the second interview and do the +/- there and then. Downside of this is that we can't offer manuscript review as an incentive. But downside of offering manuscript review as an incentive is that it may bias our sample (to people looking for help) and also may give the game away. Perhaps we could counter act that by recruiting people that we already know don't like guidelines? But would they do the writing task?-->

### Data collection instruments and technologies

<!-- Description of instruments (e.g., interview guides, questionnaires) and devices (e.g., audio recorders) used for data collection; if/how the instrument(s) changed over the course of the study -->
Interviews were conducted over Microsoft Teams, using its in-built video and audio recording. We created interview schedules (#REF) and piloted them amongst students in the department. The version of the website tested can be viewed at (#REF).

### Units of study

<!-- Number and relevant characteristics of participants, documents, or events included in the study; level of participation (could be reported in results) --> #FIXME move to Results?

### Data processing

<!-- Methods for processing data prior to and during analysis, including transcription, data entry, data management and security, verification of data integrity, data coding, and anonymization/de-identification of excerpts -->
We used Zoom to automatically transcribe audio recordings, and then manually double checked the transcripts and added context from the videos, interview notes, +/- annotations and writing sample. We imported transcripts into NVivo (#REF).

### Data analysis

<!-- Process by which inferences, themes, etc., were identified and developed, including the researchers involved in data analysis; usually references a specific paradigm or approach; rationale** -->
I created cases for participants and for the methods I used (5 second test, think aloud etc.).

Created cases for each participant. 
I uploaded the automatic transcript files from Microsoft Teams and used NVivo's function to automatically code text against participant initials. 

Created a code for each intervention component that I had enacted (47 codes). Added more codes when needed, if a participant raised something that wasn't covered by an existing intervention component. 

Coded all transcript text against these codes. 





We coded positive and negative experiences and grouped them by intervention component. We did this because we expected its output - negative and positive experiences grouped by intervention component - to be easier to act upon than if we grouped experiences by method.

<!-- # DECIDE: count similar problems? -->

### Techniques to enhance trustworthiness

<!-- Techniques to enhance trustworthiness and credibility of data analysis (e.g., member checking, audit trail, triangulation); rationale** -->
* Double checking of coding
* Member checking
* Triangulation? Read the paper Charlotte recommended. Perhaps it is about mixed method studies? Perhaps that means I could count errors / number of people that did a task successfully? [https://www.bmj.com/bmj/section-pdf/186156?path=/bmj/341/7783/Research_Methods_Reporting.full.pdf]


### Ethical issues pertaining to human subjects
<!-- Documentation of approval by an appropriate ethics review board and participant consent, or explanation for lack thereof; other confidentiality and data security issues -->
The study was approved by the Medical Sciences Interdivisional Research Ethics Committee at Oxford University. Participants gave informed consent via a web-form.


## Updates after first few interviews:

Home page colours, value statement

Notes (footnotes)

Images to home page

Quotes - added formatting and pictures

Value Prop - changed to "Writing research is hard". Then after CW changed again to "What help writing up research?"



## Things to change:

Version 1.0 to Latest version
Make home page shorter (collapse tools and benefits)
Ensure CTA on guidance page is clear
Coloured headings
What are RGs before Search for RGs button
Move logos to top
Add Oxford Logo and/or EQUATOR logo? 
At top of RG page, explain what the reader can expect to find on the page e.g., "list of item, each describing what you need to write, why it is important, and with examples and training."
The "learn more" buttons in the "what are RGs" panel are confusing. People think it is going to take them to a list of templates, or a checking tool. Replace with a single link that is clearly to a case study?
Make acronym definition more obvious
Move "What are RG button to the left"
Define study types on home page
Make formatting of FARGs better - add definitions to right hand side of later GLs?


## Themes

Do people get what RGs are?
* reporting = school


Do people get what the benefits are?

The look and feel - simplicity vs trustworthiness 

Understanding what the website is and what it will deliver. Could use NICE for inspiration?

Displaying information & consiceness

Trust and the relationship between this website, EQUATOR, and the guidance

Audience? (include geography, translations, is it about the EQUATOR?)

Understanding the guidance 
* understanding the text
* examples are important

Discussion
Perhaps the guidance would be better split into a book
Perhaps the checklist _should_ be the first thing you see. Analogy of pantry cupboard / stock room with a sign on the door. 

Need professional design

Extra evidence for identified barriers - none new

Example of CW for how submission is the worst time. Different frame of mind. Time and motivation. First time around, she dismissed development information. Second time around she found it in the FAQ and was glad. 


