---
title: "Following the behaviour change wheel guide: Workshops with reporting guideline experts"
format: 
    html:
        output-file: index.html
    docx:
        output-file: JH-chapter-workshops.docx
---

##  Introduction

Having identified influences affecting whether authors adhere to reporting guidelines (chapters {{< var chapters.synthesis >}} - {{< var chapters.web-audit >}}) and selected the Behaviour Change Wheel as a framework @michieBehaviourChangeWheel2011 (chapter {{< var chapters.bcw >}}), my next step was to use the framework to unpick these influences, diagnose what drives them and how they could be addressed. In my introduction I describe how the reporting guideline system grew organically and I justified why I wanted to redesign this system and the guidelines themselves using evidence and behaviour change theory. 

In "The Behaviour Change Wheel - A Guide To Designing Interventions"@michieBehaviourChangeWheel2014, Michie et al. suggest eight steps to help intervention designers understand behaviour and identify intervention and implementation options. The guide is aimed at intervention designers who are not behaviour science specialists, and so offered a practical way to include stakeholders from the reporting guideline ecosystem. I wanted to do this because I expected that input from experts with intimate knowledge of reporting guidelines would lead to more ideas, and that these ideas may be more likely to gain traction and have impact.

I had to be mindful of how much time I could expect stakeholders to give to my project. The eight stages outlined by Michie et al. would take many hours and require background familiarity with the COM-B model, intervention functions and policy categories. This seemed like too much to ask of strangers uninvested in this work, and so I decided to begin by involving reporting guideline experts with whom I already had a relationship, and who were already invested in the project: members of the UK EQUATOR Centre. See chapter {{< var chapters.introduction >}} for an introduction to the EQUATOR Network, and see chapters {{< var chapters.focus-groups >}} and {{< var chapters.pilot >}} for how I later sought input from wider stakeholders and authors.

In this chapter, I describe how I led members of the UK EQUATOR Centre through the intervention design process outlined in "The Behaviour Change Wheel - A Guide To Designing Interventions"@michieBehaviourChangeWheel2014. I describe the methods and results for each stage in turn.

##  Methods

Of the {{< var counts.EQUATOR_staff_invited >}} members of the UK EQUATOR Centre I invited, {{< var counts.EQUATOR_staff >}} took part in the workshops. These members had experience in developing and disseminating reporting guidelines, and in training authors how to use them.

I led the workshops and actively participated too. I felt justified drawing on my own experience from my earlier DPhil work, as an author, and developer of tools to help authors (see chapter {{< var chapters.reflexivity >}}). Together, we completed exercises through discussion. Hence my paradigm was constructivist in that knowledge was "constructed between inquirer and participant through the inquiry process itself"@m.givenSAGEEncyclopediaQualitative2008. Constructivism "rejects the idea that there is objective knowledge in some external reality for the researcher to retrieve mechanistically" and instead, "the researcher's values and dispositions influence the knowledge that is constructed through interaction with the phenomenon and participants in the inquiry"@m.givenSAGEEncyclopediaQualitative2008. 

On one hand I expected that my experience would be an asset, and would contribute to our aim of understanding and addressing the phenomena we were interested in. On the other hand, I wanted to ensure that my opinions did not restrict the group, that I remained open-minded, and that I captured the thoughts of other workshop participants accurately. And so I used a number of established techniques to enhance trustworthiness and facilitate discussion. 

I used Lincoln and Guba's criteria for trustworthiness @lincolnNaturalisticInquiry1985, which asserts that for a study to be trustworthy, the researcher must show that the findings are credible ('true'), transferable (applicable to other contexts), dependable (consistent and repeatable), and confirmable (shaped by participants, not by the researcher's bias or motivation). I describe the techniques I used to achieve each criterion in @tbl-trust-workshops.

{{< include /chapters/7_workshops/_table.qmd >}}

To encourage open discussion, I encouraged participants to rise above their own preconceptions and reassured them that there were no wrong answers, that all ideas were valid and should be documented. To facilitate rich discussion I used open-ended questioning, left space for participants to talk, and followed Michie et al.'s worksheets which structure inquiry around frameworks, models, and taxonomies.

<!-- #ASK: name all biases. Perhaps using https://www.ideatovalue.com/crea/nickskillicorn/2021/06/a-list-of-all-of-the-biases-which-affect-creativity-and-innovation-performance/ or https://catalogofbias.org/about/ -->

We met {{< var counts.EQUATOR_meetings >}} times between December 2021 and May 2022, and each online meeting lasted around 2 hours. All participants had access to the Behaviour Change Wheel book. We established some ground rules which were that no idea was a bad idea, we should favour evidence over preconceptions, and that we should aspire to challenge our own assumptions and be open minded as far as possible. We didn't seek consensus. Instead, we kept note of any disagreements that could not be resolved by discussion. 

I explained the objectives and any background theory at the start of each step. I will now summarise each step and our discussions. Our co-edited worksheets are included in Appendices. I purposefully use "we" in this chapter instead of "participants" to reflect that my voice is included.

## Results 

### 1. Defining the problem in behavioural terms 

Michie et al.'s first step is to define the problem in terms of _who_ needs to do _what_. For example, _weight loss_ is not a behaviour, but _increasing physical activity_ is, and could be specified further as _walking 10,000 steps a day_. We were all in alignment here: _we want researchers to include important details in their articles, in line with the relevant reporting guidelines._ Our notes from the workshop are in Appendix {{< var appendix.workshop_step_1 >}}.

### 2 & 3. Selecting and defining the target behaviour

"Behaviours do not exist in a vacuum but occur within the context of other behaviours of the same or other individuals" write Michie et al. @michieBehaviourChangeWheel2014 when explaining that the desired behaviour need not be the behaviour that you target. For instance, if you want a child to eat more fruit (desired behaviour), you may seek to influence what food their parent buys (target behaviour).

Step 2 involves generating a longlist of candidate target behaviours that could bring about the desired behaviour, and then selecting which behaviour(s) to focus on. 

We thought of many target behaviours involving authors, such as "reading the guidance in full" or "studying guidance (in an abstract sense)". We also considered targeting people other than authors. For instance, "peer reviewers" and "editors" could "check articles against guidelines [and] tell authors what is missing", and supervisors could "encourage" the use of guidelines.

This exercise helped us break down things that we had not questioned previously. For instance, we were forced to define what we actually meant by a "reporting guideline", noting that guidance could be distributed across publications, checklists, or supplements. We wanted people to "use the _full_ guidance (often reported in an _Example & Elaboration_ document)" and "not just the checklist". 

Similarly, where we may previously have thought of "writing" as a single task, we began to consider how authors could "use guidance when planning", "drafting", "editing [their own work]", or "checking". We discussed how these behaviours required researchers to be open-minded to assistance and wondered if we could encourage them to "ask for help when writing". 

Ultimately we ended up with a longlist of 14 possible target behaviours. The next step was to select a behaviour from our longlist by considering:

1. The expected impact if the behaviour were to be performed
2. How easy we expected behaviour change to be
3. The centrality of behaviour - how close it was to our desired behaviour
4. How easy the behaviour will be to measure

Criteria two, three, and four lead us to prioritise the behaviour of authors above that of editors or peer reviewers. We felt authors to be central as ultimately they have control over what gets written. Editors, peer reviewers and other stakeholders are less central because all they can do is ask an author to edit their writing. We also felt that changing the behaviour of editors and reviewers would be harder to instigate and to measure. Although journal policies are easy to audit, the actual behaviour of the editorial team and peer reviewers are not. In contrast, we felt authors' behaviour was easier to measure through literature audits, surveys, and web analytics for EQUATOR's website. When considering criteria one and two, we felt that authors were most likely to apply guidance if they used it early when writing.  Our workshop notes for step 2 are in Appendix {{< var appendix.workshop_step_2 >}}.

Step three of the Behaviour Change Wheel involved bringing these thoughts together and specifying the behaviour in more detail. Michie et al. suggest defining _who_ needs to perform the behaviour, _what_ do they need to do, _when_ will they do it, _where_ will they do it, how _often_ will they do it, and _with whom_ they will do it. Our final definition of our target behaviour was: **Researchers should use reporting guidance as early as possible in their research pipeline. They will do this for every piece of research, at their place of work, on their own but in the context of collaboration**

We broke this key behaviour into two sub behaviours:

1. Engage with reporting guidelines as early as possible (i.e., access and read them) and,
2. Apply the guidance to their writing as intended by the guideline developer.

By "research pipeline" we mean the many steps involved in a typical project which may include ideation, designing, writing a protocol, obtaining funding and ethics permission, drafting a manuscript, editing a manuscript, submitting a manuscript. Instead of specifying _which_ stage reporting guidance should be used, we decided to specify that we want authors to use guidance as "early as possible". We did this for a few reasons. Firstly, guidelines differ in how easily they can be used for writing protocols or applications, not all disciplines have culture of writing and publishing protocols, and not all research projects will begin with a written funding application, ethics application, or protocol. Secondly, researchers will naturally come across reporting guidelines at different stages of their work. Should a researcher discover a guideline at the point of journal submission, then we would still want them to apply the guidance then, even though this is a relatively late stage. But by specifying "as early as possible", we declare our hope that _next time_ that same researcher may decide to use a guideline at an earlier point. 

By "apply", we refer to using guidance to plan, write or edit a written description of research (e.g., within a manuscript or application). By "the guidance" we mean the fullest form of the reporting guideline available. Ideally this will be an example and elaboration paper, but for some guidelines it will just be the checklist. Applying guidance may include the use of tools like templates or checklists. Participants discussed specifying "completing a reporting checklist" as a target behaviour, but decided against it as previous research showed that authors who complete checklists upon submission don't necessarily edit their manuscript or comply with guidelines. Participants also recognised that focussing on checklists may be problematic because checklists appear administrative, are used _after_ a manuscript has been written, at which point authors are least able or motivated to edit their work. Nevertheless, participants recognised that checklists will continue to be an important part of how reporting guidance are disseminated, and so they are included within the term "apply the guidance", without being named.

We specified authors should apply guidance "as intended by the guideline developer" because thematic synthesis revealed authors may misinterpret guidelines and believe they are adhering when they are not. Our workshop notes for step 3 are in Appendix {{< var appendix.workshop_step_3 >}}.

Hence our target behaviour definition specifies the _who_, _what_, _when_,  _where_, _how often_, and _with whom_ but is broad enough to account for differences between reporting guidelines and researchers' working practices. 

### 4. Identify what needs to change

This step involved identifying what needs to change in the person and/or environment to achieve our target behaviour. Michie et al. @michieBehaviourChangeWheel2014 provide a questionnaire to facilitate this step, called the _COM-B Questionnaire_, which asks you to consider each COM-B domain in turn by considering 23 items like "To perform the target behaviour, authors would...have to know more about why it was important", "...have more time to do it", and "...have more support from others" etc.

Michie et al. @michieBehaviourChangeWheel2014 emphasise the importance of evidence in this step, recommending that data should be "collected from as many relevant sources as possible" and "triangulated", as a consistent picture of behaviour from multiple sources will "increase confidence in the analysis". Consequently, after first brainstorming freely, we went through the influences I identified in chapters {{< var chapters.synthesis >}} - {{< var chapters.web-audit >}} to support or refute our thoughts and to ensure we did not miss anything. We went through each of my descriptive themes from my qualitative evidence synthesis, each additional code from my review of survey content, and my findings from evaluating EQUATOR's website. Because our notes from that session contained duplication and because some sentences includes multiple thoughts, I consolidated our workshop output into {{< var counts.barriers >}} influences that we felt needed to change for our target behaviour to occur. We used the COM-B questionnaire to label each influence as being driven by capability, opportunity, or motivation. I added context from our discussion and my previous chapters where I felt appropriate. I circulated this list to workshop attendees for comments and revisions. You can see this list in Appendix {{< var appendix.barriers >}}.

### 5 & 6. Identify Intervention Functions and Policy Categories

Having defined our target behaviour and identified what needs to change for that behaviour to occur, the next step was to consider _how_ to achieve those changes. Michie et al. @michieBehaviourChangeWheel2014 stress the importance of "considering the full range" of possible intervention functions and policy categories available. See chapter {{< var chapters.bcw >}} for a fuller introduction to these terms but, briefly, an intervention function is a "category of means by which an intervention can change behaviour" and policy categories are options for delivering those functions. For example, the function _modelling_ could be delivered through a communication campaign or a service.

Michie et al. @michieBehaviourChangeWheel2014 recommend using the APEASE criteria to prioritise options, which stands for Affordability, Practicability, Efficacy, Acceptability, Side-effects, and Equity. They note that whilst effectiveness is key, the other criteria must also be considered as "behaviour change interventions operate within a social context".

We saw enablement, education, training, persuasion, modelling, and environmental restructuring as favourable intervention functions, ranking well on all APEASE criteria. Of these, enablement was viewed as a particularly low-hanging fruit that would likely be effective and welcomed by the research community.

We found the remaining intervention functions problematic. Incentivization and restriction (e.g., rewarding guideline adherence with funding or reduced article processing charges, or punishing non-adherence by withholding these benefits) were seen as inequitable because as long as guideline adherence is harder for some researchers than others, less-experienced, poorly resourced researchers would be at a disadvantage, as would those working in disciplines where reporting guidelines are poorly designed or harder to apply. Conversely, we felt that enablement has potential to reduce existing inequalities. In addition to being inequitable, participants voiced that restriction or punishment would be unacceptable to researchers, as would coercion (the threat of punishment). Furthermore, threats without enforcement may become known as pointless administration, lose effectiveness, and erode trust.

Regarding policy categories, we saw environmental planning as the most affordable, effective, acceptable, safe, and equitable. When talking about the _environment_, we were really talking about the _digital_ environment, within which EQUATOR has control over its own website but not websites or digital services run by others. Improving and extending the website was viewed as an affordable investment, that would be welcomed by authors (thus acceptable), and a practical way to increase the proportion of authors engaging with reporting guidelines that would increase equity without side effects.

We recognised communication as a favourable policy category that is already utilised. Communication channels included the website, mailing lists, social media, conferences, and publications. Whilst communication was rated as affordable, practicable, acceptable, safe, and equitable, its effectiveness may be limited. Although communication could promote awareness of reporting guidelines are available, what they are, and how they can be used, EQUATOR members noted that the efficacy of a communications campaign may be undermined if the campaign directs authors to a website that is difficult to use, or guidelines that are difficult to access or understand. Thus communication on its own might not be sufficient, and members suggested that it should come after improving the digital environment.

Service provision was also favoured, as long as the service was financially sustainable. So too were guidelines, specifically guidance to help reporting guideline developers create and disseminate resources. Participants felt that legislation, regulation, and fiscal measures would not be acceptable to researchers and were not practical options for EQUATOR to use. 

Our notes from steps 5 and 6 are in Appendices {{< var appendix.workshop_step_5 >}} and {{< var appendix.workshop_step_6 >}}.

### 7. Identify behaviour change techniques

The aim of step 7 was to identify possible behaviour change techniques by systematically considering items from a taxonomy of 93 techniques @michieBehaviorChangeTechnique2013 for each intervention function chosen in step 5. Michie et al suggest doing it this way because "the process of designing behaviour change interventions usually involves first of all determining the broad approach that will be adopted and then working on the specifics of the intervention design. For example, when attempting to reduce excessive antibiotic prescribing one may decide that an educational intervention is the appropriate approach. Alternatively, one may seek to incentivise appropriate prescribing or in some way penalise inappropriate prescribing. Once one has done this, one would decide on the specific intervention components". I interpret this passage as Michie et al. @michieBehaviourChangeWheel2014 suggesting that intervention designers will select only one or two intervention functions, and then consider all behaviour change techniques relevant to that function. However, picking a "broad approach" didn't feel helpful to us. Given that a system already exists for disseminating reporting guidance, we found ourselves considering how we could use multiple intervention functions to refine this system. Consequently, because we were not limiting our approach to one or two intervention functions, it meant we could not filter the list of behaviour change techniques and had to consider most of them. 

Additionally, although the taxonomy is designed to be applicable to a range of contexts and intervention types, it didn't always feel like a perfect fit for our needs. Some techniques are explicitly health-focussed (e.g., _Body changes_, _Pharmacological support_, and _Information about health consequences_) whereas for others the link with health interventions came from the examples provided. For example, the technique _practical social support_ lists the example of "Ask the partner of the patient to put their tablet on the breakfast tray so that the patient remembers to take it". It's not immediately obvious how this technique could generalise to our target behaviour. The taxonomy developers perhaps acknowledge this limitation, suggesting in their discussion that the list can be viewed as a "core" taxonomy that can be modified or extended according to context @michieBehaviorChangeTechnique2013.

Hence, given that we didn't want to choose a "broad approach", and given that this step required familiarity with the taxonomy and how it can be adapted to our context, doing this step with EQUATOR staff would have been very time consuming, as others have also found @carvalhoParticipatoryDesignBehaviour2020. Ultimately, EQUATOR staff asked for a more pragmatic approach.

Instead, I did this step on my own. I was well placed to do this because I was familiar with the taxonomy and the Behaviour Change Wheel Guide, I had led all workshops, and I was most familiar with the influences we were trying to address. I went through each intervention function the group had favoured in step 5 (Enablement, Education, Training, Persuasion, Modelling, and Environmental Restructuring), considered all relevant behaviour change techniques, and used the APEASE criteria to choose ones I felt were appropriate. 

Whilst I did that, EQUATOR members considered each of our favoured intervention functions and brainstormed solutions to influences. I then deduced the behaviour change techniques these ideas used and cross checked them against my list of favoured behaviour change techniques, adding any I had missed. The final list of 39 techniques is shown in @tbl-bcts. I describe the ideas fully – alongside ideas captured through focus groups with other stakeholders – in the next chapter.

{{< include /chapters/7_workshops/table_bcts.md >}}

### 8. Identifying delivery options

I then had to decide how to deliver our favoured behaviour change techniques, intervention functions and policy options. Michie et al.'s guidance for this step is more open ended @michieBehaviourChangeWheel2014; although they offer delivery options for communication as an example intervention function, there is no framework or systematic approach to this step and instead they recommend that designers consider the delivery options that they have at their disposal. For EQUATOR, that meant developing a new service or training programme, running a communication campaign, developing new guidance, or improving their website. It was important that my choice was feasible within my funding window, and I wanted it to have a lasting impact after my DPhil was finished.

Creating a new service felt unsustainable as it would likely stop once my funding ran out. Developing training or guidance for guideline developers could be useful and acceptable but not achievable within my time constraint. I could have developed a communications campaign, but EQUATOR members felt this was something they could do independently and that it should be done after addressing other influences – a communications campaign promoting a guideline would be less impactful if the guideline is hard to understand or use.

Instead, redesigning reporting guidelines and key parts of EQUATOR's website felt like the perfect choice for multiple reasons. We decided that displaying redesigned guidelines as a webpage would make them easy to disseminate, easy to access, and would enable functionality not available in traditional publications. These redesigned guidelines could become part of the existing EQUATOR Network website. We also felt that EQUATOR's home page was particularly important to focus on, as it is the most visited page of the website where the highest proportion of visitors disappear. 

Improving guidelines and the website aligned with our prioritised options. Planning the digital environment was our highest ranked policy category. We could use the website to deliver many of our favoured intervention functions (enablement, education, persuasion, modelling). It spoke to my skills as a software developer. Finally, these changes could be made within the time limit of my DPhil and would have a lasting impact.

##  Discussion

Following Michie et al.'s guide @michieBehaviourChangeWheel2014 helped us to specify our target behaviour, understand the behavioural drivers behind {{< var counts.barriers >}} influences on our target behaviour, select intervention functions, policy categories, behaviour change techniques, and delivery options. 

More fundamentally, the process helped the workshop participants to begin thinking about reporting guidelines as a behaviour change intervention. The process helped us break down the differences between tasks (e.g., writing vs. editing vs. reviewing research articles), users (e.g., inexperienced vs. experienced researchers, editors, reviewers), resources (e.g., explanation and elaboration documents vs. checklists). It helped EQUATOR staff to look at the current system objectively and it helped us challenge our preconceptions.

One of the most interesting parts of this process was witnessing an unexpected change of opinion amongst EQUATOR staff. Before beginning this study, a common refrain heard around the office was that in order for reporting guidelines to be successful editors had to start enforcing them and refuse to publish research that did not adhere. So it was fascinating to witness workshop participants unanimously rating restriction and coercion as their least favourite options.

I think two things happened here. Firstly, having discussed the challenges authors face when using reporting guidelines, participants felt that forcing authors to use them would be unacceptable to researchers, impractical for editors, and inequitable as some authors would face larger hurdles than others. Secondly, participants reassessed things they had taken for granted, and realised that there are many low-hanging fruit that could make guidelines easier to find and use, and that these fruits were growing in their own orchard.

Using a framework helped participants to systematically consider a full range of options, many of which may not have come to mind naturally. However, sometimes it was difficult to get participants to think "outside of the box". The default was to think about how the existing system could be improved, and it was difficult to imagine a world where we could be starting from scratch. In one sense this was an opportunity, as improving a system that already exists is easier than creating something totally new. But it could also be seen as a limitation, as our imagination may have been limited by what already exists. 

We may also have been limited by group-think @parrilloGroupthink2008. All workshop participants had worked for the EQUATOR Centre for many years and had many shared opinions and experiences. Including other stakeholders in these workshops would have helped address this but would have been impractical to coordinate. To mitigate this, I decided to gather input from guideline developers, publishers, and authors through separate pieces of work which I describe in chapters {{< var chapters.focus-groups >}} and {{< var chapters.pilot >}}.

In conclusion, by working through Michie et al's suggested approach to applying the Behaviour Change Wheel @michieBehaviourChangeWheel2014 in a series of workshops with members of the EQUATOR Network, we defined our target behaviour as _"Researchers should use reporting guidance as early as possible in their research pipeline. They will do this for every piece of research, at their place of work, on their own but in the context of collaboration"_. We broke this down into two sub behaviours: 1) engage with reporting guidelines as early as possible (i.e., access and read them) and, 2) apply the guidance to their writing as intended by the guideline developer. We identified {{< var counts.barriers >}} influences that could affect this target behaviour. We favoured Enablement, Education, Training, Persuasion, Modelling, and Environmental Restructuring as intervention functions, and we favoured Environmental Planning, Communication, Service Provision, and Guidelines as policy categories. I identified 39 behaviour change techniques that could be used, and we decided that our focus (and the focus of the rest of my thesis) should be on redesigning reporting guidelines and the EQUATOR Network home page.

## Reflections on this chapter 

At times these workshops felt a little awkward to run. I was very aware of being "just" a student and my imposter syndrome was strongest when conversations questioned EQUATOR's identity. I'm used to a corporate approach where a company's vision and objectives are set at a high level and all work is expected to support those goals. Although EQUATOR does have organisational objectives, they have not been updated since its formation and its staff are not tethered to them. As with any research group, staff are free to pursue their own research interests. This academic freedom is important, but because staff are not aligned behind a core set of values or objectives, some of our workshop discussions touched on bigger questions like _what does EQUATOR stand for?_, _what should it stand for in the future?_, or the differences between the _EQUATOR Network_, the _UK EQUATOR Centre_, and us as a research group. One of the biggest debates was around whether EQUATOR (and reporting guidelines) should be advising researchers on how to design studies. Design and reporting go hand in hand. One reason reporting is important is to allow the reader to assess the design. However, the two are separate research tasks, done at different times, and perhaps by different people. Ultimately the group decided reporting and design should be linked but kept separate, and that EQUATOR should stick to its founding objectives of improving reporting. This distinction has settled comfortably in my head, but perhaps not so much in others', especially those whose interests lie in experimental design and who perhaps see EQUATOR and reporting guidelines as trojan horses for disseminating design advice.

This is one way in which the interests of participants influenced workshop dialogue and outputs. As have conversations outside of the workshops. We are all colleagues, and continually talk about our work. At the time of these workshops a few colleagues were involved in a study assessing adherence to reporting guidelines. When they assessed the same manuscript they often disagreed whether an item was reported properly. In an attempt to solve this problem, another colleague created an assessment tool that deconstructs reporting guideline items into their smallest parts and asks assessors to confirm the presence and absence of each sub item. Implicit in these conversations is a perception that reporting is either good or bad. Information is either present or absent. On first glance this makes sense and at the time of the workshops I never really questioned it. But upon revising this chapter I realise that my thinking may have shifted since. Does this black and white mentality always hold? Some reporting items are more subjective than others, and a black and white approach to assessment makes less sense when guidelines explicitly acknowledge this subjectivity. For example, this section is partly an attempt to fulfil SRQR's item 6 (Researcher characteristics and reflexivity) which also appears in JARS-Qual, where it warns "It may not be possible for authors to estimate the depth of description desired by reviewers without guidance". To me, this reads as "good reporting is in the eye of the reader" and is an acknowledgement of subjectivity. What might satisfy one reader may not satisfy another. So a black-and-white approach to adherence might not always be possible or best. If I had raised these questions during the workshop, they might have led to additional ideas or lines of dialogue. 

What other assumptions may have shaped my workshops? In writing this section I realised that describing my "stance" is difficult because some assumptions only became clear to me when they were challenged or shifted. Describing my stance feels easier when I have something to describe it against. Perhaps I find this difficult because I'm inexperienced, or perhaps because my community &mdash; EQUATOR and the reporting guideline crowd &mdash; are quite homogenous in their own stances and so I have less to contrast myself against until my research led me to question and disagree with prevailing attitudes. Implicit assumptions have undoubtedly influenced this chapter (and others). If I continue working in this field I hope that as I and my colleagues grow these assumptions will surface as I continue to reflect on them.