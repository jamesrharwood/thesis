```
Summarize aims and objectives

Summarize results

* number of deficiencies
* number of intervention components

Biggest themes:

* design
* wanting more but also wanting less
* understanding relationships
* focussing on author worked. Beneifts to others largely neglected but nobody missed it. 

Validation of barriers. 
Even for Components that were not implemented. E.g. consistent words

Value of methods. 

* codes per method?
* each brought value 

Strengths in comparison to other studies

* recruitment
* thick data. Much better compared to thin studies. Even good feedback studies only focussed on content on guidance (PRISMA 2) or its application (SQUIRE 2), but not so much the design or dissemination channel. Other guideline groups could copy my methods. 

Limitations

Context wasn't perfect. Participants were not completely naive. 
Focussing on deficiencies occludes positives. 
Better to have had neutral interviewer? But my understanding of the components was also a strength?
Not all intervention components explored:

* Some not at all e,g, search optimization
* Some only partially e.g. search functionality and links to related guidelines, should be a navigation task

Future studies

Further refinement addressing the main results discussed at start of Discussion
Possible preference study comparing old and new system, or relative importance of components e.g. conveying benefits to others vs oneself
NEed to develop checklists and writing tools (templates)
Once live, could do a mixed methods feasibility study comparing analytics behavioural data with possible journal input. 







The context in which users encounter the website will effect how it is perceived. 

Most visitors come from journal websites, having been instructed by an editor or JAI page to use a particular reporting guideline. Others may come to the site because a a guideline has been recommended by a friend. Very few arrive completely cold turkey. 

In contrast to this study, where all participants were totally cold (unless they were already familiar with EQUATOR). 

So when one participant suggested that "What are RGs" should come above the Frequently Accessed Guidelines (Or at least that the writing, checking, planning should) this may appear sensible on the surface. But given that most authors will come already with a guideline's name in mind (even if they don't really know what it is), this ight not be necessary. 

Therefore, need a real world evaluation.

All could speak English = bias

Not all intervention components explored directly. E.g. the think aloud was good at detecting things that were salient (good or bad). Not good at detecting neutral things. 

Task based protocol or navigation task might have been better. E.g. "look for french version" or "imagine you needed to cite this, where would you look" - which I did kinda do as an interview question.

The problem of finding the right guideline still deserves exploration. Need the other guidelines to be up, and would need to either give tasks or recruit people differently (not by the kind of research they were doing). 

The data collection methods I've used have been critiqued - variation in practice makes it difficult to repeat or compare studies. And variation is not always grounded in theory. For example, 5 second test variations, think aloud variations. 

Trust became a big theme. Wasn't really included as an intervention component at top of home page. Should be added, and future 5 second tests should explore this. Fogg splits trust into 4 categories and this would be testing surface credibility (Fogg 2003, from chapter 4 of UX five second rule book. " That which can be perceived immediately (colors, layout, images, etc.) has the ability to boost credibility substantially—e.g.,the use of “stock photos” or public domain imagery can have a negative effect on perceived credibility, while using “real” images can help communicate a sense of authenticity and legitimacy.")

both de jong and Schellens and think aloud protocol advice talk about prioritising deficiencies according to....likelihood, severity...and some other metrics. I did not do this explicitly.