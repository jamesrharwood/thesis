I did not aspire to perfect the web pages, as I do not believe a single perfect design exists. Instead, I viewed the design process as iterative, and the purpose of this study was to make iterations quickly and based on evidence.

My aim, therefore, was to make iterations to the home page and redesigned SRQR guidance based on evidence. To meet this aim, my objectives were to:

* explore the experience of a diverse sample of authors,
* identify and understand limitations deductively,
* and identify possible modifications to address these limitations both inductively and deductively.

### Sampling strategy

My purposive sample of authors varied in their:

* years of academic experience,
* subject area,
* their first language,
* and their country of residence. 

This variation was important because my thematic synthesis (chapter {{< var chapters.synthesis >}}) suggested inexperienced authors may have the most to benefit from reporting guidelines, but also face the most hurdles. Inexperience may be due to early career stage, being new to a field or study design, or new to academic writing. My synthesis also suggested language barriers could hinder adherence, and my service evaluation of EQUATOR's existing website (chapter {{< var chapters.web_audit >}}) revealed a highly international userbase.

Authors were eligible to participate if they were currently engaged in research utilizing qualitative methods, and if they were able to attend an online interview conducted in English. 

I recruited through three channels:

1. I posted on X (called Twitter at the time). 
2. I advertised through my manuscript checker, Penelope.ai. Many medical journals offer the tool to submitting authors. BMJ Open is the largest of these journals, and enjoys a large, international, author base. 
3. I targeted Chinese researchers by writing to a Chinese qualitative researcher. 

All channels invited authors to signal their interest by email. To check applicants' eligibility as qualitative researchers, I asked them to describe their research methods in a few sentences over email. I excluded applicants if their descriptions made no reference of a qualitative method.

I sent all eligible applicants the participant information sheet and consent form. I used JISC Online Surveys to obtain consent and ask the following demographic questions:

* How many years have you done research?
* Please describe your research in a couple of sentences
* What is your first language?
* What country do you work and live in?

I offered participants £50 reimbursement in return for an expected 2 hour commitment. This was a delivered as an Amazon voucher to UK participants, and a bank transfer to international participants. My recruitment advert, information sheet, consent form, and email templates are in Appendix #TODO. 

Time and money limited my target sample size to 10 participants. I maximised information power @malterudSampleSizeQualitative2016 through other means. Firstly, I used methods to elicit ample information from each participant. Secondly, I used my table of intervention components as an analysis framework. Hence I anticipated a sample of 10 to sufficiently inform at least one design iteration. 

### Procedures

I wanted to replicate certain aspects of real life. Firstly, interview sessions took place online using Microsoft Teams. This meant participants could view the website on their own computer, using their normal browser, in their usual place of work. This would allow me to identify problems with slow-loading over bad internet connections and display problems on different screen resolutions, whilst avoiding difficulties of asking a participant to use an unfamiliar computer or browser. 

Secondly, I wanted to replicate the experience of encountering a new website as a naïve user, gradually exploring content, and then reading and applying guidance to one's own writing. I did this by using a variety of methods: 5 second test to capture initial reactions, think aloud to capture exploration, plus-minus task, a writing evaluation, and semi structured interviews throughout. Each method target multiple intervention components (see @tab-methods-for-components). The process was as follows:

{{< insert _process.md >}}

{{< insert _table.qmd >}}

I began sessions by introducing myself as part of team creating a new website. To encourage open and truthful feedback I reassured participants the best way they could help was by being honest, not to worry about critiquing the website or offending me, and to share positive _and_ negative feedback. I asked participants to tell me a little about themselves to relax into the interview and help them feel comfortable talking, before moving on to the first task: the 5 second test. 

#### Five second test

Until this point, participants had no idea what the website was about. My recruitment materials and interview introduction made no mention of writing nor reporting guidelines, and so participants were blind to the website's purpose.

The five second test is an "in the moment" survey method @UXFiveSecondRules2014. By sharing my screen, I showed participants the top of the home page for five seconds before removing it and asking questions. The test limits exposure to five seconds because although a participant can absorb much information (colours, words, shapes), five seconds is rarely sufficient to make sense of everything as a whole. The aim is to capture participant's immediate reactions to salient design elements (like images, large words) before they have a change to consider the content more critically. Furthermore, this five second limit was relevant because my website service evaluation found many authors leave EQUATOR's website within five seconds without interacting with it.

This test was appropriate for the top of the home page because, as per best practice, the area has little text, all relevant content is visible in one frame, and I asked few questions:

1. What do you think the website is about? 
2. How do you think this website may affect your work? 

If participants answered the first question with "reporting guidelines", I asked "what do you think reporting guidelines are?". If participant's answers did not mention writing, I asked what stages of research they might use the website.

I designed these questions to explore three intervention ingredients: describe what reporting guidelines are, how they can be best used, and their benefits. These are the main ingredients featured at the top of the home page. 

#### Semi structured interview 1 - prior experience with reporting guidelines

After the five second test it was no longer necessary to keep participants blinded, and so I asked participants about their prior awareness of, or experience with, reporting guidelines. I asked which guidelines they had used and what they had used them for.

#### Think aloud 1 - home screen

Website designers often ask participant's to "think aloud" as they complete a task or view a website as a way of exploring participants' thought processes (e.g. @szinayInfluencesUptakeHealth2021; @yardleyUnderstandingReactionsInternetdelivered2010). Think aloud as a method was first described by cognitive psychologists Ericsson and Simon @laffalEricssonAndersSimon1985. Their strict approach viewed verbalizations as "indicators of what information was heeded and in what order, a sort of time stamp of the contents of short-term memory" @borenThinkingAloudReconciling2000. As user experience testers adopted the method, they used it more flexibly to additionally capture participants' thoughts, feelings, and expectations @borenThinkingAloudReconciling2000. Whereas cognitive psychologists use the method to understand cognitive processes, usability testers use it to "support the development of usable systems by identifying system deficiencies". Because "building robust models of human cognition is not a central concern", Ericsson and Simon's fixed approach is less appropriate, and testers use a more flexible and pragmatic approach to data collection and interpretation @borenThinkingAloudReconciling2000.

As per best practice @borenThinkingAloudReconciling2000 I began by explaining the task, and giving instruction to continually verbalize a train of thought. I then demonstrated by sharing my screen, opening up a different website, and "thinking aloud" for a minute. Participant's then shared their screen as they explored the home page. Whenever participants stopped talking, I would prompt them to continue by asking "What are you thinking?". I acknowledged participants' verbalizations with non-commital sounds like "uh-hu" and "mmm". These verbalizations and prompts are also considered best practice @borenThinkingAloudReconciling2000.

#### Semi-structured interview 2 - home page

Once participants had explored the entire home page, I asked participants about any intervention components they had not talked about in the think aloud, about their overall opinions, and whether their understanding had changed since first viewing it.

#### Think aloud 2 - SRQR guideline page

I asked participants to find the relevant guideline for reporting qualitative research, and then to continue thinking aloud as they explored it. The top of the SRQR page included information about the guideline, such as its scope, and the number of journals endorsing it. 

### Semi-structured interview 3 - SRQR guideline page

Because the SRQR guidance is very long, I stopped participants from thinking aloud once they reached the guidance itself. I then used semi-structured interview questions to explore any intervention components missed by the think aloud, and to explore participants' expectations of four key features within in guidance: defined words (signified by a dotted underlines), footnotes (signified by a superscript number), links to discussion boards (signified by an icon), and drop down content (signified by a chevron icon). I pointed to an example of each and asked participants what they expected to happen if they clicked on it.

This marked the end of the first interview session. I then explained the plus-minus and writing tasks participants needed to complete before the second session.

#### Plus-Minus task:

In their review of methods to solicit text evaluations from readers, de Jong and Schellens @dejongReaderFocusedTextEvaluation1997 distinguish between evaluation goals: selection (whether readers will engage with the text), comprehension, application (being able to apply information in a real world setting), acceptance (including credibility), appreciation, and relevance and completeness. I was not interested in selection (participants had no option other than to engage with the text), and my study scope did not extend to SRQR's relevence nor completeness. Instead, I was interested in participant's experience of the design decisions I had taken in presenting the SRQR guideline, including the layout, structure, optional content, definitions, and tone of voice.

de Jong and Schellens describe methods to target comprehension, acceptance, appreciation in isolation, but because my interest included all three, I chose a nonspecific method, the Plus-Minus task. In this task, readers are asked to annotate a document with plus and minus signs to signify positive and negative reading experiences and then discuss annotations retrospectively. 

I asked participants to select and annotate 2 or 3 reporting items relevant to whatever they happened to writing-up in the time between interviews. I created duplicates of the SRQR guidance page and gave participants unique URLs so they did not see each other's annotations. I used a web annotation tool called Hypothes.is #REF. Participants could optionally add comments. Participants explained their annotations in the second interview.

As de Jong and Schellens note @dejongReaderFocusedTextEvaluation1997, the plus-minus method is advantageous over other nonspecific methods (reading think aloud #REF, and signalled stopping technique #REF) because it collect data without disturbing participants' natural reading process. Additionally, it was useful in this study as participants could make annotations in their own time, as part of their normal work pattern.

Although the plus-minus task will detect text that participants consider incomprehensible, it cannot detect whether participants comprehend guidance correctly or whether they are able to apply it to their writing. To address this, I used a writing evaluation.

#### Writing Evaluation

In the plus-minus task described above, I asked participants to select a few reporting items relevent to what they happened to be writing at the time. For the writing evaluation, I asked participants to send me the paragraphs they had written before our next interview. I read the excerpts and noted reporting items (and sub items) as present or missing. 

In the second interview, inspired by Davies et al.'s SQUIRE guidelines evaluation @daviesFindingsNovelApproach2016, I asked participants to identify parts of their writing pertaining to reporting items. When I considered an item (or sub item) to be missing, I asked the participant whether they had reported this information. If they felt they had, I asked them to point out where, and then explored any misinterpretations. If they had _not_ reported information, I asked why. 



