---
title: "What influences might researchers encounter when using reporting guidelines? Part 2: Describing the questions asked in quantitative surveys."
format: 
    html:
        output-file: index.html
    docx:
        output-file: JH-chapter-survey-content.docx
---

## Background 

In the previous chapter I describe my systematic search and thematic synthesis of qualitative research exploring authors' experiences of using reporting guidelines. I identified influences that may affect whether an author adheres to reporting guidelines. Many of the studies I included used mixed methods surveys and my search also identified purely quantitative survey studies. Despite not including these quantitative data in my qualitative synthesis, I decided the quantitative _questions_ were important to investigate for three reasons. 

My primary reason was to continue pursuing the objective of my previous chapter: to identify possible influences on whether authors adhere to reporting guidelines. As many authors of these studies were themselves users or developers of reporting guidelines the questions may reflect real influences they have experienced, witnessed, or are trying to avoid or achieve. 

Secondly, I wanted to know how many reporting guidelines had undergone any user testing, qualitative or quantitative. My qualitative synthesis found few reporting guidelines had undergone qualitative evaluation. Quantitative surveys might be easier for guideline developers to run (as most come from quantitative disciplines), and so I wanted to know which guidelines had been evaluated quantitatively and how this compared with those from my previous chapter. 

Thirdly, I expected this analysis would provide additional context for my qualitative synthesis. In mixed-method surveys, quantitative questions may bias responses to subsequent qualitative questions and, therefore, the findings of my thematic synthesis. For instance, qualitative questions like "Anything else?" or "Please elaborate" may lead respondents to neglect or repeat topics covered by the previous quantitative questions. 

My objectives, therefore, were to:

1. Describe the landscape of quantitative survey studies exploring authors' experiences of using reporting guidelines.
2. Identify additional possible influences that were absent from my qualitative evidence synthesis. 
3. Compare themes arising from the quantitative questions with those from my qualitative synthesis. 

## Methods

My qualitative synthesis included a systematic search that sought to capture all survey studies investigating reporting guidelines (see chapter {{< var chapters.synthesis >}} for full search details). I found 22 studies using quantitative survey questions, 14 of which also included one or more qualitative questions (see @tbl-study-list). My search included Chinese databases, and although I did not find any Chinese qualitative studies to include in my previous chapter, I did find two Chinese quantitative surveys [@fangSurveyAwarenessARRIVE2015; @maSurveyBasicMedical2017]. Yuting Duan from the Chinese EQUATOR Centre translated these two studies into English. I imported files into NVivo, including the full surveys where available, labelled all questions with descriptive codes, creating new codes when necessary, and then inductively grouped related codes into broad categories.

{{< include tbl_studies.md >}}

## Results

### What reporting guidelines were studied?

Between the 22 studies 25 reporting guidelines were mentioned, most frequently PRISMA (n=6), STARD (n=6), CONSORT (n=6) and ARRIVE (n=5) (see my [List of Abbreviations](/chapters/abbreviations.qmd) for the full titles of each reporting guideline). Thus, only a small proportion of the reporting guidelines indexed in EQUATOR's database [@EQUATORNetworkEnhancing] have been evaluated with quantitative questions. Fourteen studies focussed on a single guideline (n=14), with others asking questions about multiple guidelines (e.g., "which reporting guidelines [participants] had known"[@girayAssessmentKnowledgeAwareness2020a]) or guidelines in general (e.g., "whether they had used reporting guidelines in their publications"[@karadagoncelKnowledgeAwarenessOptimal2018a]). Most studies included participants from the USA, Europe, and Canada and only a few studies were conducted elsewhere (e.g., China and Turkey) (see @tbl-study-list).

In comparison, my thematic synthesis (chapter {{< var chapters.synthesis >}}) identified 18 studies that collected qualitative data. These studies covered only 12 reporting guidelines and were all conducted in western countries, hence were slightly less diverse than the quantitative survey studies.

### The focus of quantitative questions

Survey studies asked participants:

-   whether they were aware or familiar with certain reporting guidelines,
-   how often they used them and what for,
-   whether reporting guidelines had influenced their behaviour,
-   whether guidance was usable and useful,
-   their opinions on guidance content,
-   their reasons for using a reporting guideline,
-   their opinions on reporting quality in the literature,
-   whether reporting guidelines were easy to find and access,
-   whose role it was to check for compliance,
-   whether the aim of the guidance was clear,
-   opinions on things explicitly named as a barrier including the length of the guidance, the language it is written in, and the time needed to use it, and
-   opinions on things explicitly named as a facilitator or motivator including endorsements, evidence, explanatory information, training, the behaviour of peers, and the development process of the guidance.

### Comparing the focus of quantitative questions with themes derived from qualitative data

The quantitative questions included some novel influences not contained in the qualitative data (shown in bold in @tbl-codes-from-quant), such as training as a possible facilitator [@fullerWhatAffectsAuthors2015], whether authors had heard of the EQUATOR Network [@fullerWhatAffectsAuthors2015; @girayAssessmentKnowledgeAwareness2020a], and whether transparency in guideline development is important [@fullerWhatAffectsAuthors2015]. One study asked whether language may be a barrier to using reporting guidelines for some[@pragerBarriersReportingGuideline2021]. Both quantitative questions and the qualitative data mentioned journals enforcing reporting guidelines, but only quantitative questions asked whether funders and employers should also enforce them.

{{< include tbl_codes_categories.md >}}

{{< include tbl_codes_themes.md >}}

Most ideas captured in the quantitative questions also appeared in the qualitative data. This may indicate that the quantitative questions asked were pertinent, or perhaps that they influenced participants' responses to subsequent, qualitative questions.

Overall, although the quantitative questions contained some novel themes, the qualitative data contained many more ideas that were not addressed by the quantitative questions (see bold items in @tbl-codes-from-qual). These included what authors understand reporting guidelines to be, the pros and cons of itemization, ideas of how guidance could be improved, negative feelings when an item cannot be reported as desired, the pros and cons of including design advice in reporting guidance, whether optional items were understood as being optional, and frustration when the scope of a reporting guideline is too broad, narrow, or unclear.

The qualitative data sometimes provided context to or explanation for quantitative answers (see italicised items in @tbl-codes-from-qual). For example, many of the quantitative surveys asked participants whether they could understand the guidance. However, a quantitative answer to this question does not reveal *what* the participant understands, *how* they understand it, or whether they understand it *as intended*. The qualitative data contained reports of people failing to understand the wording of an item, how to report that item in practice, whether an item applies to them, whether a reporting guideline applies to them, what the intended scope of a reporting guideline is, or even what a reporting guideline is at all. One study found that although authors reported understanding an item, their writing showed that they had interpreted it differently to how the reporting guideline developers had intended[@daviesFindingsNovelApproach2016].

## Discussion 

The aim of this chapter was to reveal which reporting guidelines had been evaluated using quantitative survey questions, what influences those questions explored, and to compare those influences with the results of my previous chapter.

Few reporting guidelines have been evaluated using quantitative or mixed surveys. Given that even fewer have been tested qualitatively, this means that hardly any of the hundreds of reporting guidelines in the EQUATOR Network's database have undergone meaningful user testing. To encourage and support future guideline developers, advice on user testing could be included in an update of guidance for guideline developers, the current version of which contains little advice on how to evaluate reporting guidelines[@moherGuidanceDevelopersHealth2010].

The quantitative survey questions covered few of the themes I identified from qualitative studies. Quantitative surveys often asked about awareness, usage, usability, usefulness, importance, barriers, facilitators, content, and whether reporting guidelines had led to a change in behaviour. However, many of the themes I identified in my qualitative synthesis did not appear in the quantitative surveys. This suggests that the quantitative questions did not bias or limit the qualitative data, and also suggests that quantitative surveys may often miss themes. Often the qualitative data provided context and explanation for the quantitative data. Because quantitative surveys can miss important findings or fail to explain them, developers seeking actionable feedback should collect qualitative data when assessing how researchers understand or feel about reporting guidelines, or what could be done to improve the guidance. 

### Limitations of included studies and advice for future research

<!-- #STRETCH I do not describe limitations of this chapter, just limitations of studies. And not all of the limitations below will have limited my study. E.g., recall bias does not matter because I was looking at the questions, not the responses.  -->

All included survey studies were subject to recall bias as participants were describing past behaviour or opinions. Future studies could consider methods that allow researchers to document experiences in real time, like observation or think aloud tasks.

The included studies' participants lacked diversity. Studies should ensure participants represent expected users in terms of academic writing experience, discipline, profession, experience (or naivety) with reporting guidelines, and language, or even focus on differential experiences of specific target groups. For example, the EQUATOR Network website gets similar levels of traffic from Asia and Europe, yet very little research into usability or barriers of reporting guidelines has included authors from Asian countries (see chapter {{< var chapters.web-audit >}}). The website also sees many new visitors who abandon the site quickly, without accessing any reporting guidance. These visitors may be authors who are naïve to reporting guidelines and decide not to use one. Understanding why people choose not to use a guideline is equally important as understanding the experiences of those that do. Many of the included studies used snowball sampling via social networks connected to the researchers, and hence may have attracted participants who knew about reporting guidelines and already held opinions on them. Many studies required authors to read the guidance as part of the study itself, thereby forcing participants to engage with it. Consequently these studies do not capture perspectives of less-engaged authors, or explain why authors choose not to use a guideline.

Some included surveys contained leading questions. For example, the Likert rated statements "The STARD 2015 guidelines are easy to follow" [@pragerBarriersReportingGuideline2021] and "The time required to adhere to the STARD 2015 guidelines is a barrier to using the guidelines" [@pragerBarriersReportingGuideline2021] are both subject to acquiescence bias; the tendency for participants to agree with research statements [@lavrakasAcquiescenceResponseBias2008]. Future studies should consider using neutral questions.

Studies used lots of different words to describe reporting guidelines, including *guidelines, standards, requirements, checklist, explanation and elaboration,* or just an acronym, e.g., CONSORT. This became a problem in studies where participants were not supplied with guidance documents as part of the study, as it was not always clear which document a participant was considering. For instance, asking participants whether PRISMA is easy to understand will not tell you whether they are talking about the PRISMA checklist, statement, or explanation and elaboration document. Future studies should be specific when asking questions and reporting results.

## Conclusion

Very few reporting guidelines have been evaluated using either quantitative or qualitative methods. Reviewing the content of quantitative surveys revealed few novel influences which were absent from the qualitative data synthesised in chapter {{< var chapters.synthesis >}}. However, the qualitative data contained many more themes than the quantitative questions. This suggests that the results of my qualitative evidence synthesis were not limited by the content of quantitative survey questions within the mixed methods surveys. Because the qualitative data revealed more themes and provided explanation and context to findings, reporting guideline developers who want to make sure their resources are easy to use should consider using qualitative methods, which may produce richer, actionable insights.

Two studies asked participants whether they had heard of the EQUATOR Network, noting that it is a "valuable resource for users and potential users of reporting guidelines" that 44% (19/43) of editors[@fullerWhatAffectsAuthors2015] and 38% of authors (38/100) [@girayAssessmentKnowledgeAwareness2020a] are aware of (published in 2015 and 2020 respectively). Although these studies asked participants whether they were familiar with EQUATOR, authors' experiences of using EQUATOR's website has never been explored. In the next chapter I describe EQUATOR's website and key characteristics of its web traffic, before discussing how well it is helping authors find reporting guidance and what may be limiting its success.

## Reflections on reporting this chapter

This chapter could have been combined with the one before. They came from the same starting idea, and their findings are intertwined. I separated them because I felt like my first chapter was already getting too long, and I wanted to keep its structure simple, familiar, and in line with ENTREQ. Would I have made this decision if ENTREQ did not exist? Possibly not. Separating the chapters draws a distinction between their objectives and methods, but in drawing this line I am diverging from the true origin story of this chapter. The two were not designed as separate studies, so in presenting them as such am I twisting the truth so it falls in line with a reporting guideline? This might not matter for these exploratory studies, but it made me question whether other researchers may feel pressure to package a messy research journey into a neat-and-tidy reporting guideline format. 
