---
title: "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 2: Describing the questions asked in quantitative surveys."
format: 
    html:
        output-file: index.html
    docx:
        output-file: JH-chapter-survey-content.docx
---

::: {.callout}

Known todos:

* Some citations need fixing
* I may add a reflexivity paragraph

:::

## Background 

In the previous chapter I describe my systematic search and thematic synthesis of qualitative research exploring authors' experiences of using reporting guidelines. I identified potential influences that may affect whether an author adheres to reporting guidelines. During my search, I observed that many studies also included quantitative survey questions, and despite not considering them in my qualitative synthesis, I felt that these questions were important to investigate for two reasons. Firstly, as many of the researchers designing these surveys were themselves users or developers of reporting guidelines the questions may reflect real barriers or facilitators that they have experienced, witnessed, or are trying to avoid or achieve. Secondly, in mixed-method surveys, quantitative questions may bias responses to subsequent qualitative questions and, therefore, the findings of my thematic synthesis. For instance, qualitative questions like "Anything else?" or "Please elaborate" may lead respondents to neglect or repeat topics covered by the previous quantitative questions.

In this study, I describe the landscape and content of quantitative surveys that solicit information on author experience of using reporting guidelines. My aim was to identify additional possible influences that were absent from the qualitative evidence synthesis.

## Methods

My qualitative synthesis included a systematic search that sought to capture all survey studies investigating reporting guidelines (see (in press) for full search details). I found 22 studies that included quantitative survey questions, 14 of which also included one or more qualitative questions (see @tbl-study-list). YD translated two studies from Chinese into English[@fangSurveyAwarenessARRIVE2015; @maSurveyBasicMedical2017]. I imported files into NVivo, including the full surveys where available, labelled all questions with descriptive codes, creating new codes when necessary, and then inductively grouped related codes into broad categories (see @tbl-codes-from-quant).

## Results

### What reporting guidelines were studied?

Between the 22 studies 25 reporting guidelines were mentioned, most frequently PRISMA (n=6), STARD (n=6), CONSORT (n=6) and ARRIVE (n=5) (see my [List of Abbreviations](/chapters/abbreviations.qmd) for the full titles of each reporting guideline). Thus, only a small proportion of the reporting guidelines indexed in EQUATOR's database [@EQUATORNetworkEnhancing] have been evaluated with quantitative questions. Fourteen studies focussed on a single guideline (n=14), with others asking questions about multiple guidelines (e.g. "which reporting guidelines [participants] had known"[@girayAssessmentKnowledgeAwareness2020a]) or guidelines in general (e.g., "whether they had used reporting guidelines in their publications"[@karadagoncelKnowledgeAwarenessOptimal2018a]). Most studies included participants from the USA, Europe, and Canada and only a few studies conducted elsewhere (e.g., China and Turkey) (see @tbl-study-list).

In comparison, my thematic synthesis (chapter {{< var chapters.synthesis >}}) identified 18 studies that collected qualitative data. These studies covered only 12 reporting guidelines and were all conducted in western countries, hence were slightly less diverse than the quantitative survey studies.

### The focus of quantitative questions

Survey studies asked participants:

-   whether they were aware or familiar with certain reporting guidelines,
-   how often they used them and what for,
-   whether reporting guidelines had influenced their behaviour,
-   whether guidance was usable and useful,
-   their opinions on guidance content,
-   their reasons for using a reporting guideline,
-   their opinions on reporting quality in the literature,
-   whether reporting guidelines were easy to find and access,
-   whose role it was to check for compliance,
-   whether the aim of the guidance was clear, and
-   opinions on things explicitly named as a barrier including the length of the guidance, the language it is written in, and the time needed to use it.
-   opinions on things explicitly named as a facilitator or motivator including endorsements, evidence, explanatory information, training, the behaviour of peers, and the development process of the guidance.

### Comparing the focus of quantitative questions with themes derived from qualitative data

The quantitative questions included some novel influences not contained in the qualitative data (shown in bold in @tbl-codes-from-quant), such as training as a possible facilitator [@fullerWhatAffectsAuthors2015], whether authors had heard of the EQUATOR Network [@fullerWhatAffectsAuthors2015; @girayAssessmentKnowledgeAwareness2020a], and whether transparency in guideline development is important [@fullerWhatAffectsAuthors2015]. One study asked whether language may be a barrier to using reporting guidelines for some[@pragerBarriersReportingGuideline2021]. This concern may have been missing from the qualitative studies because they were conducted in English. Both quantitative questions and the qualitative data mentioned journals enforcing reporting guidelines, but only quantitative questions asked whether funders and employers should also enforce them.

Most ideas captured in the quantitative questions also appeared in the qualitative data. This may indicate that the quantitative questions asked were pertinent, or perhaps that they influenced participants' responses to subsequent, qualitative questions, as mixed method surveys were included in this commentary and the qualitative synthesis.

Overall, although the qualitative questions contained some novel themes, I found that the qualitative data contained many more ideas that were not addressed by the quantitative questions (see bold items in @tbl-codes-from-qual). These included what authors understand reporting guidelines to be, the pros and cons of itemization, ideas of how guidance could be improved, negative feelings when an item cannot be reported as desired, the pros and cons of including design advice in reporting guidance, whether optional items were understood as being optional, and frustration when the scope of a reporting guideline is too broad, narrow, or unclear.

The qualitative data sometimes provided context to or explanation for quantitative answers (see italicised items in @tbl-codes-from-qual). For example, many of the quantitative surveys asked participants whether they could understand the guidance. However, a quantitative answer to this question does not reveal *what* the participant understands, *how* they understand it, or whether they understand it *as intended*. The qualitative data contained reports of people failing to understand the wording of an item, how to report that item in practice, whether an item applies to them, whether a reporting guideline applies to them, what the intended scope of a reporting guideline is, or even what a reporting guideline is at all. One study found that although authors reported understanding an item, their writing showed that they had interpreted it differently to how the reporting guideline developers had intended[@daviesFindingsNovelApproach2016].

## Advice for future studies

As very few reporting guidelines have undergone any kind of user testing, I urge guideline developers to evaluate their resources to ensure researchers understand their content, aim, and applicability criteria. Advice on how to go about this could be included in an update of guidance for guideline developers, the current version of which contains little advice on how to evaluate reporting guidelines[@moherGuidanceDevelopersHealth2010].

Because quantitative surveys can miss or mask important findings, developers seeking actionable feedback should collect qualitative data when assessing how researchers understand or feel about reporting guidelines, or what could be done to improve the guidance. As survey studies are subject to recall bias when participants are describing past behaviour or opinions, future studies could consider methods that allow researchers to document experiences in real time, like observation or think aloud tasks.

Studies should ensure participants represent expected users in terms of academic writing experience, discipline, profession, experience (or naivety) with reporting guidelines, and language, or even focus on differential experiences of specific target groups. For example, the EQUATOR Network website gets similar levels of traffic from Asia and Europe, yet very little research into usability or barriers of reporting guidelines has included authors from Asian countries (see chapter {{< var chapters.web-audit >}}). The website also sees many new visitors who abandon the site quickly, without accessing any reporting guidance. These visitors may be authors who are na√Øve to reporting guidelines and decide not to use one. Most of the included studies used snowball sampling or required authors to read the guidance as part of the study itself, and so don't capture perspectives of these less-engaged authors.

Surveys should avoid leading questions. For example, the Likert rated statements "The STARD 2015 guidelines are easy to follow" [@pragerBarriersReportingGuideline2021] and "The time required to adhere to the STARD 2015 guidelines is a barrier to using the guidelines" [@pragerBarriersReportingGuideline2021] are both subject to acquiescence bias; the tendency for participants to agree with research statements [@lavrakasAcquiescenceResponseBias2008]. Future studies should consider using neutral questions, such as "Do you think the STARD 2015 guidelines are easy to follow?".

Studies used lots of different words to describe reporting guidelines, including *guidelines, standards, requirements, checklist, example and elaboration,* or just an acronym, e.g., CONSORT. This became a problem in studies where participants were not supplied with guidance documents as part of the study, as it was not always clear which document a participant was considering. For instance, asking participants whether PRISMA is easy to understand will not tell you whether they are talking about the PRISMA checklist, statement, or explanation and elaboration document. Future studies should be specific when asking questions and reporting results.

## Discussion

Very few reporting guidelines have been evaluated using either quantitative or qualitative methods. Reviewing the content of quantitative surveys revealed some novel influences which were absent from the qualitative data synthesised in chapter {{< var chapters.synthesis >}}. Quantitative surveys often asked about awareness, usage, usability, usefulness, importance, barriers, facilitators, content, and whether reporting guidelines had led to a change in behaviour but did not address many other themes identified in my qualitative synthesis. Reporting guideline developers who want to make sure their resources are easy to use should consider using qualitative methods, which may produce richer, actionable insights.

Two studies asked participants whether they had heard of the EQUATOR Network, noting that it is a "a valuable resource for users and potential users of reporting guidelines" that 44% (19/43) of editors[@fullerWhatAffectsAuthors2015] and 38% of authors (38/100) [@girayAssessmentKnowledgeAwareness2020a] are aware of. Although these studies asked participants whether they were familiar with EQUATOR, authors' experiences of using EQUATOR's website has never been explored.

In the next chapter I describe EQUATOR's website and key characteristics of its web traffic, before discussing how well it is helping authors find reporting guidance and what may be limiting its success.

::: landscape

## Tables

{{< include tbl_studies.md >}}

{{< include tbl_codes_categories.md >}}

{{< include tbl_codes_themes.md >}}

:::
<!-- landscape -->
