---
title: "What influences researchers when using reporting guidelines? Part 1: A Thematic Synthesis"
format: 
    html:
        output-file: index.html
    docx:
        output-file: JH-chapter-synthesis.docx
---

## Introduction

In chapter {{< var chapters.introduction >}} I discussed evidence that reporting guidelines have only had a modest impact on reporting quality. Although a few of these studies put forward some possible explanations for their disappointing findings, these were often based on untested assumptions and few included any qualitative exploration of why reporting guidelines were (or were not) working.

I also described how reporting guidelines are disseminated in similar ways; mostly as checklists and an accompanying “Explanation and Elaboration” (E&E) document published in academic journals, often separately. I described how these resources sit within a broader system that includes the guidance itself, tools (e.g., checklists), websites (e.g., journal instructions, the EQUATOR Network website, guideline-specific websites), and the behaviour of others (e.g., editors, peer reviewers, co-authors, colleagues). In this chapter, my objective was to identify influences originating from any part of this system that may affect whether an author adheres to reporting guidelines. I did this by synthesising qualitative studies exploring authors' experiences of reporting guidelines.

I will briefly describe what I mean by _influence_. Health behaviour researchers often talk of “barriers and enablers”, or “barriers and facilitators”. Following this tradition, I also also intended to look for barriers and facilitators when first designing this study. I expected to end up with two tidy lists; a list of things that stopped authors from adhering to guidelines, and a list of things that helped. However, whilst coding I found the data did not fall into two binary categories so neatly. What hindered one person sometimes helped another, or what helped in one scenario did not help in another. I subsequently found that other researchers struggle to fit their data into these binary categories. Haynes and Loblay @haynesRethinkingBarriersEnablers2024 address this struggle and argue against the use of “barriers and facilitators”, which they describe as an “untheorised framing device that often rests on unexamined assumptions yet, because it is familiar, is chosen as a ‘safe’ approach by inexperienced qualitative researchers and those who seek ways of making qualitative research appear more palatable to reviewers”. Haynes and Loblay argue that a barrier and enabler approach is a false friend; “If it can claim any theoretical basis”, they argue, “it would be that the approach is loosely underpinned by a linear-rationalist behavioural paradigm”, but this simplified linearity can lead to superficial findings that do not consider context nor encourage reflexivity. 

After reading Haynes and Loblay I felt justified letting go of my barrier and facilitator plan. My next challenge was one of vocabulary. If I was no longer identifying barriers and facilitators, what noun could I use in their place? Haynes and Loblay didn’t present me with any solutions here. Although they argue against using barriers and enablers, they use the terms exclusively. I tried “factors that influence reporting guideline adherence”, but an editor complained that the word "factors" had a precise and protected definition in epidemiology, and if I were to use it readers would expect a quantitative cohort, cross-section, or case-control study. I expected this review to deepen my understanding of authors' experience of reporting guidelines and the surrounding system, and to reveal parts of that experience that may influence adherence. Ultimately I settled on the word "influence" instead of factors, barriers, or facilitators. The Oxford dictionary defines an influence as something that has "capacity to have an effect on the character, development, or behaviour of someone or something". Like the term "facilitators and enablers", It is untheorized, but unapologetically so. Its definition is sufficiently vague to accommodate context, nuance, and uncertainty, and it fits comfortably within my chapter's research question: what might influence whether authors adhere to reporting guidelines?

## Methods

I performed a thematic synthesis of qualitative research that explored researchers' experiences of reporting guidelines. When writing this chapter, I used the ENTREQ guidelines for reporting qualitative evidence syntheses and the PRISMA-S guidelines for reporting systematic searches [@rethlefsenPRISMASExtensionPRISMA2021; @tongEnhancingTransparencyReporting2012].

### Researcher roles and characteristics

In the previous chapter I introduced my supervisors Gary Collins, Charlotte Albury, Jennifer de Beyer and Michael Schlüssel, and their relationships with reporting guidelines. In this chapter I also worked with Shona Kirtley, a search specialist at the UK EQUATOR Centre and an author of the PRISMA-S guidelines for reporting systematic review literature searches, and I sought help from Yuting Duan and Lingyun Zhao at the Chinese EQUATOR Centre, run by Professor Zhao-Xiang Bian. As with all work in this thesis, I conceived, coordinated, and led all research.

### Approach to searching and data sources 

My search strategy sought all research publications reporting qualitative data exploring researchers' experiences of using reporting guidelines. I wanted to capture the experiences of researchers from around the world, so I included international databases in my information sources. All data sources are listed in @tbl-info-sources.

::: {.landscape .column-page-right .content-visible unless-format="pdf"}

{{< include tbl_info_sources.qmd >}}

:::

::: {.content-visible when-format="pdf"}

\newpage 

\blandscape 

{{< include tbl_info_sources.qmd >}}

\elandscape

:::

### Inclusion and exclusion criteria

I included published research articles reporting researchers' experiences of using reporting guidelines derived through qualitative methods. I excluded articles published before 1996, the year the CONSORT statement was first published. With the assistance of Yuting Duan, Lingyun Zhai, and Michael Schlüssel, I was able to screen studies written in Chinese, Spanish, and Portuguese as well as English, and so I excluded articles written in any other language during screening.

I included articles reporting feedback from researchers as part of guideline development. I decided not to include reporting guideline development studies where this feedback came exclusively from the development group members, as I considered this context to be too different to how ordinary researchers experience reporting guidelines.

Many research articles I found used a mix of quantitative and qualitative survey questions. I did not consider categorical survey questions with a free text option for "other" to be qualitative, but I did include findings from free text questions inviting participants to provide context to a previous (not qualitative) question. I describe the quantitative studies and the questions they asked in my next chapter.

### Electronic search strategy

The UK EQUATOR Centre's search specialist (Shona Kirtley) helped develop comprehensive search strategies, which had a component for reporting guidelines and another for qualitative methods. I constructed my reporting guidelines component from the acronyms of frequently accessed guidelines (@tbl-rgs) with generic terms for reporting guidelines to capture guidelines not named explicitly. My qualitative component came from a review of search filters [@rosumeckValidationStudyRevealed2020], which recommended a sensitive qualitative filter for systematic reviews [@rogersLocatingQualitativeStudies2018]. I extended the filter to include descriptive methods because I knew some of my target records were mixed method surveys. I conducted scoping searches, but my search strategies were not peer reviewed before execution. My search strategies are reported fully in Appendix {{< var appendix.search >}}.

{{< include tbl_rgs.qmd >}}

### Screening

I screened titles and abstracts to identify articles exploring researchers' experiences and then screened full texts to identify whether those articles used qualitative methods. A colleague from the UK EQUATOR Centre (Michael Schlüssel) double-screened a random 10% sample and we resolved differences through discussion. Yuting Duan from the Chinese EQUATOR Centre screened Chinese records in the same way and asked her colleague Lingyun Zhao for second opinions when necessary.

### Describing and appraising records

I extracted study characteristics and used the Critical Appraisal Skills Programme Qualitative (CASP-Qual) checklist [@criticalappraisalskillsprogrammeCASPQualitativeChecklist2018] to critically appraise included studies. I expected this appraisal to help me consider strengths and weaknesses of each study when synthesising them.

### Synthesis methodology

I used thematic synthesis as defined by Thomas and Harden [@thomasMethodsThematicSynthesis2008] because it can handle studies with "thin" descriptions, because it allowed me to infer influences from research that may not have addressed my concern directly, and because I expected its output, grouped by themes, to be useful to guideline developers.

I imported files into NVivo 12.0 for Mac and coded all sentences from the results section and relevant supplementary materials that reported qualitative findings. I assigned each sentence one or more descriptive codes that sought to distil the essence of what was written, creating new codes when necessary and without using a framework. I then used mind-mapping software [@biernerMarkdownPreviewMermaid2022] to visualise similarities and differences between codes and aggregate them inductively into descriptive themes. These themes were descriptive in that they captured the meaning of the codes they contained. I then used my research question to infer influences from these descriptive themes, thereby producing analytic themes. I discussed all steps with Jennifer de Beyer, and we resolved conflicts through discussion.

## Results

### Search

My search yielded 18 articles (see @fig-prisma-flow-diagram for full search results). I and Michael Schlüssel double-screened 10% of the non-Chinese titles and abstracts and agreed on 98.3% of them (170/173). We resolved the remaining three through discussion and consensus. All eligible records were in English; there were no eligible articles written in Chinese, Spanish, or Portuguese. Eligible articles included surveys, semi-structured interviews, focus groups, and writing tasks.

::: {.landscape .column-page-right .content-visible unless-format="pdf"}

![PRISMA flow diagram](prisma-flow-diagram.png){#fig-prisma-flow-diagram}

:::

::: {.content-visible when-format="pdf"}

\blandscape 

![PRISMA flow diagram](prisma-flow-diagram.png){#fig-prisma-flow-diagram width=150%}

\elandscape

:::

Only 7 of the 18 records reported where participants came from. Three mixed method survey studies [@deweyImpactPerceivedValue2019; @macleodMDARMaterialsDesign2021; @sharpOnlineSurveySTROBE2020] included participants from a wide range of countries but it was not possible to tell which participants completed the optional qualitative questions. Three interview studies [@daviesSQUIREGuidelinesEvaluation2015; @fullerWhatAffectsAuthors2015; @korevaarUpdatingStandardsReporting2016] included participants exclusively from North America, Europe, and Australia, whilst the fourth [@sertARRIVEGuidelinesUpdated2020] included a participant from Brazil.

Critical appraisal of the studies using CASP-Qual rated the studies ranging from valuable to not very valuable; the less valuable studies had few qualitative components or minimal reporting of qualitative analysis or findings. Study characteristics are reported in @tbl-study-characteristics.

::: {.landscape .column-page-right .content-visible unless-format="pdf"}

{{< include tbl_study_characteristics.qmd >}}

:::

::: {.content-visible when-format="pdf"}

\blandscape 

{{< include tbl_study_characteristics.qmd >}}

\elandscape

:::

### Synthesis findings

The relationships between my codes, descriptive themes, and analytic themes are reported in @tbl-codes. I identified the following analytic themes: 1) Researchers may not understand guidance as intended or what reporting guidelines are, even if they think they do; 2) Researchers report a variety of reasons for using reporting guidelines, and that some are more important than others; 3) Researchers report using reporting guidelines for different tasks and wanting guidance to be delivered in ways that better fit their needs; 4) Using reporting guidelines has costs which researchers may feel outweigh benefits; 5) Reporting guidelines may need to be revised and updated; 6) Researchers may not be able to report all items, which can leave them feeling uncertain or worried; 7) Awareness and accessibility may limit reporting guideline usage; 8) Reporting guidelines may be more useful to less experienced researchers, but these researchers may find them harder to use; 9) Researchers want or need design advice, but reporting guidelines may not be the right place to find it; 10) Reporting guidelines can be harder to use if their scope is too broad, too narrow, or poorly defined; 11) Researchers may have to use multiple sets of reporting guidelines, multiplying complexity and costs; 12) Researchers may use checklists but never read the full guidance.

As mentioned in the introduction of this chapter, I identified that barriers and facilitators were not consistent experiences. What may be a barrier for one person might be an enabler to others or when occurring in different context, and so I refrained from labelling my analytic themes as one or the other and described them as _influences_ instead. 

{{< include tbl_codes.qmd >}}

#### 1) Researchers may not understand guidance as intended or what reporting guidelines are, even if they think they do

Researchers commonly stated that they need more information to fully understand the intention of the guideline developer. When asked about the clarity of guidance, researchers across many studies reported difficulty in understanding certain terms, concepts, or checklist items:

> 'outcome': "Does it mean the domain, or does it mean the domain measure, metric, method of aggregation, and time?" [@pageUpdatingGuidanceReporting2021].
>
> "Primary and secondary improvement related question is confusing, what does that mean?... I had a hard time with the [difference between the] improvement question and the study question." [@daviesSQUIREGuidelinesEvaluation2015]

A few researchers reported ignoring an item if they could not understand it:

> "Only one item was identified as hard to understand by more than one respondent: 'methods employed to ensure completeness of data', which two participants said they left out because of difficulty in comprehending the item" [@daviesFindingsNovelApproach2016]

Some researchers reported feeling that reporting guidelines were "simply not comprehensible" [@daviesSQUIREGuidelinesEvaluation2015]. Others reported that they had understood, but further investigation revealed that their interpretations could be "different from that intended by the developers" [@daviesFindingsNovelApproach2016]. For example, Davies et al. [@daviesFindingsNovelApproach2016] found that one SQUIRE item "was reported as used fully [in the quantitative survey], but the qualitative analysis revealed that its usage was frequently inconsistent with the intention of the developers". One reason for this may be because different researchers may interpret the guidance in different ways depending on their prior experience, the research context, or if the guidance is ambiguous. For example, the SQUIRE developers found that the word 'theory' "meant different things to different people. For some, the word 'theory' meant 'mechanism by which an intervention was expected to work', for others it meant 'lean or six sigma for example', and for still others it meant 'logic model'" [@daviesSQUIREGuidelinesEvaluation2015].

Even when researchers reported understanding what an item meant, they may not have understood why it is important or who it is important to, leading them to remark that an item "seems unnecessary" [@sertARRIVEGuidelinesUpdated2020]. Few researchers referenced the needs of evidence synthesizers or patients as consumers of research, but more reported considering whether an item would be useful to other researchers, editors, and reviewers:

> "the information provided does not matter as the reviewers do not know what to do with it''[@sharpOnlineSurveySTROBE2020]

In addition to not understanding guidance or who it is important to, many researchers expressed difficulty understanding whether an item was applicable to their work. Some reporting guidelines specify that not all items are compulsory or that some items may only apply to a subset of research articles. Researchers highlighted that this may not be obvious, especially if this nuance is buried in a long elaboration document. Some researchers therefore reported uncertainty over which items applied to them:

> "Authors asked for clarification of which items were always required and which were nonessential" [@pradyAssessingUtilityStandards2007]
>
> "Not always clear what was relevant to their study" [@sertARRIVEGuidelinesUpdated2020]
>
> "He had realised with experience and re-reading the Guidelines that SQUIRE did not require him to include every item in the manuscript".[@daviesSQUIREGuidelinesEvaluation2015]

This uncertainty may extend to the entire reporting guideline if researchers don't know when to use one over another. One researcher declared that "PRISMA guidelines can also be used rather than the MOOSE" [@struthersGoodReportsDevelopingWebsite2021], when the two are primarily for reviews of intervention studies and observational studies respectively. Sometimes there may not be a perfect reporting guideline for a given study, as one researcher commented after using ARRIVE (which focusses on experimental research involving laboratory animals):

> "Our report was an animal based cadaveric study looking at accuracy of drill guides. I were unsure which category it should fall under." [@sertARRIVEGuidelinesUpdated2020]

Even if a researcher understands the guidance, why it is important, and why it applies to them, they may not understand how to report it or "how much detail to report" [@sertARRIVEGuidelinesUpdated2020]. Some researchers "used examples [included in the guidance] to understand what should be reported" because they "demonstrate what is meant in practice" [@sertARRIVEGuidelinesUpdated2020].

At a more fundamental level, researchers varied in their understanding of what reporting guidelines are. Often researchers would talk about reporting guidelines as if they were design guidelines, e.g., describing STROBE as "woefully deﬁcient in encouraging...use of appropriate data analytic approaches" [@sharpOnlineSurveySTROBE2020]. This suggested that the researcher had not noticed the stipulation that "these recommendations are not prescriptions for designing or conducting studies" included in STROBE's explanation and elaboration document [@vonelmStrengtheningReportingObservational2007]. Other researchers wrote about STARD as is if the guidance was to be used when collecting imaging data:

> "Two comments suggested that reporting quality may be impacted by the physical environment in which [...] data are collected. These comments may indicate an incomplete understanding of reporting guidelines which pertain to reporting results during manuscript writing, not the process of imaging acquisition itself." [@pragerBarriersReportingGuideline2021]

#### 2) Researchers report a variety of reasons for using reporting guidelines, and that some are more important than others

Some researchers listed personal benefits to using reporting guidelines. Some described reporting guidelines as a "training tool" [@burfordTestingPRISMAEquity20122013] for personal development, noting that guidance helps "develop a strong foundation and habits" [@sharpOnlineSurveySTROBE2020]. Some talked about how guidance made them feel: "As a junior scientist it gives me conﬁdence to request the reporting of a certain piece of information" [@sharpOnlineSurveySTROBE2020]. Others said that reporting guidelines are "a helpful reminder" [@burfordTestingPRISMAEquity20122013] and that going through the checklist "improved their manuscripts" [@eysenbachg.CONSORTEHEALTHImplementationChecklist2013]. Some saw value in fostering a "transparent reporting process" [@deweyImpactPerceivedValue2019] and for making sure your "project is written up [...] rigorously" [@daviesSQUIREGuidelinesEvaluation2015].

A couple of researchers noted altruistic benefits, reporting that widespread reporting guideline adherence "helps in standardizing how research is reported" [@sharpOnlineSurveySTROBE2020] and calling "for more scientific reports to be published, preferably using a template or guideline to make them comparable" [@svensoyQualitativeStudyResearchers2021].

In the absence of anticipated benefits, some researchers said that they use reporting guidelines simply because "it was what was implicitly expected of them to do" [@fullerWhatAffectsAuthors2015], and that these expectations came from journals and their peers. Some used "tools promoted by journals, which often promised to ease the publishing process" [@svensoyQualitativeStudyResearchers2021] but others wrote that they found this to be an empty promise:

> ''I have never had (nor have I heard of) an editor or reviewer pushing back on a claim that all STROBE criteria were met. Therefore, when a STROBE checklist is required for manuscript submission, it seems to turn into a[n] exercise in additional administrative busywork without really improving the research.''[@sharpOnlineSurveySTROBE2020]

A few researchers reported being more likely to comply with journal requirements if they thought the journal was likely to enforce them: "Does the journal only suggest or actually require submission of a reporting guideline checklist?" [@fullerWhatAffectsAuthors2015]. Some said they were more likely to comply if "it was a high impact factor journal and I thought that I would only get one crack at it" [@fullerWhatAffectsAuthors2015].

A few researchers compared different motivations for using reporting guidance, noting that personal, guaranteed, and immediate benefits were more motivating than hypothetical benefits or benefits to others:

> "I suppose you are looking for short-term gain, short-term benefits as a writer of a report" [@svensoyQualitativeStudyResearchers2021]
>
> "it can be difﬁcult to put the energy into using STROBE (or any other) one a priori since ultimately, it depends on the journal submitted to and accepted to" [@sharpOnlineSurveySTROBE2020]
>
> "All the researchers wanted more homogenous reporting but emphasized that: "As an individual reporter, one is prone to choose the easiest and most accessible one."" [@svensoyQualitativeStudyResearchers2021]

#### 3) Researchers report using reporting guidelines for different tasks and wanting guidance to be delivered in ways that better fit their needs

Although reporting guidelines were designed to help researchers draft and check their manuscripts, many researchers mentioned using reporting guidelines for other tasks including designing research, planning, peer-reviewing, and educating. A few researchers suggested different ways that the guidance could be delivered to make their task easier. For example, some "thought [the] order of items should reflect [the] order considered when designing the experiment" [@sertARRIVEGuidelinesUpdated2020]. Others wanted "a manuscript template" [@deweyImpactPerceivedValue2019] to make writing easier. Some suggested that "online form[s]" [@macleodMDARMaterialsDesign2021] or software to "mark in the text what corresponds to each item in the list" [@struthersGoodReportsDevelopingWebsite2021] would make it easier to complete a reporting checklist as part of journal submission.

#### 4) Using reporting guidelines has costs which researchers may feel outweigh benefits

Researchers noted that some items require extra work, either to collect the necessary information or just to think about and report, and that sometimes this workload felt overly burdensome:

> "If I put the onus on everybody out there who's trying to improve care to deal with that sophisticated question [...], I just think I are putting a barrier in place that is going to be a mountain" [@daviesFindingsNovelApproach2016]

This work requires time, and "the length of time it would take to consider the items" [@burfordTestingPRISMAEquity20122013] was cited by many researchers as a cost, with some asking themselves whether "sufﬁcient time [was] available to comply with [the] reporting guideline" [@fullerWhatAffectsAuthors2015].

Researchers noted that a reporting guideline's "length and content is a key factor inﬂuencing the time needed to complete it." [@sharpOnlineSurveySTROBE2020]. Some found checklists to be "very complete, but to follow every single point is overwhelming" [@eysenbachg.CONSORTEHEALTHImplementationChecklist2013]. As a solution, many wanted to "simplify" or "shorten the checklists" [@struthersGoodReportsDevelopingWebsite2021]. A few researchers wanted a "hierarchy" to know which items were most "important to include" [@daviesFindingsNovelApproach2016]. Another suggested that checklists presented as online forms could include "logic for irrelevant [items]" [@macleodMDARMaterialsDesign2021] so that the users are presented with only items that apply to them.

Complexity was sometimes mentioned alongside time: "As the research often was performed out of work hours, the required time and complexity of the guidelines or templates may have played a crucial role [in deciding whether to use a reporting guideline]" [@svensoyQualitativeStudyResearchers2021]. Researchers had conflicting opinions about whether itemization reduces perceived complexity. Proponents noted that the "Checklist is a very helpful summary of sometimes confusing guidelines" [@deweyImpactPerceivedValue2019] and that itemization made guidance "easier to follow" and "more approachable" [@pageUpdatingGuidanceReporting2021]. But a few said that presenting guidance in small pieces made it difficult to "get the whole picture of what you are supposed to be doing" [@daviesSQUIREGuidelinesEvaluation2015] and that itemization makes "the checklist appear more daunting for users" because it adds vertical length. Consequently, "If you make the checklist too long people will see it as too complicated and then won't use it" [@pageUpdatingGuidanceReporting2021].

Another concern cited by many researchers was that following a reporting guideline can result in long reports:

> "I use SQUIRE a lot for planning---I complete the sections up through the methods at the time I design the study...[but] SQUIRE creates sort of long reports if followed exactly." [@daviesSQUIREGuidelinesEvaluation2015]
>
> "the document you create if you use SQUIRE exactly as written is unintelligible" [@daviesSQUIREGuidelinesEvaluation2015]
>
> "this [item] would require another paper" [@eysenbachg.CONSORTEHEALTHImplementationChecklist2013]

This problem was exacerbated by journal word limits:

> "I believe it is a useful instrument but it is unrealistic to assume that every single suggestion can be detailed in a 6000-words manuscript." [@eysenbachg.CONSORTEHEALTHImplementationChecklist2013]
>
> "two remarked that word limitations has necessitated removal of many items" [@pradyAssessingUtilityStandards2007]

Although a handful of researchers noted that "the relaxation of word limits" [@fullerWhatAffectsAuthors2015] would help, many researchers objected to long articles because they were bloated, harder to read, or simply "unintelligible" [@daviesSQUIREGuidelinesEvaluation2015] and requested strategies to "enhance readability" regardless of journal policies. Some wondered where they could place this information besides the article body, such as "in an appendix", an "online supplement or repository", or a figure [@pageUpdatingGuidanceReporting2021]. Some researchers preferred to report information in the checklist instead of the article body because of "space restrictions, because [it was] a minor component of the study, because they considered the information to be obvious, or because they were unsure of how to incorporate it in the manuscript." [@sertARRIVEGuidelinesUpdated2020]. Some used this strategy to report items that had "not been used or observed during the study, for example that no inclusion or exclusion criteria had been set, no data had been excluded, randomisation and blinding had not been used..." [@sertARRIVEGuidelinesUpdated2020] although it was not clear whether this was motivated by a desire for a concise article or a concern about highlighting potential weaknesses.

Faced with the costs of time, work and article length, some researchers explicitly weighed perceived benefits against costs and disagreed about the balance:

> "The manuscript has improved. However, I felt that the amount of effort was considerably greater than the degree of improvement." [@eysenbachg.CONSORTEHEALTHImplementationChecklist2013]
>
> "it also adds to the time required to put together a manuscript, and I am not sure how much it improves the chances of a manuscript being published" [@sharpOnlineSurveySTROBE2020]
>
> "it does increase the quality of the articles, it is clearly worth the time" [@sharpOnlineSurveySTROBE2020]

The balance of costs versus benefits may be most favourable when guidance is used early in the research workflow. Researchers who used reporting guidelines earlier in their workflow (e.g., for planning research or drafting) used language that implied it was something they did regularly (e.g., "I use SQUIRE a lot for planning" [@daviesSQUIREGuidelinesEvaluation2015]). Some reported that they had come to this habit by their own initiative and that reporting guideline developers should "encourage people to use the criteria early in the writing process (I have, which probably is why I only changed one thing [at the point of submission])" [@struthersGoodReportsDevelopingWebsite2021]. One researcher suggested that "policy that focuses on a front end approach would be helpful" [@sharpOnlineSurveySTROBE2020], noting that "To fully apply the criteria, I would need to systematically apply the STROBE criteria on the front end design of a project, grant, etc. rather than at the time of writing a project" [@sharpOnlineSurveySTROBE2020].

Conversely, many authors who completed a checklist during manuscript submission, very late in their in workflow, emphasised the costs, using words like "arduous" [@sharpOnlineSurveySTROBE2020] and expressing negative opinions of this process (see *Researchers may use the checklist but never read the full guidance*). This may be because researchers lack the motivation, time, or ability to edit their manuscripts at this point.

#### 5) Reporting guidelines may need to be revised and updated for different reasons

Researchers in most studies had opinions on how guidance could be improved through clarifying, reorganising, splitting, merging, adding, or deleting items, and sometimes these views fed into the revision of reporting guidelines [@daviesFindingsNovelApproach2016; @pageUpdatingGuidanceReporting2021]. This feedback may be useful for reporting guideline developers. Even if a reporting guideline was considered perfect at one point in time, researchers noted that guidance must be kept up to date in response to changes in the field and broader scientific ecosystem:

> "The evolution of the healthcare improvement scholarly literature in the intervening years since the publication of the SQUIRE Guidelines has led to the development of concepts that were not fully anticipated at the time of initial release" [@daviesSQUIREGuidelinesEvaluation2015].

Updates to one reporting guideline may necessitate the update of another. For instance, as PRISMA was being updated, a few researchers "supported referring to PRISMA for Abstracts, but suggested it also needs updating" to reflect updates being made to PRISMA [@pageUpdatingGuidanceReporting2021].

#### 6) Researchers may not be able to report all items, which can leave them feeling uncertain or worried

Some researchers described being unable to report items because of external factors, including intellectual property or data rules, disagreement between co-authors, or because "peer reviewers or editors had suggested editing out much of their [reporting guideline]-specific text" [@pradyAssessingUtilityStandards2007]. Others reported feeling unable to report an item because they did not do it, whether on purpose, due to an oversight, or because requirements had changed since the study began:

> "[This item was] not part of the study objectives" [@eysenbachg.CONSORTEHEALTHImplementationChecklist2013]
>
> "This [item] is a good idea, but I did not do this." [@eysenbachg.CONSORTEHEALTHImplementationChecklist2013]
>
> "The RCT was initiated before trial registration became customary in Norway, and therefore does not have a Trial ID number." [@eysenbachg.CONSORTEHEALTHImplementationChecklist2013]

This left some researchers fearing that "an ''incomplete'' checklist [gave] the impression that their study is ''less than 'perfect'.''[@sharpOnlineSurveySTROBE2020]. Some expressed concern that strict wording that assumed something was done may "force people to lie/mislead by asking a question they cannot answer" [@pageUpdatingGuidanceReporting2021] and suggested that guidance should instead use more agnostic language and specify what to do if an item were not addressed, such as "If no publicly accessible protocol is available, please state this" [@pageUpdatingGuidanceReporting2021].

#### 7) Awareness and accessibility may limit reporting guideline usage

Researchers may not know what guidance exists and may be more likely to use whatever is most accessible and discoverable:

> "Several of the researchers did not have extensive knowledge about the different reporting tools, so the accessibility of the guideline or template was often a decisive factor." [@svensoyQualitativeStudyResearchers2021]

One researcher wrote that "poor dissemination strategy by authors of reporting guidelines had inhibited uptake" [@fullerWhatAffectsAuthors2015], and others recognised that reporting guidelines could be "better highlighted" [@deweyImpactPerceivedValue2019] by journals or advertised on "social media platforms" [@struthersGoodReportsDevelopingWebsite2021].

#### 8) Reporting guidelines may be more useful to less experienced researchers, but these researchers may find them harder to use

Some researchers reported that they didn't need the guidance as they were experienced enough to know what they were doing:

> "One of the most prevalent themes was the expression of self-assuredness. ''[I] follow the STROBE guidelines in my reporting reasonably well without actually referring to them or using a checklist'' (group 3, ID1) and ''[I] already apply the STROBE recommendations despite not having heard of it until today''" [@sharpOnlineSurveySTROBE2020]

Sometimes this was accompanied by an acknowledgement that reporting guidance may be more beneficial to less experienced researchers:

> "Despite experienced researchers generally not seeing a beneﬁt to personally using STROBE, there were strong feelings that it is valuable to early-career researchers" [@sharpOnlineSurveySTROBE2020]
>
> "Helpful at beginning of career, but not at later stage" [@deweyImpactPerceivedValue2019]
>
> "this exercise might be good for college students but is insulting for professionals" [@eysenbachg.CONSORTEHEALTHImplementationChecklist2013]

However, less experienced researchers often reported finding "reporting guidelines being difficult to use initially" [@fullerWhatAffectsAuthors2015], or that reporting guidelines became easier with experience in medical writing in general, and with experience in using other reporting guidelines. For instance, "Participants with less experience in scholarly medical writing found the SQUIRE Guidelines harder" [@daviesSQUIREGuidelinesEvaluation2015].

#### 9) Researchers want or need design advice, but reporting guidelines may not be the right place

Many researchers reported wanting advice on design choices but disagreed on where that design guidance should go. Some researchers suggested referring researchers to other design resources through hyperlinks or citations. Others explicitly wanted design guidance to be written into reporting guidelines so that others would read it. Some went as far as calling for reporting guidelines to express an opinion and encourage one technique over another. One researcher objected to a "neutral tone" [@pageUpdatingGuidanceReporting2021] in a reporting guideline that may give the impression that a design choice (that they disapproved of) was reasonable practice.

However, other researchers objected to reporting guidelines that were opinionated about design choices. One user described STROBE as a "procedural straightjacket" [@sharpOnlineSurveySTROBE2020], suggesting that it dictates how studies should be conducted. Users who encounter the guidance late in writing may be unable to act on any design recommendations and consequently may feel fearful of reporting transparently if their design choices deviate from what the guideline recommends as best practice (see *Researchers may not be able to report all items, which can leave them feeling uncertain or worried*).

Perhaps with these concerns in mind, one wrote about the "need to make sure that the language around this elaboration gives [researchers] some flexibility" [@pageUpdatingGuidanceReporting2021], with another noting that "I am OK with the idea of emphasizing the value of [this design choice], but I cannot mandate it" [@pageUpdatingGuidanceReporting2021].

#### 10) Reporting guidelines can be harder to use if their scope is too broad, too narrow, or poorly defined

Reporting guideline developers may narrow the scope of their guidance by limiting it to certain design choices or research contexts. This frustrated some researchers, who noted that narrow "checklists cannot fit all types of research" [@deweyImpactPerceivedValue2019] and "cautioned that ''that balance between freedom and structure is important to consider'' [...] and that it is ''important to recognise that each study/analysis is unique and doesn't always ﬁt with the recommendations''.[@sharpOnlineSurveySTROBE2020]

This narrowing of scope may have been a conscious decision, as requested by this researcher giving feedback on a proposed update of the PRISMA guideline: "I need an agreement on whether PRISMA is to be only for intervention studies (as implied by the proposed modification above) or more general." [@pageUpdatingGuidanceReporting2021] However, scope may not always be clearly communicated: another PRISMA user opined that "the assessment of risk of bias, statement of risk ratio and explaining additional analyses depend on the study design ... [For] a systematic review of cross-sectional surveys or a meta-synthesis I do not need this information" [@tamPerceptionPreferredReporting2019], suggesting they were unaware of PRISMA's focus on interventional studies or that MOOSE and ENTREQ would be more appropriate for these kinds of studies (see previous themes for further discussion of awareness and understanding the applicability of reporting guidelines).

Researchers noted that scope could be made broader by removing items or, more commonly, by extending items with more options and examples:

> "omit "(benefits or harms)" from the checklist item to be more inclusive of reviews that do not examine effects of interventions" [@pageUpdatingGuidanceReporting2021]
>
> "If the new PRISMA will more explicitly embrace topics other than interventions (which I think it should), then some additional examples could be added to the parenthesis (e.g., sensitivity and specificity, disease prevalence, regression coefficient)" [@pageUpdatingGuidanceReporting2021]

However, extending guidance with options can make the guidance appear longer and means researchers must work out which parts apply to them.

#### 11) Researchers may have to use multiple sets of reporting guidelines, multiplying complexity and costs

There are now over 500 reporting guidelines indexed on the EQUATOR Network website, with more added each year as reporting guideline developers seek to cover more and more use cases. Researchers may be expected to use a second or third reporting guideline in addition to the original one. Reporting guidelines that are intended to be used in addition to another are called extensions. Some researchers "pointed out that these extensions have created needless complexity and additional confusion in reporting of observational studies [...] and that the number of extensions has become excessive, especially given that multiple extensions may apply to a single study" [@sharpOnlineSurveySTROBE2020].

One researcher wrote: "it would be good to have better connection between different checklists (perhaps using digital linking, decision-trees, etc.)" [@pageUpdatingGuidanceReporting2021]. Some showed concern that hyperlinks to extensions will go unused and so developers should "incorporate all relevant details in the [...] checklist and elaboration (in case authors don't read the extension)" [@pageUpdatingGuidanceReporting2021]. When writing about PRISMA, one researcher noted that "it would be wise to limit the number of additional documents to look up. This is only item 7, and I have already been referred to PRISMA for Abstracts and PRISMA for Searches. As a systematic review author, reviewer, or editor, I would be unlikely to go to several sources for reporting guidance" [@pageUpdatingGuidanceReporting2021].

A few researchers wrote that related reporting guidelines should be mutually updated to keep in sync with each other before linking or embedding them. Researchers wanted the instruction, terminology, and structure of different sets of reporting guidelines to be coherent, suggesting, for example, that the updated PRISMA should be structured to be "in line with PRISMA-P" [@pageUpdatingGuidanceReporting2021].

In addition to reconciling multiple reporting guidelines, researchers must also comply with journal, funder, and other scientific guidelines and expressed frustration when instructions contradicted each other. For example, some reporting guidelines specify subheadings for abstracts and one researcher pointed out that a "major issue is that journals wildly differ in requirements/what is allowed in abstracts" [@pageUpdatingGuidanceReporting2021].

#### 12) Researchers may use checklists but never read the full guidance

Reporting guidelines typically consist of the guidance itself and a checklist that serves as a summary of the guidance and a tool to demonstrate compliance. Sometimes the document containing the full guidance is called the Explanation and Elaboration (or E&E for short). When talking about a reporting guideline, it was often unclear whether the researcher was talking about the checklist or the E&E.

Some researchers implied that their only experience with reporting guidelines was completing a checklist as part of submission. I noticed that many negative statements were directed specifically at this process, describing checklists as "painful" [@deweyImpactPerceivedValue2019], "pedantic", "annoying" [@sharpOnlineSurveySTROBE2020], or a "stupid exercise" [@eysenbachg.CONSORTEHEALTHImplementationChecklist2013].

One study explored researchers' use of checklists and E&E documents, noting that "Participants used the guidelines and the E&E in different ways. Some did not read the E&E and used only the checklist, others read the E&E first and then used the checklist and a further group used the checklist and referred to the E&E for help with specific items." [@sertARRIVEGuidelinesUpdated2020]. One researcher even went as far as to say that the "E&E appeared to be redundant" [@sertARRIVEGuidelinesUpdated2020].

If some researchers only use checklists, which typically lack any nuance included in the E&E, this may explain why some described reporting guidance as inflexible and prescriptive, warning that "Blind checklists are not relevant to most work" [@struthersGoodReportsDevelopingWebsite2021] or that "Authors may fear the 'Checklist Manifesto' becoming a rigid bureaucracy, and also becoming contrived" [@sharpOnlineSurveySTROBE2020].

## Discussion

Researchers face many challenges when trying to use reporting guidelines and have many questions, opinions, and suggestions that could be useful for reporting guideline developers. Researchers also report personal benefits, especially personal benefits when using a reporting guideline early. These benefits are at odds with how reporting guidelines are typically disseminated (as pre-submission checklists) and presented (as a benefit to others). The polarity and severity of these influences differ according to context (e.g., when or how a reporting guideline used), and personal characteristics (e.g., experience in academic writing). These findings could help increase the impact of reporting guidelines if taken into consideration during their development, dissemination, and implementation phases.

<!-- #STRETCH There are studies looking at facilitators & barriers to clinical practice guidelines. They carve up influences in different ways. E.g., https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7765264/ separates clinician-related factors (e.g., lack of awareness) and external ones (e.g., patients or environments). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10246545/ separates individual factors from guideline factors. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5024051/ reports facilitators and barriers at the level of the innovation (e.g., the recommendations), whereas for the guideline as a whole, multiple levels (socio‐political, organization, user, and facilities). Should I divide my results up like this too? -->


The reporting guideline development community has typically relied on journals to promote their resources and have called on editors to better enforce reporting guideline adherence [@moherDescribingReportingGuidelines2011a]. However, the results presented here suggest that focusing solely on enforcement may be short sighted and that guideline developers have the power to address many influences, at least in part. Doing so may in turn make it easier for journals and funders to enforce reporting guidelines. For example, it is difficult to enforce a reporting guideline that is complicated to understand or if the guideline's applicability criteria are unclear.

Qualitative methods uncovered issues that may be masked by quantitative surveys. For example, Davies et al. [@daviesFindingsNovelApproach2016] found that one SQUIRE item "was reported as used fully [in the quantitative survey], but the qualitative analysis revealed that its usage was frequently inconsistent with the intention of the developers". Most studies used mixed method surveys and often the qualitative component was small, perhaps limited to a single question like "Please add your comments and suggestions in the free text below" [@deweyImpactPerceivedValue2019] or "Any other feedback?" [@macleodMDARMaterialsDesign2021], and consequently resulted in thin data. Future studies seeking richer qualitative data should consider using interviews and focus groups above surveys.

Despite there being hundreds of reporting guidelines, my search found only 18 studies that collected qualitative data, covering 12 reporting guidelines in total, and only six of the 15 reporting guidelines listed on the EQUATOR Network's homepage. Of notable absence were the COREQ and SRQR guidelines for qualitative research. Given that qualitative research differs from quantitative research in terms of its ontologies, epistemologies, and how it considers replicability and best practice, it would be interesting to know whether qualitative researchers face additional influences not covered here.

Many of the themes centred on understanding and usability. For example, not understanding what reporting guidelines are, their scope, their content, or how to use them. Reporting guideline developers may lack the funds, time, motivation, or expertise to design user-friendly resources or effectively evaluate them. The EQUATOR Network guidance for reporting guideline developers [@moherGuidanceDevelopersHealth2010], published in 2010, covers steps from inception to dissemination but largely neglects user experience or user testing. Only two short sections, totalling eight sentences of an eight-page document, address the importance of gathering user feedback, but the guidance offers no instruction on how to do this. Nor does the guidance advise on usability best-practices. Reporting guideline developers may benefit from advice on how to evaluate their resources and from infrastructure to collect feedback from researchers, such as a commenting system on the EQUATOR Network website.

### Limitations

I was limited by the availability of literature and the relative thinness of some studies' qualitative analysis. Most studies relied on participants recalling what they had done or thought in the past, and so may be subject to recall bias. Future studies could consider using 'in the moment' methods like think aloud tasks.

Surveys could be subject to question order bias. Often the qualitative question appeared at the end of a survey, and so proceeding quantitative questions could have influenced participants' responses. In the next chapter I describe how most concepts explored by the quantitative survey questions also appeared in the qualitative data and some of these quantitative questions were leadingly phrased. However, the qualitative data contained many additional themes that did not appear in the quantitative questions and so I believe the results in this chapter are not merely the results of question order bias.

I tried to capture the experience of a diverse range of researchers, but most participants of the reviewed studies were from western countries. My Chinese database searches yielded no relevant studies. Likewise, I found no studies on this topic published in Spanish or Portuguese. Around a quarter of visitors to the EQUATOR Network's website have their browser set to a language other than English (I talk about this more in chapter {{< var chapters.web-audit >}}), and non native-English speaking researchers may well face additional challenges not covered here. The studies synthesised in this chapter were all conducted in English. This may explain why language barriers did not appear as a theme, despite being identified as a potential issue in quantitative surveys [@pragerBarriersReportingGuideline2021] as discussed in the next chapter.

I did not distinguish between different guidelines and expect that the themes I found may apply to reporting guidelines to different degrees. I also expected code frequency to be biased by the questions asked in each study. I therefore decided not to prioritise themes by importance or frequency.

I considered including grey literature, commentaries, and opinion pieces. These may have contributed themes to my analysis but finding these pieces (many of which may have been on private blog posts not indexed by search tools), and the extra work of synthesising primary and secondary order constructs was not feasible. I also considered synthesising quantitative survey data. This data was mostly categorical or ordinal. I decided against it because the surveys differed in content and measure and because numbers would not give me reasons. Instead, in the next chapter I describe how I looked at the questions themselves and compared them with the themes identified in this chapter.

### Conclusions

Researchers encounter many influences when using reporting guidelines. Overall, I found few reporting guidelines have been evaluated qualitatively and the few that have may suffer from thin description, recall bias, question order bias, and lack diverse sampling. In chapter {{< var chapters.pilot >}} I describe how I avoided these limitations in my own qualitative study where I used interviews, observation, think-aloud tasks, and writing tasks to explore a diverse group of authors' experiences of a redesigned reporting guideline. Reporting guideline developers should be encouraged, supported, and funded to evaluate their resources using in-depth qualitative methods like these. Most of the studies included in this review were surveys, and during my search I found other (quantitative) surveys. In the next chapter, I explain why and how I explored the questions contained in these surveys.

### Reflections on this chapter

As with all qualitative research it is important for me to reflect on how my stance may influence my data collection and interpretation. Because I'm studying reporting guidelines I felt it was particularly important to keep a record of my own experience using them for the first time. This was the first chapter I wrote. Before Charlotte Albury joined my supervisory team none of us had ever done a qualitative synthesis before. We did not even realise our planned study _was_ a qualitative evidence synthesis and consequently did not know which reporting guideline I should use to write my protocol. One supervisor suggested the Non-Intervention, Reproducible, and Open Systematic Reviews framework @toporIntegrativeFrameworkPlanning2023 before I found the Journal Article Reporting Standards for Qualitative meta-analyses (JARS-Qual)@bantrywhiteJournalArticleReporting2019. Developed by the American Psychological Association, it felt like a good fit as my phenomena (experiences of using a reporting guideline) fell within the realm of social sciences. However, when I tried to publish my protocol the editor (who came from the medical meta-research world) insisted I use ENTREQ @tongEnhancingTransparencyReporting2012 which was developed for reporting syntheses of _health_ research, even though my phenomena was not health-related. Although I disliked being strong-armed to use a reporting guideline I felt was less relevant to my work, there was sufficient overlap to switch without much complaint. Neither ENTREQ nor JARS-Qual have ready-to-use checklists, so when I wanted to complete a reporting checklist I had to make my own. I found parts of ENTREQ confusing. The article states "the ENTREQ statement consists of 21 items grouped into five main domains" and continues to discuss each domain. But the checklist (presented as a table) does not mention domains. This makes it difficult to work out where each item is discussed in the guidance text. If I want to read more about item 15 ("software") should I look under Methods and Methodology, Literature Search and Selection, Appraisal, or Synthesis of Findings? This was my first attempt at using a reporting guideline and already I was encountering issues with selecting the right one, understanding it, and applying it to my writing. Some of the influences my synthesis had identified were beginning to ring true, and going forward I attempted to avoid giving precedence to barriers I experienced myself.