# Where do I sit?

Many of the problems being tackled by open science enthusiasts are, at their core, behaviour change projects, but they are not treated as such. When attending conferences I have found myself wondering how behaviour change could be applied to other projects. Often the intended behaviour might be implicit, or poorly defined. Projects may jump to a solution without considering all options or how the solution is intended to work. Often these solutions are whatever is most familiar (e.g., education or training course), most rewarding to the project team (a publication), determined by funding (e.g. it might be easier to get funding to develop a _thing_ rather than to run a service). 

Some projects focussed on a particular tool rather than behaviour (#REF octopus, RCT report card). This might be because tools are easier to fund. Some researchers were fixated on one particular intervention type - often regulation. 

I was excited to attend the Wellcome Trust, 2022 conference Reproducibility, Replicability and Trust in Science. Its theme was  "to position the challenge of reproducibility and replicability as a behaviour-change problem". Yet of the scores of presentations and posters, I was the only researcher using behaviour change theory. This leaves me feeling that although behaviour change theory is well established in health research, applying it to behaviour change in research systems is not commonplace. 

We now have the _UK Committee on Research Integrity_, the _UK Reproducibility Network_ and, most recently, the _University of Oxford's Research Practice Group_. But _research integrity_, _reproducbility_ and _practice_ are all different things. Some would class them all under _meta research_, which John Ioannidis defines as (https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2005468) "the study of research itself: its methods, reporting, reproducibility, evaluation, and incentives" and states that it "uses an interdisciplinary approach to study, promote, and defend robust science". Ioannidis describes how meta research offers "metaviews of science", perhaps implicitly acknowledging that most meta research is retrospective and observational (in line with the original greek meaning of 'behind', as well as the modern definition of 'self-referential'). The plain language alternative _Research on Research_ (as in the _Research on Research Institute_) feels similarly passive.

When health researchers were looking to describe "the systematic use of methods and tools to try to continuously improve quality of care and outcomes for patients"(https://www.kingsfund.org.uk/publications/making-case-quality-improvement#:~:text=The%20term%20'quality%20improvement'%20refers,Healthcare%20Improvement's%20Model%20for%20Improvement.) they coined the term _quality improvement_. Quality improvement's approach includes "using data to understand variation" which may be akin to most meta-research, but it also includes 4 other principals which, in my experience, typically don't feature in meta-research discussions:

* training staff in the nature of systems
* giving all staff the opportunity to contribute and act on ideas for improvement
* using many small-scale trials and tests as a way to learn and improve
* ensuring a continuous focus on the needs and experiences of the people served by the system

Perhaps there is space for _quality improvement in research_, or _scholarly improvement_. Beyond medicine, maybe we could learn from business studies where, as far back as 1969, _organisation development_ was defined as "an effort (1) planned, (2) organization-wide, and (3) managed from the top, to (4) increase organization effectiveness and health through (5) planned interventions in the organization’s “processes,” using behavioral-science knowledge"(Richard Beckhard (1969), from https://www.sagepub.com/sites/default/files/upm-binaries/41238_1.pdf). 

I wouldn't usually get hung up on semantics, but having a _name_ feels like an important first step to becoming a recognised field and not just a response to transient "crisis" (cite reproducibility crisis). Research systems have evolved over centuries and will continue to do so, as will our definitions of "best practice" and "wasteful research". Because it's always changing, the system will never be "fixed" - we will never be able to dust our hands and be done with it. As QI and Organizational Development seek to iteratively optimize changing health care and business systems, our research system should have continual self-improvement "baked in". Does _meta research_ deliver this? Perhaps it could, if Ioannidis' use of the word "promote" translated to instigating change and not just advocacy. Indeed, many people (including my supervisors) might describe my work as meta-research. But to me that description feels passive and doesn't reflect my aim and approach to instigate change, just as I expect a quality improvement researcher may not identify with a systematic reviewer. Perhaps if there was a field called _Scholarly Development_ or _Research Improvement_, I would put myself in that box.

I hope that this field, whatever it is named, begets dedicated departments and job titles. These require funding, and the _types_ of funding for research improvement may need to be different. As an example, EQUATOR has become an important and influential part of the publishing system. Their website is a service used by millions of researchers and, like any service, it requires maintenance and incurs running costs. Because traditional academic funding is project based, EQUATOR have struggled to finance web development, the site is becoming increasingly dated in appearance and infrastructure. The future research-improvers will need longer-term funding to develop, sustain, and refine the services and resources that comprise interventions. 
