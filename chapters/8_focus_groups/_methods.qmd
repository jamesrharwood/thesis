## Methods

The purpose of this study was to elicit ideas from experts familiar with reporting guidelines, on how to address influences that may affect adherence. My methods had two parts: 1) brainstorming ideas with EQUATOR during workshops and 2) extending this list through focus-groups with guideline developers, publishers, and guideline advocates. I then describe the combined results. 

### Workshopping Ideas with EQUATOR

I described the workshops I ran with EQUATOR in chapter {{< var chapters.workshops >}}, including the techniques I used to encourage rich discussion and to navigate my position as a participating researcher, contributing ideas myself whilst facilitating others to share their voices. In step 7 of those workshops, I asked participants to consider each intervention function in turn and suggest ideas employing it. After the workshop, I then labelled each idea with the influence(s) it would be addressing to create an "ideas table" with two columns. 

The left hand column listed influences. These came from the consolidated list of _what needs to change for our target behaviour to occur_ that came out of step four of the Behaviour Change Wheel workshops in the previous chapter. The influences were initially worded as statements (e.g. "Researchers may not know when reporting guidelines should be used"). To make discussions fluid, I rephrased each to a question (e.g. "How can we ensure researchers know when reporting guidelines should be used?"), and included context and elaboration. 

The right hand column listed the ideas of how each influence could be addressed, coming from step 7 of the workshops. The table had {{< var counts.barriers >}} rows, one for each influence, and I gave each row a full page so there was a lot of white space to add ideas to, and so that rows could be presented individually on a computer screen (see @fig-ideas-document). I invited workshop participants to review and edit this table, thereby co-producing a list of influences and linked ideas. 

### Focus groups with external stakeholders

Focus groups are researcher-facilitated group discussions that use conversation as a form of data collection @givenFocusGroups2008. A key element of focus groups is interactions between participants as they agree, disagree, challenge, and 'feed off' of each other @givenFocusGroups2008. I chose focus groups because I expected this interaction to lead to more ideas being generated than if I interviewed participants in isolation. Focus groups are also a practical way to collect data from larger groups of people. This is in contrast to in-depth interviews which are more useful in eliciting rich detail about individuals' perspectives @givenFocusGroups2008.

Before describing my focus group methods in detail, I will briefly outline two key differences to the workshops. The workshops described above were part of the sessions described in the previous chapter, where I explained how I actively contributed because I felt my experience would be useful. However, by the time the focus groups with external stakeholders began I felt I had contributed everything I had to give, and my priority for this study was for stakeholders to shape and build upon what EQUATOR and I had done. Therefore, whereas I actively contributed during the EQUATOR workshops, I tried to remain a passive facilitator during the focus groups. This was the first key difference. The second was that whereas in the workshops, EQUATOR staff and I spent many hours, across multiple sessions, working through influences one at a time, this was not feasible in the focus groups because it would have required multiple sessions and participants had limited time. Instead, focus group participants selected influences to discuss.

### Sampling

To seek variation and ideas from a broad range of stakeholders, I invited a purposive sample including the developers of popular reporting guidelines, publishing professionals, and academics that have studied reporting guidelines. My invitation email is in Appendix {{< var appendix.focus_group_emails >}}. I asked participants to extend the invite to others they felt would be appropriate. Because the Behaviour Change Wheel requires input from experts with insight into the intervention, I decided against recruiting na√Øve authors for this study (although I did elicit their opinions in a subsequent study &mdash; see chapter {{< var chapters.pilot >}}).

Following best-practice, I used information power @malterudSampleSizeQualitative2016 to guide my desired sample size. I decided to use information power above data saturation because, although the latter is commonly requested by editors and reviewers, many have argued against its (often poorly defined) use beyond its original application in grounded theory [@oreillyUnsatisfactorySaturationCritical2013, @braunSaturateNotSaturate2021]. Instead, I followed Braun and Clarke's advice to use pragmatic 'rules of thumb' to anticipate a lower sample size that could potentially generate adequate data, and then make an in-situ decision about when to stop collecting data @braunSaturateNotSaturate2021. Information power is one such set of rules. Malterud et al. posit that the more relevant information a sample holds, the fewer participants are needed. They argue that sample size sufficiency depends on five factors: 1) whether the study's aim is narrow or broad, 2) whether samples are considered dense (they have a lot of relevant experience or knowledge of the phenomena) or sparse, 3) whether the study is well supported by theory, 4) the quality of dialogue, and 5) whether data will be compared between participants/groups. My aim was narrow and well defined. My sample was dense in that participants knew a lot about how reporting guidelines are disseminated but also showed variance in terms of which guidelines they work on and which parts of the academic system they represented. I used the Behaviour Change Wheel as an applied theory. I used open questioning to encourage strong dialogue (I elaborate on this later), and I was not planning a cross-case analysis. All of these choices put my study towards the high-power end of Malterud's spectrum. Therefore, I deemed my information power sufficiently strong to justify initially recruiting 15-20 participants across 4-5 groups. 

By monitoring the number of edits to the co-produced file I could be confident that my information power was good (i.e. that my sample was generating new ideas). I used the dialogue criteria from information power to decide when to stop recruiting. Once groups began to add fewer and fewer comments, I judged that the benefit of continued recruitment was insufficient given time constraints.

### Materials

I used the ideas table to prompt discussion in the focus groups. However, because ideas generated by previous participants could bias or limit the creativity of current participants [@parrilloGroupthink2008, @s.lewis-beckSocialDesirabilityBias2004] I initially hid them by turning the text in the ideas column white before sharing the document. I would reveal the text only after participants had exhausted their own imagination (see @fig-ideas-document for an example). All participants could edit this file to record their own ideas or elaborate on other people's ideas. At the end of each focus group, I then turned the ideas column white again, ready for the next group to continue the process. In this way, each group built upon the output of the previous groups without being bias by it.

::: {#fig-ideas-document layout-ncol=1}
![Ideas initially hidden](assets/ideas-doc-b4.png)

![Ideas revealed](assets/ideas-doc-after.png)

An example row in the Ideas Table that was co-edited by participants. The left hand column contained influences that needed to be addressed for our target behaviour to occur. Existing ideas to address each influence were initially hidden (by turning the text white), and only made visible once participants had discussed their own ideas.
:::

### Focus group sessions

I conducted focus groups between May-July 2022 online using Zoom. Before each focus group, I asked participants to spend some time thinking about influences and what needed to change. I did this because I wanted participants to get into the frame of mind and come to the focus group "armed" with influences they were ready to discuss. I was also interested to see whether participants would contribute influences that I did not identify in chapters {{< var chapters.synthesis >}} - {{< var chapters.web-audit >}}. 

Each focus group lasted 2 hours. Following standard practice, I began by introducing myself and the project in a way that I hoped would help participants relax and to think open-mindedly, not defensively @mcgrathTwelveTipsConducting2019. I explained where the list of influences had come from, and that the influences were in reference to reporting guidelines _in general_, and not necessarily a comment on any guideline in particular. I encouraged participants to think beyond the guideline documents themselves, and to consider all stakeholders and resources involved. I explained the goal was to brainstorm as many ideas as possible, and not worry about whether ideas were good or bad @s.lewis-beckSocialDesirabilityBias2004.

It was not possible for a single focus group to cover all rows within a reasonable amount of time, so I allowed participants to select which influences they wanted to discuss. I did this by giving them a few minutes to read through the influences in the left hand column, raise any additional influences they felt were missing, and mark those that they wanted to talk about. I occasionally selected items to discuss myself, either because they had been neglected by previous groups or because I expected participants to have insight into them.

For each influence discussed, I would explain it and allow participants to ask questions. Inspired by the Think/Pair/Share teaching method to encourage engagement within classrooms @allenThinkPairShare2012, I then asked participants to spend a minute reflecting on the required change and brainstorming solutions on their own before discussing them as a group. I encouraged this solo reflection because I wanted all participants to engage with the problem.

To facilitate discussion, I asked open ended questions, often drawing on intervention functions from the BCW by asking questions like "how could this be easier to do?" or "how could we change how people feel about this?". I did this when participants ran out of ideas, or when they got fixated on a particular type of intervention, in which case I would reassure participants that their fixated solution was already documented and that it would be useful to think of alternatives.

Once participants had exhausted their own ideas, I revealed the ideas identified by previous groups by changing the colour of text from white to black. Participants could then edit and extend the text until it reflected all of their thoughts too. Ideas were never removed from the document, but participants could add concerns or disagreements if they wanted to. Editing the file in this way allowed participants to document their thoughts in their own words.

After each focus group I made notes on how the session went and reflected on what I could have done differently. I made a copy of the ideas document and then turned the text in the ideas column white again, ready for the next group. Taking copies after each group created a paper trail of how the document had evolved after each session. I counted the number of additions so that I could monitor how many new ideas had been added and, therefore, whether I could stop data collection. 

### Data processing and analysis

I used qualitative description for my analysis [@bradshawEmployingQualitativeDescription2017; @kimCharacteristicsQualitativeDescriptive2017], which involved aggregating and summarising ideas. I imported the final ideas document into NVivo @qsrinternationalptyltd.NVivo2020 and applied descriptive codes to ideas. If a sentence contained multiple ideas, I coded each idea separately. I also coded the influences and stakeholders that were related to each idea. I did not interpret data as doing so would erase the views captured during co-production. 

In the next chapter I describe how I used the Behaviour Change Wheel to label ideas according to their intervention function. However, I wanted the results of this chapter to be accessible and useful to the reporting guideline community (I have since shared this chapter with a few guideline development groups), and so I decided to group ideas inductively in ways that felt cohesive and made the results easy for my intended audience to understand and act upon. For example, I aggregated "ask authors to cite reporting guidelines" and "display citation metrics on reporting guideline resources" into a group about "Citations", even though they target different influences (discoverability and perceived trustworthiness) and employ different intervention functions (education and persuasion). I labelled each group with the influence(s) it addressed, and the stakeholders it involved.

I discussed and refined my coding, aggregating and summarising with one of my supervisors, Jennifer de Beyer. I sent the aggregated, summarised ideas to focus group participants and EQUATOR staff members, inviting them to check it reflected their ideas faithfully.

### Reflexivity & Trust

In chapter {{< var chapters.workshops >}} I described my active role within my workshops with EQUATOR and I argued that my subjectivity was an asset within the workshops. In contrast, I tried to remain objective when running focus groups with external stakeholders, in order to capture the perspectives of participants without influencing them. My research paradigm for the focus groups was post-positivist, in that I considered that ideas were "out there", but that differences in context, experience, and opinion would affect what I (and participants) observed, understood, and concluded.

I wanted to ensure my results could be trusted as an account of participants' views. Lincoln and Guba @lincolnNaturalisticInquiry1985 argue that for a study to be trustworthy, the researcher must show that the findings are credible ('true'), transferable (applicable to other contexts), dependable (consistent and repeatable), and confirmable (shaped by participants, not by the researcher's bias or motivation). Lincoln and Guba propose a number of techniques to achieve these criteria, and I describe the techniques I used in @tbl-trust.

{{< include _table_trust.qmd >}}

### Ethics & Data Management

The study was approved by the Medical Sciences Interdivisional Research Ethics Committee (R80414/RE001). Participants gave informed consent by completing an online form @jiscOnlineSurveys having read the participant information sheet (see Appendix {{< var appendix.focus_group_information >}}). Participant's edits to the co-produced file were anonymous. I recorded the audio of focus groups so that I could refer to them during analysis. All data and recordings were kept on secure university storage.

### Reporting

I used SRQR @obrienStandardsReportingQualitative2014 when outlining this chapter, and again to check my reporting during revision (see appendix {{< var appendix.srqr-focus-groups >}}). 