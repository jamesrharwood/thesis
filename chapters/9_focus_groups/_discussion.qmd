# Discussion

## Description of findings

<!-- 
* short summary of the main findings
* include some interpretation of the data in the context of previous findings, experiences, theory, or a guiding paradigm or approach.
* The discussion provides authors an opportunity to:
  * elaborate on their findings in relation to their research question(s) and study purpose(s);
  * connect their findings to prior empirical work, theories, and/or frameworks; 
  * and discuss implications:
    * describe how findings contribute to or advance the field
    * Implications may include transferability, or specifying the appropriate scope for generalization of the findings beyond the study (e.g., to other settings, populations, time periods, circumstances).
-->

### Summary of results

My objective was to identify ideas to address factors that may influence adherence to reporting guidelines by running workshops with the EQUATOR Network and focus groups with guideline developers, publishers, and other experts. Stakeholders identified {{< var counts.sub_items_pre_jh >}} ideas employing all intervention functions. There were ideas to consider before, during, and after creating guidance, ideas to consider when writing the guidance down, ideas about tools to help guidance application, ideas about ongoing activities to support or promote guidance use, and ideas about refining guidance over time in response to feedback. Many of these ideas could be enacted by guideline developers, publishers, and EQUATOR, but participants also saw opportunities for ethics committees, funders, academics, registries, and syllabus writers; stakeholders who are typically less frequently considered.

I believed that including perspectives from a range of stakeholders would lead to more ideas. This seems to be the case: EQUATOR staff thought of {{< var counts.EQUATOR_ideas >}} which was then expanded to {{< var counts.sub_items_pre_jh >}} through the focus groups. 

This is the first time that guideline developers from different groups have come together with publishers and academics to consider reporting guidance dissemination as a system. I had a fair response rate from guideline groups and of their responses were generally supportive, even if they were unable to take part. Multiple guideline developers volunteered their guideline to be a "guinneapig" and expressed support for my work. All stakeholders were open minded to the barriers I presented except for one developer who expressed scepticism that reporting guidelines were anything but perfect, and requested additional evidence of authors' negative experiences.

This is also the first time a behaviour change framework has been used to identify ways to encourage the application of reporting guidance and so is an important step in the journey towards an evidence based intervention underpinned with theory. Intervention functions and policy categories were an effective way to prompt discussion when participants ran out of ideas or repeatedly focussed on the same type of intervention that had already been documented. I also felt that the framework helped guideline developers detach themselves from their creations and look at them objectively. Keeping discussion focussed on guidelines _in general_ (not theirs in particular), having evidence for barriers, and including mixed perspectives within groups, also helped.

#DECIDE I think I'll take out the paragraph above. 
<!-- 
#STRETCH consider adding interpretation if asked. Do GL developers agree on what RGs are and what they aim to do? Are they the E&E? Are they the checklist? Are they encouraging transparency? Or a stepping stone to standardised design and reporting format? And second point, do developers have the skills and resources to gather and act on feedback, and to turn publications into refined resources?

### Interpretations

Two questions divided participants more than others; whether reporting guidelines should be agnostic to design choices, and whether they should be viewed as rules (standards or requirements), or just a point in the right direction (guidance). The term 'guideline' is used ambiguously in scholarly publishing. NICE defines clinical guidelines as 'recommendations', which is in line with the Cambridge dictionary definition: "information intended to advise people on how something should be done". Thus guidelines "advise" or "recommend" but don't enforce or legislate. However, _journal author guidelines_ can be perceived as _rules_ that researchers have to adhere to if they want to publish, synonymous with _instructions_. Some participants were concerned that reporting guidelines may be perceived as rules, and that authors should be reassured that "guidelines are just that: guidelines!". Conversely, a few participants argued strongly that reporting guidelines should be seen as rules.

These divisions over whether guidelines are guides or rules, and whether they should include opinions about design, may reflect differences between the scope of reporting guidelines, and the perspective of developers. For example, some reporting guidelines cover specific types of study that are homogenous in design and approach (e.g. ontology and epistemology). In this instance, developers may feel comfortable making assumptions or recommendations about design choices, and may be more likely to consider guidelines as standards that can be enforced. In comparison, developers of guidelines with a broader scope may have purposefully tried to not make assumptions about design or approach in an attempt to accommodate studies that share a single feature (e.g. population or method) but vary in all others, such as their design or ontology.

The division could also reflect variation in developers' aims. Transparency may be the end goal for most, but others may consider reporting standards to be a stepping stone towards influencing (what they consider to be) good design. -->

<!-- #ASK: connect their findings to prior empirical work, theories, and/or frameworks -->

### Implications

In chapter {{< var chapters.workshops >}} I describe how the ideas presented here fed into step 7 of the Behaviour Change Wheel intervention development process. These results will also be of interest to the reporting guideline community. Developers may find inspiration here when writing or revising guidance. At the time of writing, the EQUATOR Network was in the process of updating its advice for guideline developers, which I hope will be informed by these results. The ideas generated may also be of interest to publishers, funders, and Universities.

<!-- #### Transferability -->

Although only a few reporting guideline development groups took part in this study, most ideas identified were abstract enough to generalise to most reporting guidance, which tend to be developed and distributed in similar ways; (e.g., all development groups will have to consider what guidance to create, its scope, how to communicate it clearly and how to disseminate it). Some ideas may even generalise to other interventions to encourage good research practices (e.g., to communicate personal benefits, not to patronize researchers). Some ideas may generalise to clinical guidelines, although those are better studied.

### Limitations

The study may have benefited from more diversity in participants' backgrounds and expertise. We could have recruited participants from funders, ethics committees, or registries, behaviour change experts or experts familiar with user experience of websites or written documents. All participants were western and proficient in English. Although this is partly a limitation of who writes reporting guideline, we could have sought input from publishing houses that cater to non-western authors. Broadening the participant pool in this way would have led to more ideas. 

My focus groups were smaller than I had planned. Some would consider these group sizes too small to be called focus groups, and may instead call them paired interviews, dyads, or triads ([#REF](https://www.aqr.org.uk/glossary/triad)). A hallmark of focus groups is that they use "group interaction to produce data" @l.morganFocusGroupsQualitative1997, and these interactions may include sharing experiences and challenging each other. However, I did not feel the small group sizes to be a limitation in this study for two reasons. Firstly, because participants were co-editing a file and building upon the thoughts of previous groups, participants could react and respond to to participants from previous groups.  Secondly, participants had deep understandings of the topic (evidenced by sessions overrunning and participants dwelling on a single topic) which meant that even pairs of participants had plenty to discuss, share, and debate. If I had condensed participants into, say, 3 groups of 5-6 participants, each participants would have had less time to speak and I anticipate that many ideas would have gone un-spoken. 

### Future work

I purposefully did not seek input from authors as this study required input from experts familiar with reporting guideline dissemination. Instead, I describe how I sought input from authors in chapter {{< var chapters.pilot >}}. I also purposefully did not ask stakeholders to prioritize or rank ideas. In chapter {{< var chapters.workshops >}} I explained that prioritization is subjective and, therefore, should be done by stakeholder separately. I also described how EQUATOR began to prioritize intervention options and in the next chapter I describe how I decided which ideas to act on and how I turned them into intervention components.
