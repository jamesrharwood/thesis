## Discussion

I have demonstrated how I have used a data-driven approach, guided by behaviour theory, to re-design how reporting guidance is disseminated. I have proposed {{ < var counts.redesign.intervention-components >}} intervention components, addressing {{ < var counts.redesign.barriers-targeted >}} barriers and employing {{ < var counts.redesign.intervention-functions-used >}} intervention functions. By linking components with barriers and functions, I have justified my suggestions using evidence and described how they are theorized to work. I have then created a prototype website to demonstrate how these components could be realised.

Together, these changes amount to a complete redesign of two key parts of the existing system through which reporting guidelines are currently disseminated; the guidelines themselves, and the EQUATOR Network website which is visited by almost 1 million authors each year.

### Ranking IFs

One of the most interesting parts of this process was witnessing an unexpected change of opinion amongst EQUATOR staff. Before beginning this study, a common refrain heard around the office was that in order for reporting guidelines to be successful editors had to start enforcing them and refuse to publish research that didn't adhere. So it was fascinating to see that workshop participants unanimously rated restriction and coercion as their least favourite options.

I think two things happened here. Firstly, having discussed the barriers that authors face when trying to use reporting guidelines, participants felt that forcing authors to use them would be unacceptable to authors, impractical for editors, and inequitable as some authors would face larger hurdles than others. Secondly, participants reassessed things they had taken for granted, and realised that there are many low-hanging fruit that could make guidelines easier to find and use, and that these fruit were growing in their own orchard.

### When comparing current intervention

This reassessment required participants to take a step back and look at the current set-up with fresh eyes. We did this informally. Some participants shared long-standing frustrations with the website or guidelines. One participant shared designs she had created years ago for a redesigned EQUATOR website. Other times, after someone shared an idea, we would go to the guidelines to see how it's done now. In this way, we realised that the STROBE checklist (which is by far the most accessed resource on the EQUATOR website) doesn't link to the STROBE guideline article.

So this comparison was ad-hoc, and I have included pieces of it in this chapter purely to provide context to the proposed changes. I sought out examples of a behaviour change technique not being implemented, or being implemented poorly. I made generalisations about RGs using words like "some" or "few" to give an impression of how frequently RGs currently use a given BCT. These frequency descriptions are based on my own observation, and not on a formal audit.

I considered systematically auditing the content of EQUATOR Network website and popular guidelines to see which behaviour change techniques they employ and which of our ideas were already present. I decided not to for two reasons. Firstly, with so many  ideas and so many guidelines, this would have taken time and I decided instead to prioritize building and testing a prototype. Secondly, this audit wouldn't have dramatically influenced the intervention components we designed, but would merely quantify how different my proposed intervention is to the current set-up. Who would be interested in quantifying this difference? Perhaps my thesis examiners, and perhaps the guideline development community. But quantifying differences wouldn't bring me any further towards helping authors or impacting reporting quality, like building a prototype would. Should the guideline development community need that evidence then this audit could be done in the future, but in my opinion, the difference speaks for itself.

### Limitations & reflections on process

Using a framework and a systematic method helped participants (and I) to check our biases. Instead of relying on personal preference, we tried to ensure choices reflected the function we were trying to employ. For example, when choosing a background image, instead of asking "do you like this one?", the questions became "what feelings do you think this image conveys? Does it communicate simplicity?". Working as a group helped mitigate individual peculiarities.

However, there is no avoiding the fact that many decisions required a degree of subjectivity and, as lead researcher, designer, and developer, often these decisions landed on my shoulders. I tried to mitigate this by involving EQUATOR members in every step, prioritizing their ideas over my own, and providing many opportunities for feedback.

Our imagination may have been constrained by what already exists. Although I encouraged blue-sky thinking, participants often focussed on tweaking what already exists instead of starting from a blank slate. If reporting guidelines didn't exist, how else might we have tackled poor reporting? If EQUATOR didn't exist, would we still have recommended an organisation? How might that organisation be structured, governed, and what kind of legal entity might it be? If the publishing industry didn't exist, might we have imagined different ways of describing research that were more formulaic than free-form articles?

These imagination constraints may be a weakness, but they are also an advantage. EQUATOR is in a privileged position that in that it is known and trusted by publishers, guideline developers, and many authors. Thousands of journals and authors already use reporting checklists. So whilst the changes proposed in this chapter (and the ideas proposed in the chapter before) may be criticised for not being radical enough, for an organisation (and a PhD student) with limited time and resources, it makes sense to improve a system that already has significant buy-in from the academic community, over and above destroying that system or trying to create a new one from scratch.

Our horizons were perhaps limited by group-think. If I were to repeat the work, I would have included a small, diverse group of authors to take part in the design process. I would also have invited representatives from the publishing industry, funding community, and people more familiar with designing digital behavioural interventions. Including these diverse, informed voices in the design process could have lead to more radical design choices.

### Future work

I partly remedy this limitation by piloting the new website amongst authors in the next chapter. Defining intervention components by their target barriers and intervention functions lends itself nicely to designing a pilot study to gather feedback. I describe this in more detail in the next chapter but, briefly, because I knew what each component was supposed to be _doing_ I could design an interview schedule with questions and tasks to specifically explore components. ~For example, the purpose of the top of the home page is to communicate what reporting guidelines are and how they benefit authors. Instead of asking general questions (like "what do you think of...?"), I decided to ask more specific questions ("What do you think the website is about? How do you think it might influence your work?"). Furthermore, because the purpose was to communicate these things _quickly_, I decided to only give pilot participants 5s before asking these questions.~ Similarly, defining components in this way will facilitate future quantitative work to assess efficacy.

After I've done this pilot work and made refinements, further development work will be required before the new website can be made live. Some of this work are technical tasks that, although necessary, do not have behavioural impact. For example, I will need to integrate the new website as a subdomain of EQUATOR's existing one, and I will create automated tests that run before each deployment. However, other tasks _do_ appear on the list of intervention ideas, and will affect behaviour. For example, I intend to optimise each guideline page so that it ranks highly in search engines.

Along with this technical work, I will need to spend time uploading other popular reporting guidelines and editing items into a consistent structure, as I did with SRQR. This process revealed gaps in item description. Most commonly, there was often no guidance of what to write if an item wasn't or couldn't be done. For instance, the target sample size item had no instruction of what to write if you didn't ever have a target in mind. Some items were missing any kind of justification of why the item was important and to whom. I anticipate similar gaps for other reporting guidelines, and would seek to work alongside guideline developers to fill them.

Buy in. 

Beyond the intervention components presented here, the prioritization exercise identified other ideas that EQUATOR would like to implement, but that I chose not to act upon. For example, participants favoured developing training resources specific to individual reporting items, and creation of network of “reporting champions”, akin to the UKRN model. EQUATOR participants liked the idea of lobbying funders to require reporting guidelines be used for applications. The work in this chapter could be used to support funding applications to support these endeavours. 

### Implications

I talk about broader implications in my final discussion section.  

- Long term implications - a scalable intervention that can be used by reporting guidelines authors off the shelf.
- Example of consort and prisma websites being down for months

Why didn’t I do an analysis of existing RG system?

Perhaps a GL for RGs would be useful

Next steps - future work, pilot

### For later discussion

### How does my approach compare with the Person Based Approach?

In practice intervention designers may use different frameworks when creating interventions.

For example, @bandInterventionPlanningDigital2017

Describe person based approach steps.

Draw parallels with the steps I took. Say what was different.

Argue why PBA wasn't necessary here.
