Demonstrated a data driven, systematic approach to intervention design. Number of components, number of functions, number of barriers. 

Number of components remaining that Equator could deliver. Funding applications. 

Number of components that others could deliver. Previously unthought of. 

First realisation of a RG intervention where each part is designed and defined by BCT. Previously the website and guidelines may well have had elements, but they were there haphazardly, organically. 

Things yes to do before it can go into production

Immediate implications - pilot

Mid term implications - real-world testing

Long term implications - a scalable intervention that can be used by reporting guidelines authors off the shelf


Although data driven, still a lot of subjectivity. Eg design choices. But the intended function served as an anchor or yard stick so instead of relying on personal preferences participants could refer to intended function. Some felt that including multiple team members helped as we were less swayed by individual peculiarities. 

Limitations. Existing system is both. Limits imagination. What would we have made if EQUATOR didn’t exist? Or if RGs didn’t exist? If consort didn’t already exist, might they have taken a more Modular approach like jars? If no RGs existed, would we have still decided to create a system that mainly intervenes at publishing time? Should I have focused my time making resources for funders? But by improving something that already exists, we can be more certain of acceptability and practicality. 

It would have been useful to include authors in the design workshops. 

Why didn’t I do an analysis of existing RG system?

Next steps - future work, pilot
## Discussion

### Ranking IFs:

Interestingly, participants gave low scores to ideas centring around restriction - encouraging journals, funders, or ethics committees to enforce RG adherence - principally because of concerns that such rules would not be acceptable to researchers, would be expensive, and would not be equitable, as RGs adherence would disproportionately burden international or early career researchers, or disciplines where the guidance is less mature or harder to apply. This marked a reversal of opinion from before we started, when most members of EQUATOR were confident that enforcement was the only solution.

### How does my approach compare with the Person Based Approach?

In practice intervention designers may use different frameworks when creating interventions.

For example, @bandInterventionPlanningDigital2017

Describe person based approach steps.

Draw parallels with the steps I took. Say what was different.

Argue why PBA wasn't necessary here.

### When comparing current intervenion

I considered auditing popular guidelines to count how many currently employ any of the ideas in this list. However, with so many items, such an audit would have taken a long time:

* Debatable benefit: already confident that lots of ideas not implemented
* Those that are may not be implemented in an ideal way, e.g. not prominent. Hard to quantify.
* Even if only 1 guideline doesn't do something, most of the ideas are fairly easy to implement, so the cost:benefit ratio still makes sense. 

So instead, I made generalisations about RGs using words like "some" or "few" to give an impression of how frequently a BCT is used. I sought out examples of an item not being implemented, or being implemented poorly. As long as 1 guideline doesn't do something, or does it poorly, that justified including the item.

### Gaps in item description

This process revealed gaps in item description. Most commonly, there was often no guidance of what to write if an item wasn't or couldn't be done. For instance, the target sample size item had no instruction of what to write if you didn't ever have a target in mind. Some items were missing any kind of justification of why the item was important and to whom.

### feedback & the use of the intervention planning table

I sent drafts of the intervention to EQUATOR members. I used a tool Pastel to collect feedback. The first round elicited 39 comments from 3 team members. Some comments were supportive e.g., "I really like these two boxes!", others were suggested changes to wording or content. When members disagreed, we referred back to our intervention planning table to check the component against its intended function. For example, when one team member didn't like the background image, we consulted the table to check whether the image was doing what it was meant to. E.g. "even though you don't like it personally, do you agree that it is conveying a feeling of simplicity?".
