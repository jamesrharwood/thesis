## Discussion

### Ranking IFs:

Interestingly, participants gave low scores to ideas centring around restriction - encouraging journals, funders, or ethics committees to enforce RG adherence - principally because of concerns that such rules would not be acceptable to researchers, would be expensive, and would not be equitable, as RGs adherence would disproportionately burden international or early career researchers, or disciplines where the guidance is less mature or harder to apply. This marked a reversal of opinion from before we started, when most members of EQUATOR were confident that enforcement was the only solution.

### How does my approach compare with the Person Based Approach?

In practice intervention designers may use different frameworks when creating interventions.

For example, @bandInterventionPlanningDigital2017

Describe person based approach steps.

Draw parallels with the steps I took. Say what was different.

Argue why PBA wasn't necessary here.

### When comparing current intervenion

I considered auditing popular guidelines to count how many currently employ any of the ideas in this list. However, with so many items, such an audit would have taken a long time:

* Debatable benefit: already confident that lots of ideas not implemented
* Those that are may not be implemented in an ideal way, e.g. not prominent. Hard to quantify.
* Even if only 1 guideline doesn't do something, most of the ideas are fairly easy to implement, so the cost:benefit ratio still makes sense. 

So instead, I made generalisations about RGs using words like "some" or "few" to give an impression of how frequently a BCT is used. I sought out examples of an item not being implemented, or being implemented poorly. As long as 1 guideline doesn't do something, or does it poorly, that justified including the item.

### Gaps in item description

This process revealed gaps in item description. Most commonly, there was often no guidance of what to write if an item wasn't or couldn't be done. For instance, the target sample size item had no instruction of what to write if you didn't ever have a target in mind. Some items were missing any kind of justification of why the item was important and to whom.

### feedback & the use of the intervention planning table

I sent drafts of the intervention to EQUATOR members. I used a tool Pastel to collect feedback. The first round elicited 39 comments from 3 team members. Some comments were supportive e.g., "I really like these two boxes!", others were suggested changes to wording or content. When members disagreed, we referred back to our intervention planning table to check the component against its intended function. For example, when one team member didn't like the background image, we consulted the table to check whether the image was doing what it was meant to. E.g. "even though you don't like it personally, do you agree that it is conveying a feeling of simplicity?".