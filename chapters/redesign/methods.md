## Introduction

The output of my focus groups was a long list of ideas. To turn this list into an intervention I had to determine which of them I _could_ implement, which ones I _should_ implement, and _how_ I could turn them into intervention components. Before I could do that, I had to better define what the objective of the intervention was. Although my previous chapters focussed on _authors_ (as opposed to editors, supervisors, or peer reviewers etc.), these chapters threw up multiple behaviours that I could now choose to target. For example, did I want authors to complete a checklist upon journal submission? Read guidance from start to finish before putting pen to paper? Adhere to guidance? Visit a particular page on the EQUATOR Network website? I needed to select and define the desired behaviour more exactly.

In this chapter I explain how I a) defined the target behaviour, b) prioritized intervention functions and policy categories in order to determine which ideas to consider c) used behavioural analysis to turn ideas into intervention components which I then d) developed into an intervention prototype.

## Methods and Results

### Objective 1: Defining the target behaviour

My first step was to state the objective of the intervention in terms of key behaviours and outcomes. I did this collaboratively with EQUATOR UK during the first of the {{< var counts.EQUATOR_meetings >}} workshops attended by {{< var counts.EQUATOR_staff >}} UK EQUATOR staff members between Jan - May 2022, described in the previous chapter. As recommended by Michie et al (@michieBehaviourChangeWheel2014) we defined our key behaviour in terms of _who_ needed to do _what_, _when_, _where_, and in what _context_. Workshop participants drafted definitions on their own before discussing as a group and co-editing a shared file.

Our final definition of our target behaviour was: **Researchers should use reporting guidance as early as possible in their research pipeline. They will do this for every piece of research, at their place of work, on their own but in the context of collaboration**

Participants broke this key behaviour into two sub behaviours:

1. Engage with reporting guidelines as early as possible (ie read them) and,
2. Apply the guidance to their writing as intended by the guideline developer.

By "research pipeline" we mean the many steps involved in a typical project which may include ideation, obtaining funding and ethics permission, designing, writing a protocol, drafting a manuscript, editing a manuscript, submitting a manuscript. Instead of specifying _which_ stage reporting guidance should be used, we decided to specify that we want authors to use guidance as "early as possible". We did this for a few reasons. Firstly, guidelines differ in how easily they can be used for writing protocols or applications. Secondly, researchers will naturally come across reporting guidelines at different points on their work. Should a researcher only discover a guideline at the point of journal submission, then of course we want them to apply the guidance then. But by specifying "as early as possible", we declare our hope that _next time_ that same researcher may decide to use a guideline at an earlier point. And finally, not all research projects will begin with a written funding application, ethics application, or protocol.

By "apply", we refer to using guidance to plan, write or edit a written description of research (e.g. within a manuscript or application). Applying guidance includes the use of tools like templates or checklists. Participants discussed specifying "completing a reporting checklist" as a target behaviour, but decided against it as previous research showed that authors who complete checklists upon submission don't necessarily edit their manuscript or comply with guidelines. Participants also recognised that focussing on checklists may be problematic because checklists appear administrative, are used _after_ a manuscript has already been written, at which point authors are least able or motivated to edit their work. Nevertheless, participants recognised that checklists will continue to be an important part of how reporting guidance are disseminated, and so they are included within the term "apply the guidance", without being named.

We felt it important to specify that guidance should be applied "as intended by the guideline developer" as my previous research showed that sometimes authors who misinterpret guidelines may believe that they are adhering when they are not.

Hence our target behaviour definition specifies the _who_, _what_, and _when_ but is broad enough to account for differences between researchers' working practices, research projects, and reporting guidelines.

### Objective 2: Prioritizing intervention functions and policy categories

#### Purpose

Because I had included different stakeholders and encouraged blue-sky thinking, not all ideas were practical for EQUATOR to implement. For example, EQUATOR aren't able to offer financial rewards for good reporting, but they do control a website. And although they _could_ advocate for Universities to fire researchers that don't adhere to reporting guidelines, nobody at the UK EQUATOR Center would support such a policy. I lead a series of workshops so that we, as a group, could decide which ideas we felt we _could_ and _should_ take forward.

#### Methods

The prioritization exercise occurred during the final three workshops described in the previous chapter. Although 6 members of the UK EQUATOR Centre took part in the internal workshops, only 4 took part in the prioritisation exercise.

With over a hundred ideas, it was impractical to rank them individually. There were too many, and some were very similar, which made discussions at this level tedious and confusing. Additionally, because ideas often conflated intervention functions and policy categories, it was difficult to tease ideas apart - if we didn't like an idea, was it because we didn't like its function or its mode of delivery? For example, in the idea "journal instructions to authors should use reassuring authors", although EQUATOR members may have favoured using persuasive "reassuring" language, they may have felt that expending effort to lobby journals to change their author guidelines may be an expensive and impractical use of research budget.

Instead, I asked workshop participants to prioritize the 9 intervention functions and 7 policy categories listed by @michieBehaviourChangeWheel2011, within which all ideas could be categorised (see chapter #TODO for an introduction to the Behaviour Change Wheel, intervention functions, and policy categories). I used ideas as concrete examples of these, otherwise abstract, concepts, but I didn't ask members to rank these ideas directly.

I wanted participants to consider options' Affordability, Practicability, Efficacy, Acceptability, Side-effects, and Equity. Together, these are known as the APEASE criteria. Participants considered the APEASE criteria for each intervention function on their own before discussing as a group and adding notes to a co-edited file. I then asked them to decide which intervention function(s) they would prioritize, which they would consider but not prioritize, and which (if any) they would avoid. Participants then discussed their preferences as a group before reaching consensus.

I then repeated the exercise to prioritize policy categories before deciding which of these I could pursue within the constraints of my DPhil.

#### Results

Participants saw Enablement, Education, Training, Persuasion, Modelling, and Environmental Restructuring as ranking favourably on all APEASE criteria. Of these, Enablement was viewed as a particularly low-hanging fruit that would likely be effective and welcomed by the research community.

Participants found the remaining intervention functions problematic. Incentivization and restriction (e.g. rewarding guideline adherence with funding or reduced article processing, or punishing non-adherence by withholding these benefits) were seen as inequitable because as long as guideline adherence is harder for some researchers than others, less-experienced, poorly resourced researchers would be at a disadvantage, as would those working in disciplines where reporting guidelines are poorly designed or harder to apply. Conversely, participants stated that enablement has potential to reduce existing inequalities. In addition to being inequitable, participants voiced that restriction or punishment would be unacceptable to researchers, as would coercion (the threat of punishment). Furthermore, threats without enforcement could become known as paper tigers, lose effectiveness, and erode trust.

Regarding Policy Categories, environmental planning was seen as the most affordable, effective, acceptable, safe, and equitable. When talking about the _environment_, EQUATOR was really talking about the _digital_ environment, within which EQUATOR has control over its own website but not websites or digital services run by others. Improving and extending the website was viewed as an affordable investment, that would be welcomed by authors (thus acceptable), and a practical way to increase the proportion of authors engaging with reporting guidelines that would increase equity without side effects.

Members also recognised communication as a favourable policy category that is already utilised. Communication channels included the website, mailing lists, social media, conferences, and publications. Whilst communication was rated as affordable, practicable, acceptable, safe, and equitable, its effectiveness may be limited. Although communication could promote awareness of reporting guidelines are available, what they are, and how they can be used, EQUATOR members noted that the efficacy of a communications campaign may be undermined if the campaign is directing authors to a website that is difficult to use, or guidelines that are difficult to access or understand. Thus communication on its own might not be sufficient, and members suggested that it should come after improving the digital environment.

Service provision was also favoured, as long as the service was financially sustainable. So too where guidelines, specifically guidance to help reporting guideline developers create and disseminate resources. Participants felt that Legislation, Regulation, and Fiscal Measures would not be acceptable to researchers and were not not practical options for EQUATOR to use.

I then had to decide which of the favoured policy categories to focus on as part of my PhD; (digital) environmental planning, communication, service provision, or guidelines. It was important that whatever I chose had to be doable within my funding window, had to lend itself to writing research chapters, and I wanted it to have a lasting impact after my DPhil was finished.

Creating a new service felt unsustainable as it would likely stop once my funding ran out. Developing guidance for guideline developers could be useful and acceptable, but I didn't have enough time to organise a delphi process with guideline developers. I could have developed a communications campaign, but EQUATOR members felt this was something they could do independently and that it should be done after addressing other barriers. 

Instead, refining and extending the existing EQUATOR website felt like the perfect choice for multiple reasons. Firstly, planning the digital environment was ranked most highly as a policy category. Secondly, the website can be used as a means of delivering many of the highly ranked intervention functions (enablement, education, persuasion, modelling). Thirdly, it spoke to my skills as a software developer and is something that the UK EQUATOR staff could not do on their own. And finally, these changes could be made within the time limit of my PhD and, importantly, the impact would be sustained after I finish.

### Objective 3: Define intervention content

#### Purpose

Having decided that I would focus on refining and extending the EQUATOR website, I then had to decide which intervention changes I could feasibly make. I wanted to define these changes in terms of the barriers and behavioural drivers each change was designed to target, and the intervention function the change is employing. Defining intervention content in this way is useful because it helps intervention developers to understand why the component has been added (or removed), how it is theorised to be working and, therefore, how its effectiveness may be tested.

#### Methods

For every idea generated in chapter #TODO, I labelled which barriers (from chapters #TODO) it was addressing, which behavioural drivers it was targeting, and which intervention functions it was employing to do so.  this list was data driven, in that it was based upon the ideas and barriers generated from previous research. To give more structure and context to this list, I grouped ideas according to the sub-behaviours they targeted: 1) engaging with guidance and 2) applying it.

Once all ideas were coded, I selected ideas to implement by considering a) whether they could be incorporated into a web-based intervention, b) the priority of the intervention function (determined in objective 2), and c) whether I could feasibly deliver the idea within the time constraints of my DPhil.

#### Results

See table #TODO for all {{ < var counts.ideas >}} ideas, labelled with the barriers they address, the drivers they target, the intervention functions they use, and whether I could implement them.

### Objective 4: Building the intervention

#### Purpose

To build a working prototype that could be piloted.

#### Methods and Results

##### Designing the intervention

I began by describing how the intervention component could be realised, and contrasted this to the existing system. In doing these comparisons, I looked at how the EQUATOR website is currently, and I made generalisation about how popular reporting guidelines are currently disseminated, and the content of their Example and Elaboration documents and checklists.

Designing was iterative and collaborative. I included the same members of EQUATOR UK that had participated in the workshops. We met 3 times between November 2022 and January 2023.

We began by deciding which webpages required redesigning, and how webpages should be navigated. On the existing website, authors starting on the home page must visit up to 5 webpages to reach the full reporting guidance. We redesigned the layout of the website to reduce this to 2 webpages - the EQUATOR home page, and a reporting guideline page containing the full guidance. (See figures #TODO). These different website layouts are visualised in @fig-sankey-b4 and @fig-sankey-after.

Workshop participants then sketched ideas for how the home page and guidance pages could be laid out and where intervention components could be placed. Once participants had agreed on a layout, I created an alpha version of the new website and invited members to comment on it. These were real webpages that could be viewed in a browser, but dummy text and images. After another round of feedback I refined the alpha version, populated it with real text and images, and participants gave feedback again. The new pages can be viewed in @fig-home, @fig-rg-intro, and @fig-discussion, and can be compared with the old pages in @fig-home-b4 and @fig-db-b4.

My intention was to create guideline pages for a sample of the most frequently accessed guidelines so that the website felt real for pilot participants. However, many intervention components involved changing the wording and layout of the guidance itself. Editing multiple guidelines was neither feasible not necessary, as we only needed one edited guideline to pilot the new website.

I selected SRQR as my test guideline to edit because I was familiar with it, having used it when writing up my own research, and because I felt it would make a good guideline to test with (see next chapter for why). I got written permission from Bridget O'Brien, the lead developer or SRQR, and from the publisher. I kept Bridget up to date with my work and invited her feedback.

I began editing SRQR by pasting the text into Microsoft Word and rearranging content into categories: what to write, how/where to write it, what to write if the item wasn't/couldn't be done, why the item is important and to whom, examples. I edited sentences to speak directly to authors. E.g. "Describe X" instead of "Authors should describe X", and to use active voice. This shortened the text and also made it clearer that the primary audience is authors.

For composite items I split the sub-items into bulleted lists. E.g.

> For each X, describe:
>
> * X
> * Y
> * Z
>

I rearranged conditional sub-items so that they read "If X, then describe Y", rather than "Describe Y if X". I moved definitions into the glossary and contextual information into notes. I edited the resulting text to join it back together. I edited the tone of voice to add reassuring language. An example of the redesigned guidance can be viewed in #sec-box-item.

After development, I double checked the intervention against the initial list of intervention components to ensure I had covered all of them. I consulted with EQUATOR members to verify that the components were realised as expected and invited another round of feedback.

##### System architecture

When considering architecture options I prioritized technology that was easily maintainable by EQUATOR staff or a future PhD student. I looked for tools that would be familiar to early career researchers. I considered DIY website builders (like Wix or Squarespace) but these services can be expensive. Most offer a 'drag and drop' building experience which, although easy to use, is a laborious way of uploading and formatting large amounts of content. Should EQUATOR want to change how an item is presented, they would have to manually edit each and every item for each and every reporting guideline. Additionally, our intended intervention changes required some custom functionality that wasn't offered by these services (e.g. integration with a DOI minting service, glossary definitions, discussion boards).

Although coding languages like `html` or `javascript` are used by many software developers to create websites, I felt that few early career researchers would be familiar with them. Ultimately I decided on `markdown`, an incredibly simple language that can be learnt in just a few minutes. Markdown is a plain text language. It uses asterisks, underscores, and carets to make text **\*\*bold\*\***, _\_italic\__, or ^\^superscript\^^. Headings, URLS, and references are similarly easy, and can be simplified further by using one of many readily available editors that make writing markdown feel like writing a Microsoft Word document.

Many researchers already use markdown to write reproducible manuscripts using tools like RStudio or Quarto. Quarto can also be used to turn markdown into html (a website), and can be further customised using languages commonly used in research, like Ruby or Python. Quarto requires no technical knowledge, is easy to learn, has great documentation, and is open source.

All code is on Github, a version control repository that researchers commonly use to store and share code and data. The website is served using Github Pages which is free and easily configurable.

I wanted EQUATOR to have ultimate control over the website, but I also wanted guideline developers to have selective access to their own guideline content but not to other guidelines. I have built the website such that each reporting guideline is stored within its own repository on github, accessible only to its developers. These guideline repositories are then "pulled in" to the main EQUATOR repository, so EQUATOR can double check changes that developers make before allowing them to go live on the site.

The website source code can be viewed at {{ < var website-github-repo-url >}} and the website can be viewed at {{ < var website-url > }}.

#### Iterative feedback and development

I sent drafts of the intervention to EQUATOR members. I used a tool Pastel to collect feedback. The first round elicited 39 comments from 3 team members. Some comments were supportive e.g., "I really like these two boxes!", others were suggested changes to wording or content.