## Introduction

The output of my focus groups was a long list of ideas. To turn this list into an intervention I had to determine which of them I _could_ implement, which ones I _should_ implement, and _how_ I could turn them into intervention components.

In this chapter I explain how I a) defined the target behaviour, b) prioritized intervention functions and policy categories in order to determine which ideas to consider c) used behavioural analysis to turn ideas into intervention components which I then d) developed into an intervention prototype.

## Methods and Results 

### Objective 1: Defining the target behaviour

My first step was to state the objective of the intervention in terms of key behaviours and outcomes. I did this collaboratively with EQUATOR UK by running {{< var counts.EQUATOR_meetings >}} workshops attended by {{< var counts.EQUATOR_staff >}} between Jan - May 2022. As recommended by Michie et al (@michieBehaviourChangeWheel2014) we defined our key behaviour in terms of _who_ needed to do _what_, _when_, _where_, and in what _context_. Workshop participants drafted definitions on their own before discussing as a group and co-editing a shared file.

Our final definition of our target behaviour was: **Researchers should use reporting guidance as early as possible in their research pipeline. They will do this for every piece of research, at their place of work, on their own but in the context of collaboration**

We broke this key behaviour into two sub behaviours:

1. Engage with reporting guidelines as early as possible (ie read them) and,
2. Apply the guidance to their writing as intended by the guideline developer.

### Objective 2: Prioritizing intervention functions and policy categories

#### Purpose

Because I hope that EQUATOR will build upon the output of my PhD, it was important to include them in deciding which ideas they felt able and willing to support. Because I had included different stakeholders and encouraged blue-sky thinking, not all ideas were practical for EQUATOR to implement. For example, EQUATOR aren't able to offer financial rewards for good reporting, but they do control a website. And although they _could_ advocate for Universities to fire researchers that don't adhere to reporting guidelines, nobody at the UK EQUATOR Center would support such a policy. I lead a series of workshops so that we, as a group, could decide which ideas we felt we _could_ and _should_ take forward.

#### Methods

The prioritization exercise occurred during the final three workshops described in the previous chapter. Although 6 members of the UK EQUATOR Centre took part in the internal workshops, only 4 took part in the prioritisation exercise.

With over a hundred ideas, it was impractical to rank them individually. There were too many, and some were very similar, which made discussions at this level tedious and confusing. Additionally, because ideas often conflated intervention functions and policy categories, it was difficult to tease ideas apart - if we didn't like an idea, was it because we didn't like its function or its mode of delivery? For example, in the idea "journal instructions to authors should use reassuring authors", although EQUATOR members may have favoured using persuasive "reassuring" language, they may have felt that expending effort to lobby journals to change their author guidelines may be an expensive and impractical use of research budget.

Instead, I asked workshop participants to use APEASE to prioritize intervention functions and policy categories. I used ideas as concrete examples of these, otherwise abstract, concepts, but I didn't ask members to rank these ideas directly.

I wanted participants to consider options' Affordability, Practicability, Efficacy, Acceptability, Side-effects, and Equity. Together, these are known as the APEASE criteria. Participants considered the APEASE criteria for each intervention intervention function on their own before discussing as a group and adding notes to a co-edited file. I then asked them to decide which intervention function(s) they would prioritize, which would consider but not prioritize, and which (if any) they would avoid. Participants then discussed their preferences as a group before reaching consensus.

I then repeated the exercise to prioritize policy categories. Once workshop participants had identified their preferred policy categories, I decided which of these I could pursue within the constraints of my DPhil.

#### Results

Participants saw Enablement, Education, Training, Persuasion, Modelling, and Environmental Restructuring as ranking favourably on all APEASE criteria. Of these, Enablement was viewed as a particularly low-hanging fruit that would likely be effective and welcomed by the research community.

Participants found the remaining intervention functions problematic. Incentivization and restriction (e.g. rewarding guideline adherence with funding or reduced article processing, or punishing non-adherence by withholding these benefits) were seen as inequitable because as long as guideline adherence is harder for some researchers than others, less-experienced, poorly resourced researchers would be at a disadvantage, as would those working in disciplines where reporting guidelines are poorly designed or harder to apply. Conversely, participants felt that enablement has potential to reduce existing inequalities. In addition to being inequitable, participants felt confident that restriction or punishment would be unacceptable to researchers, as would the threat of punishment - coercion. Furthermore, threats without enforcement would become known as paper tigers, lose effectiveness, and erode trust.

Regarding policy categories, participants unanimously favoured Environmental Restructuring as their policy category of choice, followed by Education, Training, Persuasion, Modelling, which were all seen as practical, acceptable, and affordable policy categories that EQUATOR could use. Participants felt that Legislation and Restriction would not be acceptable to researchers and were not not practical options for EQUATOR to use.

Environmental planning was seen as the most affordable, effective, acceptable, safe, and equitable policy category. When talking about the _environment_, EQUATOR was really talking about the _digital_ environment, within which EQUATOR has control over its own website but not websites or digital services run by others. Improving and extending the website was viewed as a single affordable investment, that would be welcomed by authors (thus acceptable), and a practical way to increase the proportion of authors engaging with reporting guidelines with no side effects. Making the site easier to use was expected to increase equity amongst website visitors.

Members also recognised communication as a favourable policy category that is already utilised. Communication channels included the website, mailing lists, social media, conferences, and publications. Whilst communication was rated as affordable, practicable, acceptable, safe, and equitable, its effectiveness may be limited. Although communication could promote awareness of reporting guidelines are available, what they are, and how they can be used, EQUATOR members noted that the efficacy of a communications campaign may be undermined if the campaign is directing authors to a website that is difficult to use. Thus communication on its own might not be sufficient, and members suggested that it should come after improving the digital environment.

When discussing concrete ideas, the ideas that came out on top were the ones that could be implemented by:

* Web-based intervention:
  * Improving the EQUATOR website
  * Extending the website to disseminate reporting guidance
* Providing training
* Providing support services
* Creating a network of "reporting champions", akin to the UK Reproducibility Network.

Of these, I then had to decide which ones I - as a time-limited PhD student with a thesis that requires research chapters - could address. Creating a service or network felt unsustainable beyond the end of my funding. I could have developed training materials but who would then promote or deliver the training? There was a risk that training materials would go unused.

In contrast, Improving and extending the EQUATOR website was seen as an acceptable, affordable, and practicable option with no side-effects that would have a high impact on the hundreds-of-thousands of authors that visit EQUATOR's website each year, and could increase equity. These kinds of changes could be made within the time limit of my PhD and, importantly, the impact would be sustained after I finish.

### Objective 3: Define intervention content

#### Purpose

To define changes to intervention content in terms of the barriers and behavioural drivers it is trying to target, and the intervention function the change is employing. Defining intervention content in this is useful because it helps intervention developers to understand why the component has been added (or removed), how it is theorised to be working and, therefore, how its effectiveness may be tested.

#### Methods

For every idea generated in chapter #TODO, I labelled which barriers (from chapters #TODO) it was addressing, which behavioural drivers it was targeting, and which intervention functions it was employing to do so.  this list was data driven, in that it was based upon the ideas and barriers generated from previous research. To give more structure and context to this list, I grouped ideas according to the sub-behaviours they targeted: 1) engaging with guidance and 2) applying it.

Once all ideas were coded, I selected ideas to implement by considering a) whether they could be incorporated into a web-based intervention, b) the priority of the intervention function (determined in objective 2), and c) whether I could feasibly deliver the idea within the time constraints of my DPhil.

#### Results

See table #TODO for all {{ < var counts.ideas >}} ideas, labelled with the barriers they address, the drivers they target, the intervention functions they use, and whether I could implement them.

### Objective 4: Building the intervention

#### Purpose

To build a working prototype that could be piloted.

#### Methods and Results

##### Designing the intervention

I began by describing how the intervention component could be realised, and contrasted this to the existing system. In doing these comparisons, I looked at how the EQUATOR website is currently, and I made generalisation about how popular reporting guidelines are currently disseminated, and the content of their Example and Elaboration documents and checklists.

Designing was iterative and collaborative. I included the same members of EQUATOR UK that had participated in the workshops. We met 3 times between November 2022 and January 2023.

We began by deciding which webpages required redesigning, and how webpages should be navigated. On the existing website, authors starting on the home page must visit up to 5 webpages to reach the full reporting guidance. We redesigned the layout of the website to reduce this to 2 webpages - the EQUATOR home page, and a reporting guideline page containing the full guidance. (See figures #TODO). These different website layouts are visualised in @fig-sankey-b4 and @fig-sankey-after.

Workshop participants then sketched ideas for how the home page and guidance pages could be laid out and where intervention components could be placed. Once participants had agreed on a layout, I created an alpha version of the new website and invited members to comment on it. These were real webpages that could be viewed in a browser, but dummy text and images. After another round of feedback I refined the alpha version, populated it with real text and images, and participants gave feedback again. The new pages can be viewed in @fig-home, @fig-rg-intro, and @fig-discussion, and can be compared with the old pages in @fig-home-b4 and @fig-db-b4.

My intention was to create guideline pages for a sample of the most frequently accessed guidelines so that the website felt real for pilot participants. However, many intervention components involved changing the wording and layout of the guidance itself. Editing multiple guidelines was neither feasible not necessary, as we only needed one edited guideline to pilot the new website.

I selected SRQR as my test guideline to edit because I was familiar with it, having used it when writing up my own research, and because I felt it would make a good guideline to test with (see next chapter for why). I got written permission from Bridget O'Brien, the lead developer or SRQR, and from the publisher. I kept Bridget up to date with my work and invited her feedback.

I began editing SRQR by pasting the text into Microsoft Word and rearranging content into categories: what to write, how/where to write it, what to write if the item wasn't/couldn't be done, why the item is important and to whom, examples. I edited sentences to speak directly to authors. E.g. "Describe X" instead of "Authors should describe X", and to use active voice. This shortened the text and also made it clearer that the primary audience is authors.

For composite items I split the sub-items into bulleted lists. E.g.

> For each X, describe:
>
> * X
> * Y
> * Z
>

I rearranged conditional sub-items so that they read "If X, then describe Y", rather than "Describe Y if X". I moved definitions into the glossary and contextual information into notes. I edited the resulting text to join it back together. I edited the tone of voice to add reassuring language. An example of the redesigned guidance can be viewed in #sec-box-item.

After development, I double checked the intervention against the initial list of intervention components to ensure I had covered all of them. I consulted with EQUATOR members to verify that the components were realised as expected and invited another round of feedback.

##### System architecture

When considering architecture options I prioritized technology that was easily maintainable by EQUATOR staff or a future PhD student. I looked for tools that would be familiar to early career researchers. I considered DIY website builders (like Wix or Squarespace) but these services can be expensive. Most offer a 'drag and drop' building experience which, although easy to use, is a laborious way of uploading and formatting large amounts of content. Should EQUATOR want to change how an item is presented, they would have to manually edit each and every item for each and every reporting guideline. Additionally, our intended intervention changes required some custom functionality that wasn't offered by these services (e.g. integration with a DOI minting service, glossary definitions, discussion boards).

Although coding languages like `html` or `javascript` are used by many software developers to create websites, I felt that few early career researchers would be familiar with them. Ultimately I decided on `markdown`, an incredibly simple language that can be learnt in just a few minutes. Markdown is a plain text language. It uses asterisks, underscores, and carets to make text **\*\*bold\*\***, _\_italic\__, or ^\^superscript\^^. Headings, URLS, and references are similarly easy, and can be simplified further by using one of many readily available editors that make writing markdown feel like writing a Microsoft Word document.

Many researchers already use markdown to write reproducible manuscripts using tools like RStudio or Quarto. Quarto can also be used to turn markdown into html (a website), and can be further customised using languages commonly used in research, like Ruby or Python. Quarto requires no technical knowledge, is easy to learn, has great documentation, and is open source.

All code is on Github, a version control repository that researchers commonly use to store and share code and data. The website is served using Github Pages which is free and easily configurable.

I wanted EQUATOR to have ultimate control over the website, but I also wanted guideline developers to have selective access to their own guideline content but not to other guidelines. I have built the website such that each reporting guideline is stored within its own repository on github, accessible only to its developers. These guideline repositories are then "pulled in" to the main EQUATOR repository, so EQUATOR can double check changes that developers make before allowing them to go live on the site.

The website source code can be viewed at {{ < var website-github-repo-url >}} and the website can be viewed at {{ < var website-url > }}.

#### Iterative feedback and development

I sent drafts of the intervention to EQUATOR members. I used a tool Pastel to collect feedback. The first round elicited 39 comments from 3 team members. Some comments were supportive e.g., "I really like these two boxes!", others were suggested changes to wording or content.