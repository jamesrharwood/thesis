# Methods

<!-- # ASK: How should I report the meetings I help with EQUATOR first? Do I need to report that as a separate study? Or can I just describe it in the methods? If so, where? Maybe in data-collection methods? -->

## Qualitative approach and research paradigm

Our qualitative approach was active research, whereby participants and researchers generated ideas linked to theoretical concepts from the Behaviour Change Wheel (#REF). Our paradigm was post-positivist; we considered that ideas were "out there" and recognised that the opinions of the lead researchers may influence what they observe and conclude, and attempted to mitigate against this (see [_Techniques to Enhance Trustworthiness_](#sec-trust)<!--#TODO: add link-->). <!-- # ASK: Active research post-positivist? -->

## Researcher characteristics and reflexivity

CA teaches qualitative methods across a range of courses at Oxford University and, at the time of the study, was course director of Oxford Qualitative courses, through which JH received training in interviewing, focus groups, and analysing qualitative data.

Reflecting on our prior held opinions, all authors believed reporting to be important. With the exception of CA, all authors have a history of developing or advocating for reporting guidelines. JH, JdB and MS fed their ideas into the focus groups. GC had professional relationships with some of the invited guideline developers but was not involved in the focus groups which were facilitated by JH, a PhD student with no prior relationships. JH has a background in software development and user experience and as a novice researcher has experienced first hand frustration when trying to use reporting guidelines, and so came to this study with a slightly different perspective. As a relative outsider to the reporting guideline ecosystem, he hoped that participants would feel comfortable to speak candidly and non-defensively about the strengths and weaknesses of reporting guidelines. JH maintained reflexivity by reflecting on prior-held beliefs and maintaining a journal.  
<!-- # All agreed that reporting is important # ASK: Charlotte's prior held beliefs? 
# All agreed that RGs were not perfect -->

## Context

We ran focus groups over Zoom, with participants attending from their usual place of work of home.

<!-- # ASK: what else is relevant to context? Contextual factors? -->

## Sampling Strategy

We invited a purposive sample including the developers of the twenty most popular reporting guidelines, publishing professionals, and academics that have studied reporting guidelines. We asked participants to extend the invite to others they felt would be appropriate. Because the behaviour change wheel requires input from experts with insight into the intervention, we decided to elicit the opinions of authors in a separate study.

Our estimated sample size was guided by information power (#REF). (#TODO: check how others report information power). Our aim was narrow (#ASK: narrow aim), our sample was dense in that they knew a lot about how reporting guidance is disseminated but also showed variance in terms of which guidelines they work on. We were using the BCW and COM-B model as applied theories. We used open questioning and (#ASK: is there a term for eliciting views before revealing views of others?) to encourage strong dialogue. Our analysis strategy was (#ASK: analysis strategy). Given this, we felt our information power was sufficient to justify initially recruiting 15-20 participants. We stopped sampling once three focus groups contributed fewer than three new ideas as, by this point, we judged the benefit of continued sampling insufficient given time constraints.
<!-- # ASK: Is our unit of analysis the document? Ideas? Or the focus group? If the latter, shouldn't we have based sample size estimate on # of focus groups, not number of participants? -->
<!-- # ASK: is my definition of data saturation sufficient? -->

## Ethical Issues

The study was approved by the Medical Sciences Interdivisional Research Ethics Committee (R80414/RE001). Participants gave informed consent by completing an online form. Participant's edits to the co-produced file were anonymous. We recorded the audio of focus groups for JH to refer during analysis.

## Data Collection Methods and Materials

We chose focus groups to collect data so that participants with differing backgrounds could feed off each other's ideas. Focus groups took place between May-July 2022.

_Materials:_

Before the focus group began, participants were sent a link to shared, editable file (stored on the University Sharepoint) containing barriers that authors may face when using reporting guidelines. These barriers came from previous research; a thematic synthesis, review of quantitative surveys, observation of user behaviour on the EQUATOR Network, and personal experience of EQUATOR team members. Participants were allowed to add barriers to this list.

Each barrier was described on a new page. Each page had two columns: the barrier was described in the left hand column, and potential solutions in the right hand column. Text in this right hand column was formatted in white, making it invisible so that participants' thinking was not bias by previous participants.

Given that there were many barriers to discuss and many potential solutions, we felt it was not feasible to ask participants to start from a blank sheet. Therefore, JH worked with members of the EQUATOR Network to create an initial set of potential solutions by going over each barrier in depth, over # TODO: # meetings, lasting # TODO: # hours in total. Participants could then expand, edit, or add to this intial list. This method made it easier to achieve our aim of identifying as many ideas as possible. 

We did not aim to priotise ideas. Stakeholders could choose to evaluate ideas. The Behaviour Change Wheel recommends using the APEASE criteria for this (Acceptability, Practicability, Effectiveness, Affordability, Side-effects and Equity). 

An initial list of solutions was generated by members of the EQUATOR Network. 

_Focus Group introductions:_

At the start of each focus group JH explained that the goal was to brainstorm blue-sky ideas to barriers. He explained the evidence behind the barriers. Wanting guideline developers to think open-mindedly and not defensively, he explained that the barriers were in reference to reporting guidelines _in general_, and not necessarily a comment on _their_ guideline. He encouraged participants to think beyond the guideline documents themselves, and to consider all stakeholders and resources involved.

_Selecting which barriers to talk about:_

In the first focus group it became apparent that participants had a lot to say and that 2 hours was not sufficient to cover all the barriers, so JH added a table of contents to the top of the document. Participants spent the first few minutes reading through this table and marking the items that they wanted to talk about. JH then used these marks to select barriers for discussion, and also selected barriers himself - either because the barrier had been neglected by previous groups or because he expected participants' to have insight into it.

_Brainstorming solutions to chosen barriers:_

JH would explain the barrier in question and allow participants to ask questions. Once participants felt they understood it, JH asked them to spend a couple of minutes brainstorming solutions in silence. Participants then explained discussed their ideas as a group. Once all ideas had been discussed, JH would reveal the solutions identified by previous groups by changing the colour of the text in the right hand column from white (which made it invisible) to black. Participants then edited or extended the text until it reflected their thoughts. Ideas were never removed from the document, but participants could add concerns or disagreements if they wanted to.

Sometimes participants would get fixed on a particular solution and find it difficult to think of others. If this happened, JH would reassure participants that their fixated solution was documented and that it would be useful to think of alternatives. He would use intervention functions from the Behaviour Change Wheel to prompt discussion, saying something like "Could we create incentives to solve this problem?" or "What would make [behaviour] easier to do?".

_Between focus groups:_

After each focus group, JH would make notes on how the session went, make a copy of the co-produced document and turn the right hand column white again, ready for the next group.

_Units of study:_

The unit of study was #TODO: unit of study & move this section

_Feedback after the focus groups:_

_data-processing_

After the final focus group, JH imported the final co-produced file into NVivo and coded the barriers, ideas of how to address the barrier, and the stakeholders that could act on the idea. Each idea was a single intervention change, so some sentences contained multiple ideas. Sometimes the same idea appeared against multiple barriers, and most barriers had multiple associated ideas.

_data analysis_

JH then grouped ideas inductively in ways that felt cohesive and made the results easy for our intended audience (the reporting guideline community) to understand. For example, the ideas "ask authors to cite reporting guidelines" and "display citation metrics on reporting guideline resources" were placed together into a category about "Citations", even though they target different barriers (making guidelines discoverable reporting guidelines and convincing researchers that guidelines are trustworthy) and employ different intervention functions (education and persuasion). # TODO: ensure wording matches results.

JH then summarised each group of ideas and shared the draft summary with EQUATOR members and focus group participants, who were invited to ensure the summary reflected their ideas faithfully and add any further ideas. JH also invited feedback from guideline developers who had shown an interest in the study but had been unable to attend a focus group.

_Techniques to enhance trustworthiness:_ {#sec-trust}

JH maintained a reflective journal through the study and listed his own ideas before collecting data and throughout the study, whenever ideas struck him. He withheld these from participants so as to not bias their thinking, but added any ideas that _were not_ covered by others at the end. We kept a record of which ideas originated from EQUATOR, from the focus groups, from feedback comments, and from JH. Participants were asked to check and edit the results to ensure it reflected their thoughts. # ASK: Should I label each idea with its origin?
