[
  {
    "objectID": "data/eq_changes.html",
    "href": "data/eq_changes.html",
    "title": "Thesis",
    "section": "",
    "text": "website website-home website-guideline service guidance"
  },
  {
    "objectID": "data/intervention_components.html",
    "href": "data/intervention_components.html",
    "title": "Intervention Components",
    "section": "",
    "text": "barrier_ids: [what-are-rgs, when-to-use, benefits, importance, feel-not-my-job] intervention_fn_ids: [education]\nA clear, obvious description of:\n\nwhat reporting guidelines are\nhow and when to use them\nwhy authors should use them, including:\n\nwhat personal benefits to expect\nthe importance to others.\n\n\n\n\n\nDocuments in editable formats.\n\n\n\n\n\n\n\n\n\nRemove any specification of structure.\nBe explicit about flexibility where items can go.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbarrier_ids: [understanding, feel-patronized, believed-benefits, how-to-report, how-to-do] intervention_fn_ids: [enablement, persuasion]\n\nsolicit help,\nshare experiences,\nprovide feedback to guideline developers,\nand cultivate a feeling of inclusivity and community ownership.\n\n\n\n\n\n\n\nbarrier_ids: [feel-restricted, feel-transparent, believed-costs, what-are-rgs] intervention_fn_ids: [education]\n\nremove design assumptions\nadd null case\nbe explicit about agnosticism\n\n\n\n\nbarrier_ids: [feel-transparent, feel-patronized, believed-benefits, feel-not-my-job] intervention_fn_ids: persuasion\n\nUse language and design to communicate confidence and simplicity as opposed to judgement and complexity.\nEncourage explanation even when choices were unusual or sub-optimal.\nReassure authors that most research has limitations that can be addressed in Discussion sections.\nReassure authors that reporting guidelines are just guidelines.\nAvoid patronizing authors.\nConsider wording instructions directly at the intended user.JH\n\n\n\n\nbarrier_ids: [need-tools, need-right-time] intervention_fn_ids: [enablement]\n\ndiscussion points for planning research in the order decisions are made\nto-do lists for conducting research in the order data is collected\ntemplates for drafting\nchecklists for checking manuscripts that are easy to fill out, update, and cross-check\nresources for peer reviewers who wish to review reporting quality including:\n\nguidance specifically for peer reviewers.\ncommonly-used words that reviewers can search for to quickly find relevant text.JH\nsuggested text that peer reviewers can copy to request information\ntools to generate feedback reports"
  },
  {
    "objectID": "data/intervention_components.html#value-statement",
    "href": "data/intervention_components.html#value-statement",
    "title": "Intervention Components",
    "section": "",
    "text": "barrier_ids: [what-are-rgs, when-to-use, benefits, importance, feel-not-my-job] intervention_fn_ids: [education]\nA clear, obvious description of:\n\nwhat reporting guidelines are\nhow and when to use them\nwhy authors should use them, including:\n\nwhat personal benefits to expect\nthe importance to others."
  },
  {
    "objectID": "data/intervention_components.html#ready-to-use-tools",
    "href": "data/intervention_components.html#ready-to-use-tools",
    "title": "Intervention Components",
    "section": "",
    "text": "Documents in editable formats."
  },
  {
    "objectID": "data/intervention_components.html#structure-agnostic",
    "href": "data/intervention_components.html#structure-agnostic",
    "title": "Intervention Components",
    "section": "",
    "text": "Remove any specification of structure.\nBe explicit about flexibility where items can go."
  },
  {
    "objectID": "data/intervention_components.html#feedback-channels",
    "href": "data/intervention_components.html#feedback-channels",
    "title": "Intervention Components",
    "section": "",
    "text": "barrier_ids: [understanding, feel-patronized, believed-benefits, how-to-report, how-to-do] intervention_fn_ids: [enablement, persuasion]\n\nsolicit help,\nshare experiences,\nprovide feedback to guideline developers,\nand cultivate a feeling of inclusivity and community ownership."
  },
  {
    "objectID": "data/intervention_components.html#design-agnostic",
    "href": "data/intervention_components.html#design-agnostic",
    "title": "Intervention Components",
    "section": "",
    "text": "barrier_ids: [feel-restricted, feel-transparent, believed-costs, what-are-rgs] intervention_fn_ids: [education]\n\nremove design assumptions\nadd null case\nbe explicit about agnosticism"
  },
  {
    "objectID": "data/intervention_components.html#persuasion",
    "href": "data/intervention_components.html#persuasion",
    "title": "Intervention Components",
    "section": "",
    "text": "barrier_ids: [feel-transparent, feel-patronized, believed-benefits, feel-not-my-job] intervention_fn_ids: persuasion\n\nUse language and design to communicate confidence and simplicity as opposed to judgement and complexity.\nEncourage explanation even when choices were unusual or sub-optimal.\nReassure authors that most research has limitations that can be addressed in Discussion sections.\nReassure authors that reporting guidelines are just guidelines.\nAvoid patronizing authors.\nConsider wording instructions directly at the intended user.JH"
  },
  {
    "objectID": "data/intervention_components.html#tools-for-the-job",
    "href": "data/intervention_components.html#tools-for-the-job",
    "title": "Intervention Components",
    "section": "",
    "text": "barrier_ids: [need-tools, need-right-time] intervention_fn_ids: [enablement]\n\ndiscussion points for planning research in the order decisions are made\nto-do lists for conducting research in the order data is collected\ntemplates for drafting\nchecklists for checking manuscripts that are easy to fill out, update, and cross-check\nresources for peer reviewers who wish to review reporting quality including:\n\nguidance specifically for peer reviewers.\ncommonly-used words that reviewers can search for to quickly find relevant text.JH\nsuggested text that peer reviewers can copy to request information\ntools to generate feedback reports"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Refining reporting guidance dissemination: identifying and addressing barriers using behaviour change theory",
    "section": "",
    "text": "ASK Alterantively: Refining reporting guidelines using behaviour change theory.\nJames Harwood\nORCID: 0000-0003-3530-3231\nDoctor of Philosophy\nJanuary 2024\nNuffield Department of Orthopaedics, Rheumatology and Musculoskeletal Sciences\nThe University of Oxford\n\n\n\n\n\n\n\n\nHere is the working version of my thesis.\nYou’ll find chapters on the left, and can download Word files for each chapter on the right.\nChapters that I’ve not done (including this one) are this colour. Chapters in this colour are a works-in-progress.\nIf you see anything like “@something”, “?:something”, or “#TODO” then please ignore.\nSome of the tables and figures aren’t rendering nicely yet, especially in Word format. You can ignore that too.\n\n\n\n\n\nActual word counts will be lower as these numbers include tables and references for each chapter.\n\nintroduction: 5561 (inc. tables: 6340)\nreflexivity: 1317 (inc. tables: 1317)\nsynthesis: 7268 (inc. tables: 9521)\nsurvey-content: 2617 (inc. tables: 5044)\nwebsite-audit: 3732 (inc. tables: 4842)\nbcw: 1616 (inc. tables: 2203)\nworkshops: 4176 (inc. tables: 4731)\nfocus-groups: 8713 (inc. tables: 9130)\ndefining-content: 1378 (inc. tables: 6719)\nredesign: 3312 (inc. tables: 3312)\npilot: 8393 (inc. tables: 17787)\n\nTOTAL: 48083 (inc. tables: 70946)"
  },
  {
    "objectID": "index.html#word-count-tallies",
    "href": "index.html#word-count-tallies",
    "title": "Refining reporting guidance dissemination: identifying and addressing barriers using behaviour change theory",
    "section": "",
    "text": "Actual word counts will be lower as these numbers include tables and references for each chapter.\n\nintroduction: 5561 (inc. tables: 6340)\nreflexivity: 1317 (inc. tables: 1317)\nsynthesis: 7268 (inc. tables: 9521)\nsurvey-content: 2617 (inc. tables: 5044)\nwebsite-audit: 3732 (inc. tables: 4842)\nbcw: 1616 (inc. tables: 2203)\nworkshops: 4176 (inc. tables: 4731)\nfocus-groups: 8713 (inc. tables: 9130)\ndefining-content: 1378 (inc. tables: 6719)\nredesign: 3312 (inc. tables: 3312)\npilot: 8393 (inc. tables: 17787)\n\nTOTAL: 48083 (inc. tables: 70946)"
  },
  {
    "objectID": "chapters/gantt.html",
    "href": "chapters/gantt.html",
    "title": "Project Timeline",
    "section": "",
    "text": "gantt\n    dateFormat  YYYY-MM-DD\n    axisFormat %-m/%-y\n    title       \n    %% excludes    weekends\n    todayMarker on\n    %% (`excludes` accepts specific dates in YYYY-MM-DD format, days of the week (\"sunday\") or \"weekends\", but not the word \"weekdays\".)\n    section 3. Thematic synthesis\n    Article written    :milestone, done, 2022-12-01, 0d\n    Revise                       :active, ThemSyn, after BCWDraft, 2w\n\n    section 4. Survey review\n    Article written    :milestone, done, 2022-12-01, 0d\n    Revise                       :active, SurveyRev, after BCWDraft, 2w\n\n    section 5. Website review\n    Data analysed & described           :milestone, done, 2022-12-01, 0d\n    Revise                       :active, WebsiteRev, after BCWDraft, 2w\n\n    section 6. Audit\n    Data analysed                       :active, crit, AuditAnalysis, 2022-12-01, 2w\n    Write chapter                       :active, crit, AuditDraft, after AuditAnalysis, 3w\n\n    section 7. BCW \n    Draft   :done, BCWDraft, 2023-07-01, 17d\n\n    section 8. Workshops\n    Draft   :done, Workshops, 2023-06-14, 4w\n\n    section 9. Focus groups\n    Draft   :done, FocusGroups, 2023-01-04, 6w\n    Revision 1 :done, FocusGroupsRevision, 2023-05-14, 8w\n\n    section 4. Design intervention\n    Beta-version                        :milestone, done, DesignBeta, 2023-02-28, 0d\n    Draft                       :done, DesignDraft, 2023-03-25, 16w\n\n    section 5. Refining intervention\n    Collect data                        :active, PilotData, 2023-04-03, 2023-08-01\n    Analyse data                        :active, PilotAnalysis, after PilotData, 30d\n    Draft                       :active, PilotDraft, after PilotAnalysis, 4w     \n\n    section Introduction\n    Draft                      :active, IntroDraft, after ThemSyn, 30d\n\n    section Discussion\n    Draft                      :active, DiscussionDraft, after ThemSyn, 30d\n\n    section Reflexivity & Context\n    Draft                      :active, ReflexivityDraft, after ThemSyn, 30d\n\n    section Abstract, Appendices, etc., \n    Draft :active, OtherDraft, after DiscussionDraft, 2w\n\n    section Milestones\n    Complete draft     :milestone, active, CompleteDraft, after OtherDraft, \n    Revisions   :active, Revisions, after CompleteDraft, 16w\n    Submit thesis          :milestone, active, Submit, after Revisions, 0d"
  },
  {
    "objectID": "chapters/gantt.html#july-16th",
    "href": "chapters/gantt.html#july-16th",
    "title": "Project Timeline",
    "section": "",
    "text": "gantt\n    dateFormat  YYYY-MM-DD\n    axisFormat %-m/%-y\n    title       \n    %% excludes    weekends\n    todayMarker on\n    %% (`excludes` accepts specific dates in YYYY-MM-DD format, days of the week (\"sunday\") or \"weekends\", but not the word \"weekdays\".)\n    section 3. Thematic synthesis\n    Article written    :milestone, done, 2022-12-01, 0d\n    Revise                       :active, ThemSyn, after BCWDraft, 2w\n\n    section 4. Survey review\n    Article written    :milestone, done, 2022-12-01, 0d\n    Revise                       :active, SurveyRev, after BCWDraft, 2w\n\n    section 5. Website review\n    Data analysed & described           :milestone, done, 2022-12-01, 0d\n    Revise                       :active, WebsiteRev, after BCWDraft, 2w\n\n    section 6. Audit\n    Data analysed                       :active, crit, AuditAnalysis, 2022-12-01, 2w\n    Write chapter                       :active, crit, AuditDraft, after AuditAnalysis, 3w\n\n    section 7. BCW \n    Draft   :done, BCWDraft, 2023-07-01, 17d\n\n    section 8. Workshops\n    Draft   :done, Workshops, 2023-06-14, 4w\n\n    section 9. Focus groups\n    Draft   :done, FocusGroups, 2023-01-04, 6w\n    Revision 1 :done, FocusGroupsRevision, 2023-05-14, 8w\n\n    section 4. Design intervention\n    Beta-version                        :milestone, done, DesignBeta, 2023-02-28, 0d\n    Draft                       :done, DesignDraft, 2023-03-25, 16w\n\n    section 5. Refining intervention\n    Collect data                        :active, PilotData, 2023-04-03, 2023-08-01\n    Analyse data                        :active, PilotAnalysis, after PilotData, 30d\n    Draft                       :active, PilotDraft, after PilotAnalysis, 4w     \n\n    section Introduction\n    Draft                      :active, IntroDraft, after ThemSyn, 30d\n\n    section Discussion\n    Draft                      :active, DiscussionDraft, after ThemSyn, 30d\n\n    section Reflexivity & Context\n    Draft                      :active, ReflexivityDraft, after ThemSyn, 30d\n\n    section Abstract, Appendices, etc., \n    Draft :active, OtherDraft, after DiscussionDraft, 2w\n\n    section Milestones\n    Complete draft     :milestone, active, CompleteDraft, after OtherDraft, \n    Revisions   :active, Revisions, after CompleteDraft, 16w\n    Submit thesis          :milestone, active, Submit, after Revisions, 0d"
  },
  {
    "objectID": "chapters/gantt.html#feb-1st",
    "href": "chapters/gantt.html#feb-1st",
    "title": "Project Timeline",
    "section": "Feb 1st",
    "text": "Feb 1st\n\n\n\n\ngantt\n    dateFormat  YYYY-MM-DD\n    axisFormat %-m/%-y\n    title       \n    %% excludes    weekends\n    todayMarker off\n    %% (`excludes` accepts specific dates in YYYY-MM-DD format, days of the week (\"sunday\") or \"weekends\", but not the word \"weekdays\".)\n    section 2a. Thematic synthesis\n    Data analysed, article submitted    :milestone, done, 2022-12-01, 0d\n    Write chapter                       :active, ThemSyn, after DesignDraft, 3w\n\n    section 2b. Survey review\n    Data analysed, article submitted    :milestone, done, 2022-12-01, 0d\n    Write chapter                       :active, SurveyRev, after ThemSyn, 3w\n\n    section 2c. Website review\n    Data analysed & described           :milestone, done, 2022-12-01, 0d\n    Write chapter                       :active, WebsiteRev, after SurveyRev, 3w\n\n    section 2d. Audit\n    Data analysed                       :active, crit, AuditAnalysis, 2022-12-01, 2w\n    Write chapter                       :active, crit, AuditDraft, after AuditAnalysis, 3w\n\n    section 3. Focus groups\n    Methods & results written           :milestone, done, 2022-12-01, 0d\n    Write Chapter                       :active, crit, FocusGroups, 2023-01-05, 4w\n\n    section 4. Design intervention\n    Beta-version                        :active, crit, DesignBeta, 2022-12-01, 13w\n    Write chapter                       :active, DesignDraft, after FocusGroups, 4w\n\n    section 5. Refining intervention\n    Protocol                            :milestone, done, 2022-12-01, 0d\n    Collect data                        :active, PilotData, after DesignBeta, 12w\n    Analyse data                        :active, PilotAnalysis, after PilotData, 21d\n    Write chapter                       :active, PilotDraft, after PilotAnalysis, 4w     \n\n    section Intro & Discussion\n    Write chapters                      :active, after PilotDraft, 42d\n\n    section Milestones\n    Outline                             :milestone, done,   2022-11-01, 0d\n    Confirmation                        :milestone, crit, 2023-01-23, 0d\n    Submit thesis                       :milestone, active, 2023-09-01, 0d"
  },
  {
    "objectID": "chapters/gantt.html#dec-1st",
    "href": "chapters/gantt.html#dec-1st",
    "title": "Project Timeline",
    "section": "Dec 1st",
    "text": "Dec 1st\n\n\n\n\ngantt\n    dateFormat  YYYY-MM-DD\n    axisFormat %-m/%-y\n    title       \n    %% excludes    weekends\n    todayMarker off\n    %% (`excludes` accepts specific dates in YYYY-MM-DD format, days of the week (\"sunday\") or \"weekends\", but not the word \"weekdays\".)\n    section 2a. Thematic synthesis\n    Data analysed, article submitted    :milestone, done, 2022-12-01, 0d\n    Write chapter                       :active, ThemSyn, after DesignDraft, 3w\n\n    section 2b. Survey review\n    Data analysed, article submitted    :milestone, done, 2022-12-01, 0d\n    Write chapter                       :active, SurveyRev, after ThemSyn, 3w\n\n    section 2c. Website review\n    Data analysed & described           :milestone, done, 2022-12-01, 0d\n    Write chapter                       :active, WebsiteRev, after SurveyRev, 3w\n\n    section 2d. Audit\n    Data analysed                       :active, AuditAnalysis, 2022-12-01, 2w\n    Write chapter                       :active, AuditDraft, after AuditAnalysis, 3w\n\n    section 3. Focus groups\n    Methods & results written           :milestone, done, 2022-12-01, 0d\n    Write Chapter                       :active, FocusGroups, 2023-01-05, 3w\n\n    section 4. Design intervention\n    Beta-version developed              :active, DesignBeta, 2022-12-01, 5w\n    Write chapter                       :active, DesignDraft, after FocusGroups, 4w\n\n    section 5. Refining intervention\n    Protocol                            :milestone, done, 2022-12-01, 0d\n    Collect data                        :active, PilotData, 2023-01-05, 12w\n    Analyse data                        :active, PilotAnalysis, after PilotData, 21d\n    Write chapter                       :active, PilotDraft, after PilotAnalysis, 4w     \n\n    section Intro & Discussion\n    Write chapters                      :active, after PilotDraft, 42d\n\n    section Milestones\n    Outline                             :milestone, done,   2022-11-01, 0d\n    Confirmation                        :milestone, crit, 2023-01-23, 0d\n    Submit thesis                       :milestone, active, 2023-09-01, 0d"
  },
  {
    "objectID": "chapters/gantt.html#original",
    "href": "chapters/gantt.html#original",
    "title": "Project Timeline",
    "section": "Original",
    "text": "Original\n\n\n\n\ngantt\n    dateFormat  YYYY-MM-DD\n    axisFormat %-m/%-y\n    title       \n    %% excludes    weekends\n    todayMarker off\n    %% (`excludes` accepts specific dates in YYYY-MM-DD format, days of the week (\"sunday\") or \"weekends\", but not the word \"weekdays\".)\n    section 2a. Thematic synthesis\n    Data analysed, article submitted    :milestone, done, 2022-12-01, 0d\n    Write chapter                       :active, ThemSyn, after s4, 3w\n\n    section 2b. Survey review\n    Data analysed, article submitted    :milestone, done, 2022-12-01, 0d\n    Write chapter                       :active, SurveyRev, after ThemSyn, 3w\n\n    section 2c. Website review\n    Data analysed & described           :milestone, done, 2022-12-01, 0d\n    Write chapter                       :active, WebsiteRev, after SurveyRev, 3w\n\n    section 2d. Audit\n    Data analysed                       :milestone, done, 2022-12-01, 0d\n    Write chapter                       :active, s2d, 2022-12-01, 3w\n\n    section 3. Focus groups\n    Methods & results written           :milestone, done, 2022-12-01, 0d\n    Write Chapter                       :active, FocusGroups, 2023-01-05, 3w\n\n    section 4. Design intervention\n    Beta-version developed              :milestone, done, 2022-12-01, 0d\n    Write chapter                       :active, s4, after FocusGroups, 4w\n\n    section 5. Refining intervention\n    Protocol                            :milestone, done, 2022-12-01, 0d\n    Collect data                        :active, PilotData, 2023-01-05, 12w\n    Analyse data                        :active, PilotAnalysis, after PilotData, 21d\n    Write chapter                       :active, PilotDraft, after PilotAnalysis, 4w     \n\n    section Intro & Discussion\n    Write chapters                      :active, after PilotDraft, 42d\n\n    section Milestones\n    Outline                             :milestone, done,   2022-11-01, 0d\n    Confirmation                        :milestone, crit, 2023-01-23, 0d\n    Submit thesis                       :milestone, active, 2023-09-01, 0d"
  },
  {
    "objectID": "chapters/9_defining_content/index.html",
    "href": "chapters/9_defining_content/index.html",
    "title": "Defining Intervention Content",
    "section": "",
    "text": "In this chapter I describe how I brought together the outputs of all previous research chapters to create a intervention planning table containing intervention components.\nIn chapters 3 I described how I identified barriers influencing whether authors adhere to reporting guidelines. In chapter 7 I described how ran workshops with EQUATOR staff to define our target behaviour, prioritise intervention functions and policy categories, and how we decided to prioritise redesigning reporting guidelines and the EQUATOR Network’s website hope page. In chapter 8 I described how I asked guideline developers, publishers, and other stakeholders to build upon our workshop outputs by brainstorming solutions to barriers.\nConsequently, at this point in my thesis I had multiple lists; a list of barriers, lists of intervention options, and a list of ideas. Not all ideas were relevant to the the EQUATOR Network or aligned with our prioritised intervention options, and although ideas referenced barriers, the links between ideas, barriers, intervention functions, and behaviour change techniques were not fully explicit.\nWhat remained was to selected ideas to implement and turn them into intervention components, defined as [1]:\n\na designed element that uses one or more behaviour change technique,\ntheorized to work through one or more intervention functions\nto target one or more behavioural drivers.\n\nMichie et al. [1] describe this step as “identify[ing] intervention content in terms of which [behaviour change techniques] best serve intervention functions”, where a behaviour change technique is “an active component” that is “observable, replicable, [and] irreducible”. Defining intervention content in this way is useful because it helps intervention developers understand why the component added (or removed), how it is theorised to work and, therefore, how its effectiveness may be tested.\nIn this chapter I describe how I selected ideas to implement, and how I defined them as intervention components."
  },
  {
    "objectID": "chapters/9_defining_content/index.html#introduction",
    "href": "chapters/9_defining_content/index.html#introduction",
    "title": "Defining Intervention Content",
    "section": "",
    "text": "In this chapter I describe how I brought together the outputs of all previous research chapters to create a intervention planning table containing intervention components.\nIn chapters 3 I described how I identified barriers influencing whether authors adhere to reporting guidelines. In chapter 7 I described how ran workshops with EQUATOR staff to define our target behaviour, prioritise intervention functions and policy categories, and how we decided to prioritise redesigning reporting guidelines and the EQUATOR Network’s website hope page. In chapter 8 I described how I asked guideline developers, publishers, and other stakeholders to build upon our workshop outputs by brainstorming solutions to barriers.\nConsequently, at this point in my thesis I had multiple lists; a list of barriers, lists of intervention options, and a list of ideas. Not all ideas were relevant to the the EQUATOR Network or aligned with our prioritised intervention options, and although ideas referenced barriers, the links between ideas, barriers, intervention functions, and behaviour change techniques were not fully explicit.\nWhat remained was to selected ideas to implement and turn them into intervention components, defined as [1]:\n\na designed element that uses one or more behaviour change technique,\ntheorized to work through one or more intervention functions\nto target one or more behavioural drivers.\n\nMichie et al. [1] describe this step as “identify[ing] intervention content in terms of which [behaviour change techniques] best serve intervention functions”, where a behaviour change technique is “an active component” that is “observable, replicable, [and] irreducible”. Defining intervention content in this way is useful because it helps intervention developers understand why the component added (or removed), how it is theorised to work and, therefore, how its effectiveness may be tested.\nIn this chapter I describe how I selected ideas to implement, and how I defined them as intervention components."
  },
  {
    "objectID": "chapters/9_defining_content/index.html#methods",
    "href": "chapters/9_defining_content/index.html#methods",
    "title": "Defining Intervention Content",
    "section": "Methods",
    "text": "Methods\nI started with a list of 28 ideas. These were the 128 ungrouped ideas originating from my focus groups (chapter 8), plus 136 ideas I had whilst I was writing my focus group chapter (see Appendix C).\nI labelled each idea with:\n\nThe barrier(s) it addresses (derived from chapters 3, 4, 5, and 7).\nThe behavioural driver it targets (from step four of the workshops described in chapter 7).\nThe intervention functions it uses.\nThe behaviour change techniques it uses, selected from Michie et al.’s taxonomy [2].\n\nI asked a colleague with experience in behaviour change taxonomies to double check my coding, and we resolved disagreements through discussion.\nFor each idea, I then considered:\n\nWhether it was implementable by redesigning guidelines or by modifying the EQUATOR website. If not, I removed it from the list.\nWhether its intervention function aligned with the intervention options EQUATOR and I had prioritised during our workshops (see workshop steps 5 & 6 in chapter 7). If not, I removed it from the list.\n\nTo give structure to this list, I grouped intervention components according to the sub-behaviours they targeted: 1) engaging with guidance and 2) applying it (see section on specifying the target behaviour in chapter 7).\nTo provide extra context and clarity, I described each component in more detail and how it compares to the status quo. In doing these comparisons I made generalisations about how popular reporting guidelines are written and disseminated by drawing on evidence from my qualitative synthesis (3), looking at how the EQUATOR website is currently, referring back to workshop discussions (from chapter 7).\nComponents were abstract at this stage, described only as words. I had not built anything when I made the list, but to make the components easier to understand I have referenced figures from the next chapter, where I describe how I turned these hypothetical intervention components into a working prototype."
  },
  {
    "objectID": "chapters/9_defining_content/index.html#results",
    "href": "chapters/9_defining_content/index.html#results",
    "title": "Defining Intervention Content",
    "section": "Results",
    "text": "Results\nFrom the initial list of 28 I identified 63 intervention components that could be implemented by redesigning guidelines and by improving the EQUATOR Network home page. Together, these components use 18 behaviour change techniques and 6 intervention functions to target 32 barriers.\nSee table Table 1 for all intervention components, labelled with the barriers they address, the intervention functions they work through, and the behaviour change technique they employ."
  },
  {
    "objectID": "chapters/9_defining_content/index.html#discussion",
    "href": "chapters/9_defining_content/index.html#discussion",
    "title": "Defining Intervention Content",
    "section": "Discussion",
    "text": "Discussion\nIn this chapter I have demonstrated how I have used behaviour change theory to create a list of intervention components. This list brings together the outputs of all of my previous thesis chapters and links them together; each component comes from an idea (chapter 8), addresses one or more barriers (chapters 3), using intervention options ranked favourably by EQUATOR staff (chapter 7). Hence the table presented here crystallises the preceding chapters.\nThe table includes components derived from all ideas pursuant to the intervention options EQUATOR staff and I prioritised in our workshops. Because I filtered out ideas not in line with these priorities, the list is not exhaustive. Stakeholders using the same set of initial ideas would create different components because of their different priorities and implementation opportunities. For example, a funder may have generated more components related to reporting guidance for applications or protocols, or relating to their application criteria and submission systems. Stakeholders with the power to grant approval (for funding/ethics/publication) may have components related to enforcement. Consequently, although I hope this list will help readers understand the intervention changes I have made and why I have made them, I would encourage other intervention designers to go through this process themselves instead of using this list verbatim.\nThroughout the table I have drawn comparisons between the proposed components and existing reality. These comparisons are vague; I use terms like “some” or “rarely”. Where possible, I refer to images or examples. Some examples came from my qualitative synthesis (chapter 3), others came arose organically in my workshops (chapter 7) when participants shared long-standing frustrations with the website or guidelines. Other times, after discussing a barrier or idea, we would look at a few guidelines to see how things are done currently. So this comparison was ad-hoc, and I have included them only to provide context to the proposed changes. I considered making this comparison formal, by systematically coding BCTs employed by EQUATOR’s website and popular guidelines or by counting how frequently intervention components appear currently. Ultimately, I decided this would not be helpful nor practical. With so many components and so many guidelines, this would have taken time and I decided instead to prioritise building and testing a prototype. Secondly, this audit would not have dramatically influenced the intervention components we designed, but would merely quantify how different my proposed intervention is to the current set-up. I believed my prototype would be markedly different from the status quo, and so I doubted the benefit of quantifying a difference I expected would be obvious. Should need arise in the future, an audit like this would still be possible.\n\nConclusions\nBy linking components with barriers, functions, and behaviour change techniques, I have justified components using evidence and described how they are theorized to work. This table will help other intervention developers and stakeholders understand what changes I have made and why. The next two chapters make use of this table. In chapter I describe how I referred to this table when designing a study to refine components. But first, in the next chapter, I describe how I used this table to redesign a reporting guideline and the EQUATOR Network’s home page."
  },
  {
    "objectID": "chapters/9_defining_content/index.html#tables",
    "href": "chapters/9_defining_content/index.html#tables",
    "title": "Defining Intervention Content",
    "section": "Tables",
    "text": "Tables\n\n\n\n\nTable 1: Intervention Planning Table. Intervention components labelled with the behaviour change techniques and intervention functions they employ, and grouped according to the key behaviours, barriers, and behavioural drivers that they aim to target. Where possible, examples demonstrate how components were (or were not) used originally (Before) and, how they are included within the redesigned intervention (Now).\n\n\n\n\n\n\n\n\n\nIntervention Ingredient\nBehaviour Change Technique\nIntervention Function\nBefore\nNow\n\n\n\n\nKey Behaviour: Engage with (read) appropriate reporting guidance as early as possible\n\n\n\n\n\n\nTargeted barrier: Researchers may not know what reporting guidelines are\nBehavioural driver: Capability\n\n\n\n\n\n\nDescribe what reporting guidelines are where they are first encountered\nInstruction on how to perform the behavior\nEducation\nNo prominent description of what reporting guidelines are on EQUATOR home page or in reporting guidelines resources.\nExample: See Figure 3, Figure 5, Figure 7, Figure 8\nProminent definition on home page and guideline page.\nExample: See Figure 9, Figure 10\n\n\nClarify what tasks (e.g., writing, designing, or appraising research) guidelines and resources are designed for\nInstruction on how to perform the behavior\nEducation\nNo clear instruction on what tasks reporting guidelines or their resources can and cannot be used for.\nExample: See Figure 3, Figure 4, Figure 5\nClear instruction and differentiation of resources\nExample: See Figure 9, Figure 10\n\n\nTargeted barrier: Researchers may not know what reporting guidelines exist\nBehavioural driver: Capability\n\n\n\n\n\n\nInstruct authors to cite reporting guidelines so readers may learn about them\nInstruction on how to perform the behavior\nEducation\nNo consistent instruction to cite reporting guidelines\nConsistent instruction to cite reporting guidelines\n\n\nDecision tools for discovering appropriate resources*\nInstruction on how to perform the behaviour\nEnablement\nWe previously made a “reporting guideline wizard” but it was difficult to find.\nExample: see Figure 3\nNot included yet\n\n\nCollections of related reporting guidelines*\nAdding objects to the environment\nEnvironmental Restructuring\nCollections exist on EQUATOR site but are difficult to find.\nNot included yet\n\n\nLinks between related guidelines\nRestructuring the physical environment\nEnvironmental Restructuring\nGuideline publications may cite guidelines published previously, but these can be buried in text and are not updated. EQUATOR website guideline pages feature links to extensions, but these may be hard to find. Checklists do not link to related resources.\nExample: See Figure 4, Figure 5\nGuidelines prominently link to other relevant guidelines and explain when they should be used.\nExample: See Figure 10\n\n\nEmbed reporting guidelines that “fit together”*\nInstruction on how to perform the behavior\nEnablement\nChecklists and their extensions are published separately. The best example of modular guidance is perhaps the JARS guidelines, but even these are published as separate documents.\nNo change\n\n\nTargeted barrier: Guidance may be difficult to find\nBehavioural driver: Opportunity\n\n\n\n\n\n\nCentralised hosting\nRestructuring the physical environment\nEnablement\nEQUATOR maintains a database of reporting guideline meta-data, but the guidance and checklists were published and hosted in different locations and in different ways.\nA core set of frequently accessed guidelines are now presented on a single website.\n\n\nSearch function on website\nRestructuring the physical environment\nEnablement\nEQUATOR’s search function was difficult to find.\nExample: See Figure 3\nSearch function is easier to find as a recognizable icon in the navigation bar of every page. The home page includes additional ways to access search functionality.\nExample: See Figure 9\n\n\nSearch Engine Optimization\nRestructuring the physical environment\nEnablement\nEQUATOR’s website did not make use of some commonly used search optimization heuristics. It ranked well for guideline acronyms (like STROBE) but not for general terms that naive authors may use, like “observational epidemiology” or “how to write-up research”. The site wasn’t optimized for viewing on mobile devices, which will also harm google search rankings.\nExample: (Not visible)\nThe site has additional meta-data. Each reporting guideline page has its own meta-data. The site is optimized for mobiles.\nExample: (Not visible)\n\n\nPermanent document object identifiers (DOIs)*\nRestructuring the physical environment\nEnablement\nAlthough guideline publications have DOIs, tools (commonly hosted on guideline developer’s websites) do not. EQUATOR’s website does not use document object identifiers. If resources move (e.g. a website is reorganised or depreciated) then links can “die”.\nNo change\n\n\nTargeted barrier: reporting guidelines may be difficult to access\nBehavioural driver: Opportunity\n\n\n\n\n\n\nEnsure guidelines and tools are open access*\nRestructuring the physical environment\nEnablement\nSome guidelines are published behind paywalls\nNo change\n\n\nTargeted barrier: Researchers may not know whether a reporting guideline applies to them\nBehavioural driver: Capability\n\n\n\n\n\n\nDescribe the scope of a reporting guideline at the top of every resource\nInstruction on how to perform the behavior\nEducation\nSome reporting guidelines may describe their scope within a publication. Others might not, or may only describe their scope broadly without fully explaining the design assumptions within the guidance. Guideline publications rarely explain circumstances where the guidance should not be used. Checklists rarely define intended scope beyond the title of the guideline.\nThe intended scope of a guideline is clearly & prominently described. This definition includes contexts in which the guidance should not be used.\nExample: See Figure 10\n\n\nTargeted barrier: Researchers may not know what reporting guideline is their best fit\nBehavioural driver: Capability\n\n\n\n\n\n\nUse if-then rules to direct authors to more appropriate and up-to-date guidance when available\nInstruction on how to perform the behavior\nEducation\nReporting guidelines do not consistently point authors towards related resources that might be better fits. Guidelines are not updated as-and-when other guidelines become available.\nReporting guidelines clearly and consistently point authors to more appropriate guidance when appropriate, using if-then rules. These links can be updated any time.\nExample: See Figure 10\n\n\nExplicitly state when no better guidance exists for a use case\nInstruction on how to perform the behavior\nEducation\nReporting guidelines rarely explain what to do when no better guidance exists for a use case.\nReporting guidelines warn authors when no better guidance exists for a use case, and how the current guidance can be adapted instead\nExample: See Figure 10\n\n\nTargeted barrier: Researchers may not understand the language\nBehavioural driver: Opportunity\n\n\n\n\n\n\nProvide translations\nInstruction on how to perform the behavior\nEnablement\nSome guidelines have been translated, but many haven’t. Links to translations are present on reporting guideline database pages but these links may not be easy to find. The EQUATOR website has an automatic translation tool which will translate content on web pages, but this doesn’t cover the guidance itself.\nExample: See Figure 4\nTranslations are prominently listed above the guidance\nExample: See Figure 10\n\n\nTargeted barrier: Researchers may expect the costs to outweigh benefits\nBehavioural driver: Motivation\n\n\n\n\n\n\nMake guidance appear shorter by removing superfluous information, hiding optional content, splitting long guidelines, using concise language, and separating design advice\nRestructuring the physical environment\nEnvironmental restructuring\nreporting guideline publications may include lengthy explanation of development, verbose language, and can be bloated by design advice\nExample: See Figure 6\nSRQR has been edited. The only text presented immediately is instruction on what the author needs to describe. Additional information is hidden at first and can be expanded. Text is shortened through editing and by using active voice. In the case of SRQR, this reduced the text length by 60%.\nExample: See Figure 11\n\n\nCater to different kinds of user (readers vs dippers) by structuring guidance with headings, itemisation, hyperlinking to particular sections, and with optional content\nRestructuring the physical environment\nEnvironmental restructuring\nBesides being split into items, reporting guidance is largely unstructured and different items can be organised in different ways. Checklist items do not link to items within an elaboration document.\nExample: See Figure 6\nSRQR items are structured consistently, making information easier to find. Itemisation is used consistently, content is hyperlinked when useful.\nExample: See Figure 11\n\n\nInclude testimonials from researchers who were nervous about being punished for reporting transparently\nDemonstration of the behavior\nPersuasion\nNo such testimonials exist\nQuotes included alongside guideline\nExample: see Figure 11\n\n\nDecrease fear of judgement by making reporting guidelines design agnostic\nRemove aversive stimulus\nCoercion (Removal of)\nReporting guidelines may conflate reporting advice with design advice or design assumptions. The justification for why an item is important to describe is frequently presented in terms of good and bad design.\nSRQR explicitly states that it makes no assumptions about design. Inadvertent design assumptions were edited.\n\n\nRemove branding and messaging that may invoke feelings of judgement, complexity, or administrative red-tape\nRemove aversive stimulus\nCoercion (Removal of)\nEQUATOR’s website looked cluttered and visually unappealing. Guidance published in articles can look unappealing and dense. When justifying why authors should use reporting guidelines, guideline developers frequently referenced research waste, (lack of) transparency, bias, and poor design.\nExample: See Figure 3\nA clean, simple interface for the home page and guidance pages. Text makes less use of to judgemental phrases and fewer references to the negative consequences of poor reporting.\nExample: See Figure 9\n\n\nReassure that all research has limitations to encourage explanation over perfect design\nSocial support (unspecified)\nPersuasion\nFew guidelines would include this kind of reassurance\nExample: See Figure 5\nThis reassurance appears on the home page and all guidance pages\nExample: See Figure 10\n\n\nTargeted barrier: Researchers may feel that checking reporting is someone else’s job.\nBehavioural driver: Motivation\n\n\n\n\n\n\nAddress communications to authors\nInstruction on how to perform a behaviour\nPersuasion\nIt wasn’t immediately clear whether the EQUATOR guideline website was aimed at authors, editors, reviewers, or all.\nAll resources and website copy are directed predominantly at authors.\n\n\nCommunicate why reporting is primarily the responsibility of the author*\nInstruction on how to perform a behaviour\nEducation\nBecause it wasn’t clear how reporting guidelines and checklists should be used, they (especially checklists) could appear as administrative tasks that should be the responsibility of the editor or reviewer.\nClear explanation of why guidelines and tools should be used by authors primarily, although can also be used by others.\n\n\nTargeted barrier: Researchers may not consider writing as reporting\nBehavioural driver: Motivation\n\n\n\n\n\n\nEducate authors about writing as a process\nInstruction on how to perform a behaviour\nEducation\nMany researchers don’t get trained on writing as a process, they just do it. EQUATOR provided education about how to write but this wasn’t advertised on guidelines.\nSome SRQR items now link to relevant EQUATOR materials and courses.\nExample: See Figure 11\n\n\nKey Behaviour: Apply reporting guidance to writing\n\n\n\n\n\n\nTargeted barrier: Researchers may not know what resources exist for a reporting guideline\nBehavioural driver: Capability\n\n\n\n\n\n\nlink all resources to each other\nRestructuring the physical environment\nEnvironmental restructuring\nReporting guideline development articles, example and elaboration articles, and checklist files may not link to each other.\nExample: See Figure 7\nGuidance links to all tools and development article\nExample: See Figure 10\n\n\nTargeted barrier: Researchers may not know what benefits to expect\nBehavioural driver: Capability\n\n\n\n\n\n\nDescribe personal benefits and benefits to others where reporting guidelines are introduced (home page, on resources, in communications)\nInformation about emotional consequences, Information about others’ approval, Information about social and environmental consequences\nEducation\nBenefits are not prominently described on EQUATOR’s home page, nor within the guideline publications. Benefits that are described may by hard to find, and often focus on hypothetical benefits to the research community, but not personal benefits to the author.\nExample: See Figure 3\nBenefits are prominently and consistently displayed across the home page and guidance pages. Descriptions prioritize personal benefits to the authors above hypothetical benefits to others.\nExample: See Figure 10\n\n\nTargeted barrier: Researchers may not believe stated benefits\nBehavioural driver: Motivation\n\n\n\n\n\n\nGather and communicate evidence for benefits\nInformation about emotional consequences, Information about others’ approval, Information about social and environmental consequences\nPersuasion\nBenefits often presented without evidence (if at all)\nDummy quotes provides evidence for experienced benefits.\nExample: See Figure 11\n\n\nInclude design, features, and language to foster trust\nCredible source, Social comparison\nPersuasion\nWebsite design looked amateur. Citation metrics are available for guideline publications, but are not displayed on the EQUATOR website or within the guidance publications or checklists themselves. Guidelines were often preceeded by lengthy explanations of development.\nExample: See Figure 4\nProfessional design. EQUATOR’s Logo remains prominent. Citation metrics are presented at the top the reporting guidance. Information about who developed the guidelines, how they developed it, and why the guidance is credible is still provided, and easily findable from the top of the guidance.\nExample: See Figure 10\n\n\nCreate spaces for authors to discuss reporting guidelines with others\nSocial comparison, Credible source, Adding objects to the environment\nPersuasion\nThere were no official on- or offline spaces for authors to discuss guidelines.\nEach reporting item has its own discussion board.\nExample: See Figure 12\n\n\nUse tone of voice and design to communicate personal benefits; confidence and simplicity\nFraming/reframing\nPersuasion\nGuidance text made little use of a reassuring tone or words. The EQUATOR website and guideline articles looked dense and complex.\nExample: See Figure 3 and Figure 5\nA clean, simple interface for the home page and guidance pages. Text uses phrases like “confidence”, “quick”, “maximum impact”.\nExample: See Figure 9 and Figure 10\n\n\nTargeted barrier: Researchers may not care about the benefits of using a reporting guideline\nBehavioural driver: Motivation\n\n\n\n\n\n\nInclude testimonials from research users who benefit from complete reporting\nSalience of consequence\nPersuasion\nTestimonials not included in reporting guidelines.\nSRQR includes dummy testimonials and quotes from research users\nExample: See Figure 11\n\n\nTargeted barrier: Researchers may misunderstand\nBehavioural driver: Capability\n\n\n\n\n\n\nUse plain language\nInstruction on how to perform the behavior\nEnablement\nAlthough developers aspire to write clearly, authors may misinterpret guidance or fail to understand it completely.\nSRQR is edited to use plainer language.\n\n\nDefine key terms\nInstruction on how to perform the behavior\nEducation\nFew guidelines came with a glossary. Some key terms may be defined within the guideline text. Including definitions this way makes them hard to find and elongates the guidance.\nSRQR now has a glossary, and text is marked-up with definitions that appear upon click.\nExample: See Figure 11\n\n\nUse consistent terms*\nInstruction on how to perform the behavior\nEnablement\nGuidelines may use different terms to refer to the same thing (or the converse - use the same term to refer to different things). A single guideline may do this too.\nSRQR uses consistent terms across items.\n\n\nProvide translations\nSee above\nNone\nSee above\n\n\n\nCreate spaces for authors to discuss reporting guidelines with others (see above)\nSee above\nNone\n\n\n\n\nTargeted barrier: Researchers may not know why items are important\nBehavioural driver: Capability\n\n\n\n\n\n\nFor each item, explain why the information is important and to whom (not just what constitutes “good” design)\nInformation about social and environmental consequences\nEducation\nSometimes there was no explanation as to why an item should be reported. Other times the justification would be about why a particular design choice was important.\nExample: See Figure 6\nInformation added when necessary\nExample: See Figure 11\n\n\nExplain importance of complete reporting to the scientific community\nInformation about social and environmental consequences\nEducation\nEQUATOR and most reporting guidelines do this already\nContinue to do this\n\n\nTargeted barrier: Researchers may not know how to do an item\nBehavioural driver: Capability\n\n\n\n\n\n\nProvide links to other resources that explain how an item can be done\nInstruction on how to perform the behavior\nEducation\nSome reporting guideline publications (or elaboration articles) include instruction in text but many don’t. SRQR did not.\nLinks included when relevant.\nExample: See Figure 11\n\n\nTargeted barrier: Researchers may not know how to report an item in practice\nBehavioural driver: Capability\n\n\n\n\n\n\nFor each item, provide clear instruction of what needs to be described\nInstruction on how to perform the behavior\nEducation\nWriting instructions are often mixed in with other explanation and context.\nExample: See Figure 6\nWriting instruction occurs first for each item.\nExample: See Figure 11\n\n\nFor each item, provide examples of reporting in different contexts\nDemonstration of the behavior\nModelling\nNot all reporting guidelines provide examples. Examples may not cover different contexts.\nExample: See @ fig-item-b4\nSRQR already had some examples. No more examples added\nExample: See Figure 11\n\n\nCreate spaces for authors to discuss reporting guidelines with others (see above)\nSee above\nNone\n\n\n\n\nTargeted barrier: Researchers may not know what to write when they cannot report an item\nBehavioural driver: Capability\n\n\n\n\n\n\nProvide clear instruction of what needs to be described when an item was not done, could not be done, or does not apply\nInstruction on how to perform the behavior\nEducation\nRarely instructed\nExample: See Figure 6\nInstructed where relevant\nExample: See Figure 11\n\n\nProvide examples of reporting “imperfect” items*\nDemonstration of the behavior\nModelling\nExamples not provided\nNo changes made\n\n\nTargeted barrier: Researchers have limited time\nBehavioural driver: Opportunity\n\n\n\n\n\n\nEnsure all resources and tools (e.g., checklists and templates) are in ready-to-use formats*\nAdding objects to the environment\nEnablement\nSome checklists not in immediately usable formats e.g., PDFs\nNo changes made\n\n\nStructure guideline items to make them quicker to digest\nRestructuring the physical environment\nEnablement\nE&E documents not structured below the item level\nExample: See Figure 6\nItems have consistent structure and use bullet points consistently\nExample: See Figure 11\n\n\nTell authors how long the guidance will take to read\nInstruction on how to perform the behavior\nEducation\nEstimated reading time not given\nEstimated reading time given\nExample: See Figure 10\n\n\nTell authors how long guidance will take to apply*\nInstruction on how to perform the behavior\nEducation\nNo advice given\nNo changes made\nExample: No changes made\n\n\nTargeted barrier: Researchers may not know when reporting guidelines should be used\nBehavioural driver: Capability\n\n\n\n\n\n\nTell authors when to use reporting guidelines, or that reporting guidelines are best used as early as possible\nInstruction on how to perform the behavior\nEducation\nRarely stated prominently\nStated prominently\nExample: See Figure 10\n\n\nClarify what tasks (e.g., writing, designing, or appraising research) guidelines and resources are designed for (see above)\nSee above\nNone\n\n\n\n\nTargeted barrier: Researchers may not encounter reporting guidelines early enough to act on them\nBehavioural driver: Opportunity\n\n\n\n\n\n\nOptimize websites for search terms aimed at early use like “how to write”, or “funding application”. (See Search Engine Optimization above)\nSee above\nNone\n\n\n\n\nCreate prompts / communication campaigns to target authors early in their research*\nPrompts/cues\nEnablement\nEQUATOR had no way to do this\nNo changes made\n\n\nCreate tools to be used for early writing tasks\nAdding objects to the environment\nEnablement\nMost reporting guidelines come with a checklist but none come with a template, or tools/guidance specific to protocols or funding applications.\nNo changes made\n\n\nTargeted barrier: Researchers may struggle to keep writing concise\nBehavioural driver: Opportunity\n\n\n\n\n\n\nProvide instruction as to how and where information can be reported without breaching word count limits or making articles bloated.\nInstruction on how to perform the behavior\nEducation\nFew reporting guidelines include this information\nAdded instruction at top of reporting guideline and in some items where most useful\nExample: See Figure 10\n\n\nProvide examples of concise reporting*\nDemonstration of the behavior\nModelling\nNo examples specifically to display concise reporting\nNo changes made\n\n\nTargeted barrier: Researchers may not have tools for the job at hand\nBehavioural driver: Opportunity\n\n\n\n\n\n\nCreate guidance for planning research, or for writing protocols/funding applications (see Create tools to be used for early writing tasks)*\nSee above\nSee above\n\n\n\n\nCreate to-do lists in the order research is conducted, to help authors collect information they will need to report (see Create tools to be used for early writing tasks)*\nSee above\nSee above\n\n\n\n\nCreate templates for drafting (see Create tools to be used for early writing tasks)*\nSee above\nSee above\n\n\n\n\nCreate tools to facilitate checklist completion*\nAdding objects to the environment\nEnablement\nUpdating page numbers in a checklist is time consuming. It takes editors time to double check page numbers and content. Checklists may not include instructions of how to complete them.\nExample: See Figure 7\nNo changes made\n\n\nCreate tools to facilitate particular reporting items*\nAdding objects to the environment\nEnablement\nSome tools exist (e.g., PRISMA flow chart diagram maker, COBWEB)\nNo changes made\n\n\nCreate tools to help collaborators check each other’s work*\nAdding objects to the environment\nEnablement\nChecklists exist but aren’t specifically designed for collaborators\nNo changes made\n\n\nCreate tools to help peer reviewers check reporting and request missing information*\nAdding objects to the environment\nEnablement\nChecklists are reporting guidelines are not specifically aimed at peer reviewers\nNo changes made\n\n\nTargeted barrier: reporting guidelines can become outdated\nBehavioural driver: Opportunity\n\n\n\n\n\n\nProvide feedback channels to help developers keep guidance updated (see Create spaces for authors to discuss reporting guidelines with others)\nSee above\nNone\n\n\n\n\nMake it possible for guideline developers to make small edits without having to publish new articles\nRestructuring the physical environment\nEnablement\nDevelopers would have to publish a new article\nDevelopers can make small updates any time\n\n\nTargeted barrier: Researchers may struggle to reconcile multiple sets of guidance\nBehavioural driver: Opportunity\n\n\n\n\n\n\nExplain when reporting guidelines do not intended to prescribe structure\nInstruction on how to perform the behavior\nEducation\nNot always stated. Not always prominent\nExample: see Figure 5\nExplained at top of guidance\nExample: see Figure 10\n\n\nProvide instruction as to how and where information can be reported without breaching word count limits or making articles bloated (see Provide instruction as to how and where information can be reported without breaching word count limits or making articles bloated)\nSee above\nNone\n\n\n\n\nEmbed reporting guidelines that “fit together” (see above)*\nSee above\nNone\n\n\n\n\nTargeted barrier: Researchers may be asked to remove reporting guideline content\nBehavioural driver: Opportunity\n\n\n\n\n\n\nProvide advice regarding how to respond if asked to remove reporting guideline content by a colleague, editor, or reviewer\nProblem solving\nEducation\nNo advice given\nAdvice given in FAQ\nExample: See Figure 10\n\n\nTargeted barrier: reporting guideline resources may not be in usable formats\nBehavioural driver: Opportunity\n\n\n\n\n\n\nEnsure all resources and tools (e.g., checklists and templates) are in ready-to-use formats (see above)*\nSee above\nNone\n\n\n\n\nTargeted barrier: Researchers may feel afraid to report transparently\nBehavioural driver: Motivation\n\n\n\n\n\n\nPresent design advice separately to reporting advice*\nRestructuring the physical environment\nCoercion (removal of)\nSome reporting guideline E&Es include design advice\nNo changes made\n\n\nMake reporting guidelines agnostic to design choices (see Decrease fear of judgement by making reporting guidelines design agnostic)\nSee above\nNone\n\n\n\n\nEncourage explanation even when choices are unusual or not optimal\nInstruction on how to perform the behavior\nEducation\nNot always present\nExample: See Figure 6\nAdded to items\nExample: See Figure 11\n\n\nReassure authors that all research has limitations (see Reassure that all research has limitations to encourage explanation over perfect design)\nSee above\nNone\n\n\n\n\nInclude testimonials from researchers who were nervous about being punished for reporting transparently (see Include testimonials from researchers who were nervous about being punished for reporting transparently)\nSee above\nNone\n\n\n\n\nTargeted barrier: Researchers may feel restricted if reporting guidelines prescribe design\nBehavioural driver: Motivation\n\n\n\n\n\n\nPresent design advice separately and remain design agnostic (see Decrease fear of judgement by making reporting guidelines design agnostic)\nSee above\nNone\n\n\n\n\nReassure when guidelines are just guidelines\nSocial support\nPersuasion\nNot always present or prominent\nExample: See Figure 5\nProminently displayed at top of reporting guideline\nExample: See Figure 10\n\n\nTargeted barrier: Researchers may feel patronized\nBehavioural driver: Motivation\n\n\n\n\n\n\nCreate spaces for authors to discuss reporting guidelines with others (see Create spaces for authors to discuss reporting guidelines with others)\nSee above\nPersuasion\n\n\n\n\nAvoid patronizing language\nRemove aversive stimulus\nPersuasion\nAlthough authors may feel patronized when asked to adhere to a reporting guideline, reporting guidelines themselves rarely use patronizing language\nContinue to avoid using patronizing language\n\n\nExplain how the guidance was developed and why it can be trusted\nCredible source\nEducation\nMost reporting guidelines explain this in a published article. Checklists do not\nExample: See Figure 5\nBrief description included on home page and at top of reporting guideline, links to full to development information\nExample: See Figure 10\n\n\nKey Behaviour: Repeat engagement with reporting guidelines for subsequent studies\n\n\n\n\n\n\nTargeted barrier: Researchers may forget to use reporting guidelines at earlier research stages\nBehavioural driver: Opportunity\n\n\n\n\n\n\nCreate prompts / communication campaigns to target authors early in their research (see above)*\nSee above\nNone"
  },
  {
    "objectID": "chapters/9_defining_content/index.html#figures",
    "href": "chapters/9_defining_content/index.html#figures",
    "title": "Defining Intervention Content",
    "section": "Figures",
    "text": "Figures\nI explain how I created these figures in chapter 10. I have included them in this chapter to give context to the components in the table above.\n\n\n\n\n\nFigure 1: A simplified layout of the existing EQUATOR Network website, as of the 5th of April, 2023. Users must navigate through up to 5 different web pages to reach reporting guidance. The proportion of users navigating between each step is shown in the width of the links. Links in grey are estimated proportions. “Exit” means leaving the website. For simplicity, I have not included the 7% of authors viewing pages within the “Library” subdirectory, nor the 1% visiting other content categories.\n\n\n\n\n\n\nFigure 2: The layout of the new web-intervention. Users must now only need to navigate to 2 pages to access reporting guidance. The proportion of users navigating between each step is shown in the width of the links. All links in are estimated proportions, based on a realistic aim to reduce the exit rate from the home page and database page by 50%.\n\n\n\n\n\n\n\nFigure 3: The existing EQUATOR home page, as captured on 5th of April, 2023. Limitations include: 1) No prominent description of what RGs are 2) No clear instruction on what tasks RGs can and cannot be used for 3) Search function hard to find (area A) 4) Decision tool for identifying which RG to use was hard to find (area B) 5) The page looked cluttered and unappealing 6) The tone of voice was functional. It was not particularly judgemental but not reassuring either. 7) There was little description of benefits of using a reporting guideline besides the mention of ‘quality’ and ‘transparency’ in the definition of EQUATOR, reference to ‘high-impact research’, ‘improve your writing’, and ‘enhance your peer review’ in the header. 8) No reassurance that most research has limitations 9) Frequently accessed guidelines are fairly easy to find (area C).\n\n\n\n\n\n\nFigure 4: The existing EQUATOR page for SRQR, as captured on 5th April, 2023. Limitations include:\n1) The actual guidance is hard to find. Area A includes 3 links. The first two send users to an article describing how SRQR was developed. The actual guidance appears in a supplement of that article, which is the third link in area A. The label “relevant URLs” is vague. 2) Little instruction regarding what the RG is or can be used for other than “Qualitative research” 3) Links to related guidelines that are hard to find or, for SRQR, absent 4) No metrics around how many authors use this RG (e.g. citation counts) 5) The French translation of the guidance is well labelled and fairly easy to find (area B), but to the right of it is a box prominently labelled “Translations”, and the link in here would actually take the user further away from the translated guidance.\n\n\n\n\n\n\nFigure 5: The SRQR publication, captured on the 5th April, 2023. Limitations of reporting guideline publications may include:\n1) RG publications often focus on how the guidance was developed. The actual guidance (see area C) or checklist (area B) may be relegated to a box, table, or a linked supplement. 2) Not all RGs describe what RGs are or what they can be used for, and these descriptions can be hard to find (areas A). 3) RG publications may not reassure authors that most research has limitations, and that transparency is OK 4) Publications may not be written with a reassuring tone of voice. Instead, guideline developers may justify their work by emphasising the negative impact of research waste. This may be how developers justify their work to themselves, editors, reviewers, or readers. As a result, to a naive author considering using the guidance, the tone of voice may come across as judgemental. 5) Benefits to the user may be hard to find or (as with this RG) not described at all. Benefits to others are more likely to be described, including a focus on how transparent, complete reporting benefits the research community or, conversely, how poor reporting is wasteful. 6) Instruction on when RGs do/do not intend to prescribe structure, or instruction may be hard to find (see area D) or missing. 7) Instructions on whether a RG intends to be a strict standard vs. ‘just’ a guideline may be hard to find (see area D) or missing. 8) Links to related resources only include those that were created before the RG was published. Some guidelines don’t include any links.\n9) No clear instruction on whether to use the guideline in a situation that it wasn’t designed for, but when no better guidance exists.\n\n\n\n\n\n\nFigure 6: An example item from the SRQR guideline. Limitations may include:\n1) Text is unstructured, so it is difficult to immediately identify what needs to be written.\n2) Text uses verbose, passive language\n3) The text appears long and difficult to digest\n4) Terms aren’t always defined\n5) Not all reporting items are justified\n6) Not all items include instruction of what to write if the item could not/was not done.\n\n\n\n\n\n\n\nFigure 7: The SRQR checklist. Limitations of RG checklists may include:\n1) Checklists may not define what RGs are, what they can be used for, or their benefits. 2) Checklists may not be in a usable format (e.g. a PDF that cannot be filled in, or a table that cannot be copied) 3) Checklists may not include instruction of how to complete them. 4) Checklists may not link to the underlying guidance, or other related resources. 5) Content may lack nuance of full guidance and may appear dictatorial and administrative\n\n\n\n\n\n\nFigure 8: Author instructions for BMJ Open, a typical journal, captured on 5th April, 2023. Limitations of Journal instruction to authors may include:\n1) Instructions advise authors to use RGs, but don’t define what RGs are, what they can be used for, or the benefits or using them. 2) Advice regarding reporting guidelines may be hard to find amongst lengthy instruction pages (see area A)\n\n\n\n\n\n\n\nFigure 9: Intervention home page. Intervention changes made to the homepage include the following:\n1) RGs are now clearly defined (areas A) 2) The site looks simple and has plenty of white space\n3) Personal benefits are described explicitly and communicated through reassuring language and quotes (see areas B)\n4) Search and browse buttons are easy to find (area C) 5) Frequently accessed guidelines are still easy to find (area D) 6) The site describes what tasks RGs can be used for, and differentiates tools by task (area E)\n\n\n\n\n\n\nFigure 10: Intervention reporting guideline page. Intervention changes made to RG introductions include:\n1) Clear description of what the RG is, what it can and cannot be used for, the benefits to the author and to society, and how and when it can be used. (area A) 2) Description of whether the RG is intended to be a standard or ‘just’ a guideline (area A) 3) Tools are clearly differentiated by task (area B) 4) Related guidelines and other resources are linked. These links can be updated as and when newer guidelines are published (area C) 5) Clear instruction on whether a RG can be used in a situation that it wasn’t designed for, but where no better guidance exists (area D) 6) Links to translations (area E) 7) Reassuring language throughout, and reassuring quotes from editors, readers, and authors (e.g., area F) 8) Citation metrics (area G) 9) An estimation of how long guidance will take to read (area H) 10) Advice on how or where to report items so as not to breach word count limits and when RGs do or do not intend to prescribe structure (area I) 11) Full guidance (area J, see Figure 11) 12) Citation information (area K) 13) Information on how the guidance was developed and why it can be trusted (area L)\n\n\n\n\n\n\nFigure 11: A re-designed item from the SRQR reporting guideline. Intervention changes include:\n1) Content is separated into what to write (area A), why information is important (area B), examples (area C), and any additional background information (not shown). 2) Areas B and C are presented as expandable content, so the only instruction immediately visible is what to write (area A). This means that the guidance is easier to digest and less intimidating. 3) Definitions are presented as pop-ups for technical terms 4) Quotes provide reassurance and persuasion (area D) 5) Language is direct and edited for clarity and brevity 6) Each item has its own discussion page (linked to from the top right of area A)\n\n\n\n\n\n\nFigure 12: Intervention discussion page. Every reporting item now has its own discussion page where authors can ask and answer questions, and provide feedback to guideline developers.\n\n\n\n\n\n\n1. Michie S, Atkins L, West R (2014) The Behaviour Change Wheel: A Guide to Designing Interventions. Silverback Publishing, London\n\n\n2. Michie S, Richardson M, Johnston M, Abraham C, Francis J, Hardeman W, Eccles MP, Cane J, Wood CE (2013) The behavior change technique taxonomy (v1) of 93 hierarchically clustered techniques: Building an international consensus for the reporting of behavior change interventions. Annals of Behavioral Medicine: A Publication of the Society of Behavioral Medicine 46:81–95"
  },
  {
    "objectID": "chapters/3_synthesis/tbl_codes.html",
    "href": "chapters/3_synthesis/tbl_codes.html",
    "title": "Thesis",
    "section": "",
    "text": "Table 1: Codes (left, not bold), descriptive themes (bold), and analytic themes (right)\n\n\n\n\n\n\nDescriptive themes\nCodes\nAnalytic Themes\n\n\nWhat does this mean?\nWhat does this term mean?[1–5]\nWhat does this item mean? [1–6]\nHow are these items different?[2, 4, 6, 7]\nHave I understood this as intended? [1, 2]\nExamples help me understand items [4, 8, 9]\nWhy is this item important?\nWhy is this item important? [2–4, 10]\nWho is this item important to? [2, 4, 11]\nDoes this apply to me?\nHave I understood the guideline’s scope as intended? [4, 5]\nDoes this item apply to me? [2, 4–7]\nIs this item optional? [2, 6]\nI do not understand what reporting guidelines are\nWhat are reporting guidelines? [11, 12]\nHow should I use a reporting guideline? [13]\nResearchers may not understand the guidance as intended, or what reporting guidelines are, even if they think they do\n\n\nReporting guidelines benefit me\nI find guidelines useful in general [5, 14]\nGuidelines make me feel confident [11]\nGuidelines help me develop as a researcher [11, 15]\nGuidelines may help me improve my manuscript [2, 7, 11, 14, 15]\nI believe guidelines may help me publish more easily [16]\nI use guidelines because of other people\nI may use guidelines because journals and editors tell me to [11, 13, 15, 16]\nI may use guidelines because other researchers expect it [13, 16]\nGuidelines benefit others\nStandardized reporting benefits the community [11, 16, 17]\nSome benefits are more important than others\nImmediate benefits are more important than hypothetical ones [11, 16]\nPersonal benefits are more important than benefits to others [16]\nResearchers report a variety of reasons for using reporting guidelines, and that some are more important than others\n\n\nResearchers use reporting guidelines for different tasks\nI use reporting guidelines for planning research [2, 11]\nI use reporting guidelines for designing research [6, 11, 12, 14]\nI use reporting guidelines for writing [2, 6, 11, 14]\nI use reporting guidelines for checking my own or other people’s writing [11, 12]\nI use reporting guidelines to appraise the quality of other people’s reporting [3]\nI use reporting guidelines for peer reviewing [11]\nI want guidance presented in formats that are better suited to the task I am doing\nI want items presented in the order in which I must do them [; [17]; [9]]\nI want design or methods advice [2, 4, 11]\nI want templates for writing [14]\nI want checklists that are easy to fill in [5, 18]\nI want checklists embedded into journal submission workflows [14]\nI want items embedded into data collection tools [15]\nResearchers report using reporting guidelines for different tasks and wanting guidance to be delivered in ways that better fit their needs\n\n\nGuidelines take time\nGuidelines take time to read, understand and apply [13, 15, 16]\nSome items require extra work which takes time and effort [1, 2, 19]\nI want an indication of which items to prioritize [2, 6]\nPerceived complexity [2, 14, 16, 18]\nLong guidelines are off-putting [5, 7, 11, 15]\nItemization may decrease costs\nItemization helps me navigate guidance[4]\nItemization summarizes the guidance[14]\nItemization may increase perceived costs\nItemization makes guidance appear longer[4]\nItemization blocks the bigger picture[2]\nI think guidelines make my manuscripts long and bloated\nFollowing reporting guidance can result in long, bloated articles [2, 6, 7, 15]\nLong, bloated articles may exceed journal word limits [6, 7, 13, 18]\nI want options for where to report this item [1, 2, 4, 7, 11, 13]\nThe benefits of using a reporting guideline may not outweigh the costs\nThe benefits of using a reporting guideline may not outweigh the costs [7, 11, 13]\nThe balance of benefits vs costs may be more favourable when guidelines are used early\nGuidelines are more valuable when used early [2, 5, 11, 14]\nUsing reporting guidelines has costs, and researchers may not feel that benefits outweigh the costs\n\n\nI think the guidance could be improved\nI would clarify this item [4, 6]\nI would move this item [1, 2]\nI would split this item into two [2, 4, 9]\nI would add or remove items from this guideline [2–4, 6]\nI would add or remove requirements from this item [4, 6, 8, 10, 11]\nGuidelines need to be kept updated\nGuidelines can become out of date [2]\nGuidelines need to be updated [4]\nReporting guidelines may need to be revised and updated for different reasons\n\n\nI feel unable to report this\nI cannot report this because I didn’t do it [2, 4, 6, 7]\nI cannot report this because of intellectual property issues [7]\nI cannot report this because it clashes with journal guidelines [4]\nI cannot report this because data was missing from my primary studies [15]\nEditors, reviewers or co-authors asked me to remove this item [6, 19]\nI feel nervous or uncertain if I am unable to report an item\nI feel uncertain because I don’t know how to say that I didn’t do it [4]\nI feel worried that I will be judged for transparently reporting something I didn’t do [4, 11]\nResearchers may not be able to report all items which can leave them feeling uncertain or worried\n\n\nI can only use what I know about and have\nI may not know that reporting guidelines exist, or what guidance exists [3, 5, 13, 14, 16]\nI may not be able to easily access guidance [5, 16]\nAwareness and accessibility may limit reporting guideline usage\n\n\nReporting guidelines are more valuable to inexperienced researchers\nReporting guidelines may be less valuable to experienced researchers [7, 11, 14]\nExperienced researchers feel that they already know how to report [2, 11, 14]\nExperienced researchers find guidance patronizing and feel untrusted [4, 7, 13, 18]\nReporting guidelines can be hard to use at first but get easier with experience\nReporting guidelines can be hard to use at first but get easier with experience [2, 13, 16]\nReporting guidelines may be more useful to less experienced researchers, but less experienced researchers may find them harder to use\n\n\nI want or need design advice\nI want design or methodological advice [4, 11, 18]\nI don’t know how to do this item [2, 4, 6]\nI think this guidance prescribes how research should be designed\nGuidelines are procedural straightjackets [11]\nThis guideline is too prescriptive [4, 10, 11]\nResearchers want or need design advice, but reporting guidelines may not be the right place\n\n\nA guideline’s scope can be unclear\nThe guideline’s applicability criteria are not clear [3, 5, 14]\nA guideline can be too narrow\nThis guideline isn’t a perfect fit for me [5]\nThis guideline doesn’t generalise [4, 10, 11, 14, 18]\nA guideline’s scope can be too broad\nI don’t want to see optional items that only apply to other types of study [5, 6]\nReporting guidelines can be harder to use if their scope is too broad, too narrow, or poorly defined\n\n\nI often need to adhere to multiple sets of guidance\nI need to adhere to journal guidelines or other research guidelines [4, 6, 13, 14]\nI might need to use multiple reporting guidelines [11]\nI want guidelines to harmonize\nI want reporting guidelines to be linked or embedded [3, 4]\nI want reporting guidelines to use similar structure [4]\nI want reporting guidelines to use similar terms [4]\nResearchers may have to use multiple sets of reporting guidelines, multiplying complexity and costs\n\n\nI experience reporting guidelines primarily as, or through, checklists\nI don’t like checklists[5, 7, 11, 14]\nI may use the checklist instead of the full guidance [8]\nResearchers may use checklists but never read the full guidance\n\n\n\n\n\n\n\n\n1. Davies L, Donnelly KZ, Goodman DJ, Ogrinc G (2016) Findings from a novel approach to publication guideline revision: User road testing of a draft version of SQUIRE 2.0. BMJ quality & safety 25:265–272\n\n\n2. Davies L, Batalden P, Davidoff F, Stevens D, Ogrinc G (2015) The SQUIRE Guidelines: An evaluation from the field, 5years post release. BMJ quality & safety 24:769–775\n\n\n3. Korevaar DA, Cohen JF, Reitsma JB, et al (2016) Updating standards for reporting diagnostic accuracy: The development of STARD 2015. Research integrity and peer review 1:7\n\n\n4. Page MJ, McKenzie JE, Bossuyt PM, Boutron I, Hoffmann TC, Mulrow CD, Shamseer L, Tetzlaff JM, Moher D (2021) Updating guidance for reporting systematic reviews: Development of the PRISMA 2020 statement. Journal of Clinical Epidemiology 134:103–112\n\n\n5. Struthers C, Harwood J, de Beyer JA, Dhiman P, Logullo P, Schlüssel M (2021) GoodReports: Developing a website to help health researchers find and use reporting guidelines. BMC medical research methodology 21:217\n\n\n6. Prady SL, MacPherson H (2007) Assessing the utility of the standards for reporting trials of acupuncture (STRICTA): A survey of authors. The Journal of Alternative and Complementary Medicine 13:939–943\n\n\n7. Eysenbach G. (2013) CONSORT-EHEALTH: Implementation of a checklist for authors and editors to improve reporting of web-based and mobile randomized controlled trials. Studies in health technology and informatics 192:657–661\n\n\n8. Sert NP du, Hurst V, Ahluwalia A, et al (2020) The ARRIVE guidelines 2.0: Updated guidelines for reporting animal research. PLOS Biology 18:e3000410\n\n\n9. de Vries RBM, Hooijmans CR, Langendam MW, van Luijk J, Leenaars M, Ritskes-Hoitinga M, Wever KE (2015) A protocol format for the preparation, registration and publication of systematic reviews of animal intervention studies. Evidence-based Preclinical Medicine 2:e00007\n\n\n10. Tam WWS, Tang A, Woo B, Goh SYS (2019) Perception of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement of authors publishing reviews in nursing journals: A cross-sectional online survey. BMJ Open 9:e026271\n\n\n11. Sharp MK, Glonti K, Hren D (2020) Online survey about the STROBE statement highlighted diverging views about its content, purpose, and value. Journal of clinical epidemiology 123:100–106\n\n\n12. Prager R, Gagnon L, Bowdridge J, Unni RR, McGrath TA, Cobey K, Bossuyt PM, McInnes MDF (2021) Barriers to reporting guideline adherence in point-of-care ultrasound research: A cross-sectional survey of authors and journal editors. BMJ Evidence-Based Medicine bmjebm-2020-111604\n\n\n13. Fuller T, Pearson M, Peters J, Anderson R (2015) What affects authors’ and editors’ use of reporting guidelines? Findings from an online survey and qualitative interviews. PLoS ONE 10:e0121585\n\n\n14. Dewey M, Levine D, Bossuyt PM, Kressel HY (2019) Impact and perceived value of journal reporting guidelines among Radiology authors and reviewers. European Radiology 29:3986–3995\n\n\n15. Burford BJ, Welch V, Waters E, Tugwell P, Moher D, O’Neill J, Koehlmoos T, Petticrew M (2013) Testing the PRISMA-Equity 2012 reporting guideline: The perspectives of systematic review authors. PloS one 8:e75122\n\n\n16. Svensøy JN, Nilsson H, Rimstad R (2021) A qualitative study on researchers’ experiences after publishing scientific reports on major incidents, mass-casualty incidents, and disasters. Prehospital and Disaster Medicine 36:536–542\n\n\n17. Page MJ, McKenzie JE, Bossuyt PM, et al (2021) The PRISMA 2020 statement: An updated guideline for reporting systematic reviews. BMJ (Clinical research ed) 372:n71\n\n\n18. Macleod M, Collings AM, Graf C, Kiermer V, Mellor D, Swaminathan S, Sweet D, Vinson V (2021) The MDAR (Materials Design Analysis Reporting) Framework for transparent reporting in the life sciences. Proceedings of the National Academy of Sciences 118:e2103238118\n\n\n19. Rader T., Mann M., Stansfield C., Cooper C., Sampson M. (2014) Methods for documenting systematic review searches: A discussion of common issues. Research synthesis methods 5:98–115"
  },
  {
    "objectID": "chapters/3_synthesis/index.html",
    "href": "chapters/3_synthesis/index.html",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 1: A thematic synthesis",
    "section": "",
    "text": "Known todos:\n\nReflexivity paragraph at end (needed for all chapters)"
  },
  {
    "objectID": "chapters/3_synthesis/index.html#introduction",
    "href": "chapters/3_synthesis/index.html#introduction",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 1: A thematic synthesis",
    "section": "Introduction",
    "text": "Introduction\nIn chapter 1 I described a number of studies that found that reporting guidelines have only had a modest impact on reporting quality. Although a few of these studies put forward some possible explanations for their disappointing findings, these were often based on assumption and none of these studies included a qualitative components to better understand why reporting guidelines were (or were not) working.\nResearcher’s behaviour may be influenced by any part of the reporting guideline system, including the guidance itself (typically published in an academic article), tools (e.g., checklists), websites (e.g., journal instructions, the EQUATOR Network website, guideline-specific websites), and the behaviour of others (e.g., editors, peer reviewers, co-authors, colleagues). Because reporting guidelines are generally disseminated in similar ways, barriers and facilitators identified for one guideline are likely to generalise to another.\nIn this chapter, I explain how I synthesised qualitative studies exploring authors’ experiences of reporting guidelines in order to identify behavioural influences that may affect whether an author adheres to reporting guidelines."
  },
  {
    "objectID": "chapters/3_synthesis/index.html#methods",
    "href": "chapters/3_synthesis/index.html#methods",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 1: A thematic synthesis",
    "section": "Methods",
    "text": "Methods\nI performed a thematic synthesis of qualitative research that explored researchers’ experiences of reporting guidelines. When writing this chapter, I used the ENTREQ guidelines for reporting qualitative evidence syntheses and the PRISMA-S guidelines for reporting systematic searches [1, 2].\n\nApproach to searching and data sources\nMy search strategy sought all research articles that collected qualitative data exploring researchers’ experiences of using reporting guidelines. I wanted to capture the experiences of researchers from around the world, so I included international databases in my information sources. All data sources are listed in Table 1.\n\n\nInclusion and exclusion criteria\nI included studies that reported researchers’ experiences of using reporting guidelines derived through qualitative methods. I excluded articles written before 1996, the year that the CONSORT statement was first published. Any articles not written in English, Chinese, Spanish, or Portuguese were excluded during screening.\nI included records reporting feedback from researchers as part of guideline development. I decided not to include reporting guideline development studies where this feedback came exclusively from the development group members, as I considered this context to be too different to how ordinary researchers experience reporting guidelines.\nI found many studies that used a mix of quantitative and qualitative questions. I did not consider categorical survey questions with a free text option for “other” to be qualitative, but I did include findings from free text questions that invited participants to provide context to a previous (not qualitative) question. I describe the quantitative studies and the kinds of questions they asked in an accompanying commentary (in press).\n\n\nElectronic search strategy\nASK: How do I refer to people that weren’t part of my supervisory team? In an article they would be authors, but in a thesis?\nA search specialist (SK) helped develop the comprehensive search strategies, which had a component for reporting guidelines and another for qualitative methods. I constructed my reporting guidelines component from the acronyms of frequently accessed guidelines (Table 2) with generic terms for reporting guidelines to capture guidelines not named explicitly. My qualitative component came from a review of search filters[3], which recommended a sensitive qualitative filter for systematic reviews[4]. I extended the filter to include descriptive methods because I knew some of my target records were mixed method surveys. I conducted scoping searches, but my search strategies were not peer reviewed before execution. My search strategies are reported fully in the Appendix.\n\n\nScreening\nYD screened Chinese records and asked LZ for second opinions when necessary. I screened all other records, of which MS double-screened a random 10% sample and differences were resolved through discussion. YD and I screened titles and abstracts to identify records that explored researchers’ experiences and then screened full texts to identify whether those articles used qualitative methods.\n\n\nDescribing and appraising records\nI extracted study characteristics and used the Critical Appraisal Skills Programme Qualitative (CASP-Qual) checklist [5] to critically appraise included studies, which helped him and JdB consider the strengths and weaknesses of each study when synthesising them.\n\n\nSynthesis methodology\nI used thematic synthesis as defined by Thomas and Harden[6] because it can handle studies with “thin” descriptions, it allowed us to infer facilitators and barriers from research that may not have addressed my concern directly, and because I expected its output, grouped by themes, to be useful to guideline developers.\nI imported files into NVivo 12.0 for Mac and coded all sentences from the results section and relevant supplementary materials that reported qualitative findings. I assigned each sentence one or more descriptive codes that sought to distil the essence of what was written, creating new codes when necessary and without using a framework. I then used mind-mapping software[7] to visualise similarities and differences between codes and aggregate them inductively into descriptive themes that captured the meaning of the codes they contained. I then used my research question to infer facilitators and barriers from these descriptive themes and to understand the context in which these occur, thereby producing analytic themes. I discussed all steps with JdB, resolving conflicts through discussion when necessary."
  },
  {
    "objectID": "chapters/3_synthesis/index.html#results",
    "href": "chapters/3_synthesis/index.html#results",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 1: A thematic synthesis",
    "section": "Results",
    "text": "Results\n\nSearch\nMy search yielded 18 articles (see Figure 1 for full search results). I and MS double-screened 10% of the non-Chinese titles and abstracts and agreed on 98.3% of them (170/173). The remaining three were resolved after discussion and consensus. All eligible records were written in English and included surveys, semi-structured interviews, focus groups, and writing tasks.\n\n\n\n\n\nFigure 1: PRISMA flow diagram\n\n\n\n\nOnly 7 of the 18 records reported where participants came from. Three mixed method survey studies [8–10] included participants from a wide range of countries but it was not possible to tell which participants completed the optional qualitative questions. Four interview studies [11–13] included participants who were almost exclusively from North America, Europe, and Australia, with one participant from Brazil.\nCritical appraisal of the studies using CASP-Qual rated the studies ranging from valuable to not very valuable; the less valuable studies had few qualitative components or minimal reporting of qualitative analysis or findings. Study characteristics are reported in Table 3.\n\n\nSynthesis findings\nThe relationships between my codes, descriptive themes, and analytic themes are reported in Table 4. I identified the following analytic themes: 1) Researchers may not understand guidance as intended or what reporting guidelines are, even if they think they do; 2) Researchers report a variety of reasons for using reporting guidelines, and that some are more important than others; 3) Researchers report using reporting guidelines for different tasks and wanting guidance to be delivered in ways that better fit their needs; 4) Using reporting guidelines has costs which researchers may feel outweigh benefits; 5) Reporting guidelines may need to be revised and updated; 6) Researchers may not be able to report all items, which can leave them feeling uncertain or worried; 7) Awareness and accessibility may limit reporting guideline usage; 8) Reporting guidelines may be more useful to less experienced researchers, but these researchers may find them harder to use; 9) Researchers want or need design advice, but reporting guidelines may not be the right place to find it; 10) Reporting guidelines can be harder to use if their scope is too broad, too narrow, or poorly defined; 11) Researchers may have to use multiple sets of reporting guidelines, multiplying complexity and costs; 12) Researchers may use checklists but never read the full guidance.\nI identified that ‘barriers’ and ‘facilitators’ were not consistent experiences. What may be a barrier for one person might be a facilitator to others or when occurring in different context, and so I refrained from labelling my analytic themes as one or the other.\n\n1) Researchers may not understand guidance as intended or what reporting guidelines are, even if they think they do\nResearchers commonly stated that they need more information to fully understand the intention of the guideline developer. When asked about the clarity of guidance, researchers across many studies reported difficulty in understanding certain terms, concepts, or checklist items:\n\n‘outcome’: “Does it mean the domain, or does it mean the domain measure, metric, method of aggregation, and time?”[14].\n“Primary and secondary improvement related question is confusing, what does that mean?… I had a hard time with the [difference between the] improvement question and the study question.”[11]\n\nA few researchers reported ignoring an item if they could not understand it:\n\n“Only one item was identified as hard to understand by more than one respondent: ‘methods employed to ensure completeness of data’, which two participants said they left out because of difficulty in comprehending the item”[15]\n\nSome researchers reported feeling that reporting guidelines were “simply not comprehensible”[11]. Others reported that they had understood, but further investigation revealed that their interpretations could be “different from that intended by the developers”[15]. For example, Davies et al. [15] found that one SQUIRE item “was reported as used fully [in the quantitative survey], but the qualitative analysis revealed that its usage was frequently inconsistent with the intention of the developers”. One reason for this may be because different researchers may interpret the guidance in different ways depending on their prior experience, the research context, or if the guidance is ambiguous. For example, the SQUIRE developers found that the word ‘theory’ “meant different things to different people. For some, the word ‘theory’ meant ‘mechanism by which an intervention was expected to work’, for others it meant ‘lean or six sigma for example’, and for still others it meant ‘logic model’”[11].\nEven when researchers reported understanding what an item meant, they may not have understood why it is important or who it is important to, leading them to remark that an item “seems unnecessary“[16]. Few researchers referenced the needs of evidence synthesizers or patients as consumers of research, but more reported considering whether an item would be useful to other researchers, editors, and reviewers:\n\n“the information provided does not matter as the reviewers do not know what to do with it’’[10]\n\nIn addition to not understanding guidance or who it is important to, many researchers expressed difficulty understanding whether an item was applicable to their work. Some reporting guidelines specify that not all items are compulsory or that some items may only apply to a subset of research articles. Researchers highlighted that this may not be obvious, especially if this nuance is buried in a long elaboration document. Some researchers therefore reported uncertainty over which items applied to them:\n\n“Authors asked for clarification of which items were always required and which were nonessential”[17]\n“Not always clear what was relevant to their study”[16]\n“He had realised with experience and re-reading the Guidelines that SQUIRE did not require him to include every item in the manuscript”.[11]\n\nThis uncertainty may extend to the entire reporting guideline if researchers don’t know when to use one over another. One researcher declared that “PRISMA guidelines can also be used rather than the MOOSE”[18], when the two are primarily for reviews of intervention studies and observational studies respectively. Sometimes there may not be a perfect reporting guideline for a given study, as one researcher commented after using ARRIVE (which focusses on experimental research involving laboratory animals):\n\n“Our report was an animal based cadaveric study looking at accuracy of drill guides. I were unsure which category it should fall under.”[16]\n\nEven if a researcher understands the guidance, why it is important, and why it applies to them, they may not understand how to report it or “how much detail to report”[16]. Some researchers “used examples [included in the guidance] to understand what should be reported” because they “demonstrate what is meant in practice”[16].\nAt a more fundamental level, researchers varied in their understanding of what reporting guidelines are. Often researchers would talk about reporting guidelines as if they were design guidelines, e.g., describing STROBE as “woefully deﬁcient in encouraging…use of appropriate data analytic approaches”[10]. This suggested that the researcher had not noticed the stipulation that “these recommendations are not prescriptions for designing or conducting studies” included in STROBE’s explanation and elaboration document[19]. Other researchers wrote about STARD as is if the guidance was to be used when collecting imaging data:\n\n“Two comments suggested that reporting quality may be impacted by the physical environment in which […] data are collected. These comments may indicate an incomplete understanding of reporting guidelines which pertain to reporting results during manuscript writing, not the process of imaging acquisition itself.”[20]\n\n\n\n2) Researchers report a variety of reasons for using reporting guidelines, and that some are more important than others\nSome researchers listed personal benefits to using reporting guidelines. Some described reporting guidelines as a “training tool”[21] for personal development, noting that guidance helps “develop a strong foundation and habits”[10]. Some talked about how guidance made them feel: “As a junior scientist it gives me conﬁdence to request the reporting of a certain piece of information”[10]. Others said that reporting guidelines are “a helpful reminder”[21] and that going through the checklist “improved their manuscripts”[22]. Some saw value in fostering a “transparent reporting process”[8] and for making sure your “project is written up as rigorously”[11].\nA couple of researchers noted altruistic benefits, reporting that widespread reporting guideline adherence “helps in standardizing how research is reported”[10] and calling “for more scientific reports to be published, preferably using a template or guideline to make them comparable”[23].\nIn the absence of anticipated benefits, some researchers said that they use reporting guidelines simply because “it was what was implicitly expected of them to do”[12], and that these expectations came from journals and their peers. Some used “tools promoted by journals, which often promised to ease the publishing process”[23] but others wrote that they found this to be an empty promise:\n\n‘’I have never had (nor have I heard of) an editor or reviewer pushing back on a claim that all STROBE criteria were met. Therefore, when a STROBE checklist is required for manuscript submission, it seems to turn into a[n] exercise in additional administrative busywork without really improving the research.’’[10]\n\nA few researchers reported being more likely to comply with journal requirements if they thought the journal was likely to enforce them: “Does the journal only suggest or actually require submission of a reporting guideline checklist?”[12]. Some said they were more likely to comply if “it was a high impact factor journal and I thought that I would only get one crack at it”[12].\nA few researchers compared different motivations for using reporting guidance, noting that personal, guaranteed, and immediate benefits were more motivating than hypothetical benefits or benefits to others:\n\n“I suppose you are looking for short-term gain, short-term benefits as a writer of a report”[23]\n“it can be difﬁcult to put the energy into using STROBE (or any other) one a priori since ultimately, it depends on the journal submitted to and accepted to”[10]\n“All the researchers wanted more homogenous reporting but emphasized that:”As an individual reporter, one is prone to choose the easiest and most accessible one.”“[23]\n\n\n\n3) Researchers report using reporting guidelines for different tasks and wanting guidance to be delivered in ways that better fit their needs\nAlthough reporting guidelines were designed to help researchers draft and check their manuscripts, many researchers mentioned using reporting guidelines for other tasks including designing research, planning, peer-reviewing, and educating. A few researchers suggested different ways that the guidance could be delivered to make their task easier. For example, some “thought [the] order of items should reflect [the] order considered when designing the experiment”[16]. Others wanted “a manuscript template”[8] to make writing easier. Some suggested that “online form[s]”[9] or software to “mark in the text what corresponds to each item in the list”[18] would make it easier to complete a reporting checklist as part of journal submission.\n\n\n4) Using reporting guidelines has costs which researchers may feel outweigh benefits\nResearchers noted that some items require extra work, either to collect the necessary information or just to think about and report, and that sometimes this workload felt overly burdensome:\n\n“If I put the onus on everybody out there who’s trying to improve care to deal with that sophisticated question […], I just think I are putting a barrier in place that is going to be a mountain”[15]\n\nThis work requires time, and “the length of time it would take to consider the items”[21] was cited by many researchers as a cost, with some asking themselves whether “sufﬁcient time [was] available to comply with [the] reporting guideline”[12].\nResearchers noted that a reporting guideline’s “length and content is a key factor inﬂuencing the time needed to complete it.”[10]. Some found checklists to be “very complete, but to follow every single point is overwhelming”[22]. As a solution, many wanted to “simplify” or “shorten the checklists”[18]. A few researchers wanted a “hierarchy” to know which items were most “important to include”[15]. Another suggested that checklists presented as online forms could include “logic for irrelevant [items]”[9] so that the users are presented with only items that apply to them.\nComplexity was sometimes mentioned alongside time: “As the research often was performed out of work hours, the required time and complexity of the guidelines or templates may have played a crucial role [in deciding whether to use a reporting guideline]”[23]. Researchers had conflicting opinions about whether itemization reduces perceived complexity. Proponents noted that the “Checklist is a very helpful summary of sometimes confusing guidelines”[8] and that itemization made guidance “easier to follow” and “more approachable”[14]. But a few said that presenting guidance in small pieces made it difficult to “get the whole picture of what you are supposed to be doing”[11] and that itemization makes “the checklist appear more daunting for users:”If you make the checklist too long people will see it as too complicated and then won’t use it”“[14].\nAnother concern cited by many researchers was that following a reporting guideline can result in long reports:\n\n“I use SQUIRE a lot for planning—I complete the sections up through the methods at the time I design the study…[but] SQUIRE creates sort of long reports if followed exactly.”[11]\n“the document you create if you use SQUIRE exactly as written is unintelligible”[11]\n“this [item] would require another paper”[22]\n\nThis problem was exacerbated by journal word limits:\n\n“I believe it is a useful instrument but it is unrealistic to assume that every single suggestion can be detailed in a 6000-words manuscript.”[22]\n“two remarked that word limitations has necessitated removal of many items”[17]\n\nAlthough a handful of researchers noted that “the relaxation of word limits”[12] would help, many researchers objected to long articles because they were bloated, harder to read, or simply “unintelligible”[11] and requested strategies to “enhance readability” regardless of journal policies. Some wondered where they could place this information besides the article body, such as “in an appendix”, an “online supplement or repository”, or a figure[14]. Some researchers preferred to report information in the checklist instead of the article body because of “space restrictions, because [it was] a minor component of the study, because they considered the information to be obvious, or because they were unsure of how to incorporate it in the manuscript.”[16]. Some used this strategy to report items that had “not been used or observed during the study, for example that no inclusion or exclusion criteria had been set, no data had been excluded, randomisation and blinding had not been used…”[16] although it was not clear whether this was motivated by a desire for a concise article or a concern about highlighting potential weaknesses.\nFaced with the costs of time, work and article length, some researchers explicitly weighed perceived benefits against costs and disagreed about the balance:\n\n“The manuscript has improved. However, I felt that the amount of effort was considerably greater than the degree of improvement.”[22]\n“it also adds to the time required to put together a manuscript, and I am not sure how much it improves the chances of a manuscript being published”[10]\n“it does increase the quality of the articles, it is clearly worth the time”[10]\n\nThe balance of costs versus benefits may be most favourable when guidance is used early in the research workflow. Researchers who used reporting guidelines earlier in their workflow (e.g., for planning research or drafting) used language that implied it was something they did regularly (e.g., “I use SQUIRE a lot for planning”[11]). Some reported that they had come to this habit by their own initiative and that reporting guideline developers should “encourage people to use the criteria early in the writing process (I have, which probably is why I only changed one thing [at the point of submission])”[18]. One researcher suggested that “policy that focuses on a front end approach would be helpful”[10], noting that “To fully apply the criteria, I would need to systematically apply the STROBE criteria on the front end design of a project, grant, etc. rather than at the time of writing a project”[10].\nConversely, many authors who completed a checklist during manuscript submission, very late in their in workflow, emphasised the costs, using words like “arduous”[10] and expressing negative opinions of this process (see Researchers may use the checklist but never read the full guidance). This may be because researchers lack the motivation, time, or ability to edit their manuscripts at this point.\n\n\n5) Reporting guidelines may need to be revised and updated for different reasons\nResearchers in most studies had opinions on how guidance could be improved through clarifying, reorganising, splitting, merging, adding, or deleting items, and sometimes these views fed into the revision of reporting guidelines[14, 15]. This feedback may be useful for reporting guideline developers. Even if a reporting guideline was considered perfect at one point in time, researchers noted that guidance must be kept up to date in response to changes in the field and broader scientific ecosystem:\n\n“The evolution of the healthcare improvement scholarly literature in the intervening years since the publication of the SQUIRE Guidelines has led to the development of concepts that were not fully anticipated at the time of initial release”[11].\n\nUpdates to one reporting guideline may necessitate the update of another. For instance, as PRISMA was being updated, a few researchers “supported referring to PRISMA for Abstracts, but suggested it also needs updating” to reflect updates being made to PRISMA[14].\n\n\n6) Researchers may not be able to report all items, which can leave them feeling uncertain or worried\nSome researchers described being unable to report items because of external factors, including intellectual property or data rules, disagreement between co-authors, or because “peer reviewers or editors had suggested editing out much of their [reporting guideline]-specific text”[17]. Others reported feeling unable to report an item because they did not do it, whether on purpose, due to an oversight, or because requirements had changed since the study began:\n\n“[This item was] not part of the study objectives”[22]\n“This [item] is a good idea, but I did not do this.”[22]\n“The RCT was initiated before trial registration became customary in Norway, and therefore does not have a Trial ID number.”[22]\n\nThis left some researchers fearing that “an ‘’incomplete’’ checklist [gave] the impression that their study is ‘’less than ’perfect’.’’[10]. Some expressed concern that strict wording that assumed something was done may”force people to lie/mislead by asking a question they cannot answer”[14] and suggested that guidance should instead use more agnostic language and specify what to do if an item were not addressed, such as “If no publicly accessible protocol is available, please state this”[14].\n\n\n7) Awareness and accessibility may limit reporting guideline usage\nResearchers may not know what guidance exists and may be more likely to use whatever is most accessible and discoverable:\n\n“Several of the researchers did not have extensive knowledge about the different reporting tools, so the accessibility of the guideline or template was often a decisive factor.”[23]\n\nOne researcher wrote that “poor dissemination strategy by authors of reporting guidelines had inhibited uptake”[12], and others recognised that reporting guidelines could be “better highlighted”[8] by journals or advertised on “social media platforms”[18].\n\n\n8) Reporting guidelines may be more useful to less experienced researchers, but these researchers may find them harder to use\nSome researchers reported that they didn’t need the guidance as they were experienced enough to know what they were doing:\n\n“One of the most prevalent themes was the expression of self-assuredness. ’‘[I] follow the STROBE guidelines in my reporting reasonably well without actually referring to them or using a checklist’’ (group 3, ID1) and ’‘[I] already apply the STROBE recommendations despite not having heard of it until today’’”[10]\n\nSometimes this was accompanied by an acknowledgement that reporting guidance may be more beneficial to less experienced researchers:\n\n“Despite experienced researchers generally not seeing a beneﬁt to personally using STROBE, there were strong feelings that it is valuable to early-career researchers”[10]\n“Helpful at beginning of career, but not at later stage”[8]\n“this exercise might be good for college students but is insulting for professionals”[22]\n\nHowever, less experienced researchers often reported finding “reporting guidelines being difficult to use initially”[12], or that reporting guidelines became easier with experience in medical writing in general, and with experience in using other reporting guidelines. For instance, “Participants with less experience in scholarly medical writing found the SQUIRE Guidelines harder”[11].\n\n\n9) Researchers want or need design advice, but reporting guidelines may not be the right place\nMany researchers reported wanting advice on design choices but disagreed on where that design guidance should go. Some researchers suggested referring researchers to other design resources through hyperlinks or citations. Others explicitly wanted design guidance to be written into reporting guidelines so that others would read it. Some went as far as calling for reporting guidelines to express an opinion and encourage one technique over another. One researcher objected to a “neutral tone”[14] in a reporting guideline that may give the impression that a design choice (that they disapproved of) was reasonable practice.\nHowever, other researchers objected to reporting guidelines that were opinionated about design choices. One user described STROBE as a “procedural straightjacket”[10], suggesting that it dictates how studies should be conducted. Users who encounter the guidance late in writing may be unable to act on any design recommendations and consequently may feel fearful of reporting transparently if their design choices deviate from what the guideline recommends as best practice (see Researchers may not be able to report all items, which can leave them feeling uncertain or worried).\nPerhaps with these concerns in mind, one wrote that “I need to make sure that the language around this elaboration gives [researchers] some flexibility”[14], with another noting that “I am OK with the idea of emphasizing the value of [this design choice], but I cannot mandate it”[14].\n\n\n10) Reporting guidelines can be harder to use if their scope is too broad, too narrow, or poorly defined\nReporting guideline developers may narrow the scope of their guidance by limiting it to certain design choices or research contexts. This frustrated some researchers, who noted that narrow “checklists cannot fit all types of research”[8] and “cautioned that ‘’that balance between freedom and structure is important to consider’’ […] and that it is ‘’important to recognise that each study/analysis is unique and doesn’t always ﬁt with the recommendations’’.[10]\nThis narrowing of scope may have been a conscious decision, as requested by this researcher giving feedback on a proposed update of the PRISMA guideline: “I need an agreement on whether PRISMA is to be only for intervention studies (as implied by the proposed modification above) or more general.”[14] However, scope may not always be clearly communicated: another PRISMA user “opined that”the assessment of risk of bias, statement of risk ratio and explaining additional analyses depend on the study design … [For] a systematic review of cross-sectional surveys or a meta-synthesis I do not need this information”“[24], suggesting they were unaware of PRISMA’s focus on interventional studies or that MOOSE and ENTREQ would be more appropriate for these kinds of studies (see previous themes for further discussion of awareness and understanding the applicability of reporting guidelines).\nResearchers noted that scope could be made broader by removing items or, more commonly, by extending items with more options and examples:\n\n“omit”(benefits or harms)” from the checklist item to be more inclusive of reviews that do not examine effects of interventions”[14]\n“If the new PRISMA will more explicitly embrace topics other than interventions (which I think it should), then some additional examples could be added to the parenthesis (e.g. sensitivity and specificity, disease prevalence, regression coefficient)”[14]\n\nHowever, extending guidance with options can make the guidance appear longer and means researchers must work out which parts apply to them.\n\n\n11) Researchers may have to use multiple sets of reporting guidelines, multiplying complexity and costs\nThere are now over 500 reporting guidelines indexed on the EQUATOR Network website, with more added each year as reporting guideline developers seek to cover more and more use cases. Researchers may be expected to use one reporting guideline instead of another. Other times they may be expected to use a second or third reporting guideline in addition to the original one. Some researchers “pointed out that these extensions have created needless complexity and ‘’additional confusion in reporting of observational studies’’ […] and that the ‘’number of extensions has become excessive, especially given that multiple extensions may apply to a single study’’”[10].\nOne researcher wrote: “it would be good to have better connection between different checklists (perhaps using digital linking, decision-trees, etc.)”[14]. Some showed concern that hyperlinks will go unused and so developers should “incorporate all relevant details in the […] checklist and elaboration (in case authors don’t read the extension)”[14]. When writing about PRISMA, one researcher noted that “it would be wise to limit the number of additional documents to look up. This is only item 7, and I have already been referred to PRISMA for Abstracts and PRISMA for Searches. As a systematic review author, reviewer, or editor, I would be unlikely to go to several sources for reporting guidance”[14].\nA few researchers wrote that related reporting guidelines should be mutually updated to keep in sync with each other before linking or embedding them. Researchers wanted the instruction, terminology, and structure of different sets of reporting guidelines to be coherent, suggesting, for example, that the updated PRISMA should be structured to be “in line with PRISMA-P”[14].\nIn addition to reconciling multiple reporting guidelines, researchers must also comply with journal, funder, and other scientific guidelines and expressed frustration when instructions contradicted each other. For example, some reporting guidelines specify subheadings for abstracts and one researcher pointed out that a “major issue is that journals wildly differ in requirements/what is allowed in abstracts”[14].\n\n\n12) Researchers may use checklists but never read the full guidance\nReporting guidelines typically consist of the guidance itself and a checklist that serves as a summary of the guidance and a tool to demonstrate compliance. Sometimes the document containing the full guidance is called the Explanation and Elaboration (or E&E for short). When talking about a reporting guideline, it was often unclear whether the researcher was talking about the checklist or the E&E.\nSome researchers implied that their only experience with reporting guidelines was completing a checklist as part of submission. I noticed that many negative statements were directed specifically at this process, describing checklists as “painful”[8], “pedantic”, “annoying”[10], or a “stupid exercise”[22].\nOne study explored researchers’ use of checklists and E&E documents, noting that “Participants used the guidelines and the E&E in different ways. Some did not read the E&E and used only the checklist, others read the E&E first and then used the checklist and a further group used the checklist and referred to the E&E for help with specific items.”[16]. One researcher even went as far as to say that the “E&E appeared to be redundant”[16].\nIf some researchers only use checklists, which typically lack any nuance included in the E&E, this may explain why some described reporting guidance as inflexible and prescriptive, warning that “Blind checklists are not relevant to most work”[18] or that “Authors may ‘’fear the ’Checklist Manifesto’ becoming a rigid bureaucracy, and also becoming contrived’’”[10]."
  },
  {
    "objectID": "chapters/3_synthesis/index.html#discussion",
    "href": "chapters/3_synthesis/index.html#discussion",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 1: A thematic synthesis",
    "section": "Discussion",
    "text": "Discussion\nResearchers face many challenges when trying to use reporting guidelines and have many questions, opinions, and suggestions that could be useful for reporting guideline developers. These findings could help increase the impact of reporting guidelines if taken into consideration during their development, dissemination, and implementation phases.\nThe reporting guideline development community has typically relied on journals to promote their resources and have called on editors to better enforce reporting guideline adherence[25]. However, the results presented here suggest that focusing solely on enforcement may be short sighted and that guideline developers have the power to address the usage barriers identified here themselves, at least in part. Doing so may in turn make it easier for journals and funders to enforce reporting guidelines. For example, it is difficult to enforce a reporting guideline that is difficult to understand or if the guideline’s applicability criteria are unclear.\nQualitative methods uncovered issues that may be masked by quantitative surveys. For example, Davies et al. [15] found that one SQUIRE item “was reported as used fully [in the quantitative survey], but the qualitative analysis revealed that its usage was frequently inconsistent with the intention of the developers”. Most studies used mixed method surveys and often the qualitative component was small, perhaps limited to a single question like “Please add your comments and suggestions in the free text below”[8] or “Any other feedback?”[9], and consequently resulted in thin data. Future studies seeking richer qualitative data should consider using interviews and focus groups above surveys.\nDespite there being hundreds of reporting guidelines, my search found only 18 studies that collected qualitative data, covering 12 reporting guidelines in total, and only six of the 15 reporting guidelines listed on the EQUATOR Network’s homepage. One noticeable absence was the COREQ guidelines for reporting qualitative interviews and focus groups, the third most frequently accessed reporting guideline on EQUATOR website. Given that qualitative research differs from quantitative research in terms of its ontologies, epistemologies, and how it considers replicability and best practice, it would be interesting to know whether qualitative researchers face additional hurdles not covered here.\nReporting guideline developers may lack the funds, time, motivation, or expertise to design user-friendly resources or effectively evaluate them. The EQUATOR Network guidance for reporting guideline developers[26], published in 2010, covers steps from inception to dissemination but largely neglects user experience or user testing. Only two short sections, totalling eight sentences of an eight-page document, address the importance of gathering user feedback, but the guidance offers no instruction on how to do this. Nor does the guidance advise on usability best-practices. Reporting guideline developers may benefit from advice on how to evaluate their resources and from infrastructure to collect feedback from researchers, such as a commenting system on the EQUATOR Network website.\n\nLimitations\nI were limited by the availability of literature and the relative thinness of some studies’ qualitative analysis. Most studies relied on participants recalling what they had done or thought in the past, and so may be subject to recall bias. Future studies could consider using ‘in the moment’ methods like think aloud tasks.\nSurveys could be subject to question order bias. Often the qualitative question appeared at the end of a survey, and so participants’ responses could have been influenced by the proceeding quantitative questions. In my accompanying commentary (in press) I describe how most of the concepts covered by the quantitative survey questions also appeared in the qualitative data and some of these quantitative questions were phrased leadingly. However, the qualitative data contained many additional themes that did not appear in the quantitative questions.\nI tried to capture the experience of a diverse range of researchers, but most participants of the reviewed studies were from western countries. My Chinese database searches yielded no relevant studies. Likewise, I found no studies on this topic published in Spanish or Portuguese. Around a quarter of visitors to the EQUATOR Network’s website have their browser set to a language other than English (in press), and it is likely that researchers who are not native English speakers face additional challenges not covered here. As all of the qualitative research I found was conducted in English, it is unsurprising that language barriers did not appear as a theme, despite being identified as a potential issue in quantitative surveys[20].\nI considered including grey literature, commentaries, and opinion pieces. These may have contributed themes to my analysis but finding these pieces (many of which may have been on private blog posts not indexed by search tools), and the extra work of synthesising primary and secondary order constructs was not feasible. I also considered synthesising quantitative survey data, most of which collected ordinal or categorical data. I decided that synthesising these quantitatively would not add value to my analysis. However, I have categorised the kinds of quantitative questions asked and compared them with the themes identified here in an accompanying commentary (in press).\nI did not distinguish between different guidelines and expect that the themes I found may apply to reporting guidelines to different degrees. I also expected code frequency to be biased by the questions asked in each study. I therefore decided not to prioritise themes by importance or frequency.\nTODO: Inclusion of grey literature / editorials / opinion pieces / social media / non research\nTODO: rework the end of discussion to link to next chapter\n\n\nImplications for future research\nAddressing the issues identified here may make reporting guidelines easier to use and increase their impact, thereby improving the quality of published research. Although a lot of consideration has been given to how reporting guidelines should be developed, the way in which that guidance is evaluated and disseminated also deserves to be studied and optimised. The results of this study will inform my future work to improve the dissemination of reporting guidelines.\n\n\nConclusions\nResearchers encounter many barriers and facilitators when using reporting guidelines. In chapters 7 I describe how I identified ways to address these influences. Reporting guideline developers should consider using qualitative methods when piloting and refining their resources. In chapter 11 I describe how I used interviews, observation, think-aloud tasks, and writing tasks to explore authors’ experiences of a redesigned reporting guideline. Developers could consider using methods like these over and above surveys and questionnaires. Most of the studies included in this review were surveys, and during my search I found other (quantitative) surveys. In the next chapter, I explain why and how I explored the questions contained in these surveys.\n\n\nTables\n\n\nTable 1: Information sources and record management\n\n\n\n\n\n\n\n\nSource\nSearch platform\nDate searched\nRecord management\n\n\n\n\nChinese Biomedical Literature Database[27]\nChinese Biomedical Literature Service System[28]\n25/10/2021\nYD used Zotero to manage, deduplicate and screen records.\n\n\nChina National Knowledge Infrastructure[29]\nhttps://www.cnki.net/\n\"\n\"\n\n\nWanfang Data[30]\nhttp://www.wanfangdata.com/\n\"\n\"\n\n\nVIP Chinese Medical Journal Database[31]\nhttp://www.cqvip.com/\n\"\n\"\n\n\nMedline\nOvid\n08/12/2021\nI used Zotero to manage and deduplicate records. He used Rayyan to do a second deduplication and screen records.\n\n\nEmbase\n\"\n\"\n\"\n\n\nAllied Complementary Medicine Database (AMED)\n\"\n\"\n\"\n\n\nPsycInfo\n\"\n\"\n\"\n\n\nLatin American and Caribbean Health Sciences Literature[32]\nWHO Global Index Medicus (GIM)[33]\n08/12/2021\n\"\n\n\nAfrican Index Medicus[34]\n\"\n\"\n\"\n\n\nWestern Pacific Region Index Medicus[35]\n\"\n\"\n\"\n\n\nIndex Medicus for South-East Asia region[36]\n\"\n\"\n\"\n\n\nIndex Medicus for the Eastern Mediterranean Region[37]\n\"\n\"\n\"\n\n\nScientific Electronic Library Online[38]\nhttps://scielo.org/en/\n08/12/2021\n\"\n\n\nOpen Science Framework (OSF)\nhttps://osf.io/\n15/12/2021\n\"\n\n\nMethods in Research on Research website [39]\nhttp://miror-ejd.eu/publications/\n14/12/2021\n\"\n\n\nEmailing developers of guidelines listed in Table 2\nn/a\n08/01/2022\n\"\n\n\n\n\n\n\nTable 2: Reporting guidelines featured on the EQUATOR Network’s home page\n\n\n\n\n\n\nName\nFull name\n\n\n\n\nCONSORT\nConsolidated Standards of Reporting Trials\n\n\nSTROBE\nStrengthening the Reporting of Observational Studies in Epidemiology\n\n\nPRISMA\nPreferred Reporting Items for Systematic Reviews and Meta-Analyses\n\n\nPRISMA-P\nPRISMA for systematic review protocols\n\n\nSPIRIT\nStandard Protocol Items: Recommendations for Interventional Trials\n\n\nSTARD\nSTAndards for Reporting Diagnostic accuracy studies\n\n\nTRIPOD\nTransparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis\n\n\nCARE\nguidelines for CAse REports\n\n\nAGREE\nAppraisal of Guidelines, REsearch and Evaluation\n\n\nRIGHT\nReporting Items for practice Guidelines in HealThcare\n\n\nSRQR\nStandards for Reporting Qualitative Research\n\n\nCOREQ\nCOnsolidated criteria for REporting Qualitative research\n\n\nARRIVE\nAnimal Research: Reporting of In Vivo Experiments\n\n\nSQUIRE\nStandards for QUality Improvement Reporting Excellence\n\n\nCHEERS\nConsolidated Health Economic Evaluation Reporting Standards\n\n\n\n\n\n\nTable 3: Study characteristics\n\n\n\n\n\n\n\n\n\n\n\nAuthors\nParticipants\nParticipant’s country\nGuideline(s) considered\nMethods\nPhenomena of interest\nCASP rating\n\n\n\n\nBurford et al., 2013 [21]\n151 systematic review authors\nNot reported\nPRISMA Equity\nMixed methods survey\nPerceived utility, facilitators, and barriers\nFairly valuable\n\n\nDavies et al., 2015 [11]\n18 experts and 29 end users\nUSA, Canada, Sweden, the UK, and Norway\nSQUIRE\nFocus groups and interviews\nExperiences with and impressions of the SQUIRE Guidelines\nValuable\n\n\nDavies et al., 2016 [15]\n44 graduates faculty, and directors of healthcare\nNot reported\nSQUIRE\nMixed methods survey and written exercise\n“whether SQUIRE 1.6 was understood and implemented as intended by the developers”\nValuable\n\n\nDe Vries et al. 2015 [40]\n7 researchers\nNot reported\nSystematic review protocols of animal intervention studies\n\nFeedback on usability, missing items, possibilities for improvement and clarity\nNot very valuable\n\n\nDewey et al., 2019 [8]\n74 out of 831 survey respondents that provided (optional) free text comments\nThe full survey was answered by respondents in the USA, Canada, China, South Korea, Japan, Germany, France, Italy, UK, Other European countries, Middle East, Latin America, Other. It's unclear where respondents for the free text answers came from.\nCONSORT, STROBE, PRISMA, STARD\nMixed methods survey\n\"(1) When and how are reporting guidelines and checklists used by authors and reviewers? (2) What is their impact on the content of final manuscript drafts according to authors? and (3) How do authors and reviewers perceive the value of reporting guidelines and related checklists?\"\nFairly valuable\n\n\nEysenbach 2013 [22]\n61 authors\nNot reported\nCONSORT Ehealth\nMixed methods survey\nViews on completing the checklist as part of submission\nFairly valuable\n\n\nFuller et al., 2015 [12]\n5 authors\nUSA and Australia\nTREND and Reporting Guidelines in general\nSemi structured interviews\nFactors that affected authors’ use of TREND and other reporting guidelines\nValuable\n\n\nKorevaar et al., 2016 [13]\n4 radiology residents, 8 laboratory medicine experts\nRadiology residents were from the Netherlands. No geographical details provided for experts\nSTARD\nInterview (residents) and mixed methods survey (experts)\nTo identify items that were vague, ambiguous, difficult to interpret, or missing\nFairly valuable\n\n\nMacleod et al., 2021 [9]\n211 authors, but only some answered the free text question\nThe full survey was answered by participants in the USA, China, Japan, Germany, EU, and “Other” areas. It is unclear who answered the free text question.\nMaterials Design Analysis Reporting framework\nMixed methods survey\nWhether the checklist was clear and useful\nFairly valuable\n\n\nPage et al., 2021 [14]\n110 systematic review authors and experts\nNot reported\nPRISMA\nMixed methods survey\nOpinions on PRISMA and proposed changes\nValuable\n\n\nPrady et al., 2007 [17]\n40 authors\nNot reported\nStandards for Reporting Interventions in Controlled Trials of Acupuncture\nMixed methods survey\nExperiences using PRISMA\nFairly valuable\n\n\nPrager et al., 2021 [20]\n5 of 18 survey respondents that answered the free text question\nNot reported\nSTARD\nMixed methods survey\nBarriers to STARD 2015 adherence\nFairly valuable\n\n\nRader et al., 2014 [41]\n263 systematic reviewers\nNot reported\nPRISMA\nMixed methods survey\nBarriers or difficulties in meeting more detailed reporting standards in PRISMA\nFairly valuable\n\n\ndu Sert et al., 2020 [16] 11 authors\nUK, USA, Belgium, Br\nazil ARRIVE\nInterview and writin\ng task Authors’ opinions, i\nnterpretation, and experiences of updated ARRIVE guidelines Fairly valuable\n\n\n\nSharp et al., 2020 [10]\n203 of 1015 researchers that answered free text questions\nThe full survey was answered by participants in Africa, Asia, Europe, North and South America, Middle East, and Pacific Region. It is unclear who answered the free text question.\nSTROBE\nMixed methods survey\nExperiences and attitudes towards STROBE\nValuable\n\n\nStruthers et al., 2021 [18]\n623 authors, 274 of whom answered free the text question\nNot reported\nReporting guidelines in general\nMixed methods survey\nThe question asked, “What could I do to improve the guideline?”\nFairly valuable\n\n\nSvensøy et al., 2021 [23]\n10 authors\nNot reported\nNot specified\nSemi structured interviews\nExperiences using guidelines or templates\nValuable\n\n\nTam et al., 2019 [24]\n230 authors, 62 of whom answered the open-ended questions\nNot reported\nPRISMA\nMixed methods survey\nOpinions on PRISMA\nFairly valuable\n\n\n\n\n\n\nTable 4: Codes (left, not bold), descriptive themes (bold), and analytic themes (right)\n\n\n\n\n\n\nDescriptive themes\nCodes\nAnalytic Themes\n\n\nWhat does this mean?\nWhat does this term mean?[11, 13–15, 18]\nWhat does this item mean? [11, 13–15, 17, 18]\nHow are these items different?[11, 14, 17, 22]\nHave I understood this as intended? [11, 15]\nExamples help me understand items [14, 16, 40]\nWhy is this item important?\nWhy is this item important? [11, 13, 14, 24]\nWho is this item important to? [10, 11, 14]\nDoes this apply to me?\nHave I understood the guideline’s scope as intended? [14, 18]\nDoes this item apply to me? [11, 14, 17, 18, 22]\nIs this item optional? [11, 17]\nI do not understand what reporting guidelines are\nWhat are reporting guidelines? [10, 20]\nHow should I use a reporting guideline? [12]\nResearchers may not understand the guidance as intended, or what reporting guidelines are, even if they think they do\n\n\nReporting guidelines benefit me\nI find guidelines useful in general [8, 18]\nGuidelines make me feel confident [10]\nGuidelines help me develop as a researcher [10, 21]\nGuidelines may help me improve my manuscript [8, 10, 11, 21, 22]\nI believe guidelines may help me publish more easily [23]\nI use guidelines because of other people\nI may use guidelines because journals and editors tell me to [10, 12, 21, 23]\nI may use guidelines because other researchers expect it [12, 23]\nGuidelines benefit others\nStandardized reporting benefits the community [10, 23, 42]\nSome benefits are more important than others\nImmediate benefits are more important than hypothetical ones [10, 23]\nPersonal benefits are more important than benefits to others [23]\nResearchers report a variety of reasons for using reporting guidelines, and that some are more important than others\n\n\nResearchers use reporting guidelines for different tasks\nI use reporting guidelines for planning research [10, 11]\nI use reporting guidelines for designing research [8, 10, 17, 20]\nI use reporting guidelines for writing [8, 10, 11, 17]\nI use reporting guidelines for checking my own or other people’s writing [10, 20]\nI use reporting guidelines to appraise the quality of other people’s reporting [13]\nI use reporting guidelines for peer reviewing [10]\nI want guidance presented in formats that are better suited to the task I am doing\nI want items presented in the order in which I must do them [; [42]; [40]]\nI want design or methods advice [10, 11, 14]\nI want templates for writing [8]\nI want checklists that are easy to fill in [9, 18]\nI want checklists embedded into journal submission workflows [8]\nI want items embedded into data collection tools [21]\nResearchers report using reporting guidelines for different tasks and wanting guidance to be delivered in ways that better fit their needs\n\n\nGuidelines take time\nGuidelines take time to read, understand and apply [12, 21, 23]\nSome items require extra work which takes time and effort [11, 15, 41]\nI want an indication of which items to prioritize [11, 17]\nPerceived complexity [8, 9, 11, 23]\nLong guidelines are off-putting [10, 18, 21, 22]\nItemization may decrease costs\nItemization helps me navigate guidance[14]\nItemization summarizes the guidance[8]\nItemization may increase perceived costs\nItemization makes guidance appear longer[14]\nItemization blocks the bigger picture[11]\nI think guidelines make my manuscripts long and bloated\nFollowing reporting guidance can result in long, bloated articles [11, 17, 21, 22]\nLong, bloated articles may exceed journal word limits [9, 12, 17, 22]\nI want options for where to report this item [10–12, 14, 15, 22]\nThe benefits of using a reporting guideline may not outweigh the costs\nThe benefits of using a reporting guideline may not outweigh the costs [10, 12, 22]\nThe balance of benefits vs costs may be more favourable when guidelines are used early\nGuidelines are more valuable when used early [8, 10, 11, 18]\nUsing reporting guidelines has costs, and researchers may not feel that benefits outweigh the costs\n\n\nI think the guidance could be improved\nI would clarify this item [14, 17]\nI would move this item [11, 15]\nI would split this item into two [11, 14, 40]\nI would add or remove items from this guideline [11, 13, 14, 17]\nI would add or remove requirements from this item [10, 14, 16, 17, 24]\nGuidelines need to be kept updated\nGuidelines can become out of date [11]\nGuidelines need to be updated [14]\nReporting guidelines may need to be revised and updated for different reasons\n\n\nI feel unable to report this\nI cannot report this because I didn’t do it [11, 14, 17, 22]\nI cannot report this because of intellectual property issues [22]\nI cannot report this because it clashes with journal guidelines [14]\nI cannot report this because data was missing from my primary studies [21]\nEditors, reviewers or co-authors asked me to remove this item [17, 41]\nI feel nervous or uncertain if I am unable to report an item\nI feel uncertain because I don’t know how to say that I didn’t do it [14]\nI feel worried that I will be judged for transparently reporting something I didn’t do [10, 14]\nResearchers may not be able to report all items which can leave them feeling uncertain or worried\n\n\nI can only use what I know about and have\nI may not know that reporting guidelines exist, or what guidance exists [8, 12, 13, 18, 23]\nI may not be able to easily access guidance [18, 23]\nAwareness and accessibility may limit reporting guideline usage\n\n\nReporting guidelines are more valuable to inexperienced researchers\nReporting guidelines may be less valuable to experienced researchers [8, 10, 22]\nExperienced researchers feel that they already know how to report [8, 10, 11]\nExperienced researchers find guidance patronizing and feel untrusted [9, 12, 14, 22]\nReporting guidelines can be hard to use at first but get easier with experience\nReporting guidelines can be hard to use at first but get easier with experience [11, 12, 23]\nReporting guidelines may be more useful to less experienced researchers, but less experienced researchers may find them harder to use\n\n\nI want or need design advice\nI want design or methodological advice [9, 10, 14]\nI don’t know how to do this item [11, 14, 17]\nI think this guidance prescribes how research should be designed\nGuidelines are procedural straightjackets [10]\nThis guideline is too prescriptive [10, 14, 24]\nResearchers want or need design advice, but reporting guidelines may not be the right place\n\n\nA guideline’s scope can be unclear\nThe guideline’s applicability criteria are not clear [8, 13, 18]\nA guideline can be too narrow\nThis guideline isn’t a perfect fit for me [18]\nThis guideline doesn’t generalise [8–10, 14, 24]\nA guideline’s scope can be too broad\nI don’t want to see optional items that only apply to other types of study [17, 18]\nReporting guidelines can be harder to use if their scope is too broad, too narrow, or poorly defined\n\n\nI often need to adhere to multiple sets of guidance\nI need to adhere to journal guidelines or other research guidelines [8, 12, 14, 17]\nI might need to use multiple reporting guidelines [10]\nI want guidelines to harmonize\nI want reporting guidelines to be linked or embedded [13, 14]\nI want reporting guidelines to use similar structure [14]\nI want reporting guidelines to use similar terms [14]\nResearchers may have to use multiple sets of reporting guidelines, multiplying complexity and costs\n\n\nI experience reporting guidelines primarily as, or through, checklists\nI don’t like checklists[8, 10, 18, 22]\nI may use the checklist instead of the full guidance [16]\nResearchers may use checklists but never read the full guidance\n\n\n\n\n\n\n\n\n\n\n\n1. Rethlefsen ML, Kirtley S, Waffenschmidt S, et al (2021) PRISMA-S: An extension to the PRISMA statement for reporting literature searches in systematic reviews. Systematic Reviews 10:39\n\n\n2. Tong A, Flemming K, McInnes E, Oliver S, Craig J (2012) Enhancing transparency in reporting the synthesis of qualitative research: ENTREQ. BMC Medical Research Methodology 12:181\n\n\n3. Rosumeck S, Wagner M, Wallraf S, Euler U (2020) A validation study revealed differences in design and performance of search filters for qualitative research in PsycINFO and CINAHL. J Clin Epidemiol 128:101–108\n\n\n4. Rogers M, Bethel A, Abbott R (2018) Locating qualitative studies in dementia on MEDLINE, EMBASE, CINAHL, and PsycINFO: A comparison of search strategies. Research Synthesis Methods 9:579–586\n\n\n5. Programme CAS (2018) CASP Qualitative Checklist. CASP - Critical Appraisal Skills Programme \n\n\n6. Thomas J, Harden A (2008) Methods for the thematic synthesis of qualitative research in systematic reviews. BMC Medical Research Methodology 8:45\n\n\n7. Bierner M (2022) Markdown Preview Mermaid Support. \n\n\n8. Dewey M, Levine D, Bossuyt PM, Kressel HY (2019) Impact and perceived value of journal reporting guidelines among Radiology authors and reviewers. European Radiology 29:3986–3995\n\n\n9. Macleod M, Collings AM, Graf C, Kiermer V, Mellor D, Swaminathan S, Sweet D, Vinson V (2021) The MDAR (Materials Design Analysis Reporting) Framework for transparent reporting in the life sciences. Proceedings of the National Academy of Sciences 118:e2103238118\n\n\n10. Sharp MK, Glonti K, Hren D (2020) Online survey about the STROBE statement highlighted diverging views about its content, purpose, and value. Journal of clinical epidemiology 123:100–106\n\n\n11. Davies L, Batalden P, Davidoff F, Stevens D, Ogrinc G (2015) The SQUIRE Guidelines: An evaluation from the field, 5years post release. BMJ quality & safety 24:769–775\n\n\n12. Fuller T, Pearson M, Peters J, Anderson R (2015) What affects authors’ and editors’ use of reporting guidelines? Findings from an online survey and qualitative interviews. PLoS ONE 10:e0121585\n\n\n13. Korevaar DA, Cohen JF, Reitsma JB, et al (2016) Updating standards for reporting diagnostic accuracy: The development of STARD 2015. Research integrity and peer review 1:7\n\n\n14. Page MJ, McKenzie JE, Bossuyt PM, Boutron I, Hoffmann TC, Mulrow CD, Shamseer L, Tetzlaff JM, Moher D (2021) Updating guidance for reporting systematic reviews: Development of the PRISMA 2020 statement. Journal of Clinical Epidemiology 134:103–112\n\n\n15. Davies L, Donnelly KZ, Goodman DJ, Ogrinc G (2016) Findings from a novel approach to publication guideline revision: User road testing of a draft version of SQUIRE 2.0. BMJ quality & safety 25:265–272\n\n\n16. Sert NP du, Hurst V, Ahluwalia A, et al (2020) The ARRIVE guidelines 2.0: Updated guidelines for reporting animal research. PLOS Biology 18:e3000410\n\n\n17. Prady SL, MacPherson H (2007) Assessing the utility of the standards for reporting trials of acupuncture (STRICTA): A survey of authors. The Journal of Alternative and Complementary Medicine 13:939–943\n\n\n18. Struthers C, Harwood J, de Beyer JA, Dhiman P, Logullo P, Schlüssel M (2021) GoodReports: Developing a website to help health researchers find and use reporting guidelines. BMC medical research methodology 21:217\n\n\n19. von Elm E, Altman DG, Egger M, Pocock SJ, Gøtzsche PC, Vandenbroucke JP (2007) Strengthening the reporting of observational studies in epidemiology (STROBE) statement: Guidelines for reporting observational studies. BMJ : British Medical Journal 335:806–808\n\n\n20. Prager R, Gagnon L, Bowdridge J, Unni RR, McGrath TA, Cobey K, Bossuyt PM, McInnes MDF (2021) Barriers to reporting guideline adherence in point-of-care ultrasound research: A cross-sectional survey of authors and journal editors. BMJ Evidence-Based Medicine bmjebm-2020-111604\n\n\n21. Burford BJ, Welch V, Waters E, Tugwell P, Moher D, O’Neill J, Koehlmoos T, Petticrew M (2013) Testing the PRISMA-Equity 2012 reporting guideline: The perspectives of systematic review authors. PloS one 8:e75122\n\n\n22. Eysenbach G. (2013) CONSORT-EHEALTH: Implementation of a checklist for authors and editors to improve reporting of web-based and mobile randomized controlled trials. Studies in health technology and informatics 192:657–661\n\n\n23. Svensøy JN, Nilsson H, Rimstad R (2021) A qualitative study on researchers’ experiences after publishing scientific reports on major incidents, mass-casualty incidents, and disasters. Prehospital and Disaster Medicine 36:536–542\n\n\n24. Tam WWS, Tang A, Woo B, Goh SYS (2019) Perception of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement of authors publishing reviews in nursing journals: A cross-sectional online survey. BMJ Open 9:e026271\n\n\n25. Moher D, Weeks L, Ocampo M, et al (2011) Describing reporting guidelines for health research: A systematic review. Journal of Clinical Epidemiology 64:718–742\n\n\n26. Moher D, Schulz KF, Simera I, Altman DG (2010) Guidance for developers of health research reporting guidelines. PLOS Medicine 7:e1000217\n\n\n27. Chinese Biomedical Literature Database. \n\n\n28. SinoMed. \n\n\n29. China National Knowledge Infrastructure. \n\n\n30. Wanfang Data - A Leading Provider of Electronic Resources for China Studies. \n\n\n31. VIP Chinese Medical Journal Database. \n\n\n32. | LILACS. \n\n\n33. Alves B/O/O-M Global Index Medicus World Health Organization. \n\n\n34. Health and Medical Articles Database - African Index Medicus. \n\n\n35. Western Pacific Region Index Medicus. \n\n\n36. IMSEAR at SEARO: Home. \n\n\n37. WHO EMRO | IMEMR | Library. \n\n\n38. About SciELO. \n\n\n39. Projet MiRoR. Projet MiRoR \n\n\n40. de Vries RBM, Hooijmans CR, Langendam MW, van Luijk J, Leenaars M, Ritskes-Hoitinga M, Wever KE (2015) A protocol format for the preparation, registration and publication of systematic reviews of animal intervention studies. Evidence-based Preclinical Medicine 2:e00007\n\n\n41. Rader T., Mann M., Stansfield C., Cooper C., Sampson M. (2014) Methods for documenting systematic review searches: A discussion of common issues. Research synthesis methods 5:98–115\n\n\n42. Page MJ, McKenzie JE, Bossuyt PM, et al (2021) The PRISMA 2020 statement: An updated guideline for reporting systematic reviews. BMJ (Clinical research ed) 372:n71"
  },
  {
    "objectID": "chapters/3_synthesis/index.html#tables",
    "href": "chapters/3_synthesis/index.html#tables",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 1: A thematic synthesis",
    "section": "Tables",
    "text": "Tables\n\n\nTable 1: Information sources and record management\n\n\n\n\n\n\n\n\nSource\nSearch platform\nDate searched\nRecord management\n\n\n\n\nChinese Biomedical Literature Database[27]\nChinese Biomedical Literature Service System[28]\n25/10/2021\nYD used Zotero to manage, deduplicate and screen records.\n\n\nChina National Knowledge Infrastructure[29]\nhttps://www.cnki.net/\n\"\n\"\n\n\nWanfang Data[30]\nhttp://www.wanfangdata.com/\n\"\n\"\n\n\nVIP Chinese Medical Journal Database[31]\nhttp://www.cqvip.com/\n\"\n\"\n\n\nMedline\nOvid\n08/12/2021\nI used Zotero to manage and deduplicate records. He used Rayyan to do a second deduplication and screen records.\n\n\nEmbase\n\"\n\"\n\"\n\n\nAllied Complementary Medicine Database (AMED)\n\"\n\"\n\"\n\n\nPsycInfo\n\"\n\"\n\"\n\n\nLatin American and Caribbean Health Sciences Literature[32]\nWHO Global Index Medicus (GIM)[33]\n08/12/2021\n\"\n\n\nAfrican Index Medicus[34]\n\"\n\"\n\"\n\n\nWestern Pacific Region Index Medicus[35]\n\"\n\"\n\"\n\n\nIndex Medicus for South-East Asia region[36]\n\"\n\"\n\"\n\n\nIndex Medicus for the Eastern Mediterranean Region[37]\n\"\n\"\n\"\n\n\nScientific Electronic Library Online[38]\nhttps://scielo.org/en/\n08/12/2021\n\"\n\n\nOpen Science Framework (OSF)\nhttps://osf.io/\n15/12/2021\n\"\n\n\nMethods in Research on Research website [39]\nhttp://miror-ejd.eu/publications/\n14/12/2021\n\"\n\n\nEmailing developers of guidelines listed in Table 2\nn/a\n08/01/2022\n\"\n\n\n\n\n\n\nTable 2: Reporting guidelines featured on the EQUATOR Network’s home page\n\n\n\n\n\n\nName\nFull name\n\n\n\n\nCONSORT\nConsolidated Standards of Reporting Trials\n\n\nSTROBE\nStrengthening the Reporting of Observational Studies in Epidemiology\n\n\nPRISMA\nPreferred Reporting Items for Systematic Reviews and Meta-Analyses\n\n\nPRISMA-P\nPRISMA for systematic review protocols\n\n\nSPIRIT\nStandard Protocol Items: Recommendations for Interventional Trials\n\n\nSTARD\nSTAndards for Reporting Diagnostic accuracy studies\n\n\nTRIPOD\nTransparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis\n\n\nCARE\nguidelines for CAse REports\n\n\nAGREE\nAppraisal of Guidelines, REsearch and Evaluation\n\n\nRIGHT\nReporting Items for practice Guidelines in HealThcare\n\n\nSRQR\nStandards for Reporting Qualitative Research\n\n\nCOREQ\nCOnsolidated criteria for REporting Qualitative research\n\n\nARRIVE\nAnimal Research: Reporting of In Vivo Experiments\n\n\nSQUIRE\nStandards for QUality Improvement Reporting Excellence\n\n\nCHEERS\nConsolidated Health Economic Evaluation Reporting Standards\n\n\n\n\n\n\nTable 3: Study characteristics\n\n\n\n\n\n\n\n\n\n\n\nAuthors\nParticipants\nParticipant’s country\nGuideline(s) considered\nMethods\nPhenomena of interest\nCASP rating\n\n\n\n\nBurford et al., 2013 [21]\n151 systematic review authors\nNot reported\nPRISMA Equity\nMixed methods survey\nPerceived utility, facilitators, and barriers\nFairly valuable\n\n\nDavies et al., 2015 [11]\n18 experts and 29 end users\nUSA, Canada, Sweden, the UK, and Norway\nSQUIRE\nFocus groups and interviews\nExperiences with and impressions of the SQUIRE Guidelines\nValuable\n\n\nDavies et al., 2016 [15]\n44 graduates faculty, and directors of healthcare\nNot reported\nSQUIRE\nMixed methods survey and written exercise\n“whether SQUIRE 1.6 was understood and implemented as intended by the developers”\nValuable\n\n\nDe Vries et al. 2015 [40]\n7 researchers\nNot reported\nSystematic review protocols of animal intervention studies\n\nFeedback on usability, missing items, possibilities for improvement and clarity\nNot very valuable\n\n\nDewey et al., 2019 [8]\n74 out of 831 survey respondents that provided (optional) free text comments\nThe full survey was answered by respondents in the USA, Canada, China, South Korea, Japan, Germany, France, Italy, UK, Other European countries, Middle East, Latin America, Other. It's unclear where respondents for the free text answers came from.\nCONSORT, STROBE, PRISMA, STARD\nMixed methods survey\n\"(1) When and how are reporting guidelines and checklists used by authors and reviewers? (2) What is their impact on the content of final manuscript drafts according to authors? and (3) How do authors and reviewers perceive the value of reporting guidelines and related checklists?\"\nFairly valuable\n\n\nEysenbach 2013 [22]\n61 authors\nNot reported\nCONSORT Ehealth\nMixed methods survey\nViews on completing the checklist as part of submission\nFairly valuable\n\n\nFuller et al., 2015 [12]\n5 authors\nUSA and Australia\nTREND and Reporting Guidelines in general\nSemi structured interviews\nFactors that affected authors’ use of TREND and other reporting guidelines\nValuable\n\n\nKorevaar et al., 2016 [13]\n4 radiology residents, 8 laboratory medicine experts\nRadiology residents were from the Netherlands. No geographical details provided for experts\nSTARD\nInterview (residents) and mixed methods survey (experts)\nTo identify items that were vague, ambiguous, difficult to interpret, or missing\nFairly valuable\n\n\nMacleod et al., 2021 [9]\n211 authors, but only some answered the free text question\nThe full survey was answered by participants in the USA, China, Japan, Germany, EU, and “Other” areas. It is unclear who answered the free text question.\nMaterials Design Analysis Reporting framework\nMixed methods survey\nWhether the checklist was clear and useful\nFairly valuable\n\n\nPage et al., 2021 [14]\n110 systematic review authors and experts\nNot reported\nPRISMA\nMixed methods survey\nOpinions on PRISMA and proposed changes\nValuable\n\n\nPrady et al., 2007 [17]\n40 authors\nNot reported\nStandards for Reporting Interventions in Controlled Trials of Acupuncture\nMixed methods survey\nExperiences using PRISMA\nFairly valuable\n\n\nPrager et al., 2021 [20]\n5 of 18 survey respondents that answered the free text question\nNot reported\nSTARD\nMixed methods survey\nBarriers to STARD 2015 adherence\nFairly valuable\n\n\nRader et al., 2014 [41]\n263 systematic reviewers\nNot reported\nPRISMA\nMixed methods survey\nBarriers or difficulties in meeting more detailed reporting standards in PRISMA\nFairly valuable\n\n\ndu Sert et al., 2020 [16] 11 authors\nUK, USA, Belgium, Br\nazil ARRIVE\nInterview and writin\ng task Authors’ opinions, i\nnterpretation, and experiences of updated ARRIVE guidelines Fairly valuable\n\n\n\nSharp et al., 2020 [10]\n203 of 1015 researchers that answered free text questions\nThe full survey was answered by participants in Africa, Asia, Europe, North and South America, Middle East, and Pacific Region. It is unclear who answered the free text question.\nSTROBE\nMixed methods survey\nExperiences and attitudes towards STROBE\nValuable\n\n\nStruthers et al., 2021 [18]\n623 authors, 274 of whom answered free the text question\nNot reported\nReporting guidelines in general\nMixed methods survey\nThe question asked, “What could I do to improve the guideline?”\nFairly valuable\n\n\nSvensøy et al., 2021 [23]\n10 authors\nNot reported\nNot specified\nSemi structured interviews\nExperiences using guidelines or templates\nValuable\n\n\nTam et al., 2019 [24]\n230 authors, 62 of whom answered the open-ended questions\nNot reported\nPRISMA\nMixed methods survey\nOpinions on PRISMA\nFairly valuable\n\n\n\n\n\n\nTable 4: Codes (left, not bold), descriptive themes (bold), and analytic themes (right)\n\n\n\n\n\n\nDescriptive themes\nCodes\nAnalytic Themes\n\n\nWhat does this mean?\nWhat does this term mean?[11, 13–15, 18]\nWhat does this item mean? [11, 13–15, 17, 18]\nHow are these items different?[11, 14, 17, 22]\nHave I understood this as intended? [11, 15]\nExamples help me understand items [14, 16, 40]\nWhy is this item important?\nWhy is this item important? [11, 13, 14, 24]\nWho is this item important to? [10, 11, 14]\nDoes this apply to me?\nHave I understood the guideline’s scope as intended? [14, 18]\nDoes this item apply to me? [11, 14, 17, 18, 22]\nIs this item optional? [11, 17]\nI do not understand what reporting guidelines are\nWhat are reporting guidelines? [10, 20]\nHow should I use a reporting guideline? [12]\nResearchers may not understand the guidance as intended, or what reporting guidelines are, even if they think they do\n\n\nReporting guidelines benefit me\nI find guidelines useful in general [8, 18]\nGuidelines make me feel confident [10]\nGuidelines help me develop as a researcher [10, 21]\nGuidelines may help me improve my manuscript [8, 10, 11, 21, 22]\nI believe guidelines may help me publish more easily [23]\nI use guidelines because of other people\nI may use guidelines because journals and editors tell me to [10, 12, 21, 23]\nI may use guidelines because other researchers expect it [12, 23]\nGuidelines benefit others\nStandardized reporting benefits the community [10, 23, 42]\nSome benefits are more important than others\nImmediate benefits are more important than hypothetical ones [10, 23]\nPersonal benefits are more important than benefits to others [23]\nResearchers report a variety of reasons for using reporting guidelines, and that some are more important than others\n\n\nResearchers use reporting guidelines for different tasks\nI use reporting guidelines for planning research [10, 11]\nI use reporting guidelines for designing research [8, 10, 17, 20]\nI use reporting guidelines for writing [8, 10, 11, 17]\nI use reporting guidelines for checking my own or other people’s writing [10, 20]\nI use reporting guidelines to appraise the quality of other people’s reporting [13]\nI use reporting guidelines for peer reviewing [10]\nI want guidance presented in formats that are better suited to the task I am doing\nI want items presented in the order in which I must do them [; [42]; [40]]\nI want design or methods advice [10, 11, 14]\nI want templates for writing [8]\nI want checklists that are easy to fill in [9, 18]\nI want checklists embedded into journal submission workflows [8]\nI want items embedded into data collection tools [21]\nResearchers report using reporting guidelines for different tasks and wanting guidance to be delivered in ways that better fit their needs\n\n\nGuidelines take time\nGuidelines take time to read, understand and apply [12, 21, 23]\nSome items require extra work which takes time and effort [11, 15, 41]\nI want an indication of which items to prioritize [11, 17]\nPerceived complexity [8, 9, 11, 23]\nLong guidelines are off-putting [10, 18, 21, 22]\nItemization may decrease costs\nItemization helps me navigate guidance[14]\nItemization summarizes the guidance[8]\nItemization may increase perceived costs\nItemization makes guidance appear longer[14]\nItemization blocks the bigger picture[11]\nI think guidelines make my manuscripts long and bloated\nFollowing reporting guidance can result in long, bloated articles [11, 17, 21, 22]\nLong, bloated articles may exceed journal word limits [9, 12, 17, 22]\nI want options for where to report this item [10–12, 14, 15, 22]\nThe benefits of using a reporting guideline may not outweigh the costs\nThe benefits of using a reporting guideline may not outweigh the costs [10, 12, 22]\nThe balance of benefits vs costs may be more favourable when guidelines are used early\nGuidelines are more valuable when used early [8, 10, 11, 18]\nUsing reporting guidelines has costs, and researchers may not feel that benefits outweigh the costs\n\n\nI think the guidance could be improved\nI would clarify this item [14, 17]\nI would move this item [11, 15]\nI would split this item into two [11, 14, 40]\nI would add or remove items from this guideline [11, 13, 14, 17]\nI would add or remove requirements from this item [10, 14, 16, 17, 24]\nGuidelines need to be kept updated\nGuidelines can become out of date [11]\nGuidelines need to be updated [14]\nReporting guidelines may need to be revised and updated for different reasons\n\n\nI feel unable to report this\nI cannot report this because I didn’t do it [11, 14, 17, 22]\nI cannot report this because of intellectual property issues [22]\nI cannot report this because it clashes with journal guidelines [14]\nI cannot report this because data was missing from my primary studies [21]\nEditors, reviewers or co-authors asked me to remove this item [17, 41]\nI feel nervous or uncertain if I am unable to report an item\nI feel uncertain because I don’t know how to say that I didn’t do it [14]\nI feel worried that I will be judged for transparently reporting something I didn’t do [10, 14]\nResearchers may not be able to report all items which can leave them feeling uncertain or worried\n\n\nI can only use what I know about and have\nI may not know that reporting guidelines exist, or what guidance exists [8, 12, 13, 18, 23]\nI may not be able to easily access guidance [18, 23]\nAwareness and accessibility may limit reporting guideline usage\n\n\nReporting guidelines are more valuable to inexperienced researchers\nReporting guidelines may be less valuable to experienced researchers [8, 10, 22]\nExperienced researchers feel that they already know how to report [8, 10, 11]\nExperienced researchers find guidance patronizing and feel untrusted [9, 12, 14, 22]\nReporting guidelines can be hard to use at first but get easier with experience\nReporting guidelines can be hard to use at first but get easier with experience [11, 12, 23]\nReporting guidelines may be more useful to less experienced researchers, but less experienced researchers may find them harder to use\n\n\nI want or need design advice\nI want design or methodological advice [9, 10, 14]\nI don’t know how to do this item [11, 14, 17]\nI think this guidance prescribes how research should be designed\nGuidelines are procedural straightjackets [10]\nThis guideline is too prescriptive [10, 14, 24]\nResearchers want or need design advice, but reporting guidelines may not be the right place\n\n\nA guideline’s scope can be unclear\nThe guideline’s applicability criteria are not clear [8, 13, 18]\nA guideline can be too narrow\nThis guideline isn’t a perfect fit for me [18]\nThis guideline doesn’t generalise [8–10, 14, 24]\nA guideline’s scope can be too broad\nI don’t want to see optional items that only apply to other types of study [17, 18]\nReporting guidelines can be harder to use if their scope is too broad, too narrow, or poorly defined\n\n\nI often need to adhere to multiple sets of guidance\nI need to adhere to journal guidelines or other research guidelines [8, 12, 14, 17]\nI might need to use multiple reporting guidelines [10]\nI want guidelines to harmonize\nI want reporting guidelines to be linked or embedded [13, 14]\nI want reporting guidelines to use similar structure [14]\nI want reporting guidelines to use similar terms [14]\nResearchers may have to use multiple sets of reporting guidelines, multiplying complexity and costs\n\n\nI experience reporting guidelines primarily as, or through, checklists\nI don’t like checklists[8, 10, 18, 22]\nI may use the checklist instead of the full guidance [16]\nResearchers may use checklists but never read the full guidance"
  },
  {
    "objectID": "chapters/3_synthesis/tbl_info_sources.html",
    "href": "chapters/3_synthesis/tbl_info_sources.html",
    "title": "Thesis",
    "section": "",
    "text": "Table 1: Information sources and record management\n\n\n\n\n\n\n\n\nSource\nSearch platform\nDate searched\nRecord management\n\n\n\n\nChinese Biomedical Literature Database[1]\nChinese Biomedical Literature Service System[2]\n25/10/2021\nYD used Zotero to manage, deduplicate and screen records.\n\n\nChina National Knowledge Infrastructure[3]\nhttps://www.cnki.net/\n\"\n\"\n\n\nWanfang Data[4]\nhttp://www.wanfangdata.com/\n\"\n\"\n\n\nVIP Chinese Medical Journal Database[5]\nhttp://www.cqvip.com/\n\"\n\"\n\n\nMedline\nOvid\n08/12/2021\nI used Zotero to manage and deduplicate records. He used Rayyan to do a second deduplication and screen records.\n\n\nEmbase\n\"\n\"\n\"\n\n\nAllied Complementary Medicine Database (AMED)\n\"\n\"\n\"\n\n\nPsycInfo\n\"\n\"\n\"\n\n\nLatin American and Caribbean Health Sciences Literature[6]\nWHO Global Index Medicus (GIM)[7]\n08/12/2021\n\"\n\n\nAfrican Index Medicus[8]\n\"\n\"\n\"\n\n\nWestern Pacific Region Index Medicus[9]\n\"\n\"\n\"\n\n\nIndex Medicus for South-East Asia region[10]\n\"\n\"\n\"\n\n\nIndex Medicus for the Eastern Mediterranean Region[11]\n\"\n\"\n\"\n\n\nScientific Electronic Library Online[12]\nhttps://scielo.org/en/\n08/12/2021\n\"\n\n\nOpen Science Framework (OSF)\nhttps://osf.io/\n15/12/2021\n\"\n\n\nMethods in Research on Research website [13]\nhttp://miror-ejd.eu/publications/\n14/12/2021\n\"\n\n\nEmailing developers of guidelines listed in Table 2\nn/a\n08/01/2022\n\"\n\n\n\n\n\n\n\n\n1. Chinese Biomedical Literature Database. \n\n\n2. SinoMed. \n\n\n3. China National Knowledge Infrastructure. \n\n\n4. Wanfang Data - A Leading Provider of Electronic Resources for China Studies. \n\n\n5. VIP Chinese Medical Journal Database. \n\n\n6. | LILACS. \n\n\n7. Alves B/O/O-M Global Index Medicus World Health Organization. \n\n\n8. Health and Medical Articles Database - African Index Medicus. \n\n\n9. Western Pacific Region Index Medicus. \n\n\n10. IMSEAR at SEARO: Home. \n\n\n11. WHO EMRO | IMEMR | Library. \n\n\n12. About SciELO. \n\n\n13. Projet MiRoR. Projet MiRoR"
  },
  {
    "objectID": "chapters/1_introduction/9_structure.html",
    "href": "chapters/1_introduction/9_structure.html",
    "title": "Thesis",
    "section": "",
    "text": "Chapter 2 - Reflections on starting my DPhil\nI reflect on my background and my prior held opinions, and those of my supervision team, and how these may have influenced the direction of this thesis.\nChapter 3 - What facilitators and barriers might researchers encounter when using reporting guidelines? Part 1: A thematic synthesis\nThe next three chapters pertain to my first objective - to identify possible reasons as to why reporting guidelines have had only a limited impact on reporting quality. This chapter describes a thematic synthesis of studies that qualitatively explored authors’ experiences of using reporting guidelines, where I sought to identify what may influence whether an author successfully adheres to reporting guidance.\nChapter 4 - What facilitators and barriers might researchers encounter when using reporting guidelines? Part 2: Describing the questions asked in quantitative surveys.\nThis chapter builds on the previous one by identifying additional possible influences from the content of quantitative survey questions.\nChapter 5 - A service evaluation of equator-network.org\nThis chapter describes a service evaluation of the EQUATOR Network website which, although an important piece of the reporting guideline infrastructure, was rarely explored by studies reviewed in the previous two chapters. From this evaluation, I then infer possible barriers that authors may encounter when trying to find and access reporting guidelines from EQUATOR’s website.\nChapter 6 - Selecting the Behaviour Change Wheel framework\nThe next 4 chapters pertain to my second objective - identifying intervention changes. This chapter introduces the Behaviour Change Wheel, which is a framework for designing and defining behaviour change interventions. I explain how my thesis gained form at this point in time; my view of reporting guidelines as a system crystallised, and Charlotte Albury joined my supervision team as my plans took an unexpected qualitative turn. I wanted my thesis to accurately reflect the twists and turns of my DPhil, and so I introduce my chosen framework in this middle chapter instead of the introduction which may be more customary.\nChapter 7 - Following the BCW Guide: Workshops with EQUATOR\nThis chapter describes how I lead workshops with UK EQUATOR center staff to identify intervention options using Behaviour Change Wheel framework.\nChapter 8 - Generating ideas to address factors limiting reporting guideline impact: workshops with EQUATOR and focus groups with developers, publishers, and experts\nThis chapter reports focus groups where I collected ideas on how intervention options could be realised.\nChapter 9 - Defining Intervention Content\nIn this chapter, I bring together the outputs of the previous two chapters to create a table of intervention components.\nChapter 10 - Developing intervention components into a prototype\nThis chapter concerns my third objective; implementing the intervention changes by redesigning a reporting guideline (SRQR) and the EQUATOR Network website’s home page.\nChapter 11 - Refining the intervention: qualitative study with authors\nIn this chapter I address my final objective by refining the intervention in response to feedback from authors. I describe a qualitative study where I used observation, think aloud, structured interviews, and a writing evaluation, to gather feedback from an international sample of authors."
  },
  {
    "objectID": "chapters/1_introduction/9_structure.html#thesis-structure",
    "href": "chapters/1_introduction/9_structure.html#thesis-structure",
    "title": "Thesis",
    "section": "",
    "text": "Chapter 2 - Reflections on starting my DPhil\nI reflect on my background and my prior held opinions, and those of my supervision team, and how these may have influenced the direction of this thesis.\nChapter 3 - What facilitators and barriers might researchers encounter when using reporting guidelines? Part 1: A thematic synthesis\nThe next three chapters pertain to my first objective - to identify possible reasons as to why reporting guidelines have had only a limited impact on reporting quality. This chapter describes a thematic synthesis of studies that qualitatively explored authors’ experiences of using reporting guidelines, where I sought to identify what may influence whether an author successfully adheres to reporting guidance.\nChapter 4 - What facilitators and barriers might researchers encounter when using reporting guidelines? Part 2: Describing the questions asked in quantitative surveys.\nThis chapter builds on the previous one by identifying additional possible influences from the content of quantitative survey questions.\nChapter 5 - A service evaluation of equator-network.org\nThis chapter describes a service evaluation of the EQUATOR Network website which, although an important piece of the reporting guideline infrastructure, was rarely explored by studies reviewed in the previous two chapters. From this evaluation, I then infer possible barriers that authors may encounter when trying to find and access reporting guidelines from EQUATOR’s website.\nChapter 6 - Selecting the Behaviour Change Wheel framework\nThe next 4 chapters pertain to my second objective - identifying intervention changes. This chapter introduces the Behaviour Change Wheel, which is a framework for designing and defining behaviour change interventions. I explain how my thesis gained form at this point in time; my view of reporting guidelines as a system crystallised, and Charlotte Albury joined my supervision team as my plans took an unexpected qualitative turn. I wanted my thesis to accurately reflect the twists and turns of my DPhil, and so I introduce my chosen framework in this middle chapter instead of the introduction which may be more customary.\nChapter 7 - Following the BCW Guide: Workshops with EQUATOR\nThis chapter describes how I lead workshops with UK EQUATOR center staff to identify intervention options using Behaviour Change Wheel framework.\nChapter 8 - Generating ideas to address factors limiting reporting guideline impact: workshops with EQUATOR and focus groups with developers, publishers, and experts\nThis chapter reports focus groups where I collected ideas on how intervention options could be realised.\nChapter 9 - Defining Intervention Content\nIn this chapter, I bring together the outputs of the previous two chapters to create a table of intervention components.\nChapter 10 - Developing intervention components into a prototype\nThis chapter concerns my third objective; implementing the intervention changes by redesigning a reporting guideline (SRQR) and the EQUATOR Network website’s home page.\nChapter 11 - Refining the intervention: qualitative study with authors\nIn this chapter I address my final objective by refining the intervention in response to feedback from authors. I describe a qualitative study where I used observation, think aloud, structured interviews, and a writing evaluation, to gather feedback from an international sample of authors."
  },
  {
    "objectID": "chapters/1_introduction/8_aims_and_objectives.html",
    "href": "chapters/1_introduction/8_aims_and_objectives.html",
    "title": "Thesis",
    "section": "",
    "text": "My aim was to identify and address barriers preventing authors from adhering to reporting guidelines. I wanted to explore the entire reporting guideline system, and I wanted to be thorough: I wanted to identify as many barriers as possible, and as many solutions as possible, before deciding which to implement.\nMy objectives were:\n\nTo identify factors that may limit reporting guideline impact by synthesising existing research and by evaluating the EQUATOR Network website (addressed in chapters 3)\nTo work with key stakeholders to identify intervention changes to address these limiting factors (addressed in chapters 6)\nTo implement these changes (described in 10)\nTo refine the new intervention in response to feedback from authors (addressed in chapter 11)\n\nMy thesis bares many of the hallmarks of pragmatism. I used both qualitative and quantitative methods. Constraints (like time and access to participants) influenced my decisions. I balanced participants’ views with my own; I sought to remove my views as much as possible in all chapters except for the workshops I conducted with EQUATOR (chapter 7) and when designing the intervention (chapter 10). I balanced inductive and deductive reasoning; my early chapters were exploratory and inductive, and my later chapters became increasingly deductive as my focus narrowed and I relied more heavily on a framework.\nASK Do I need to justify why I did not seek to quantify the system, or to define it using behaviour change techniques?"
  },
  {
    "objectID": "chapters/1_introduction/8_aims_and_objectives.html#aims-and-objectives",
    "href": "chapters/1_introduction/8_aims_and_objectives.html#aims-and-objectives",
    "title": "Thesis",
    "section": "",
    "text": "My aim was to identify and address barriers preventing authors from adhering to reporting guidelines. I wanted to explore the entire reporting guideline system, and I wanted to be thorough: I wanted to identify as many barriers as possible, and as many solutions as possible, before deciding which to implement.\nMy objectives were:\n\nTo identify factors that may limit reporting guideline impact by synthesising existing research and by evaluating the EQUATOR Network website (addressed in chapters 3)\nTo work with key stakeholders to identify intervention changes to address these limiting factors (addressed in chapters 6)\nTo implement these changes (described in 10)\nTo refine the new intervention in response to feedback from authors (addressed in chapter 11)\n\nMy thesis bares many of the hallmarks of pragmatism. I used both qualitative and quantitative methods. Constraints (like time and access to participants) influenced my decisions. I balanced participants’ views with my own; I sought to remove my views as much as possible in all chapters except for the workshops I conducted with EQUATOR (chapter 7) and when designing the intervention (chapter 10). I balanced inductive and deductive reasoning; my early chapters were exploratory and inductive, and my later chapters became increasingly deductive as my focus narrowed and I relied more heavily on a framework.\nASK Do I need to justify why I did not seek to quantify the system, or to define it using behaviour change techniques?"
  },
  {
    "objectID": "chapters/1_introduction/2_poor_reporting.html",
    "href": "chapters/1_introduction/2_poor_reporting.html",
    "title": "Thesis",
    "section": "",
    "text": "Facing an uncertain choice during treatment for multiple myeloma, epidemiologist Alessandro Liberati wrote “Why was I forced to make my decision knowing that information was somewhere but not available?”[1]. When I started my PhD, governments may have been asking the same question. The world was in the grip of COVID-19 and decision makers were wading through a deluge of patchy research articles missing important information [2]. In the years since, friends and family have had to make treatment decisions where the evidence is of “low certainty” because key details are missing from research articles.\nA selfish silver-lining of these tumultuous years was that my family and friends finally understood the problem my thesis addresses: when medical researchers inadequately describe what they did or what they found, other people cannot understand, replicate, or use their work. Research costs huge amounts of time, money, and effort, and the written account is typically its sole legacy. When details are omitted they are lost. The remaining gaps are sources of doubt; are they accidental omissions? Oversights? Cover-ups? Whatever their source, the gaps fragment the full picture, and the potential value to patients drains away.\nEarly concern over reporting quality often came from frustrated reviewers unable to find the data they needed within research reports. For example, in 1963, Glick [3] found many reports of psychiatric therapy used ambiguous descriptions of treatment duration like “at least two months” or “from one to several months”. These descriptions were so vague they were “unsuitable for comparative purposes”. More recently, Dechartes found systematic reviewers could not judge the potential for bias in a third of clinical trials because of poorly described methods, thereby limiting the confidence of conclusions [4].\nReviewers are not the only people affected. When interventions are poorly described, researchers cannot appraise or repeat research. Carp [5] described how a third of 241 brain imaging studies missed information necessary to interpret and repeat them, like the number of examinations, examination duration, and the resolution of images. Doctors and service providers also need clear descriptions to replicate interventions [6]. As Feinstein noted in 1974 [7], it is difficult enough for a clinician to understand the value of unfamiliar procedure, but “it is much more difficult when he is not told what that procedure was”. For example, Davidson et al. [8] reviewed trials describing exercise interventions for chronic back pain and found authors often did not describe interventions sufficiently for other healthcare providers to copy them.\nThese are a mere handful of many studies documenting poor reporting in medical literature. A 2023 systematic review found 148 published between 2020-2022 alone [9]. All investigated reporting quality in different medical research disciplines, and almost all concluded reporting was sub-optimal. Hence, poor reporting is a long-standing problem, plagues all disciplines, devalues research, and derails the uptake of new knowledge into clinical practice.\n\n\n\n\n1. Liberati A (2004) An unfinished trip through uncertainties. BMJ : British Medical Journal 328:531\n\n\n2. Ziemann S, Paetzolt I, Grüßer L, Coburn M, Rossaint R, Kowark A (2022) Poor reporting quality of observational clinical studies comparing treatments of COVID-19 a retrospective cross-sectional study. BMC Medical Research Methodology 22:23\n\n\n3. Glick BS (1963) Inadequacies in the reporting of clinical drug research. The Psychiatric Quarterly 37:234–244\n\n\n4. Dechartres A, Trinquart L, Atal I, Moher D, Dickersin K, Boutron I, Perrodeau E, Altman DG, Ravaud P (2017) Evolution of poor reporting and inadequate methods over time in 20 920 randomised controlled trials included in Cochrane reviews: Research on research study. BMJ 357:j2490\n\n\n5. Carp J (2012) The secret lives of experiments: Methods reporting in the fMRI literature. NeuroImage 63:289–300\n\n\n6. Glasziou P, Meats E, Heneghan C, Shepperd S (2008) What is missing from descriptions of treatment in trials and reviews? BMJ 336:1472–1474\n\n\n7. Feinstein AR (1974) Clinical biostatistics. XXV. A survey of the statistical procedures in general medical journals. Clinical Pharmacology and Therapeutics 15:97–107\n\n\n8. Davidson SRE, Kamper SJ, Haskins R, Robson E, Gleadhill C, da Silva PV, Williams A, Yu Z, Williams CM (2021) Exercise interventions for low back pain are poorly reported: A systematic review. Journal of Clinical Epidemiology 139:279–286\n\n\n9. Santo TD, Rice DB, Amiri LSN, Tasleem A, Li K, Boruff JT, Geoffroy M-C, Benedetti A, Thombs BD (2023) Methods and results of studies on reporting guideline adherence are poorly reported: A meta-research study. Journal of Clinical Epidemiology 159:225–234"
  },
  {
    "objectID": "chapters/1_introduction/2_poor_reporting.html#the-problem-of-poor-reporting-in-health-research",
    "href": "chapters/1_introduction/2_poor_reporting.html#the-problem-of-poor-reporting-in-health-research",
    "title": "Thesis",
    "section": "",
    "text": "Facing an uncertain choice during treatment for multiple myeloma, epidemiologist Alessandro Liberati wrote “Why was I forced to make my decision knowing that information was somewhere but not available?”[1]. When I started my PhD, governments may have been asking the same question. The world was in the grip of COVID-19 and decision makers were wading through a deluge of patchy research articles missing important information [2]. In the years since, friends and family have had to make treatment decisions where the evidence is of “low certainty” because key details are missing from research articles.\nA selfish silver-lining of these tumultuous years was that my family and friends finally understood the problem my thesis addresses: when medical researchers inadequately describe what they did or what they found, other people cannot understand, replicate, or use their work. Research costs huge amounts of time, money, and effort, and the written account is typically its sole legacy. When details are omitted they are lost. The remaining gaps are sources of doubt; are they accidental omissions? Oversights? Cover-ups? Whatever their source, the gaps fragment the full picture, and the potential value to patients drains away.\nEarly concern over reporting quality often came from frustrated reviewers unable to find the data they needed within research reports. For example, in 1963, Glick [3] found many reports of psychiatric therapy used ambiguous descriptions of treatment duration like “at least two months” or “from one to several months”. These descriptions were so vague they were “unsuitable for comparative purposes”. More recently, Dechartes found systematic reviewers could not judge the potential for bias in a third of clinical trials because of poorly described methods, thereby limiting the confidence of conclusions [4].\nReviewers are not the only people affected. When interventions are poorly described, researchers cannot appraise or repeat research. Carp [5] described how a third of 241 brain imaging studies missed information necessary to interpret and repeat them, like the number of examinations, examination duration, and the resolution of images. Doctors and service providers also need clear descriptions to replicate interventions [6]. As Feinstein noted in 1974 [7], it is difficult enough for a clinician to understand the value of unfamiliar procedure, but “it is much more difficult when he is not told what that procedure was”. For example, Davidson et al. [8] reviewed trials describing exercise interventions for chronic back pain and found authors often did not describe interventions sufficiently for other healthcare providers to copy them.\nThese are a mere handful of many studies documenting poor reporting in medical literature. A 2023 systematic review found 148 published between 2020-2022 alone [9]. All investigated reporting quality in different medical research disciplines, and almost all concluded reporting was sub-optimal. Hence, poor reporting is a long-standing problem, plagues all disciplines, devalues research, and derails the uptake of new knowledge into clinical practice.\n\n\n\n\n1. Liberati A (2004) An unfinished trip through uncertainties. BMJ : British Medical Journal 328:531\n\n\n2. Ziemann S, Paetzolt I, Grüßer L, Coburn M, Rossaint R, Kowark A (2022) Poor reporting quality of observational clinical studies comparing treatments of COVID-19 a retrospective cross-sectional study. BMC Medical Research Methodology 22:23\n\n\n3. Glick BS (1963) Inadequacies in the reporting of clinical drug research. The Psychiatric Quarterly 37:234–244\n\n\n4. Dechartres A, Trinquart L, Atal I, Moher D, Dickersin K, Boutron I, Perrodeau E, Altman DG, Ravaud P (2017) Evolution of poor reporting and inadequate methods over time in 20 920 randomised controlled trials included in Cochrane reviews: Research on research study. BMJ 357:j2490\n\n\n5. Carp J (2012) The secret lives of experiments: Methods reporting in the fMRI literature. NeuroImage 63:289–300\n\n\n6. Glasziou P, Meats E, Heneghan C, Shepperd S (2008) What is missing from descriptions of treatment in trials and reviews? BMJ 336:1472–1474\n\n\n7. Feinstein AR (1974) Clinical biostatistics. XXV. A survey of the statistical procedures in general medical journals. Clinical Pharmacology and Therapeutics 15:97–107\n\n\n8. Davidson SRE, Kamper SJ, Haskins R, Robson E, Gleadhill C, da Silva PV, Williams A, Yu Z, Williams CM (2021) Exercise interventions for low back pain are poorly reported: A systematic review. Journal of Clinical Epidemiology 139:279–286\n\n\n9. Santo TD, Rice DB, Amiri LSN, Tasleem A, Li K, Boruff JT, Geoffroy M-C, Benedetti A, Thombs BD (2023) Methods and results of studies on reporting guideline adherence are poorly reported: A meta-research study. Journal of Clinical Epidemiology 159:225–234"
  },
  {
    "objectID": "chapters/1_introduction/complexity_table.html",
    "href": "chapters/1_introduction/complexity_table.html",
    "title": "Thesis",
    "section": "",
    "text": "Table 1: Sources of complexity within reporting guidelines and the system drives their use.\n\n\n\n\n\n\nSource of complexity\nExample\n\n\n\n\nNumber of components involved\nReporting guidelines often consist of guidance documents, checklists, and flow diagrams, and other tools. These are disseminated through websites, publishing platforms, submission systems, and they are endorsed and enforced by staff at stakeholders including publishers, the EQUATOR Network, conference organisers, and pre-print platforms.\n\n\nRange of behaviours targeted\nGuidelines comprise “reporting items”. Some items are relatively simple, like asking the author to specify their study design in the title. Others are harder, perhaps because they require time, expertise, or prerequisite tasks. For instance, some items may require authors to have conducted their study or analysis in a certain way, or to have collected particular information.\n\n\nExpertise and skills required by those delivering and receiving the intervention\nAcademics from a particular field write reporting guidelines for their peers (as opposed to a lay audience), and so authors, editors, and reviewers must have sufficient expertise to use them.\n\n\nThe number of groups, settings, or levels targeted\nGroups: Users of reporting guidelines differ in their field of expertise, their experience, place of work.\n\nSettings: Although mostly written with authoring in mind, most guideline developers may also hope their resources are used by editors or peer reviewers for checking or appraising research articles.\n\nGuidelines are written with individuals in mind, but their efficacy is generally measured at group level (e.g. articles from a particular field published in a period time).\n\n\nFlexibility of the intervention or its components\nThere is variation between guideline content, resources, and the implementation strategies that development groups, publishers, and other stakeholders employ."
  },
  {
    "objectID": "chapters/1_introduction/1_intro.html",
    "href": "chapters/1_introduction/1_intro.html",
    "title": "Thesis",
    "section": "",
    "text": "In this chapter I introduce the evidence gap I have addressed and the approach I have taken to address it. I begin by describing the prevalence and consequences of poorly reported medical research. I introduce reporting guidelines and position them as part of a complex behaviour change intervention. This intervention has had a disappointing impact, as medical research is still poorly reported. This brings me to my evidence gap: how can we get more authors to adhere to reporting guidelines? I then outline my aims, objectives, and thesis structure."
  },
  {
    "objectID": "chapters/1_introduction/rg_table.html",
    "href": "chapters/1_introduction/rg_table.html",
    "title": "Thesis",
    "section": "",
    "text": "Table 1: A selection of highly cited reporting guidelines\n\n\n\n\n\n\n\n\n\n\n\n\n\nGuideline acronym\nDefinition\nApplicable study type\nPublication year\nDevelopment article?\nFillable checklist?\nExplanatory document?\nOther resources\nInfluences design?\n\n\nCONSORT\nConsolidated Standards of Reporting Trials\nRandomised controlled trials\n1996 #REF updated in 2001 #REF and 2010 #REF\nYes\nYes\nYes, as a separate article\nFlow diagram\nWebsite\nCOBWEB writing tool #REF\nNo\n\n\nPRISMA\nPreferred Reporting Items for Systematic Reviews and Meta-Analyses\nSystematic Reviews and Meta-Analyses\n2009 #REF\nUpdated in 2021 #REF\nYes\nYes\nYes\nFlow diagram\nWebsite\nNo\n\n\nARRIVE\nAnimal Research: Reporting of In Vivo Experiments\nPublications describing research involving live animals\n2010 #REF\nUpdated in 2020 #REF\nYes\nYes\nYes\nWebsite\nAction Plans\nCompliance questionnaire\nNot explicitly, but does contain design guidance\n\n\nSRQR\nStandards for Reporting Qualitative Research\nQualitative health research\n2014 [1]\nYes\nNo\nYes, as supplementary material that is hard to find\n\n\n\n\ne.t.c. for all guidelines mentioned on EQUATOR’s home page\n\n#TODO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. O’Brien BC, Harris IB, Beckman TJ, Reed DA, Cook DA (2014) Standards for Reporting Qualitative Research: A Synthesis of Recommendations. Academic Medicine 89:1245–1251"
  },
  {
    "objectID": "chapters/1_introduction/policy_table.html",
    "href": "chapters/1_introduction/policy_table.html",
    "title": "Thesis",
    "section": "",
    "text": "Table 1: Examples of how journals have introduced reporting guidelines into their policies. Journals also differ in the reporting guidelines they enforce. For example, some journals may only have policies for randomised trials or systematic reviews, whereas other journals may enforce guidelines for other study types. To my knowledge, no journal explicitly advises against using a reporting guideline.\n\n\n\n\n\n\nEnforcement type\nExample\n\n\n\n\nEnforcing adherence\nAn editor or peer reviewer checks the article body for reporting guideline adherence and asks the author to add missing items.\n\n\nRequesting peer reviewers use reporting guidelines\nEditors ask peer reviewers to consider reporting guideline adherence as part of their review. Some editors may supply the reviewer with the relevant checklist. The reviewer can choose whether to review reporting.\n\n\nEnforcing checklist submission\nEditorial staff require authors to submit a completed reporting checklist as part of manuscript submission. Some journals may refuse to process a submission when the checklist is missing. Some journal submission systems may include fields for authors to upload their checklists, whereas other journals may expect authors to upload checklists as a supplementary file.\n\n\nJournal endorsement\nThe journal’s instructions to authors recommends authors follow reporting guidelines. Guidelines may be specified, in which case journals may link to guideline specific websites, to the guideline publications, or to the EQUATOR Network website. Sometimes journals include a general statement but do not name guidelines, instead referring authors to the EQUATOR website with an instruction to follow “relevant guidance”.\n\n\nPublisher endorsement\nSometimes reporting guideline policies are set at the level of the publisher, as is commonly done for editorial policies. Individual journals may point authors to their publisher policies.\n\n\nNo policy\nJournals have no policies regarding reporting guidelines."
  },
  {
    "objectID": "chapters/abstract.html",
    "href": "chapters/abstract.html",
    "title": "Abstract",
    "section": "",
    "text": "Abstract"
  },
  {
    "objectID": "chapters/4_survey_content/tbl_codes_themes.html",
    "href": "chapters/4_survey_content/tbl_codes_themes.html",
    "title": "Thesis",
    "section": "",
    "text": "Table 1: Codes and descriptive themes identified from a qualitative evidence synthesis. Items in bold did not appear in the quantitative questions. Items in italic offer possible explanations to some quantitative findings.\n\n\n\n\n\n\nCodes\nDescriptive Themes\n\n\n\n\nWhat does this term mean? [1–5]\nWhat does this item mean? [1–6]\nHow are these items different? [2, 4, 6, 7]\nHave I understood this as intended? [1, 2]\nExamples help me understand items [[CSL STYLE ERROR: reference with no printed form.]; [4]; [8]]\nWhat does this mean?\n\n\nWhy is this item important? [2–4, 9]\nWho is this item important to? [2, 4, 10]\nWhy is this item important?\n\n\nHave I understood the guideline’s scope as intended? [4, 5]\nDoes this item apply to me? [2, 4–7]\nIs this item optional? [2, 6]\nDoes this apply to me?\n\n\nWhat are reporting guidelines? [10, 11]\nHow should I use a reporting guideline? [12]\nI don’t understand what reporting guidelines are\n\n\nI find guidelines useful in general [5, 13]\nGuidelines make me feel confident [10]\nGuidelines help me develop as a researcher [10, 14]\nGuidelines may help me improve my manuscript [2, 7, 10, 13, 14]\nI believe guidelines may help me publish more easily [15]\nGuidelines benefit me\n\n\nI may use guidelines because journals and editors tell me to [10, 12, 14, 15]\nI may use guidelines because other researchers expect it [12, 15]\nI use guidelines because of other people\n\n\nStandardized reporting benefits the community [10, 15, 16]\nGuidelines benefit others\n\n\nImmediate benefits are more important than hypothetical ones [10, 15]\nPersonal benefits are more important than benefits to others [15]\nSome benefits are more important than others\n\n\nI use reporting guidelines for planning research [2, 10]\nI use reporting guidelines for designing research [6, 10, 11, 13]\nI use reporting guidelines for writing [2, 6, 10, 13]\nI use reporting guidelines for checking my own or other people’s writing [10, 11]\nI use reporting guidelines to appraise the quality of other people’s reporting [3]\nI use reporting guidelines for peer reviewing [10]\nResearchers use reporting guidelines for different tasks\n\n\nI want items presented in the order in which I must do them [[CSL STYLE ERROR: reference with no printed form.]; [16]; [8]]\nI want design or methods advice [2, 4, 10]\nI want templates for writing [13]\nI want checklists that are easy to fill in [5, 17]\nI want checklists embedded into journal submission workflows [13]\nI want items embedded into data collection tools [14]\nI want guidance presented in formats that are better suited to the task I am doing\n\n\nGuidelines take time to read, understand and apply [12, 14, 15]\nSome items require extra work which takes time and effort [1, 2, 18]\nI want an indication of which items to prioritize [2, 6]\nPerceived complexity [2, 13, 15, 17]\nLong guidelines are off-putting [5, 7, 10, 14]\nGuidelines take time\n\n\nItemization helps me navigate guidance[4]\nItemization summarizes the guidance[13]\nItemization may decrease costs\n\n\nItemization makes guidance appear longer[4]\nItemization blocks the bigger picture[2]\nItemization may increase perceived costs\n\n\nFollowing reporting guidance can result in long, bloated articles [2, 6, 7, 14]\nLong, bloated articles may exceed journal word limits [6, 7, 12, 17]\nI want options for where to report this item [1, 2, 4, 7, 10, 12]\nI think guidelines make my manuscripts long and bloated\n\n\nThe benefits of using a reporting guideline may not outweigh the costs [7, 10, 12]\nThe benefits of using a reporting guideline may not outweigh the costs\n\n\nGuidelines are more valuable when used early [2, 5, 10, 13]\nThe balance of benefits vs costs may be more favourable when guidelines are used early\n\n\nI would clarify this item [4, 6]\nI would move this item [1, 2]\nI would split this item into two [2, 4, 8]\nI would add or remove items from this guideline [2–4, 6]\nI would add or remove requirements from this item [[CSL STYLE ERROR: reference with no printed form.]; [4]; [6]; [10]; [9]]\nI think the guidance could be improved\n\n\nGuidelines can become out of date [2]\nGuidelines need to be updated [4]\nGuidelines need to be kept updated\n\n\nI cannot report this because I didn’t do it [6]\nI cannot report this because of intellectual property issues [7]\nI cannot report this because it clashes with journal guidelines [4]\nI cannot report this because data was missing from my primary studies [14]\nEditors, reviewers or co-authors asked me to remove this item [6, 18]\nI feel unable to report this\n\n\nI feel uncertain because I don’t know how to say that I didn’t do it [4]\nI feel worried that I will be judged for transparently reporting something I didn’t do [4, 10]\nI feel nervous or uncertain if I am unable to report an item\n\n\nI may not know that reporting guidelines exist [3, 5, 12, 13, 15]\nI may not be able to easily access guidance [5, 15]\nI can only use what I know about and have\n\n\nReporting guidelines may be less valuable to experienced researchers [7, 10, 13]\nExperienced researchers feel that they already know how to report [2, 10, 13]\nExperienced researchers find guidance patronizing and feel untrusted [4, 7, 12, 17]\nReporting guidelines are more valuable to inexperienced researchers\n\n\nReporting guidelines can be hard to use at first but get easier with experience [2, 12, 15]\nReporting guidelines can be hard to use at first but get easier with experience\n\n\nI want design or methodological advice [4, 10, 17]\nI don’t know how to do this item [2, 4, 6]\nI want or need design advice\n\n\nGuidelines are procedural straightjackets [10]\nThis guideline is too prescriptive [4, 9, 10]\nI think this guidance prescribes how research should be designed\n\n\nThe guideline’s applicability criteria are not clear [3, 5, 13]\nA guideline’s scope can be unclear\n\n\nThis guideline isn’t a perfect fit for me [5]\nThis guideline doesn’t generalise [4, 9, 10, 13, 17]\nThis guideline is too prescriptive [4, 9, 10]\nA guideline can be too narrow\n\n\nI don’t want to see optional items that only apply to other types of study [5, 6]\nA guideline’s scope can be too broad\n\n\nI need to adhere to journal guidelines or other research guidelines [4, 6, 12, 13]\nI might need to use multiple reporting guidelines [10]\nAuthors often need to adhere to multiple sets of guidance\n\n\nI want reporting guidelines to be linked or embedded [3, 4]\nI want reporting guidelines to use similar structure [4]\nI want reporting guidelines to use similar terms [4]\nI want guidelines to harmonize\n\n\nI don’t like checklists[5, 7, 10, 13]\nI may use the checklist instead of the full guidance [[CSL STYLE ERROR: reference with no printed form.]]\nI may use the checklist before I read the full guidance [[CSL STYLE ERROR: reference with no printed form.]]\nI experience reporting guidelines primarily as, or through, checklists\n\n\n\n\n\n\n\n\n1. Davies L, Donnelly KZ, Goodman DJ, Ogrinc G (2016) Findings from a novel approach to publication guideline revision: User road testing of a draft version of SQUIRE 2.0. BMJ quality & safety 25:265–272\n\n\n2. Davies L, Batalden P, Davidoff F, Stevens D, Ogrinc G (2015) The SQUIRE Guidelines: An evaluation from the field, 5years post release. BMJ quality & safety 24:769–775\n\n\n3. Korevaar DA, Cohen JF, Reitsma JB, et al (2016) Updating standards for reporting diagnostic accuracy: The development of STARD 2015. Research integrity and peer review 1:7\n\n\n4. Page MJ, McKenzie JE, Bossuyt PM, Boutron I, Hoffmann TC, Mulrow CD, Shamseer L, Tetzlaff JM, Moher D (2021) Updating guidance for reporting systematic reviews: Development of the PRISMA 2020 statement. Journal of Clinical Epidemiology 134:103–112\n\n\n5. Struthers C, Harwood J, de Beyer JA, Dhiman P, Logullo P, Schlüssel M (2021) GoodReports: Developing a website to help health researchers find and use reporting guidelines. BMC medical research methodology 21:217\n\n\n6. Prady SL, MacPherson H (2007) Assessing the utility of the standards for reporting trials of acupuncture (STRICTA): A survey of authors. The Journal of Alternative and Complementary Medicine 13:939–943\n\n\n7. Eysenbach G. (2013) CONSORT-EHEALTH: Implementation of a checklist for authors and editors to improve reporting of web-based and mobile randomized controlled trials. Studies in health technology and informatics 192:657–661\n\n\n8. de Vries RBM, Hooijmans CR, Langendam MW, van Luijk J, Leenaars M, Ritskes-Hoitinga M, Wever KE (2015) A protocol format for the preparation, registration and publication of systematic reviews of animal intervention studies. Evidence-based Preclinical Medicine 2:e00007\n\n\n9. Tam WWS, Tang A, Woo B, Goh SYS (2019) Perception of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement of authors publishing reviews in nursing journals: A cross-sectional online survey. BMJ Open 9:e026271\n\n\n10. Sharp MK, Glonti K, Hren D (2020) Online survey about the STROBE statement highlighted diverging views about its content, purpose, and value. Journal of clinical epidemiology 123:100–106\n\n\n11. Prager R, Gagnon L, Bowdridge J, Unni RR, McGrath TA, Cobey K, Bossuyt PM, McInnes MDF (2021) Barriers to reporting guideline adherence in point-of-care ultrasound research: A cross-sectional survey of authors and journal editors. BMJ Evidence-Based Medicine bmjebm-2020-111604\n\n\n12. Fuller T, Pearson M, Peters J, Anderson R (2015) What affects authors’ and editors’ use of reporting guidelines? Findings from an online survey and qualitative interviews. PLoS ONE 10:e0121585\n\n\n13. Dewey M, Levine D, Bossuyt PM, Kressel HY (2019) Impact and perceived value of journal reporting guidelines among Radiology authors and reviewers. European Radiology 29:3986–3995\n\n\n14. Burford BJ, Welch V, Waters E, Tugwell P, Moher D, O’Neill J, Koehlmoos T, Petticrew M (2013) Testing the PRISMA-Equity 2012 reporting guideline: The perspectives of systematic review authors. PloS one 8:e75122\n\n\n15. Svensøy JN, Nilsson H, Rimstad R (2021) A qualitative study on researchers’ experiences after publishing scientific reports on major incidents, mass-casualty incidents, and disasters. Prehospital and Disaster Medicine 36:536–542\n\n\n16. Page MJ, McKenzie JE, Bossuyt PM, et al (2021) The PRISMA 2020 statement: An updated guideline for reporting systematic reviews. BMJ (Clinical research ed) 372:n71\n\n\n17. Macleod M, Collings AM, Graf C, Kiermer V, Mellor D, Swaminathan S, Sweet D, Vinson V (2021) The MDAR (Materials Design Analysis Reporting) Framework for transparent reporting in the life sciences. Proceedings of the National Academy of Sciences 118:e2103238118\n\n\n18. Rader T., Mann M., Stansfield C., Cooper C., Sampson M. (2014) Methods for documenting systematic review searches: A discussion of common issues. Research synthesis methods 5:98–115"
  },
  {
    "objectID": "chapters/4_survey_content/index.html",
    "href": "chapters/4_survey_content/index.html",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 2: Describing the questions asked in quantitative surveys.",
    "section": "",
    "text": "Known todos:\n\nSome citations need fixing\nI may add a reflexivity paragraph"
  },
  {
    "objectID": "chapters/4_survey_content/index.html#background",
    "href": "chapters/4_survey_content/index.html#background",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 2: Describing the questions asked in quantitative surveys.",
    "section": "Background",
    "text": "Background\nIn the previous chapter I describe my systematic search and thematic synthesis of qualitative research exploring authors’ experiences of using reporting guidelines. I identified potential influences that may affect whether an author adheres to reporting guidelines. During my search, I observed that many studies also included quantitative survey questions, and despite not considering them in my qualitative synthesis, I felt that these questions were important to investigate for two reasons. Firstly, as many of the researchers designing these surveys were themselves users or developers of reporting guidelines the questions may reflect real barriers or facilitators that they have experienced, witnessed, or are trying to avoid or achieve. Secondly, in mixed-method surveys, quantitative questions may bias responses to subsequent qualitative questions and, therefore, the findings of my thematic synthesis. For instance, qualitative questions like “Anything else?” or “Please elaborate” may lead respondents to neglect or repeat topics covered by the previous quantitative questions.\nIn this study, I describe the landscape and content of quantitative surveys that solicit information on author experience of using reporting guidelines. My aim was to identify additional possible influences that were absent from the qualitative evidence synthesis."
  },
  {
    "objectID": "chapters/4_survey_content/index.html#methods",
    "href": "chapters/4_survey_content/index.html#methods",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 2: Describing the questions asked in quantitative surveys.",
    "section": "Methods",
    "text": "Methods\nMy qualitative synthesis included a systematic search that sought to capture all survey studies investigating reporting guidelines (see (in press) for full search details). I found 22 studies that included quantitative survey questions, 14 of which also included one or more qualitative questions (see Table 1). YD translated two studies from Chinese into English[1, 2]. I imported files into NVivo, including the full surveys where available, labelled all questions with descriptive codes, creating new codes when necessary, and then inductively grouped related codes into broad categories (see Table 2)."
  },
  {
    "objectID": "chapters/4_survey_content/index.html#results",
    "href": "chapters/4_survey_content/index.html#results",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 2: Describing the questions asked in quantitative surveys.",
    "section": "Results",
    "text": "Results\n\nWhat reporting guidelines were studied?\nBetween the 22 studies 25 reporting guidelines were mentioned, most frequently PRISMA (n=6), STARD (n=6), CONSORT (n=6) and ARRIVE (n=5) (see my List of Abbreviations for the full titles of each reporting guideline). Thus, only a small proportion of the reporting guidelines indexed in EQUATOR’s database [3] have been evaluated with quantitative questions. Fourteen studies focussed on a single guideline (n=14), with others asking questions about multiple guidelines (e.g. “which reporting guidelines [participants] had known”[4]) or guidelines in general (e.g., “whether they had used reporting guidelines in their publications”[5]). Most studies included participants from the USA, Europe, and Canada and only a few studies conducted elsewhere (e.g., China and Turkey) (see Table 1).\nIn comparison, my thematic synthesis (chapter 3) identified 18 studies that collected qualitative data. These studies covered only 12 reporting guidelines and were all conducted in western countries, hence were slightly less diverse than the quantitative survey studies.\n\n\nThe focus of quantitative questions\nSurvey studies asked participants:\n\nwhether they were aware or familiar with certain reporting guidelines,\nhow often they used them and what for,\nwhether reporting guidelines had influenced their behaviour,\nwhether guidance was usable and useful,\ntheir opinions on guidance content,\ntheir reasons for using a reporting guideline,\ntheir opinions on reporting quality in the literature,\nwhether reporting guidelines were easy to find and access,\nwhose role it was to check for compliance,\nwhether the aim of the guidance was clear, and\nopinions on things explicitly named as a barrier including the length of the guidance, the language it is written in, and the time needed to use it.\nopinions on things explicitly named as a facilitator or motivator including endorsements, evidence, explanatory information, training, the behaviour of peers, and the development process of the guidance.\n\n\n\nComparing the focus of quantitative questions with themes derived from qualitative data\nThe quantitative questions included some novel influences not contained in the qualitative data (shown in bold in Table 2), such as training as a possible facilitator [6], whether authors had heard of the EQUATOR Network [4, 6], and whether transparency in guideline development is important [6]. One study asked whether language may be a barrier to using reporting guidelines for some[7]. This concern may have been missing from the qualitative studies because they were conducted in English. Both quantitative questions and the qualitative data mentioned journals enforcing reporting guidelines, but only quantitative questions asked whether funders and employers should also enforce them.\nMost ideas captured in the quantitative questions also appeared in the qualitative data. This may indicate that the quantitative questions asked were pertinent, or perhaps that they influenced participants’ responses to subsequent, qualitative questions, as mixed method surveys were included in this commentary and the qualitative synthesis.\nOverall, although the qualitative questions contained some novel themes, I found that the qualitative data contained many more ideas that were not addressed by the quantitative questions (see bold items in Table 3). These included what authors understand reporting guidelines to be, the pros and cons of itemization, ideas of how guidance could be improved, negative feelings when an item cannot be reported as desired, the pros and cons of including design advice in reporting guidance, whether optional items were understood as being optional, and frustration when the scope of a reporting guideline is too broad, narrow, or unclear.\nThe qualitative data sometimes provided context to or explanation for quantitative answers (see italicised items in Table 3). For example, many of the quantitative surveys asked participants whether they could understand the guidance. However, a quantitative answer to this question does not reveal what the participant understands, how they understand it, or whether they understand it as intended. The qualitative data contained reports of people failing to understand the wording of an item, how to report that item in practice, whether an item applies to them, whether a reporting guideline applies to them, what the intended scope of a reporting guideline is, or even what a reporting guideline is at all. One study found that although authors reported understanding an item, their writing showed that they had interpreted it differently to how the reporting guideline developers had intended[8]."
  },
  {
    "objectID": "chapters/4_survey_content/index.html#advice-for-future-studies",
    "href": "chapters/4_survey_content/index.html#advice-for-future-studies",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 2: Describing the questions asked in quantitative surveys.",
    "section": "Advice for future studies",
    "text": "Advice for future studies\nAs very few reporting guidelines have undergone any kind of user testing, I urge guideline developers to evaluate their resources to ensure researchers understand their content, aim, and applicability criteria. Advice on how to go about this could be included in an update of guidance for guideline developers, the current version of which contains little advice on how to evaluate reporting guidelines[9].\nBecause quantitative surveys can miss or mask important findings, developers seeking actionable feedback should collect qualitative data when assessing how researchers understand or feel about reporting guidelines, or what could be done to improve the guidance. As survey studies are subject to recall bias when participants are describing past behaviour or opinions, future studies could consider methods that allow researchers to document experiences in real time, like observation or think aloud tasks.\nStudies should ensure participants represent expected users in terms of academic writing experience, discipline, profession, experience (or naivety) with reporting guidelines, and language, or even focus on differential experiences of specific target groups. For example, the EQUATOR Network website gets similar levels of traffic from Asia and Europe, yet very little research into usability or barriers of reporting guidelines has included authors from Asian countries (see chapter 5). The website also sees many new visitors who abandon the site quickly, without accessing any reporting guidance. These visitors may be authors who are naïve to reporting guidelines and decide not to use one. Most of the included studies used snowball sampling or required authors to read the guidance as part of the study itself, and so don’t capture perspectives of these less-engaged authors.\nSurveys should avoid leading questions. For example, the Likert rated statements “The STARD 2015 guidelines are easy to follow” [7] and “The time required to adhere to the STARD 2015 guidelines is a barrier to using the guidelines” [7] are both subject to acquiescence bias; the tendency for participants to agree with research statements [10]. Future studies should consider using neutral questions, such as “Do you think the STARD 2015 guidelines are easy to follow?”.\nStudies used lots of different words to describe reporting guidelines, including guidelines, standards, requirements, checklist, example and elaboration, or just an acronym, e.g., CONSORT. This became a problem in studies where participants were not supplied with guidance documents as part of the study, as it was not always clear which document a participant was considering. For instance, asking participants whether PRISMA is easy to understand will not tell you whether they are talking about the PRISMA checklist, statement, or explanation and elaboration document. Future studies should be specific when asking questions and reporting results."
  },
  {
    "objectID": "chapters/4_survey_content/index.html#discussion",
    "href": "chapters/4_survey_content/index.html#discussion",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 2: Describing the questions asked in quantitative surveys.",
    "section": "Discussion",
    "text": "Discussion\nVery few reporting guidelines have been evaluated using either quantitative or qualitative methods. Reviewing the content of quantitative surveys revealed some novel influences which were absent from the qualitative data synthesised in chapter 3. Quantitative surveys often asked about awareness, usage, usability, usefulness, importance, barriers, facilitators, content, and whether reporting guidelines had led to a change in behaviour but did not address many other themes identified in my qualitative synthesis. Reporting guideline developers who want to make sure their resources are easy to use should consider using qualitative methods, which may produce richer, actionable insights.\nTwo studies asked participants whether they had heard of the EQUATOR Network, noting that it is a “a valuable resource for users and potential users of reporting guidelines” that 44% (19/43) of editors[6] and 38% of authors (38/100) [4] are aware of. Although these studies asked participants whether they were familiar with EQUATOR, authors’ experiences of using EQUATOR’s website has never been explored.\nIn the next chapter I describe EQUATOR’s website and key characteristics of its web traffic, before discussing how well it is helping authors find reporting guidance and what may be limiting its success.\n\n\nTables\n\n\nTable 1: Studies that collected quantitative data to explore researcher’s experiences of reporting guidelines\n\n\n\n\n\n\n\n\n\n\nCitation\nTitle\nGuidelines studied\nSample geographics\nSample size\nQuantitative or mixed methods\n\n\n\n\nBrouwers et al. 2016 [11]\nThe AGREE Reporting Checklist: a tool to improve reporting of clinical practice guidelines\nAGREE Reporting Checklist\nNot reported\n15\nQuantitative\n\n\nBurford, Welch, Waters et al., 2013 [12]\nTesting the PRISMA-Equity 2012 reporting guideline: the perspectives of systematic review authors\nPRISMA-Equity Checklist items embedded into survey\nNot reported\n151\nMixed methods\n\n\nDavies, Donnelly, Goodman, Ogrinc 2016 [8]\nFindings from a novel approach to publication guideline revision: user road testing of a draft version of SQUIRE 2.0\nSQUIRE Guidelines, which are presented as a checklist\nNot reported but invited participants were from USA, UK Lebanon, Sweden.\n44\nMixed methods\n\n\nDewey, Levine, Bossuyt et al., 2019 [13]\nImpact and perceived value of journal reporting guidelines among Radiology authors and reviewers\nCONSORT, STROBE, PRISMA, STARD checklists\nUSA, Canada, China, South Korea, Japan, Germany, France , Italy, UK, Other European countries, Middle East, Latin America and ‘Other’.\n831\nMixed methods\n\n\nEysenbach, 2013 [14]\nCONSORT-EHEALTH: Implementation of a Checklist for Authors and editors to improve reporting of web-based and mobile randomized controlled trials\nCONSORT-Ehealth checklist\nNot reported\n61\nMixed methods\n\n\nFang, Xi, Liu et al. 2016 [1]\nA survey on awareness of the ARRIVE Guideline and GSPC in researchers field in animal experiments field in Lanzhou City\nARRIVE Guidelines and Gold Standard Publication Checklist\nChina\n287\nQuantitative\n\n\nFuller, Pearson, Peters, Anderson, 2015 [6]\nWhat affects authors’ and editors’ use of reporting guidelines? Findings from an online survey and qualitative interviews\nTREND and reporting guidelines in general\nPredominantly North America\n56\nMixed methods\n\n\nGiray et al. 2020 [4]\nAssessment of the knowledge and awareness of a sample of young researcher physicians on reporting guidelines and the EQUATOR network: A single center cross-sectional study\nCONSORT, PRISMA, CARE, GRASS, STARD, STROBE, ARRIVE, SAMPL guidelines\nTurkey\n100\nQuantitative\n\n\nGuo, Qi, Yang et al., 2018 [15]\nRecognition status of quality assessment and standards for reporting randomized controlled trials of traditional Chinese medicine researchers\nCONSORT Statement, STRICTA guidelines and CONSORT extension for Traditional Chinese Medicine\nChina\n180\nQuantitative\n\n\nKorevaar, Cohen, Reitsma, et al, 2016 [16]\nUpdating standards for reporting diagnostic accuracy: the development of STARD 2015\nSTARD checklist\nNot reported for quantitative survey\n12\nMixed methods\n\n\nMa et al. 2017 [2]\nSurvey of basic medical researchers on the awareness of animal experimental designs and reporting standards in China\nARRIVE guidelines and Gold Standard Publication Checklist\nChina\n266\nQuantitative\n\n\nMacleod, Collings, Graf et al. 2021 [17]\nThe MDAR (Materials Design Analysis Reporting) Framework for transparent reporting in the life sciences\nMDAR checklist\nUSA, China, Japan, Germany, Other EU, ‘Other’\n211\nMixed methods\n\n\nMcDonough et al. 2011 [18]\nFamiliarity of non-industry authors with good publication practice and clinical data reporting guidelines\nCONSORT guidelines\nUSA, UK, Canada, South Africa, Israel, China\n23\nQuantitative\n\n\nÖncel et al. 2018 [5]\nKnowledge and awareness of optimal use of reporting guidelines in paediatricians: A cross-sectional study\nCONSORT guidelines, STROBE, PRISMA, CARE, SRQR, STARD, SQUIRE, CHEERS, SPIRIT, ARRIVE, TREND, STREGA, the Conference on Guideline Standardization (COGS), Outbreak Reports and Intervention Studies Of Nosocomial infection (ORION)\nTurkey\n244\nQuantitative\n\n\nPage, McKenzie, Bossuyt et al. 2021 [19]\nUpdating guidance for reporting systematic reviews: development of the PRISMA 2020 statement\nPRISMA statement\nNot reported\n110\nMixed methods\n\n\nPrady & MacPherson 2007 [20]\nAssessing the Utility of the Standards for Reporting Trials of Acupuncture (STRICTA): A survey of authors\nSTRICTA\nNot reported\n28\nMixed methods\n\n\nPrager, Gannon, Bowdridge et al. 2021 [7]\nBarriers to reporting guideline adherence in point-of care ultrasound research: a cross- sectional survey of authors and journal editors\nSTARD\nNot reported\n18\nMixed methods\n\n\nPhillips et al 2015 [21]\nPilot testing of the Guideline for Reporting of Evidence-Based Practice Educational Interventions and Teaching (GREET)\nGREET checklist and E&E\nNot reported\n31\nQuantitative\n\n\nRader, Mann, Stransfield et al., 2014 [22]\nMethods for documenting systematic review searches: a discussion of common issues\nPRISMA statement\nNot reported\n263\nMixed methods\n\n\nSharp, Glonti, Hren, 2020 [23]\nUsing the STROBE statement: survey ﬁndings emphasized the role of journals in enforcing reporting guidelines\nSTROBE statement\nThe full survey was answered by participants in Africa, Asia, Europe, North and South America, Middle East, and Pacific Region. It is unclear who answered the free text question.\n1015\nMixed methods\n\n\nStruthers, Harwood, de Beyer et al., 2021 [24]\nGoodReports: developing a website to help health researchers find and use reporting guidelines\nReporting guidelines in general\nNot reported\n274\nMixed methods\n\n\nTam, Tang, Woo, Goh 2019 [25]\nPerception of the Preferred Reporting Items for Systematic Reviews and Meta Analyses (PRISMA) statement of authors publishing reviews in nursing journals: a cross-sectional online survey\nPRISMA statement\nNot reported\n230\nMixed methods\n\n\n\n\n\n\nTable 2: Codes describing the focus of questions asked and their code categories. Items in bold did not appear in the qualitative data.\n\n\n\n\n\n\nCode**\nCategory\n\n\n\n\nParticipant’s experience [1, 2, 4–8, 12, 13, 15, 16, 18, 23]\nParticipant’s speciality [1, 2, 4, 6–8, 12, 16, 22, 23, 25]\nParticipant’s age [1, 2, 4, 5, 7, 15, 23, 25]\nParticipant’s gender [1, 4, 5, 7, 15, 23, 25]\nParticipant’s geography [6, 13, 18, 23]\nParticipant’s stage of current research project [12]\nDemographics\n\n\nAwareness of a particular guideline [1, 2, 4–7, 12, 18, 20, 23, 25]\nAwareness of EQUATOR [4, 6]\nHow did they first hear about guidelines or EQUATOR? [5, 6, 23]\nWhen did they first learn about a guideline? [6]\nAwareness\n\n\nHow frequently do they use guidelines? [4–8, 12, 13, 23, 25]\nWhen should guidelines be used? [4–7, 13, 23]\nWould they use a guideline, hypothetically [11, 12, 23]\nUsage\n\n\nDid the guidance impact subsequent behaviour? [8, 11–14, 20, 24]\nImpact on behaviour\n\n\nIs the guidance usable? [7, 21, 23]\nIs the guidance easy to understand? [7, 8, 15, 16, 23, 24]\nUsability\n\n\nIs the guidance useful? [2, 6, 11, 13, 17, 23, 24]\nUsefulness\n\n\nIs the guidance important? [2, 6, 8, 14, 15, 25]\nImportance\n\n\nAre time and length barriers? [6, 7, 14, 23, 24]\nIs language of guidance a barrier? [7]\nAre guidelines lacking for study type? [6]\nBarriers\n\n\nIs the layout OK? [11, 16, 23]\nShould the content be modified? [11, 16, 19]\nIs the guidance relevant? [24]\nAre guidelines prescriptive? [6]\nOpinions on content\n\n\nWill using a guideline benefit the manuscript? [7, 11, 12, 23]\nProductivity benefits of using guidelines [23]\nUsing guidelines because of journal requirements [6, 23]\nUsing guidelines because of funder requirements [6]\nUsing guidelines because of employment requirements [6]\nUsing guidelines because of other researchers expecting it [23]\nReasons for using a guideline\n\n\nOpinions on reporting quality of the literature [1, 2, 6, 7]\nOpinions on reporting quality\n\n\nAre guidelines easy to find and access [2, 6, 7]\nAccessibility\n\n\nWho should complete the checklist? [6, 7]\nRoles\n\n\nAre endorsements a facilitator? [6]\nIs evidence of increased chance of publication a facilitator? [6]\nIs evidence of improved reporting quality a facilitator? [6]\nIs explanatory information a facilitator? [6]\nIs training a facilitator? [6]\nIs the behaviour of peers a facilitator or motivator? [6]\nIs the evidence base underlying a reporting guideline a motivator? [6]\nIs transparency in guideline development a motivator? [6]\nFacilitators and motivators\n\n\nIs the aim of the guidance clear? [16]\nAim of guidance\n\n\n\n\n\n\nTable 3: Codes and descriptive themes identified from a qualitative evidence synthesis. Items in bold did not appear in the quantitative questions. Items in italic offer possible explanations to some quantitative findings.\n\n\n\n\n\n\nCodes\nDescriptive Themes\n\n\n\n\nWhat does this term mean? [8, 16, 19, 24, 26]\nWhat does this item mean? [8, 16, 19, 20, 24, 26]\nHow are these items different? [14, 19, 20, 26]\nHave I understood this as intended? [8, 26]\nExamples help me understand items [[CSL STYLE ERROR: reference with no printed form.]; [19]; [27]]\nWhat does this mean?\n\n\nWhy is this item important? [16, 19, 25, 26]\nWho is this item important to? [19, 26, 28]\nWhy is this item important?\n\n\nHave I understood the guideline’s scope as intended? [19, 24]\nDoes this item apply to me? [14, 19, 20, 24, 26]\nIs this item optional? [20, 26]\nDoes this apply to me?\n\n\nWhat are reporting guidelines? [7, 28]\nHow should I use a reporting guideline? [6]\nI don’t understand what reporting guidelines are\n\n\nI find guidelines useful in general [13, 24]\nGuidelines make me feel confident [28]\nGuidelines help me develop as a researcher [12, 28]\nGuidelines may help me improve my manuscript [12–14, 26, 28]\nI believe guidelines may help me publish more easily [29]\nGuidelines benefit me\n\n\nI may use guidelines because journals and editors tell me to [6, 12, 28, 29]\nI may use guidelines because other researchers expect it [6, 29]\nI use guidelines because of other people\n\n\nStandardized reporting benefits the community [28–30]\nGuidelines benefit others\n\n\nImmediate benefits are more important than hypothetical ones [28, 29]\nPersonal benefits are more important than benefits to others [29]\nSome benefits are more important than others\n\n\nI use reporting guidelines for planning research [26, 28]\nI use reporting guidelines for designing research [7, 13, 20, 28]\nI use reporting guidelines for writing [13, 20, 26, 28]\nI use reporting guidelines for checking my own or other people’s writing [7, 28]\nI use reporting guidelines to appraise the quality of other people’s reporting [16]\nI use reporting guidelines for peer reviewing [28]\nResearchers use reporting guidelines for different tasks\n\n\nI want items presented in the order in which I must do them [[CSL STYLE ERROR: reference with no printed form.]; [30]; [27]]\nI want design or methods advice [19, 26, 28]\nI want templates for writing [13]\nI want checklists that are easy to fill in [17, 24]\nI want checklists embedded into journal submission workflows [13]\nI want items embedded into data collection tools [12]\nI want guidance presented in formats that are better suited to the task I am doing\n\n\nGuidelines take time to read, understand and apply [6, 12, 29]\nSome items require extra work which takes time and effort [8, 22, 26]\nI want an indication of which items to prioritize [20, 26]\nPerceived complexity [13, 17, 26, 29]\nLong guidelines are off-putting [12, 14, 24, 28]\nGuidelines take time\n\n\nItemization helps me navigate guidance[19]\nItemization summarizes the guidance[13]\nItemization may decrease costs\n\n\nItemization makes guidance appear longer[19]\nItemization blocks the bigger picture[26]\nItemization may increase perceived costs\n\n\nFollowing reporting guidance can result in long, bloated articles [12, 14, 20, 26]\nLong, bloated articles may exceed journal word limits [6, 14, 17, 20]\nI want options for where to report this item [6, 8, 14, 19, 26, 28]\nI think guidelines make my manuscripts long and bloated\n\n\nThe benefits of using a reporting guideline may not outweigh the costs [6, 14, 28]\nThe benefits of using a reporting guideline may not outweigh the costs\n\n\nGuidelines are more valuable when used early [13, 24, 26, 28]\nThe balance of benefits vs costs may be more favourable when guidelines are used early\n\n\nI would clarify this item [19, 20]\nI would move this item [8, 26]\nI would split this item into two [19, 26, 27]\nI would add or remove items from this guideline [16, 19, 20, 26]\nI would add or remove requirements from this item [[CSL STYLE ERROR: reference with no printed form.]; [19]; [20]; [28]; [25]]\nI think the guidance could be improved\n\n\nGuidelines can become out of date [26]\nGuidelines need to be updated [19]\nGuidelines need to be kept updated\n\n\nI cannot report this because I didn’t do it [20]\nI cannot report this because of intellectual property issues [14]\nI cannot report this because it clashes with journal guidelines [19]\nI cannot report this because data was missing from my primary studies [12]\nEditors, reviewers or co-authors asked me to remove this item [20, 22]\nI feel unable to report this\n\n\nI feel uncertain because I don’t know how to say that I didn’t do it [19]\nI feel worried that I will be judged for transparently reporting something I didn’t do [19, 28]\nI feel nervous or uncertain if I am unable to report an item\n\n\nI may not know that reporting guidelines exist [6, 13, 16, 24, 29]\nI may not be able to easily access guidance [24, 29]\nI can only use what I know about and have\n\n\nReporting guidelines may be less valuable to experienced researchers [13, 14, 28]\nExperienced researchers feel that they already know how to report [13, 26, 28]\nExperienced researchers find guidance patronizing and feel untrusted [6, 14, 17, 19]\nReporting guidelines are more valuable to inexperienced researchers\n\n\nReporting guidelines can be hard to use at first but get easier with experience [6, 26, 29]\nReporting guidelines can be hard to use at first but get easier with experience\n\n\nI want design or methodological advice [17, 19, 28]\nI don’t know how to do this item [19, 20, 26]\nI want or need design advice\n\n\nGuidelines are procedural straightjackets [28]\nThis guideline is too prescriptive [19, 25, 28]\nI think this guidance prescribes how research should be designed\n\n\nThe guideline’s applicability criteria are not clear [13, 16, 24]\nA guideline’s scope can be unclear\n\n\nThis guideline isn’t a perfect fit for me [24]\nThis guideline doesn’t generalise [13, 17, 19, 25, 28]\nThis guideline is too prescriptive [19, 25, 28]\nA guideline can be too narrow\n\n\nI don’t want to see optional items that only apply to other types of study [20, 24]\nA guideline’s scope can be too broad\n\n\nI need to adhere to journal guidelines or other research guidelines [6, 13, 19, 20]\nI might need to use multiple reporting guidelines [28]\nAuthors often need to adhere to multiple sets of guidance\n\n\nI want reporting guidelines to be linked or embedded [16, 19]\nI want reporting guidelines to use similar structure [19]\nI want reporting guidelines to use similar terms [19]\nI want guidelines to harmonize\n\n\nI don’t like checklists[13, 14, 24, 28]\nI may use the checklist instead of the full guidance [[CSL STYLE ERROR: reference with no printed form.]]\nI may use the checklist before I read the full guidance [[CSL STYLE ERROR: reference with no printed form.]]\nI experience reporting guidelines primarily as, or through, checklists\n\n\n\n\n\n\n\n\n\n\n\n1. Fang Z.-P., Leng X., Liu Y.-L., Liu W.-B., Hu W.-J., Zhang Z.-J., Ma B., Li D.-M. (2015) A survey on awareness of the ARRIVE guideline and GSPC in researchers field in animal experiments field in Lanzhou city. Chinese Journal of Evidence-Based Medicine 15:797–801\n\n\n2. Ma B, Xu J, Wu W, Liu H, Kou C, Liu N, Zhao L (2017) Survey of basic medical researchers on the awareness of animal experimental designs and reporting standards in China. PLoS ONE 12:e0174530\n\n\n3. The EQUATOR Network | Enhancing the QUAlity and Transparency Of Health Research. \n\n\n4. Gi̇ray E, Coskun OK, Karacaatli M, Gunduz OH, Yagci İ (2020) Assessment of the knowledge and awareness of a sample of young researcher physicians on reporting guidelines and the EQUATOR network: A single center cross-sectional study. Marmara Medical Journal 33:1–6\n\n\n5. Karadağ Öncel E, Başaranoğlu ST, Aykaç K, Kömürlüoğlu A, Akman AÖ, Kıran S (2018) Knowledge and awareness of optimal use of reporting guidelines in paediatricians: A cross-sectional study. Turk Pediatri Arsivi 53:163–168\n\n\n6. Fuller T, Pearson M, Peters J, Anderson R (2015) What affects authors’ and editors’ use of reporting guidelines? Findings from an online survey and qualitative interviews. PLoS ONE 10:e0121585\n\n\n7. Prager R, Gagnon L, Bowdridge J, Unni RR, McGrath TA, Cobey K, Bossuyt PM, McInnes MDF (2021) Barriers to reporting guideline adherence in point-of-care ultrasound research: A cross-sectional survey of authors and journal editors. BMJ Evidence-Based Medicine bmjebm-2020-111604\n\n\n8. Davies L, Donnelly KZ, Goodman DJ, Ogrinc G (2016) Findings from a novel approach to publication guideline revision: User road testing of a draft version of SQUIRE 2.0. BMJ quality & safety 25:265–272\n\n\n9. Moher D, Schulz KF, Simera I, Altman DG (2010) Guidance for developers of health research reporting guidelines. PLOS Medicine 7:e1000217\n\n\n10. (2008) Acquiescence Response Bias. Encyclopedia of Survey Research Methods. https://doi.org/10.4135/9781412963947.n3\n\n\n11. Brouwers MC, Kerkvliet K, Spithoff K, Consortium ANS (2016) The AGREE Reporting Checklist: A tool to improve reporting of clinical practice guidelines. BMJ 352:i1152\n\n\n12. Burford BJ, Welch V, Waters E, Tugwell P, Moher D, O’Neill J, Koehlmoos T, Petticrew M (2013) Testing the PRISMA-Equity 2012 reporting guideline: The perspectives of systematic review authors. PloS one 8:e75122\n\n\n13. Dewey M, Levine D, Bossuyt PM, Kressel HY (2019) Impact and perceived value of journal reporting guidelines among Radiology authors and reviewers. European Radiology 29:3986–3995\n\n\n14. Eysenbach G. (2013) CONSORT-EHEALTH: Implementation of a checklist for authors and editors to improve reporting of web-based and mobile randomized controlled trials. Studies in health technology and informatics 192:657–661\n\n\n15. Guo S, Qi S, Yang L, Wang X, Zhu Q, Meng X, Zeng Y, Institute M and A of, China SMC of A (2018) Recognition status of quality assessment and standards for reporting randomized controlled trials of traditional Chinese medicine researchers. China Journal of Traditional Chinese Medicine and Pharmacy 1077–1081\n\n\n16. Korevaar DA, Cohen JF, Reitsma JB, et al (2016) Updating standards for reporting diagnostic accuracy: The development of STARD 2015. Research integrity and peer review 1:7\n\n\n17. Macleod M, Collings AM, Graf C, Kiermer V, Mellor D, Swaminathan S, Sweet D, Vinson V (2021) The MDAR (Materials Design Analysis Reporting) Framework for transparent reporting in the life sciences. Proceedings of the National Academy of Sciences 118:e2103238118\n\n\n18. McDonough J., O’Dunne A., B. C, Margerum B., Sutton D. (2011) Familiarity of non-industry authors with good publication practice and clinical data reporting guidelines. Current Medical Research and Opinion 27:S9\n\n\n19. Page MJ, McKenzie JE, Bossuyt PM, Boutron I, Hoffmann TC, Mulrow CD, Shamseer L, Tetzlaff JM, Moher D (2021) Updating guidance for reporting systematic reviews: Development of the PRISMA 2020 statement. Journal of Clinical Epidemiology 134:103–112\n\n\n20. Prady SL, MacPherson H (2007) Assessing the utility of the standards for reporting trials of acupuncture (STRICTA): A survey of authors. The Journal of Alternative and Complementary Medicine 13:939–943\n\n\n21. Phillips A., Lewis L.K., McEvoy M.P., Galipeau J., Glasziou P., Moher D., Tilson J.K., Williams M.T. (2015) Pilot testing of the guideline for reporting of evidence-based practice educational interventions and teaching (greet). Physiotherapy (United Kingdom) 101:eS1203–eS1204\n\n\n22. Rader T., Mann M., Stansfield C., Cooper C., Sampson M. (2014) Methods for documenting systematic review searches: A discussion of common issues. Research synthesis methods 5:98–115\n\n\n23. Sharp MK, Bertizzolo L, Rius R, Wager E, Gómez G, Hren D (2019) Using the STROBE statement: Survey findings emphasized the role of journals in enforcing reporting guidelines. Journal of Clinical Epidemiology 116:26–35\n\n\n24. Struthers C, Harwood J, de Beyer JA, Dhiman P, Logullo P, Schlüssel M (2021) GoodReports: Developing a website to help health researchers find and use reporting guidelines. BMC medical research methodology 21:217\n\n\n25. Tam WWS, Tang A, Woo B, Goh SYS (2019) Perception of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement of authors publishing reviews in nursing journals: A cross-sectional online survey. BMJ Open 9:e026271\n\n\n26. Davies L, Batalden P, Davidoff F, Stevens D, Ogrinc G (2015) The SQUIRE Guidelines: An evaluation from the field, 5years post release. BMJ quality & safety 24:769–775\n\n\n27. de Vries RBM, Hooijmans CR, Langendam MW, van Luijk J, Leenaars M, Ritskes-Hoitinga M, Wever KE (2015) A protocol format for the preparation, registration and publication of systematic reviews of animal intervention studies. Evidence-based Preclinical Medicine 2:e00007\n\n\n28. Sharp MK, Glonti K, Hren D (2020) Online survey about the STROBE statement highlighted diverging views about its content, purpose, and value. Journal of clinical epidemiology 123:100–106\n\n\n29. Svensøy JN, Nilsson H, Rimstad R (2021) A qualitative study on researchers’ experiences after publishing scientific reports on major incidents, mass-casualty incidents, and disasters. Prehospital and Disaster Medicine 36:536–542\n\n\n30. Page MJ, McKenzie JE, Bossuyt PM, et al (2021) The PRISMA 2020 statement: An updated guideline for reporting systematic reviews. BMJ (Clinical research ed) 372:n71"
  },
  {
    "objectID": "chapters/4_survey_content/index.html#tables",
    "href": "chapters/4_survey_content/index.html#tables",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 2: Describing the questions asked in quantitative surveys.",
    "section": "Tables",
    "text": "Tables\n\n\nTable 1: Studies that collected quantitative data to explore researcher’s experiences of reporting guidelines\n\n\n\n\n\n\n\n\n\n\nCitation\nTitle\nGuidelines studied\nSample geographics\nSample size\nQuantitative or mixed methods\n\n\n\n\nBrouwers et al. 2016 [11]\nThe AGREE Reporting Checklist: a tool to improve reporting of clinical practice guidelines\nAGREE Reporting Checklist\nNot reported\n15\nQuantitative\n\n\nBurford, Welch, Waters et al., 2013 [12]\nTesting the PRISMA-Equity 2012 reporting guideline: the perspectives of systematic review authors\nPRISMA-Equity Checklist items embedded into survey\nNot reported\n151\nMixed methods\n\n\nDavies, Donnelly, Goodman, Ogrinc 2016 [8]\nFindings from a novel approach to publication guideline revision: user road testing of a draft version of SQUIRE 2.0\nSQUIRE Guidelines, which are presented as a checklist\nNot reported but invited participants were from USA, UK Lebanon, Sweden.\n44\nMixed methods\n\n\nDewey, Levine, Bossuyt et al., 2019 [13]\nImpact and perceived value of journal reporting guidelines among Radiology authors and reviewers\nCONSORT, STROBE, PRISMA, STARD checklists\nUSA, Canada, China, South Korea, Japan, Germany, France , Italy, UK, Other European countries, Middle East, Latin America and ‘Other’.\n831\nMixed methods\n\n\nEysenbach, 2013 [14]\nCONSORT-EHEALTH: Implementation of a Checklist for Authors and editors to improve reporting of web-based and mobile randomized controlled trials\nCONSORT-Ehealth checklist\nNot reported\n61\nMixed methods\n\n\nFang, Xi, Liu et al. 2016 [1]\nA survey on awareness of the ARRIVE Guideline and GSPC in researchers field in animal experiments field in Lanzhou City\nARRIVE Guidelines and Gold Standard Publication Checklist\nChina\n287\nQuantitative\n\n\nFuller, Pearson, Peters, Anderson, 2015 [6]\nWhat affects authors’ and editors’ use of reporting guidelines? Findings from an online survey and qualitative interviews\nTREND and reporting guidelines in general\nPredominantly North America\n56\nMixed methods\n\n\nGiray et al. 2020 [4]\nAssessment of the knowledge and awareness of a sample of young researcher physicians on reporting guidelines and the EQUATOR network: A single center cross-sectional study\nCONSORT, PRISMA, CARE, GRASS, STARD, STROBE, ARRIVE, SAMPL guidelines\nTurkey\n100\nQuantitative\n\n\nGuo, Qi, Yang et al., 2018 [15]\nRecognition status of quality assessment and standards for reporting randomized controlled trials of traditional Chinese medicine researchers\nCONSORT Statement, STRICTA guidelines and CONSORT extension for Traditional Chinese Medicine\nChina\n180\nQuantitative\n\n\nKorevaar, Cohen, Reitsma, et al, 2016 [16]\nUpdating standards for reporting diagnostic accuracy: the development of STARD 2015\nSTARD checklist\nNot reported for quantitative survey\n12\nMixed methods\n\n\nMa et al. 2017 [2]\nSurvey of basic medical researchers on the awareness of animal experimental designs and reporting standards in China\nARRIVE guidelines and Gold Standard Publication Checklist\nChina\n266\nQuantitative\n\n\nMacleod, Collings, Graf et al. 2021 [17]\nThe MDAR (Materials Design Analysis Reporting) Framework for transparent reporting in the life sciences\nMDAR checklist\nUSA, China, Japan, Germany, Other EU, ‘Other’\n211\nMixed methods\n\n\nMcDonough et al. 2011 [18]\nFamiliarity of non-industry authors with good publication practice and clinical data reporting guidelines\nCONSORT guidelines\nUSA, UK, Canada, South Africa, Israel, China\n23\nQuantitative\n\n\nÖncel et al. 2018 [5]\nKnowledge and awareness of optimal use of reporting guidelines in paediatricians: A cross-sectional study\nCONSORT guidelines, STROBE, PRISMA, CARE, SRQR, STARD, SQUIRE, CHEERS, SPIRIT, ARRIVE, TREND, STREGA, the Conference on Guideline Standardization (COGS), Outbreak Reports and Intervention Studies Of Nosocomial infection (ORION)\nTurkey\n244\nQuantitative\n\n\nPage, McKenzie, Bossuyt et al. 2021 [19]\nUpdating guidance for reporting systematic reviews: development of the PRISMA 2020 statement\nPRISMA statement\nNot reported\n110\nMixed methods\n\n\nPrady & MacPherson 2007 [20]\nAssessing the Utility of the Standards for Reporting Trials of Acupuncture (STRICTA): A survey of authors\nSTRICTA\nNot reported\n28\nMixed methods\n\n\nPrager, Gannon, Bowdridge et al. 2021 [7]\nBarriers to reporting guideline adherence in point-of care ultrasound research: a cross- sectional survey of authors and journal editors\nSTARD\nNot reported\n18\nMixed methods\n\n\nPhillips et al 2015 [21]\nPilot testing of the Guideline for Reporting of Evidence-Based Practice Educational Interventions and Teaching (GREET)\nGREET checklist and E&E\nNot reported\n31\nQuantitative\n\n\nRader, Mann, Stransfield et al., 2014 [22]\nMethods for documenting systematic review searches: a discussion of common issues\nPRISMA statement\nNot reported\n263\nMixed methods\n\n\nSharp, Glonti, Hren, 2020 [23]\nUsing the STROBE statement: survey ﬁndings emphasized the role of journals in enforcing reporting guidelines\nSTROBE statement\nThe full survey was answered by participants in Africa, Asia, Europe, North and South America, Middle East, and Pacific Region. It is unclear who answered the free text question.\n1015\nMixed methods\n\n\nStruthers, Harwood, de Beyer et al., 2021 [24]\nGoodReports: developing a website to help health researchers find and use reporting guidelines\nReporting guidelines in general\nNot reported\n274\nMixed methods\n\n\nTam, Tang, Woo, Goh 2019 [25]\nPerception of the Preferred Reporting Items for Systematic Reviews and Meta Analyses (PRISMA) statement of authors publishing reviews in nursing journals: a cross-sectional online survey\nPRISMA statement\nNot reported\n230\nMixed methods\n\n\n\n\n\n\nTable 2: Codes describing the focus of questions asked and their code categories. Items in bold did not appear in the qualitative data.\n\n\n\n\n\n\nCode**\nCategory\n\n\n\n\nParticipant’s experience [1, 2, 4–8, 12, 13, 15, 16, 18, 23]\nParticipant’s speciality [1, 2, 4, 6–8, 12, 16, 22, 23, 25]\nParticipant’s age [1, 2, 4, 5, 7, 15, 23, 25]\nParticipant’s gender [1, 4, 5, 7, 15, 23, 25]\nParticipant’s geography [6, 13, 18, 23]\nParticipant’s stage of current research project [12]\nDemographics\n\n\nAwareness of a particular guideline [1, 2, 4–7, 12, 18, 20, 23, 25]\nAwareness of EQUATOR [4, 6]\nHow did they first hear about guidelines or EQUATOR? [5, 6, 23]\nWhen did they first learn about a guideline? [6]\nAwareness\n\n\nHow frequently do they use guidelines? [4–8, 12, 13, 23, 25]\nWhen should guidelines be used? [4–7, 13, 23]\nWould they use a guideline, hypothetically [11, 12, 23]\nUsage\n\n\nDid the guidance impact subsequent behaviour? [8, 11–14, 20, 24]\nImpact on behaviour\n\n\nIs the guidance usable? [7, 21, 23]\nIs the guidance easy to understand? [7, 8, 15, 16, 23, 24]\nUsability\n\n\nIs the guidance useful? [2, 6, 11, 13, 17, 23, 24]\nUsefulness\n\n\nIs the guidance important? [2, 6, 8, 14, 15, 25]\nImportance\n\n\nAre time and length barriers? [6, 7, 14, 23, 24]\nIs language of guidance a barrier? [7]\nAre guidelines lacking for study type? [6]\nBarriers\n\n\nIs the layout OK? [11, 16, 23]\nShould the content be modified? [11, 16, 19]\nIs the guidance relevant? [24]\nAre guidelines prescriptive? [6]\nOpinions on content\n\n\nWill using a guideline benefit the manuscript? [7, 11, 12, 23]\nProductivity benefits of using guidelines [23]\nUsing guidelines because of journal requirements [6, 23]\nUsing guidelines because of funder requirements [6]\nUsing guidelines because of employment requirements [6]\nUsing guidelines because of other researchers expecting it [23]\nReasons for using a guideline\n\n\nOpinions on reporting quality of the literature [1, 2, 6, 7]\nOpinions on reporting quality\n\n\nAre guidelines easy to find and access [2, 6, 7]\nAccessibility\n\n\nWho should complete the checklist? [6, 7]\nRoles\n\n\nAre endorsements a facilitator? [6]\nIs evidence of increased chance of publication a facilitator? [6]\nIs evidence of improved reporting quality a facilitator? [6]\nIs explanatory information a facilitator? [6]\nIs training a facilitator? [6]\nIs the behaviour of peers a facilitator or motivator? [6]\nIs the evidence base underlying a reporting guideline a motivator? [6]\nIs transparency in guideline development a motivator? [6]\nFacilitators and motivators\n\n\nIs the aim of the guidance clear? [16]\nAim of guidance\n\n\n\n\n\n\nTable 3: Codes and descriptive themes identified from a qualitative evidence synthesis. Items in bold did not appear in the quantitative questions. Items in italic offer possible explanations to some quantitative findings.\n\n\n\n\n\n\nCodes\nDescriptive Themes\n\n\n\n\nWhat does this term mean? [8, 16, 19, 24, 26]\nWhat does this item mean? [8, 16, 19, 20, 24, 26]\nHow are these items different? [14, 19, 20, 26]\nHave I understood this as intended? [8, 26]\nExamples help me understand items [[CSL STYLE ERROR: reference with no printed form.]; [19]; [27]]\nWhat does this mean?\n\n\nWhy is this item important? [16, 19, 25, 26]\nWho is this item important to? [19, 26, 28]\nWhy is this item important?\n\n\nHave I understood the guideline’s scope as intended? [19, 24]\nDoes this item apply to me? [14, 19, 20, 24, 26]\nIs this item optional? [20, 26]\nDoes this apply to me?\n\n\nWhat are reporting guidelines? [7, 28]\nHow should I use a reporting guideline? [6]\nI don’t understand what reporting guidelines are\n\n\nI find guidelines useful in general [13, 24]\nGuidelines make me feel confident [28]\nGuidelines help me develop as a researcher [12, 28]\nGuidelines may help me improve my manuscript [12–14, 26, 28]\nI believe guidelines may help me publish more easily [29]\nGuidelines benefit me\n\n\nI may use guidelines because journals and editors tell me to [6, 12, 28, 29]\nI may use guidelines because other researchers expect it [6, 29]\nI use guidelines because of other people\n\n\nStandardized reporting benefits the community [28–30]\nGuidelines benefit others\n\n\nImmediate benefits are more important than hypothetical ones [28, 29]\nPersonal benefits are more important than benefits to others [29]\nSome benefits are more important than others\n\n\nI use reporting guidelines for planning research [26, 28]\nI use reporting guidelines for designing research [7, 13, 20, 28]\nI use reporting guidelines for writing [13, 20, 26, 28]\nI use reporting guidelines for checking my own or other people’s writing [7, 28]\nI use reporting guidelines to appraise the quality of other people’s reporting [16]\nI use reporting guidelines for peer reviewing [28]\nResearchers use reporting guidelines for different tasks\n\n\nI want items presented in the order in which I must do them [[CSL STYLE ERROR: reference with no printed form.]; [30]; [27]]\nI want design or methods advice [19, 26, 28]\nI want templates for writing [13]\nI want checklists that are easy to fill in [17, 24]\nI want checklists embedded into journal submission workflows [13]\nI want items embedded into data collection tools [12]\nI want guidance presented in formats that are better suited to the task I am doing\n\n\nGuidelines take time to read, understand and apply [6, 12, 29]\nSome items require extra work which takes time and effort [8, 22, 26]\nI want an indication of which items to prioritize [20, 26]\nPerceived complexity [13, 17, 26, 29]\nLong guidelines are off-putting [12, 14, 24, 28]\nGuidelines take time\n\n\nItemization helps me navigate guidance[19]\nItemization summarizes the guidance[13]\nItemization may decrease costs\n\n\nItemization makes guidance appear longer[19]\nItemization blocks the bigger picture[26]\nItemization may increase perceived costs\n\n\nFollowing reporting guidance can result in long, bloated articles [12, 14, 20, 26]\nLong, bloated articles may exceed journal word limits [6, 14, 17, 20]\nI want options for where to report this item [6, 8, 14, 19, 26, 28]\nI think guidelines make my manuscripts long and bloated\n\n\nThe benefits of using a reporting guideline may not outweigh the costs [6, 14, 28]\nThe benefits of using a reporting guideline may not outweigh the costs\n\n\nGuidelines are more valuable when used early [13, 24, 26, 28]\nThe balance of benefits vs costs may be more favourable when guidelines are used early\n\n\nI would clarify this item [19, 20]\nI would move this item [8, 26]\nI would split this item into two [19, 26, 27]\nI would add or remove items from this guideline [16, 19, 20, 26]\nI would add or remove requirements from this item [[CSL STYLE ERROR: reference with no printed form.]; [19]; [20]; [28]; [25]]\nI think the guidance could be improved\n\n\nGuidelines can become out of date [26]\nGuidelines need to be updated [19]\nGuidelines need to be kept updated\n\n\nI cannot report this because I didn’t do it [20]\nI cannot report this because of intellectual property issues [14]\nI cannot report this because it clashes with journal guidelines [19]\nI cannot report this because data was missing from my primary studies [12]\nEditors, reviewers or co-authors asked me to remove this item [20, 22]\nI feel unable to report this\n\n\nI feel uncertain because I don’t know how to say that I didn’t do it [19]\nI feel worried that I will be judged for transparently reporting something I didn’t do [19, 28]\nI feel nervous or uncertain if I am unable to report an item\n\n\nI may not know that reporting guidelines exist [6, 13, 16, 24, 29]\nI may not be able to easily access guidance [24, 29]\nI can only use what I know about and have\n\n\nReporting guidelines may be less valuable to experienced researchers [13, 14, 28]\nExperienced researchers feel that they already know how to report [13, 26, 28]\nExperienced researchers find guidance patronizing and feel untrusted [6, 14, 17, 19]\nReporting guidelines are more valuable to inexperienced researchers\n\n\nReporting guidelines can be hard to use at first but get easier with experience [6, 26, 29]\nReporting guidelines can be hard to use at first but get easier with experience\n\n\nI want design or methodological advice [17, 19, 28]\nI don’t know how to do this item [19, 20, 26]\nI want or need design advice\n\n\nGuidelines are procedural straightjackets [28]\nThis guideline is too prescriptive [19, 25, 28]\nI think this guidance prescribes how research should be designed\n\n\nThe guideline’s applicability criteria are not clear [13, 16, 24]\nA guideline’s scope can be unclear\n\n\nThis guideline isn’t a perfect fit for me [24]\nThis guideline doesn’t generalise [13, 17, 19, 25, 28]\nThis guideline is too prescriptive [19, 25, 28]\nA guideline can be too narrow\n\n\nI don’t want to see optional items that only apply to other types of study [20, 24]\nA guideline’s scope can be too broad\n\n\nI need to adhere to journal guidelines or other research guidelines [6, 13, 19, 20]\nI might need to use multiple reporting guidelines [28]\nAuthors often need to adhere to multiple sets of guidance\n\n\nI want reporting guidelines to be linked or embedded [16, 19]\nI want reporting guidelines to use similar structure [19]\nI want reporting guidelines to use similar terms [19]\nI want guidelines to harmonize\n\n\nI don’t like checklists[13, 14, 24, 28]\nI may use the checklist instead of the full guidance [[CSL STYLE ERROR: reference with no printed form.]]\nI may use the checklist before I read the full guidance [[CSL STYLE ERROR: reference with no printed form.]]\nI experience reporting guidelines primarily as, or through, checklists"
  },
  {
    "objectID": "chapters/todo.html",
    "href": "chapters/todo.html",
    "title": "Thesis",
    "section": "",
    "text": "Todo…"
  },
  {
    "objectID": "chapters/10_redesign/index.html",
    "href": "chapters/10_redesign/index.html",
    "title": "Developing intervention components into a prototype",
    "section": "",
    "text": "In chapter 7 I explained how EQUATOR staff and I decided to prioritise redesigning reporting guidelines and the EQUATOR Network website’s home page. In chapter 9 I described intervention components we could include in this redesign. In this chapter, I describe how I turned this list of components into a working prototype.\nI wanted to include EQUATOR staff and guideline developers in the redesign process for multiple reasons. Firstly, I expected their experience and expertise to help decision making. Secondly, I wanted them to understand and like the redesign. I wanted them to feel ownership, and not feel displaced by efforts. As a student, relatively new to the world of reporting guidelines, at times I felt like an intruder meddling with other peoples’ creations. By making sure the redesign reflected the preferences of stakeholders, I hoped the result would feel like “ours”, not “mine”. Finally, I wanted to ensure what I made would be sustainable. I wanted EQUATOR staff and guideline developers to be able to maintain and extend it without requiring a software developer. I wanted to use technologies simple enough for stakeholders to control, cheap enough to run without funding, secure enough to meet privacy requirements, and powerful enough to do what we wanted.\nI did not have time to consult multiple guideline groups. Nor did I have time to redesign multiple guidelines. For the purpose of building a prototype I decided to focus on a single reporting guideline; the Standards for Reporting Qualitative Research (SRQR). I chose this guideline because I was familiar with it, understood it, and it’s lead developer expressed interest in collaborating. Additionally, because it is applicable to all medical qualitative research I expected its users to be plentiful and varied. For example, authors may include students, clinicians, public health experts, quantitative researchers dabbling in qualitative methods within a positivist framework, or experienced qualitative researchers working within other paradigms. This broad and varied author base would be useful for a subsequent evaluation study (see next chapter).\nIn this chapter, I describe how I worked with EQUATOR staff and SRQR’s lead developer to co-design a revamped home page and reporting guideline including intervention components from chapter 9."
  },
  {
    "objectID": "chapters/10_redesign/index.html#introduction",
    "href": "chapters/10_redesign/index.html#introduction",
    "title": "Developing intervention components into a prototype",
    "section": "",
    "text": "In chapter 7 I explained how EQUATOR staff and I decided to prioritise redesigning reporting guidelines and the EQUATOR Network website’s home page. In chapter 9 I described intervention components we could include in this redesign. In this chapter, I describe how I turned this list of components into a working prototype.\nI wanted to include EQUATOR staff and guideline developers in the redesign process for multiple reasons. Firstly, I expected their experience and expertise to help decision making. Secondly, I wanted them to understand and like the redesign. I wanted them to feel ownership, and not feel displaced by efforts. As a student, relatively new to the world of reporting guidelines, at times I felt like an intruder meddling with other peoples’ creations. By making sure the redesign reflected the preferences of stakeholders, I hoped the result would feel like “ours”, not “mine”. Finally, I wanted to ensure what I made would be sustainable. I wanted EQUATOR staff and guideline developers to be able to maintain and extend it without requiring a software developer. I wanted to use technologies simple enough for stakeholders to control, cheap enough to run without funding, secure enough to meet privacy requirements, and powerful enough to do what we wanted.\nI did not have time to consult multiple guideline groups. Nor did I have time to redesign multiple guidelines. For the purpose of building a prototype I decided to focus on a single reporting guideline; the Standards for Reporting Qualitative Research (SRQR). I chose this guideline because I was familiar with it, understood it, and it’s lead developer expressed interest in collaborating. Additionally, because it is applicable to all medical qualitative research I expected its users to be plentiful and varied. For example, authors may include students, clinicians, public health experts, quantitative researchers dabbling in qualitative methods within a positivist framework, or experienced qualitative researchers working within other paradigms. This broad and varied author base would be useful for a subsequent evaluation study (see next chapter).\nIn this chapter, I describe how I worked with EQUATOR staff and SRQR’s lead developer to co-design a revamped home page and reporting guideline including intervention components from chapter 9."
  },
  {
    "objectID": "chapters/10_redesign/index.html#methods",
    "href": "chapters/10_redesign/index.html#methods",
    "title": "Developing intervention components into a prototype",
    "section": "Methods",
    "text": "Methods\nI invited the EQUATOR staff who participated in the workshops (chapter 7). As before, I used established, best-practice techniques to facilitate open discussion [1]. I made it clear there were no right or wrong answers. I reflected on my own opinions before meeting, and held them back until EQUATOR staff had finished talking. When disagreements arose, I took time to explore and understand both sides.\nI made heavy use of my intervention planning table (see chapter 9) to make decisions based on theory over and above personal preferences. Instead of asking EQUATOR members whether they liked something (e.g., Do you like this font?), I asked whether they felt it reflected the intended intervention function instead (e.g., Does the font convey simplicity?). By prioritising theory above preference and others’ opinions above my own, I reduced my subjectivity.\nI met with EQUATOR staff 3 times between November 2022 and January 2023. Throughout our meetings we kept our target audience in mind. Building user personas is a common best practice in web development [2] [3] [4]. For example, as part of their work to help NHS staff adopt new technologies and become a digital workforce, Health Education England identified five archetypes: digital creators, end users, embedders, change drivers, and shapers [5]. They then developed five imaginary personas around these archetypes, each with their own needs, motivations, and challenges. Developing personas can be a significant task. Personas should come from evidence, and some product teams go as far as interviewing 5 or more representatives of each persona in an iterative, co-creation process.\nThis was not feasible within my PhD and so our persona generation process was less formal but still evidence based. Referring to my website service evaluation (chapter 5), we recalled most website visitors are new and visit only once, and so we assumed many will be naïve to reporting guidelines and the EQUATOR Network. Website visitors come from all over the world, so we assumed some may speak english as a second language (or not at all). We had no way of knowing how much research experience our website visitors have, but we assumed the distribution would reflect real-world numbers, where students and early career researchers outnumber experienced professors. Although the EQUATOR website has content aimed at editors, librarians, and guideline developers, this content is rarely accessed and these users are not part of our target behaviour (see chapter 7), and so we did not create personas for them.\nConsequently, two personas crystallised in our minds: inexperienced authors who face the most barriers and need the most help, and experienced authors who face fewer barriers.\n\nInexperienced author:\n\n\nmay not be familiar with EQUATOR or reporting guidelines\nmay find authoring / publishing difficult\nmay need help in finding and using a reporting guideline\nmay speak English as a second language\n\n\nExperienced author:\n\n\nmay be familiar with EQUATOR and/or reporting guidelines\nmay have used checklists before, but may not realise full guidance exists, and may have never used reporting guidelines for for drafting.\nmay feel patronized or restricted by reporting guidelines, especially if they believe them to be design requirements\nmay see checklists as red tape necessary for publication, or they may be reporting guideline “converts” already convinced of their value.\n\nMany authors may fall somewhere between these two personas. We decided to prioritise inexperienced authors because we believed them to be more numerous, and because qualitative evidence suggests inexperienced authors may benefit more from reporting guidelines (see chapter 3). By focussing on helping inexperienced authors, we hoped any spill-over effects may also help experienced authors.\nUser personas typically include motivations. There is an important difference between user motivations - what authors are wanting to do when they visit EQUATOR’s website - and our target behaviour - what we want authors to do when they visit EQUATOR’s website. Because most visitors come from journal websites or submission systems and because checklists are the most accessed resourced (chapter 5), we assumed most authors visit the website because they want to fill out a reporting checklist as part of journal submission. In contrast, we want authors to read the full reporting guidelines as early as possible, i.e. when planning or drafting research, way before they submit to a journal. We could have designed a website to focus entirely on drafting manuscripts, but this would have abandoned authors seeking checklists. Instead, we took a more pragmatic approach and decided to continue catering for authors seeking checklists, but to nudge them towards using the full guidance for drafting research in the future.\nIn our first meeting, we decided how the home page and redesigned guideline should join. On EQUATOR’s existing website, authors starting on the home page must navigate through 5 webpages to reach the full reporting guidance; the home page, guideline database page, PubMed, a publication, and then sometimes a supplement. From my website service evaluation (chapter 5), we knew many authors leave at each step (see ?@fig-sankey-b4). Therefore, we wanted our redesigned home page to link directly to the most frequently used guidelines, thereby reducing this this journey to 2 steps, with the aim to increase the proportion of authors reaching the full guidance (see ?@fig-sankey-after).\nIn our second meeting, EQUATOR staff and I sketched ideas for how the home page and reporting guideline page could be laid out and for the positioning of intervention components. These sketches were wireframes: simple illustrations focussing on space allocation, functionalities, and intended behaviours. Wireframes do not include styling. There are no colours, images are represented as blank boxes, and squiggles represent blocks of text.\nAfter the second meeting, once participants had agreed on a layout, I created an alpha version of the new home page and guideline page. These were real webpages, viewable in a browser, but I used dummy text and images because I wanted to solicit feedback on layout, structure, and functionality, not on the content. This is a common practice in web development, as people can become distracted by wording or stylistic choices. I used a web annotation tool called Pastel to collect feedback from EQUATOR staff [6] and then refined the alpha version based on this feedback.\nIn our third meeting, we co-created text for the home page. We began by listing the intervention components the text needed to address. The text needed to explain what reporting guidelines were, how they can be used, and the benefits they bring to authors. EQUATOR staff drafted text on their own before discussing and editing as a group. We also discussed style and imagery in this meeting. Again, we began by listing the intervention components the images needed to address. These included communicating what reporting guidelines are, who should use them, communicating simplicity and confidence. I invited EQUATOR staff to contribute websites and images they admired for inspiration. We ended up discussing websites run by the National Health Service [7], the International Organisation for Standardisation [8], and the National Institute for Health and Care Excellence [9]. We looked through examples of free-to-use images from a number of libraries [10]; [11]; [12]; [13]. I also consulted usability best-practices [14]. I then populated the alpha version with the text and images discussed in this meeting.\nAfter the third meeting I began redesigning the SRQR guideline. I got written permission from SRQR’s publisher and lead developer, Bridget O’Brien. I began redesigning SRQR by pasting the text into Microsoft Word and rearranging content into categories: what to write, how/where to write it, what to write if the item was not/could not be done, why the item is important and to whom, and examples. I edited sentences to speak directly to authors and to use active voice. E.g. “Describe X” instead of “X should be described”. This shortened the text and made it clearer that the primary audience is authors.\nFor composite items I split the sub-items into bulleted lists. E.g.\n\nFor each X, describe:\n\nA\nB\nC\n\n\nI rearranged conditional sub-items to read as “If X, then describe Y”, instead of “Describe Y if X”. I moved definitions into a glossary and contextual information into notes. I edited the tone of voice to add reassuring language. An example redesigned SRQR item is in #sec-box-item. I asked SRQR’s developer to provide feedback on the redesigned guideline and made refinements based on her comments.\nAfter development, I double checked the intervention against my intervention planning table (see chapter 9) to ensure I had included all components. I invited another round of feedback from EQUATOR staff and made more refinements.\n\nSystem architecture\nWhen considering architecture options I prioritized technology that could feasibly be maintained by EQUATOR staff or a future PhD student. I looked for tools that would be familiar to early career researchers. I considered DIY website builders (like Wix [15] or Squarespace [16]) but these services can be expensive. Most offer a ‘drag and drop’ building experience which, although easy to use, is a laborious way of uploading and formatting large amounts of content. Should EQUATOR want to change how reporting items are presented (for example, move the positioning of examples), they would have to manually edit each item for each reporting guideline. Additionally, our intended intervention changes required custom functionality not offered by these services (e.g., glossary definitions, discussion boards).\nAlthough coding languages like HTML (Hyper Text Markup Language) or javascript are used by many software developers to create websites, few early career researchers are familiar with them. In contrast, many researchers write reproducible manuscripts in markdown. Markdown is a simple language and takes minutes to learn. It uses asterisks, underscores, and carets to make text **bold**, _italic_, or ^superscript^. Headings, URLS, and references are similarly easy, and free editing software makes writing markdown feel like writing a Microsoft Word document. Markdown converts into lots of other formats, including docx (Microsoft Word files), LaTeX, PDF, and HTML.\nMany researchers already use tools like RStudio [17] or Quarto [18] to convert markdown into other formats. I decided to use Quarto because it is open source, has great documentation, and its functionality can be extended with other programming languages commonly used by researchers and statisticians, like Python or Ruby.\nI’ve made all code available on Github [19], an industry-leading version control system commonly used by academics. I’ve used Github Pages [20] to serve the website itself, because it is free, beginner friendly, configurable, and integrates (almost) seamlessly with Github’s version control system."
  },
  {
    "objectID": "chapters/10_redesign/index.html#results",
    "href": "chapters/10_redesign/index.html#results",
    "title": "Developing intervention components into a prototype",
    "section": "Results",
    "text": "Results\nThe redesigned home page and SRQR guideline can are shown in ?@fig-home, ?@fig-rg-intro, and ?@fig-discussion. For comparison, the old versions are shown in ?@fig-home-b4 and ?@fig-db-b4. I managed to implement 46 of the 63 intervention components identified in chapter 9. I have included the components I have built in the “After” column of the intervention planning table in chapter 9, thereby linking each component with the its behavioural technique, intervention function, and the barrier it addresses. Figures illustrating the redesign are also in chapter 9.\nThe website source code is viewable at https://github.com/jamesrharwood/equator-guidelines-website and the live website is viewable at https://jamesrharwood.github.io/equator-guidelines-website/."
  },
  {
    "objectID": "chapters/10_redesign/index.html#discussion",
    "href": "chapters/10_redesign/index.html#discussion",
    "title": "Developing intervention components into a prototype",
    "section": "Discussion",
    "text": "Discussion\nUsing the intervention planning table from my previous chapter, I have created functional prototypes of a redesigned reporting guidelines and the EQUATOR Network home page. If EQUATOR chooses to adopt these changes and apply them to other guidelines, hundreds of thousands of authors would access these redesigned resources each year.\nThese redesigned resources have potential to benefit authors directly, and also to help other stakeholders. For example, publishers may find enforcing guidelines easier if our redesigned resources prove more user friendly. Guideline developers will benefit from having a ready-to-use dissemination platform based on evidence, with built-in channels for collecting feedback from authors. This feedback may help guideline developers refine their resources further, and could act as evidence to support future funding applications.\nUsing a framework and a systematic method helped EQUATOR staff and I to make decisions based on evidence and theory, and to reduce the influence of our own subjectivity. Instead of relying on personal preference, we tried to ensure choices reflected the function we were trying to employ. For example, when choosing a background image, instead of asking “do you like this one?”, the questions became “what feelings do you think this image conveys? Does it communicate simplicity?”. Similarly, when participants disagreed, it was useful to delve into why. For example, when sketching layouts for the home page, some EQUATOR staff drew a single, prominent search button. Others drew a plethora of options like “view guidelines by speciality”, “view guidelines A-Z”. Discussion revealed that whereas some staff prefer to search directly for what they want with laser-like focus, others prefer to “explore”, especially when they are not certain what they want or what the website is about. In this instance, the final design takes both use cases into account, but other times we resolved disagreement by referring to the intervention planning table or to similar websites. Hence using a framework and exploring disagreements as a group helped mitigate personal preferences.\nHowever, many decisions required a degree of subjectivity and, as lead researcher, designer, and developer, often these decisions landed on my shoulders. I tried to mitigate this by prioritizing other people’s ideas over my own, and providing many opportunities for feedback. But the result undeniably has my “stamp”. If someone else had built it using the same table of intervention components then some things might be the same (like simplifying the user journey from 5 steps to 2, or the conventional home page layout) but other things would look different (like the choice of wording and images).\nMy design may have benefited from input from other stakeholders. I describe how I obtained feedback from authors in my next chapter, but I would have liked to include authors, publishers, funders, and other stakeholders from the start of the design process. If EQUATOR decide to take my designs forward, these consultations could still take place, but they were not feasible within the time constraints of my PhD.\nInput from user experience experts and graphic designers would also be useful. We found images to be a time consuming pain point. None of us had the skills to create professional looking graphics ourselves, and we found most free stock images were generic and did not communicate what we needed.\nMy experience of working with SRQR’s lead developer, Bridget, was was positive; she was supportive, liked the result, and she was interested when my process revealed gaps in SRQR item description. For example, often there was no guidance of what to write if a reporting item was not or could not be done. Some items did not explain why they are important and to whom. Filling these blanks required time and input that SRQR’s development team were unable to give at present, and so I left these gaps unfilled for now. I anticipate other guidelines will have similar gaps. I hope other guideline developers will be as open-minded as Bridget was, but I expect others may feel less able or motivated to engage with a redesign, or may feel protective over their writing and resistant to change.\nIn addition to filling these gaps, making my redesign “live” would require further technical work. Some of these tasks are administrative and have no behaviour change impact, but there are still 17 intervention components outstanding. These components were too difficult or time consuming to include at this stage. For example, I intended to include more examples of reporting items, and to optimise reporting guideline pages so they rank highly in search engines. EQUATOR could add these intervention components at a later date.\nSTRETCH Describe how new site differs to GoodReports"
  },
  {
    "objectID": "chapters/10_redesign/index.html#conclusions",
    "href": "chapters/10_redesign/index.html#conclusions",
    "title": "Developing intervention components into a prototype",
    "section": "Conclusions",
    "text": "Conclusions\nIn summary, I have described how I involved EQUATOR staff and a guideline developer through a co-design process to redesign the EQUATOR Network’s home page and the SRQR guideline. These new designs include 46 of the 63 intervention components identified in chapter 9. Although my designs may have benefited from including other stakeholders, I explained how I facilitated open discussion, prioritised other’s opinions, and used my intervention planning table to make decisions. In the next chapter I explain how I refined these designs further by interviewing and observing authors.\n\n\n\n\n1. Lincoln YS, Guba EG (1985) Naturalistic Inquiry. SAGE\n\n\n2. ISO 9241-210:2019(en), Ergonomics of human-system interaction Part 210: Human-centred design for interactive systems. \n\n\n3. Velsen LS van, Gemert-Pijnen JEWC van, Nijland N, Beaujean D, Steenbergen J van (2012) Personas: The Linking Pin in Holistic Design for eHealth. In: eTELEMED 2012 : The Fourth International Conference on eHealth, Telemedicine, and Social Medicine. pp 128–133\n\n\n4. Experience WL in R-BU Personas Make Users Memorable for Product Team Members. Nielsen Norman Group \n\n\n5. Personas | Health Education England. Health Education England | Digital Transformation \n\n\n6. Pastel | Fastest visual website feedback tool for web designers, developers and agencies. \n\n\n7. (16 Aug 2018, 12:27 a.m.) The NHS website. nhs.uk \n\n\n8. (2023) ISO - International Organization for Standardization. ISO \n\n\n9. NICE | The National Institute for Health and Care Excellence. NICE \n\n\n10. 8Guild, madebyoliver Search and download Free vector icons, stickers, illustrations, UI Kits and more. Smashicons | The largest icon set in the world. \n\n\n11. Download Free Vectors, Images, Stock Photos & Stock Videos. Vecteezy \n\n\n12. Free Icons and Stickers - Millions of images to download. Flaticon \n\n\n13. Freepik: Download Free Videos, Vectors, Photos, and PSD. Freepik \n\n\n14. Experience WL in R-BU Nielsen Norman Group: UX Training, Consulting, & Research. Nielsen Norman Group \n\n\n15. Your website, your business, your future｜Wix.com. wix.com \n\n\n16. Website Builder Create a Website in Minutes. Squarespace \n\n\n17. Posit The RStudio Integrated Development Environment (IDE) is the preferred tools for data scientists who develop in R & Python. Posit \n\n\n18. Quarto: An open source technical publishing system for creating beautiful articles, websites, blogs, books, slides, and more. Supports Python, R, Julia, and JavaScript. Quarto \n\n\n19. jamesrharwood (2023) EQUATOR Guidelines Website. \n\n\n20. GitHub Pages. GitHub Pages"
  },
  {
    "objectID": "chapters/appendix/synthesis_search_strategies.html",
    "href": "chapters/appendix/synthesis_search_strategies.html",
    "title": "Thesis",
    "section": "",
    "text": "I did not seek any external peer review of my search. I did not record the database versions at the time of searching due to an oversight. I performed forward and backwards citation searching but found no additional records. I did not set up email alerts.\n\n\nDatabases: Medline, Embase, AMED, PsycINFO.\nSearch date: 08/12/2021\nI used a federated search. The kw field does not exist in PsycINFO or AMED and so was ignored by these databases. The tw field does not exist in AMED either and was mapped to af instead.\n\n((reporting or writ$ or author$) adj2 (checklist$ or statement$ or guid$ or template$ or standard$ or recommendation$)).ti,kw.\n((consort$ or strobe$ or stard$ or prisma$ or moose$ or squire$ or arrive$ or remark$ or tripod$ or cheers$ or spirit$ or srqr$ or coreq$) adj3 (guid$ or statement$ or checklist$)).ti,kw.\n(experience$ or interview$ or survey$ or questionnaire$ or \"focus group$\" or facilitat$ or barrier$).af.\nqualitative.tw.\n1 or 2\n3 or 4\n5 and 6\n\nPlatform-specific filter applied: 1996 – current year\n\n\n\nDatabases: Latin American and Caribbean Health Sciences Literature, African Index Medicus, Western Pacific Region Index Medicus, Index Medicus for South-East Asia Region, and Index Medicus for the Eastern Mediterranean Region, searched using Global Index Medicus (https://www.globalindexmedicus.net/); Scientific Electronic Library Online (https://scielo.org/en/).\nSearch date: 08/12/2021\nti:(((reporting OR writ* OR author*) AND (checklist* OR statement* OR guid* OR template* OR standard* OR recommendation* OR experience* OR interview* OR survey* OR questionnaire* OR \"focus group*\" OR facilitat* OR barrier* OR qualitative*)))\nPlatform-specific filter applied: 1996 – current year\n\n\n\nDatabase URL: https://www.imicams.ac.cn/\nSearch Date: 25/10/2021\n1. 报告 OR 撰写 OR 作者\n2. 清单 OR 声明 OR 指导 OR 规范 OR 指南 OR 共识 OR 模板 OR 标准 OR 推荐意见\n3. CONSORT OR PRISMA OR STROBE OR SPIRIT OR STARD OR SRQR OR ARRIVE OR SQUIRE OR CHEERS OR TRIPOD OR COREQ\n4. 经历 OR 体验 OR 访谈OR 调查 OR 问卷调查 OR 焦点小组 OR 焦点群众\n5. 促进 OR 阻碍\n6. 质性研究 OR 定性研究\n7. (#2) AND (#1)\n8. (#7) OR (#3)\n9. (#6) OR (#5) OR (#4)\n10. (#9) AND (#8)\n11. ((#9) AND (#8)) AND (\"循证文献\"[文献类型] OR \"临床试验\"[文献类型])\n\n\n\nDatabase URL: https://www.cnki.net/\nSearch Date: 25/10/2021\n( ( ( TI = '报告' OR TI = '撰写' OR TI = '作者') AND (TI = '清单' OR TI = '声明' OR TI = '指导' OR TI = '规范' OR TI = '指南' OR TI = '共识' OR TI = '模板' OR TI = '标准' OR TI = '推荐意见' ) ) OR ( TI = 'CONSORT' OR TI = 'STROBE' OR TI = 'PRISMA' OR TI = 'SPIRIT' OR TI = 'STARD' OR TI = 'SRQR' OR TI = 'ARRIVE' OR TI = 'SQUIRE' OR TI = 'CHEERS' OR TI = 'TRIPOD' OR TI = 'COREQ' ) ) AND (TI = '经历' OR TI = '体验' OR TI = '访谈' OR TI = '调查' OR TI = '问卷调查' OR TI = '焦点群众' OR TI = '焦点小组' OR TI = '促进' OR TI = '阻碍' OR TI = '质性研究' OR TI = '定性研究' )\n\n\n\nDatabase URL: http://www.wanfangdata.com/\n(((题名或关键词:(报告 or 撰写 or 作者)) and (题名或关键词:(清单 or 声明 or 指导 or 规范 or 指南 or 共识 or 模板 or 标准 or 推荐意见))) or (题名或关键词:(CONSORT or STROBE or STARD or PRISMA or MOOSE or SQUIRE or ARRIVE or REMARK or TRIPOD or CHEERS or SPIRIT or SRQR or COREQ))) and (题名或关键词:(经历 or 体验 or 访谈 or 访问 or 采访 or 调查 or 问卷调查 or 焦点小组 or 焦点群众 or 促进 or 阻碍 or 质性研究 or 定性研究))\n\n\n\nDatabase URL: http://www.cqvip.com/\n(((M=(报告 OR 撰写 OR 作者)) AND (M=(清单 OR 声明 OR 指导 OR 规范 OR 指南 OR 共识 OR 模板 OR 标准 OR 推荐意见))) OR (M=(CONSORT OR PRISMA OR STROBE OR SPIRIT OR STARD OR SRQR OR ARRIVE OR SQUIRE OR CHEERS OR TRIPOD OR COREQ))) AND (M=(体验 OR 访谈 OR 调查 OR 问卷调查 OR 焦点群众 OR 焦点小组 OR 质性研究 OR 定性研究))\n\n\n\nURL: https://osf.io/\nSearch Date: 15/12/2021\ntitle:(((reporting OR writ* OR author*) AND (checklist* OR statement* OR guid* OR template* OR standard* OR recommendation* OR experience* OR interview* OR survey* OR questionnaire* OR \"focus group*\" OR facilitat* OR barrier* OR qualitative*)))\n\n\n\nURL: http://miror-ejd.eu/publications/\nSearch Date: 14/12/2021\nI manually searched the list of publications."
  },
  {
    "objectID": "chapters/appendix/synthesis_search_strategies.html#app-search-strategies",
    "href": "chapters/appendix/synthesis_search_strategies.html#app-search-strategies",
    "title": "Thesis",
    "section": "",
    "text": "I did not seek any external peer review of my search. I did not record the database versions at the time of searching due to an oversight. I performed forward and backwards citation searching but found no additional records. I did not set up email alerts.\n\n\nDatabases: Medline, Embase, AMED, PsycINFO.\nSearch date: 08/12/2021\nI used a federated search. The kw field does not exist in PsycINFO or AMED and so was ignored by these databases. The tw field does not exist in AMED either and was mapped to af instead.\n\n((reporting or writ$ or author$) adj2 (checklist$ or statement$ or guid$ or template$ or standard$ or recommendation$)).ti,kw.\n((consort$ or strobe$ or stard$ or prisma$ or moose$ or squire$ or arrive$ or remark$ or tripod$ or cheers$ or spirit$ or srqr$ or coreq$) adj3 (guid$ or statement$ or checklist$)).ti,kw.\n(experience$ or interview$ or survey$ or questionnaire$ or \"focus group$\" or facilitat$ or barrier$).af.\nqualitative.tw.\n1 or 2\n3 or 4\n5 and 6\n\nPlatform-specific filter applied: 1996 – current year\n\n\n\nDatabases: Latin American and Caribbean Health Sciences Literature, African Index Medicus, Western Pacific Region Index Medicus, Index Medicus for South-East Asia Region, and Index Medicus for the Eastern Mediterranean Region, searched using Global Index Medicus (https://www.globalindexmedicus.net/); Scientific Electronic Library Online (https://scielo.org/en/).\nSearch date: 08/12/2021\nti:(((reporting OR writ* OR author*) AND (checklist* OR statement* OR guid* OR template* OR standard* OR recommendation* OR experience* OR interview* OR survey* OR questionnaire* OR \"focus group*\" OR facilitat* OR barrier* OR qualitative*)))\nPlatform-specific filter applied: 1996 – current year\n\n\n\nDatabase URL: https://www.imicams.ac.cn/\nSearch Date: 25/10/2021\n1. 报告 OR 撰写 OR 作者\n2. 清单 OR 声明 OR 指导 OR 规范 OR 指南 OR 共识 OR 模板 OR 标准 OR 推荐意见\n3. CONSORT OR PRISMA OR STROBE OR SPIRIT OR STARD OR SRQR OR ARRIVE OR SQUIRE OR CHEERS OR TRIPOD OR COREQ\n4. 经历 OR 体验 OR 访谈OR 调查 OR 问卷调查 OR 焦点小组 OR 焦点群众\n5. 促进 OR 阻碍\n6. 质性研究 OR 定性研究\n7. (#2) AND (#1)\n8. (#7) OR (#3)\n9. (#6) OR (#5) OR (#4)\n10. (#9) AND (#8)\n11. ((#9) AND (#8)) AND (\"循证文献\"[文献类型] OR \"临床试验\"[文献类型])\n\n\n\nDatabase URL: https://www.cnki.net/\nSearch Date: 25/10/2021\n( ( ( TI = '报告' OR TI = '撰写' OR TI = '作者') AND (TI = '清单' OR TI = '声明' OR TI = '指导' OR TI = '规范' OR TI = '指南' OR TI = '共识' OR TI = '模板' OR TI = '标准' OR TI = '推荐意见' ) ) OR ( TI = 'CONSORT' OR TI = 'STROBE' OR TI = 'PRISMA' OR TI = 'SPIRIT' OR TI = 'STARD' OR TI = 'SRQR' OR TI = 'ARRIVE' OR TI = 'SQUIRE' OR TI = 'CHEERS' OR TI = 'TRIPOD' OR TI = 'COREQ' ) ) AND (TI = '经历' OR TI = '体验' OR TI = '访谈' OR TI = '调查' OR TI = '问卷调查' OR TI = '焦点群众' OR TI = '焦点小组' OR TI = '促进' OR TI = '阻碍' OR TI = '质性研究' OR TI = '定性研究' )\n\n\n\nDatabase URL: http://www.wanfangdata.com/\n(((题名或关键词:(报告 or 撰写 or 作者)) and (题名或关键词:(清单 or 声明 or 指导 or 规范 or 指南 or 共识 or 模板 or 标准 or 推荐意见))) or (题名或关键词:(CONSORT or STROBE or STARD or PRISMA or MOOSE or SQUIRE or ARRIVE or REMARK or TRIPOD or CHEERS or SPIRIT or SRQR or COREQ))) and (题名或关键词:(经历 or 体验 or 访谈 or 访问 or 采访 or 调查 or 问卷调查 or 焦点小组 or 焦点群众 or 促进 or 阻碍 or 质性研究 or 定性研究))\n\n\n\nDatabase URL: http://www.cqvip.com/\n(((M=(报告 OR 撰写 OR 作者)) AND (M=(清单 OR 声明 OR 指导 OR 规范 OR 指南 OR 共识 OR 模板 OR 标准 OR 推荐意见))) OR (M=(CONSORT OR PRISMA OR STROBE OR SPIRIT OR STARD OR SRQR OR ARRIVE OR SQUIRE OR CHEERS OR TRIPOD OR COREQ))) AND (M=(体验 OR 访谈 OR 调查 OR 问卷调查 OR 焦点群众 OR 焦点小组 OR 质性研究 OR 定性研究))\n\n\n\nURL: https://osf.io/\nSearch Date: 15/12/2021\ntitle:(((reporting OR writ* OR author*) AND (checklist* OR statement* OR guid* OR template* OR standard* OR recommendation* OR experience* OR interview* OR survey* OR questionnaire* OR \"focus group*\" OR facilitat* OR barrier* OR qualitative*)))\n\n\n\nURL: http://miror-ejd.eu/publications/\nSearch Date: 14/12/2021\nI manually searched the list of publications."
  },
  {
    "objectID": "chapters/appendix/ideas.html",
    "href": "chapters/appendix/ideas.html",
    "title": "Ideas generated from workshops and focus groups",
    "section": "",
    "text": "Consider creating reporting guidance to help authors write protocols, funding applications, and ethics applications.\n\nWho could do this: Guideline developers\n\n\nBarriers addressed:\n\nResearchers may not have tools for the job at hand\n\n\nResearchers may not encounter reporting guidelines early enough to act on them\n\n\n\n\n\n\nTo avoid duplicating resources, before commencing a new reporting guideline:\n\nconsult EQUATOR’s register of reporting guidelines under development;\n\nEQUATOR could make this register easier to find and search;\n\ncontact the developers of related reporting guidelines;\njournals could ask reporting guideline developers to prove that they have registered their guideline with EQUATOR (like they do for clinical trials).\n\nWhen a new reporting guideline is justified, build upon existing reporting guidelines instead of starting from scratch. This could mean extending or replacing subsets of items instead of publishing a totally new reporting guideline.\nConsider making modular guidance. For instance, the Journal Article Reporting Standards (JARS) are a set of reporting guidelines for psychology. JARS has a main guideline for quantitative studies. The Design item can be extended by one of three modules depending on whether the study involved experimental manipulation or was conducted on a single individual. the experimental manipulation module can be further extended by modules for random assignment, non-random assignment, and clinical trials. Other extension modules exist for longitudinal and replication studies.\n\n\nWho could do this: Guideline developers, EQUATOR Network, Publishers\n\n\nBarriers addressed:\n\nResearchers may not know what reporting guidelines exist\n\n\nResearchers may struggle to reconcile multiple sets of guidance\n\n\nResearchers may not know what reporting guideline is their best fit"
  },
  {
    "objectID": "chapters/appendix/ideas.html#before-developing-guidance",
    "href": "chapters/appendix/ideas.html#before-developing-guidance",
    "title": "Ideas generated from workshops and focus groups",
    "section": "",
    "text": "Consider creating reporting guidance to help authors write protocols, funding applications, and ethics applications.\n\nWho could do this: Guideline developers\n\n\nBarriers addressed:\n\nResearchers may not have tools for the job at hand\n\n\nResearchers may not encounter reporting guidelines early enough to act on them\n\n\n\n\n\n\nTo avoid duplicating resources, before commencing a new reporting guideline:\n\nconsult EQUATOR’s register of reporting guidelines under development;\n\nEQUATOR could make this register easier to find and search;\n\ncontact the developers of related reporting guidelines;\njournals could ask reporting guideline developers to prove that they have registered their guideline with EQUATOR (like they do for clinical trials).\n\nWhen a new reporting guideline is justified, build upon existing reporting guidelines instead of starting from scratch. This could mean extending or replacing subsets of items instead of publishing a totally new reporting guideline.\nConsider making modular guidance. For instance, the Journal Article Reporting Standards (JARS) are a set of reporting guidelines for psychology. JARS has a main guideline for quantitative studies. The Design item can be extended by one of three modules depending on whether the study involved experimental manipulation or was conducted on a single individual. the experimental manipulation module can be further extended by modules for random assignment, non-random assignment, and clinical trials. Other extension modules exist for longitudinal and replication studies.\n\n\nWho could do this: Guideline developers, EQUATOR Network, Publishers\n\n\nBarriers addressed:\n\nResearchers may not know what reporting guidelines exist\n\n\nResearchers may struggle to reconcile multiple sets of guidance\n\n\nResearchers may not know what reporting guideline is their best fit"
  },
  {
    "objectID": "chapters/appendix/ideas.html#when-developing-guidance",
    "href": "chapters/appendix/ideas.html#when-developing-guidance",
    "title": "Ideas generated from workshops and focus groups",
    "section": "When developing guidance",
    "text": "When developing guidance\n\n1: Avoid prescribing structure\n\nAvoid prescribing structure of a journal article as it may clash with journal requirements or other reporting guidelines.\nInstead, give options for where items can be reported.\nInclude options beyond the article body where authors can report information, like tables, figures, or appendices be.\n\n\n\nWho could do this: Guideline developers\n\n\nBarriers addressed:\n\nResearchers may struggle to reconcile multiple sets of guidance\n\n\nResearchers may struggle to keep writing concise\n\n\n\n\n2: Keep reporting guidelines agnostic to design choices\n\nAsk authors to describe methods transparently without making assumptions about, or prescribing, methods or design choices. For example, an instruction to “describe how you determined your sample size” may be more helpful than “report your sample size calculation” for authors who encounter checklists at submission and did not perform a sample size calculation before collecting data.\nAvoid recommending or admonishing design choices within the reporting guidance because:\n\ndoing so may make authors feel nervous or ashamed, and therefore less likely to report transparently;\ndesign advice elongates reporting guidelines;\nincluding design advice may give the impression that the reporting guideline is for designing or appraising design.\n\nConsider linking to external design or appraisal tools instead.\n\nWho could do this: Guideline developers\n\n\nBarriers addressed:\n\nResearchers may feel restricted if reporting guidelines prescribe design\n\n\nResearchers may feel afraid to report transparently\n\n\nResearchers may expect the costs to outweigh benefits\n\n\nResearchers may not know what reporting guidelines are\n\n\n\n\n3: Describe reporting items fully\nFor each item, authors may need to know the following:\n\nWhat needs to be reported – a brief description could go in all resources (checklists, templates etc) with a longer description in the full guideline document.\nWhy the information is important, and to whom\nAny circumstances where the item is not applicable and what to write\nIndicate priority, and any circumstances that modify importance\nWhere the item can be reported, including beyond the main article body (e.g., section, table, figure, appendix)\nWhat to write if an item wasn’t, or couldn’t be done\nWhat to write if an item cannot be reported for external reasons. For example, if items cannot be reported because of intellectual property restrictions.\nExamples, which could be real or generated, including:\n\nexamples of good and bad reporting with explanations.\nexamples of concise or word-count-friendly reporting, perhaps in alternative formats like tables and figures.JH\nexamples of well reported “imperfect” items (items that were not done)\nexamples from different research contexts\n\nLinks to external design or appraisal advice\n\n\nWho could do this: Guideline developers\n\n\nBarriers addressed:\n\nResearchers may not know whether a reporting guideline applies to them\n\n\nResearchers may not know how to report an item in practice\n\n\nResearchers may not know how to do an item\n\n\nResearchers may not know what to write when they cannot report an item\n\n\nResearchers may struggle to keep writing concise\n\n\nResearchers may not know why items are important\n\n\nResearchers may not care about the benefits of using a reporting guideline\n\n\nResearchers may be asked to remove reporting guideline content\n\n\nResearchers have limited time\n\n\nResearchers may struggle to keep writing concise\n\n\n\n\n4: Describe each reporting guideline fully\nFor each reporting guideline, authors may need the following information:\n\nA clear definition of the reporting guideline’s intended scope in plain language.\nIf-then rules to direct authors to other, more appropriate reporting guidelines. For example, CONSORT could point authors writing protocols to SPIRIT.\nIf no better guidance exists then indicate which items do/do not apply. For example, no guideline exists for authors writing protocols for observational epidemiology. Their best option currently is to use STROBE, but only some items will be required in a protocol.\nWhat tasks the reporting guideline can and cannot be used for\nHow long the resource will take to use\nWhy the guidance should be trusted and link to how it was developed\n\n\nWho could do this: Guideline developers, EQUATOR Network\n\n\nBarriers addressed:\n\nResearchers may not know whether a reporting guideline applies to them\n\n\nResearchers may not know what reporting guidelines exist\n\n\nResearchers may not know what reporting guideline is their best fit\n\n\nResearchers may not know when reporting guidelines should be used\n\n\nResearchers may feel that checking reporting is someone else’s job.\n\n\nResearchers have limited time\n\n\nResearchers may feel patronized\n\n\n\n\n5: Keep guidance short\nKeep guidance as a short as possible:\n\nBe concise but clear.\nBe realistic about what to expect from authors as each additional item increases the chances an author will be put off\nLink to other guidance elsewhere if desired.\nConsider splitting broad guidance that tries to cater for different options into shorter, modular guidance (modularity avoids duplication).\n\n\nWho could do this: Guideline developers\n\n\nBarriers addressed:\n\nResearchers have limited time\n\n\nResearchers may expect the costs to outweigh benefits"
  },
  {
    "objectID": "chapters/appendix/ideas.html#when-writing-guidance-down-and-creating-resources",
    "href": "chapters/appendix/ideas.html#when-writing-guidance-down-and-creating-resources",
    "title": "Ideas generated from workshops and focus groups",
    "section": "When writing guidance down and creating resources",
    "text": "When writing guidance down and creating resources\n\n1: Make resources ready-to-use\nEnsure resources are ready-to-use e.g., checklists as Word files, not as tables within published articles.\n\nWho could do this: Guideline developers, EQUATOR Network\n\n\nBarriers addressed:\n\nreporting guideline resources may not be in usable formats\n\n\nResearchers have limited time\n\n\n\n\n2: Make reporting guidelines easy to understand\n\nUse plain language.\nDefine key terms.\nUse consistent terms across related resources.\nProvide translations.\nUpdate guidance in response to user feedback.\n\n\nWho could do this: Guideline developers\n\n\nBarriers addressed:\n\nResearchers may misunderstand\n\n\nResearchers may not understand the language\n\n\n\n\n3: Use persuasive language and design\n\nUse language and design to communicate confidence and simplicity as opposed to judgement and complexity.\nEncourage explanation even when choices were unusual or sub-optimal.\nReassure authors that most research has limitations that can be addressed in Discussion sections.\nReassure authors that reporting guidelines are just guidelines.\nAvoid patronizing authors.\nConsider wording instructions directly at the intended user.JH\n\n\nWho could do this: Guideline developers, EQUATOR Network, Publishers, Funders, Ethics committees, Institutions, Conference organisers, Registries, Preprint servers\n\n\nBarriers addressed:\n\nResearchers may feel afraid to report transparently\n\n\nResearchers may feel patronized\n\n\nResearchers may not believe stated benefits\n\n\nResearchers may feel that checking reporting is someone else’s job.\n\n\n\n\n4: Create additional tools\nCreate tools for different tasks:\n\ndiscussion points for planning research in the order decisions are made\nto-do lists for conducting research in the order data is collected\ntemplates for drafting\nwriting assistance tools (e.g., COBWEB)\nchecklists for checking manuscripts that are easy to fill out, update, and cross-check\ntools for co-researchers to check each other’s work\ntools for generating tables and figures\nresources for peer reviewers who wish to review reporting quality including:\n\nguidance specifically for peer reviewers.\ncommonly-used words that reviewers can search for to quickly find relevant text.JH\nsuggested text that peer reviewers can copy to request information\ntools to generate feedback reports\n\njournal articles where reporting guideline items are annotated/highlighted\n\n\nWho could do this: Guideline developers, EQUATOR Network, Funders, Ethics committees, Publishers\n\n\nBarriers addressed:\n\nResearchers may not have tools for the job at hand\n\n\nResearchers may not encounter reporting guidelines early enough to act on them\n\n\n\n\n5: Make resources easy to discover and find\nLink resources:\n\nEnsure all resources link to each other. For example, checklists should link to example and elaboration documents and vice versa.\nRelated reporting guidelines should link to each other.\nReporting guidelines and resources should link to translations\nLinks should be permanent (e.g. DOIs) where possible and old links should be maintained or redirected. Broken links should be replaced.\n\nMake searching easy:\n\nHost resources somewhere consistent, like the EQUATOR Network website and database.\nProvide easy-to-use website search functions\nWeb pages should be optimized for search engines JH\nCreated curated collections for study types\nCreate decision tools for identifying reporting guidelines\n\nNames reporting guidelines to make them easy to discover and find:\n\n\nReporting guideline names could be descriptive, as acronyms may be meaningless to novice users.\nRelated reporting guidelines should use consistent names to show relationships (e.g. PRISMA and PRISMA-P appear more related than CONSORT and SPRIT).\n\n\nWho could do this: Guideline developers, EQUATOR Network\n\n\nBarriers addressed:\n\nResearchers may not know what reporting guidelines exist\n\n\nResearchers may not know what resources exist for a reporting guideline\n\n\nGuidance may be difficult to find\n\n\nResearchers may not know what reporting guideline is their best fit\n\n\n\n\n6: Make information digestible\nOrganise information so it is easy to navigate and not overwhelming.\n\nCater to users that read from start to finish, and those that dip in and out.\nStructure text with headings.\nUse section URLs to send authors directly to relevant parts of guidance.\nConsider hyperlinking related resources\nConsider embedding reporting guidelines that “fit together”, like PRISMA and PRISMA-Abstracts\nFor information presented online, consider showing/hiding information as required. For example, if PRISMA-Abstracts were embedded into PRISMA, users could choose to expand or collapse it. Or you could show/hide guidance depending on whether the author is writing a funding application, protocol, manuscript.\n\n\nWho could do this: Guideline developers, EQUATOR Network\n\n\nBarriers addressed:\n\nResearchers have limited time\n\n\nResearchers may expect the costs to outweigh benefits\n\n\nGuidance may be difficult to find"
  },
  {
    "objectID": "chapters/appendix/ideas.html#when-disseminating-resources",
    "href": "chapters/appendix/ideas.html#when-disseminating-resources",
    "title": "Ideas generated from workshops and focus groups",
    "section": "When disseminating resources",
    "text": "When disseminating resources\n\n1: Describe reporting guidelines where they are encountered\n\nWhen authors first encounter reporting guidelines they may need to know:\n\nwhat reporting guidelines are\nhow and when to use them\nwhy authors should use them, including:\n\nwhat personal benefits to expect\nthe importance to others.\n\n\nDescriptions could be succinct (e.g. on journal instruction pages) or long (e.g. in publications) JH\nA generalised description can go where authors first encounter reporting guidelines e.g., journal author guidelines, EQUATOR’s home page.\nA reporting guideline-specific description could go at the top of guidance documents, checklists, and templates.\n\nConsider specifying whether the reporting guideline is also a design guideline.\nSpecify whether the reporting guidelines are just guidelines, or whether they are intended to be requirements. Name the resource appropriately - words like guideline, standards, criteria, recommended, preferred, and templates, have different meanings.\n\n\n\nWho could do this: Publishers, EQUATOR Network, Guideline developers, Funders, Ethics committees, Institutions, Registries, Preprint servers, Conference organisers\n\n\nBarriers addressed:\n\nResearchers may not know what reporting guidelines are\n\n\nResearchers may not know when reporting guidelines should be used\n\n\nResearchers may not know what benefits to expect\n\n\nResearchers may not know why items are important\n\n\nResearchers may feel that checking reporting is someone else’s job.\n\n\n\n\n2: Make resources accessible\nEnsure resources are open access. This allows access to authors without journal subscriptions and allows others to build upon the guidance.\n\nWho could do this: Guideline developers, EQUATOR Network\n\n\nBarriers addressed:\n\nreporting guidelines may be difficult to access\n\n\n\n\n3: Show and encourage citations\n\nDisplay usage data (like citations or downloads) alongside the guidelines as a form of social proof.\nEncourage authors to cite the reporting guideline so readers discover it.\n\n\nWho could do this: Guideline developers, EQUATOR Network, Publishers, Conference organisers, Preprint servers\n\n\nBarriers addressed:\n\nResearchers may not know what reporting guidelines exist\n\n\nResearchers may not believe stated benefits\n\n\n\n\n4: Provide testimonials\nTestimonials can be short quotes or longer case studies. They could come from:\n\nresearchers who have had positive experiences using reporting guidelines, including researchers that were nervous about transparency,\ndecision makers (e.g., editors/grant managers) that value good reporting and/or check for reporting as part of their evaluation,\npeer reviewers that use reporting guidelines to check for good reporting,\npatients who are affected by research waste,\nand researchers who need to understand, synthesise, or apply research articles.\n\n\nWho could do this: Guideline developers, EQUATOR Network\n\n\nBarriers addressed:\n\nResearchers may not know what benefits to expect\n\n\nResearchers may not believe stated benefits\n\n\nResearchers may expect the costs to outweigh benefits\n\n\nResearchers may not care about the benefits of using a reporting guideline\n\n\nResearchers may feel afraid to report transparently"
  },
  {
    "objectID": "chapters/appendix/ideas.html#on-an-ongoing-basis",
    "href": "chapters/appendix/ideas.html#on-an-ongoing-basis",
    "title": "Ideas generated from workshops and focus groups",
    "section": "On an ongoing basis",
    "text": "On an ongoing basis\n\n1: Budget for reporting\nFunders and research supervisors could encourage researchers to allocate sufficient time and money for documenting and reporting results of their research.\n\nWho could do this: Funders, Institutions\n\n\nBarriers addressed:\n\nResearchers have limited time\n\n\nResearchers may not consider writing as reporting\n\n\n\n\n2: Create rewards\nStakeholders could create new rewards:\n\njournals could fast-track submissions or review for papers that followed a reporting guideline,\njournals could offer discounts on article processing charges for papers that followed a reporting guideline,\njournals, preprint servers, or peer review platforms could badge well reported articles,\nEQUATOR could offer a certification service,\nfunders could reward good reporting financially,\ninstitutions could offer prizes for good reporting.\n\n\nWho could do this: Guideline developers, EQUATOR Network, Publishers, Funders, Institutions, Preprint servers, Registries\n\n\nBarriers addressed:\n\nResearchers may not care about the benefits of using a reporting guideline\n\n\n\n\n3: Create discussion spaces\nCreate spaces for authors to discuss reporting and reporting guidelines. These could be:\n\nonline (forums, social media, email),\nor offline (meet-ups, clubs).\n\nTry to make spaces accessible to researchers from all nationalities, professional disciplines and other demographics. Spaces will allow authors to:\n\nsolicit help,\nshare experiences,\nprovide feedback to guideline developers,\nand cultivate a feeling of inclusivity and community ownership.\n\n\nWho could do this: EQUATOR Network, Guideline developers, Institutions\n\n\nBarriers addressed:\n\nResearchers may misunderstand\n\n\nResearchers may feel patronized\n\n\nResearchers may not believe stated benefits\n\n\nResearchers may not know how to report an item in practice\n\n\nResearchers may not know how to do an item\n\n\n\n\n4: Create ways to catch authors earlier\n\nConsider creating email campaigns to prompt researchers at early stages.\nThe EQUATOR website could encourage visitors to use reporting guidelines for planning and drafting research.\nWebsites could be optimised for search terms like “how to write [study type]”, “protocol”, “research plan” or “funding application”. For example, reporting guideline pages on EQUATOR’s website rank highly in Google searches for “STROBE checklist” but not “How to write an observational epidemiology study”.JH\nWriting clubs and writing training could flag reporting guidelines.\n\n\nWho could do this: EQUATOR Network, Guideline developers, Publishers, Funders, Ethics committees, Institutions, Conference organisers, Preprint servers, Registries\n\n\nBarriers addressed:\n\nResearchers may forget to use reporting guidelines at earlier research stages\n\n\nResearchers may not encounter reporting guidelines early enough to act on them\n\n\nResearchers have limited time\n\n\nResearchers may not know when reporting guidelines should be used\n\n\n\n\n5: Endorse and enforce reporting guidelines\nStakeholders could:\n\nendorse reporting guidelines\nenforce their use by mandating checklists or (preferably) checking adherence to items.\nFunders could ask about reporting guidelines or checklists when collecting updates from grant recipients.\n\n\nWho could do this: Publishers, Institutions, Ethics committees, Funders, Registries, Conference organisers, Preprint servers\n\n\nBarriers addressed:\n\nResearchers may not know what reporting guidelines exist\n\n\nResearchers may expect the costs to outweigh benefits\n\n\n\n\n6: Evidence the benefits\nEvidence any stated benefits:\n\nQuantifiable benefits could be evidenced with data (e.g., acceptance rates, publishing speed, writing speed).\nExperiential benefits could be evidenced by collecting case studies from authors who find that reporting guidelines help them feel confident and write more easily, and from readers who value well-reported research.\n\n\nWho could do this: Guideline developers, EQUATOR Network, Publishers\n\n\nBarriers addressed:\n\nResearchers may not believe stated benefits\n\n\n\n\n7: Make reporting guidelines appear as a priority\nJournals, funders and ethics committees could make reporting guidelines appear as a priority:\n\nMake them prominent in author instructions.\nPlacing checklists earlier in the PDFs that are automatically created by journal submission systems.\nPublicize when reporting guidelines are used by reviewers.\n\n\nWho could do this: Publishers, Funders, Ethics committees, Institutions, Preprint servers, Conference organisers, Registries\n\n\nBarriers addressed:\n\nResearchers may not believe stated benefits\n\n\nResearchers may not care about the benefits of using a reporting guideline\n\n\n\n\n8: Promote reporting guidelines\n\nPromote reporting guidelines on and offline.\n\nOnline may include websites, email campaigns, social media, and blogs.\nOffline may include appearing at conferences, seminars, and workshops.\n\nInstitutions could promote reporting guidelines in their curricula, learning materials, or through reporting champions. Reporting guideline developers or EQUATOR could push for reporting guidelines to be included in text books.\nPromotion can begin before a reporting guideline has been published so that researchers know about guidelines being developed.\n\nNB. Promotion is different to endorsement; a journal could run an email campaign to promote reporting guidelines without having an endorsement policy.\n\nWho could do this: Institutions, Publishers, Guideline developers, EQUATOR Network, Ethics committees, Funders, Societies, Registries, Conference organisers, Preprint servers\n\n\nBarriers addressed:\n\nResearchers may not know what reporting guidelines are\n\n\nResearchers may not know what reporting guidelines exist\n\n\n\n\n9: Install reporting champions\nAll stakeholders could have members to promote and facilitate the usage of reporting guidelines.\n\nThis could follow a local network model with EQUATOR as the central organiser.\nCould make use of existing networks, like regional reproducibility networks.\n\n\nWho could do this: EQUATOR Network, Guideline developers, Institutions, Funders, Ethics committees, Publishers, Conference organisers, Preprint servers, Registries\n\n\nBarriers addressed:\n\nResearchers may not know what reporting guidelines are\n\n\nResearchers may not know what benefits to expect\n\n\nResearchers may misunderstand\n\n\nResearchers may not know when reporting guidelines should be used\n\n\nResearchers may not know why items are important\n\n\n\n\n10: Provide additional teaching\nProvide education or training (e.g., courses, videos) specific to particular reporting guidelines.\nMore generally, students could:\n\nlearn about writing as a process and workflows for documenting and communicating research,\nlearn about research waste from poor reporting,JH\nattempt a replication to learn about the importance of complete reporting,\nand use a reporting guideline as part of their studies.\n\n\nWho could do this: Guideline developers, EQUATOR Network, Publishers, Institutions, Funders, Ethics committees\n\n\nBarriers addressed:\n\nResearchers may not consider writing as reporting\n\n\nResearchers may misunderstand\n\n\nResearchers may not know why items are important\n\n\nResearchers may not know how to report an item in practice\n\n\nResearchers may not encounter reporting guidelines early enough to act on them\n\n\nResearchers may not care about the benefits of using a reporting guideline\n\n\n\n\n11: Make updating guidelines easier\nUpdate guidance in response to user feedback or changes in the field. This would be easier if:\n\nreporting guideline developers could easily collect feedback from authors.\nsmall updates or refinements could be made without publishing a new article.\nreporting guideline developers had funding to evaluate, refine, and update their resources.JH\n\n\nWho could do this: EQUATOR Network, Funders\n\n\nBarriers addressed:\n\nreporting guidelines can become outdated\n\n\nResearchers may misunderstand"
  },
  {
    "objectID": "chapters/5_website_audit/index.html",
    "href": "chapters/5_website_audit/index.html",
    "title": "A service evaluation of equator-network.org",
    "section": "",
    "text": "Known todos:\n\nSome tables need fixing. Conversion put numbers in wrong columns\nFinish definitions table"
  },
  {
    "objectID": "chapters/5_website_audit/index.html#introduction",
    "href": "chapters/5_website_audit/index.html#introduction",
    "title": "A service evaluation of equator-network.org",
    "section": "Introduction",
    "text": "Introduction\nIn chapter 4 I found two studies that exploring authors’ awareness of EQUATOR as an organisation [1, 2]. Although some authors will know EQUATOR from their training programmes or publications, most will know EQUATOR from its website. Despite running on a shoestring budget and without any in-house expertise in software, design, or user experience, the website’s traffic has increased from 100,000 users to almost 1 million over the last 10 years, attracting visitors from all around the world. For context, around 4.3 million authors published 2.4 million academic articles in 2014 [3], and the STM association estimated that 11 million people working in research and development in 2018 [4]. Whilst these numbers will have grown since, they suggest that EQUATOR’s website traffic numbers are within a single order of magnitude of its potential audience size.\nHence what started as a research-group’s attempt to catalogue reporting guidelines has become a significant part of the academic eco-system. And yet, EQUATOR has never formally considered how successfully their website is performing, or how it could be improved. This process - assessing how effectively a website contributes towards an organisation’s objectives - is generally referred to as a website service evaluation. There is no standard definition nor approach [5]. Some evaluations use in-depth qualitative methods like interviews, card-sorting, and observed browsing or task completion. In chapter 11 I describe how I use some of these techniques to refine the website, but at this stage, my objective was to perform a quick and broad evaluation, and so I decided to use automated tools to explore all user activity.\nIn this chapter I describe how I worked with EQUATOR to identify key metrics of success and how I explored those metrics using Google Analytics, Google Tag Manager, and an automated exit survey. In the discussion section, I infer how the website may need improving.\nPublished article volume estimates are for research in general. I haven’t found numbers specific to medicine yet.\nTODO: dashboard no longer works"
  },
  {
    "objectID": "chapters/5_website_audit/index.html#methods",
    "href": "chapters/5_website_audit/index.html#methods",
    "title": "A service evaluation of equator-network.org",
    "section": "Methods",
    "text": "Methods\nGoogle Analytics [6] is a web analytics service that helps website owners track and understand users’ activity. Used by 85% of websites globally, it is the most popular web analytics service by far [7]. EQUATOR has used Google Analytics to collect data since creating their website. Mostly they use the data to report high level impact metrics to funders (such as number of visitors) but they have never used it to evaluate their site in depth.\nWhen evaluating a website, there are a huge number of metrics you can consider; Google Analytics collects over 50 by default [8, 9], and Google Tag Manager [10] allows you to collect additional custom metrics (see Table 1 for definitions of Google Analytics terms and metrics). With so many options, the first step of evaluation is to decide what metrics are most important for your website’s objectives.\nI met with three members of the UK EQUATOR Centre in October 2021. When I asked what the main purposes of their website [11] was, I received many answers. EQUATOR has content aimed at authors, editors, peer reviewers, educators, librarians, and guideline developers. There are web pages promoting training courses, toolkits for writing and reviewing research, newsletters, and blogs highlighting work done by EQUATOR and guideline developers.\nHowever, at its core, the EQUATOR staff I spoke to agreed that the purpose of the website is to help the global research community learn about and access reporting guidelines. They want website visitors to access the guidance that is right for them and come back to the website whenever they need guidance.\nTo explore how far reality meets this vision, I used Google Analytics to answer the following questions:\n\nHow many people visit the website each year?\nWhere (in the world) are visitors from, and how does this compare with the global distribution of researchers?\nHow often do visitors come back?\nHow do visitors get to the website?\nDo visitors engage with the website?\nWhat content do visitors view?\nWhat reporting guideline database records do visitors view?\nHow many visitors continue to access guidance on a third party site?\nHow may visitors access publications vs. checklists?\n\nCould elaborate on these but my gut is to keep chapter short.\nThe last two questions could not be answered by Google Analytics’ default configuration; it recorded which database records visitors looked at, but not how many people went on to view guidance on third party websites, or whether visitors were viewing checklists or full guidance. Therefore I used Google Tag Manager to create two custom metrics: one which counted when visitors downloaded a reporting checklist file, and another which counted when visitors accessed a third party website.\nTODO name and reference technique\nEQUATOR staff felt it was important that website users were able to access the right resource. Although Google Analytics could tell us what pages visitors access, it can’t tell us what they needed, or why. To explore this, I decided to use PopupSmart [12] to build an exit survey (an online questionnaire that pops up when a user appears to be leaving the site, for instance, if their mouse moves towards the close button or URL bar). I decided to use an open ended question in the hope of receiving richer responses. I didn’t want to annoy users or block them from using the website, and I wanted to keep the survey short in the hope of maximizing repose rate, and so I decided to limit the survey to a single question.\nWe decided against asking “What were you looking for today?” as that wouldn’t tell us whether users had found what they needed. We considered “How easy was it to find what you were looking for?” but decided it was too closed, the word “easy” was too subjective, and it assumed too much about the users’ intent. Instead, given that the questionnaire would only appear as users prepared to leave the site, we decided to ask “Why are you leaving?”. We intended to add narrower follow up questions in the future, depending on the responses to this initial, broad question. We hoped that the question was open enough to cater to users who had found what they needed, those that hadn’t, and users that were leaving for any other reason. For example, one EQUATOR member joked “We might get answers like ‘because it was not the right website I was looking for, I was looking for geography websites’…! haha” (spoiler alert: this wasn’t far off the mark!).\n\n\nTable 1: Definitions of terms used in this chapter\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nReporting guideline database record\nEQUATOR maintains a database of reporting guidelines. Each database entry is viewable as a webpage that displays the name of the guideline, bibliographic data, meta data, and links to the associated publications and files. The database page does not include the guidance itself.\n\n\nGoogle Analytics\n#TODO\n\n\nGoogle Tag Manager\n\n\n\nGoogle Search Console\n\n\n\nPopup Smart\n\n\n\nUser\n\n\n\nSession\n\n\n\nBounce\nA session where a user visits a single page and performs no actions. In our setup, we considered outbound link clicks to be actions (because we want authors to click through to guidance). So if a user visited a reporting guideline database record and then immediately clicked an outbound link to a guidance publication, this would not be counted as a bounce.\n\n\nBounce Rate\nThe ratio of bounce sessions to total sessions within a given time duration."
  },
  {
    "objectID": "chapters/5_website_audit/index.html#results",
    "href": "chapters/5_website_audit/index.html#results",
    "title": "A service evaluation of equator-network.org",
    "section": "Results",
    "text": "Results\n\nHow many people visit the website?\n830 134 users visited equator-network.org in 2021 (see Table 2).\n\n\nWhere (in the world) are visitors from, and how does this compare with the global distribution of researchers?\n\nDECIDE Could add a table of how many citable medical research documents come out of the top 10 countries\nOver 800,000 users visited the EQUATOR website in 2021 (Table 2), 150,000 more than the previous year, thus continuing growth seen since attracting 20,000 visitors in 2008. A third of users were from the United States or United Kingdom. The geographic distribution of users didn’t always align with global publishing trends. For example, Brazil accounted for the same proportion of users as the UK (7%) despite producing far fewer citable, medical documents [13]. Conversely, China produces twice as many citable documents as the UK but accounts for only 5% of users. Two fifths of users had their browsers set to a language other than English (Table 4), most frequently Portuguese, Chinese, and Spanish.\n\n\nHow often do visitors come back?\nGoogle Analytics classified almost all users as new (see Table 2), which means they had not visited the site within the previous 2 years (the default expiry limit for Google Analytics cookies). Most users only visited the site once within 2021.\n\n\nTable 2: User demographics equator-network.org for the year 2021.\n\n\n\n\n\n\n\n\nNumber\nPercentage of total users\n\n\n\n\nTotal users 830,\n134\n\n\n\nUsers who had not visited before 823,087\n99%\n\n\n\nUsers by country (top 10)\n\n\n\n\nUnited States\n195,217\n24%\n\n\nUnited Kingdom\n61,292\n7%\n\n\nBrazil\n55,894\n7%\n\n\nChina\n41,628\n5%\n\n\nIndia\n41,196\n5%\n\n\nAustralia\n30,647\n4%\n\n\nCanada\n29,620\n4%\n\n\nNetherlands\n25,426\n3%\n\n\nGermany\n24,561\n3%\n\n\nSpain\n23,020\n3%\n\n\nUsers by browser language setting (top 10)\n\n\n\n\nEnglish (all varieties*) 498,286 60%\n\n\n\n\nEnglish (United States)\n371,331\n45%\n\n\nEnglish (Great Britain)\n86,580\n10%\n\n\nPortuguese (Brazil)\n50,172\n6%\n\n\nChinese\n42,951\n5%\n\n\nSpanish\n36,204\n4%\n\n\nFrench\n14,708\n2%\n\n\nDutch\n14,331\n2%\n\n\nSpanish (Latin America and Caribbean)\n14,259\n2%\n\n\nJapanese\n13,381\n2%\n\n\nGerman\n12,460\n2%\n\n\nUsers by the number of sessions they made within 2021\n\n\n\n\n1 or more sessions\n818,512\n99%\n\n\n2 or more sessions\n176,570\n21%\n\n\n&gt; 3 sessions\n72,886\n9%\n\n\n\n\n* Includes 105 English locales like United States, India, Ireland etc.\n\n\nHow do visitors get to the website?\nThe source of half of the traffic was labelled as “unknown”, meaning that Google Analytics had no way of knowing where that traffic came from (see Table 3). A third of users arrive at the site via a search engine. An eighth of traffic was explicitly labelled as referrals from other websites, most commonly Wiley and Elsevier journals and Manuscript Central.\nA third of sessions begin with users arriving on the home page (see Table 4). Most other sessions begin with authors arriving directly on a reporting guideline database record page.\n\n\nDo visitors engage with the website?\nOver half of sessions (53%) ended without the user interacting with the site at all (see Table 3). Google Analytics calls this behaviour bouncing. Bounce behaviour was similar regardless of which webpage user arrive on: 45% for the home page, &gt;50% for most reporting guideline database record pages. Two thirds of sessions lasted less than 10 seconds, suggesting little interaction.\n\n\nTable 3: Session information for EQUATOR-Network.org for the year 2021. A session is defined as group of user interactions that take place within a given time frame. If a user visited the site twice within a year, then that would appear as two sessions. A session ends after 30 minutes of inactivity.\n\n\n\n\n\n\n\n\nNumber\nPercent\n\n\n\n\nNumber of sessions\n1,209,420\n\n\n\nMean number of sessions per user\n1.5\n\n\n\nNumber of sessions originating from…\n\n\n\n\na referral from another website\n142,158\n12%\n\n\na search engine\n417,671\n35%\n\n\nunknown sources\n590,659\n49%\n\n\nNumber of sessions lasting…\n\n\n\n\n&lt; 10s\n760,967\n63%\n\n\n&lt; 1 min\n908,516\n75%\n\n\nNumber of pages viewed within a session\n\n\n\n\n0\n1,451\n0%\n\n\n1\n676,670\n56%\n\n\n2\n223,864\n19%\n\n\n3\n81,878\n7%\n\n\n4 or more\n225,557\n19%\n\n\nNumber of sessions ending without interaction\n648,370\n54%\n\n\nNumber of sessions including views of…\n\n\n\n\nreporting guidelines\n605,570\n50%\n\n\nlanding page\n411,429\n34%\n\n\nlibrary\n83,238\n7%\n\n\ntoolkits\n15,574\n1%\n\n\nstudy design info\n15,268\n1%\n\n\n(All other categories of content were viewed in less than 1% of sessions)\n\n\n\n\n\n\n\n\nTable 4: Web pages where visitors began their session and their bounce rates between 1st of January and 1st of July 2022. Bounces are sessions where a visitor leaves without interacting with the website at all.\n\n\n\n\n\n\n\n\nLanding Page\nSessions starting on page\n% of total s\nessions Bounce rate\n\n\n\n\nAll pages\n602,921 100%\n53%\n\n\n\nHome page\n218,260 36%\n45%\n\n\n\nSTROBE\n68,274 11%\n56%\n\n\n\nPRISMA\n29,398 5%\n60%\n\n\n\nReporting guidelines\n27,122 4%\n40%\n\n\n\nCONSORT\n24,615 4%\n55%\n\n\n\nCOREQ\n19,962 3%\n73%\n\n\n\nSTARD\n19,316 3%\n57%\n\n\n\nSRQR\n12,039 2%\n65%\n\n\n\nCARE\n10,759 2%\n56%\n\n\n\nTRIPOD\n10,342 2%\n58%\n\n\n\n\n\n\n\nWhat content do visitors view?\nReporting guideline database record pages were viewed in half of all sessions (see Table 3). The home page was viewed in a third of sessions. Only 7% of sessions included a view of any page listed within the /library directory. All other content categories, including pages within the /toolkits directory, were viewed in 1% of sessions or less.\n\n\nWhat guideline database records do visitors view?\nSTROBE was the most viewed reporting guideline, with 231,207 unique page views in 2021. These numbers dropped rapidly: SQUIRE, the tenth most viewed reporting guideline record, was viewed ten times less frequently than STROBE. Only 65 reporting guideline pages were viewed in more than 0.1% of sessions. There are over 500 reporting guidelines indexed in EQUATOR’s database, but hardly any are viewed regularly. Only 13 guideline database records were viewed in more than 1% of visits, all of which appear in the list of “reporting guidelines for main study types” featured on EQUATOR’s home page and in a side bar on all reporting guideline sub-pages.\n\n\nTable 5: How frequently reporting guideline pages were viewed and resources accessed between the 1st of January 2022 – 1st of July 2022. Outbound links were mainly links to publications and files were mainly checklists, but a few guidelines also have flow diagrams which were rarely downloaded. The ten most accessed guidelines are shown.\n\n\n\n\n\n\n\n\n\nReporting Guideline\nSessions where database page was viewed\n\nUsers that accessed an outbound link\nUsers that accessed a checklist\n\n\n\n\nSTROBE\n\n110,910\n2,502\n38,780\n\n\nPRISMA\n\n50,146\n17,240\n15,522\n\n\nCONSORT\n\n45,832\n2,465\n13,862\n\n\nCOREQ\n\n30,487\n11,152\n-\n\n\nSTARD\n\n27,301\n987\n7,803\n\n\nSRQR\n\n26,132\n10,035\n-\n\n\nCARE\n\n21,892\n8,761\n8,350\n\n\nTRIPOD\n\n15,240\n4,678\n3,960\n\n\nSPIRIT\n\n11,108\n3,985\n3,244\n\n\nSQUIRE\n\n11,061\n1,022\n3,061\n\n\n\n\n\n\nHow many visitors continue to access guidance on a third party site?\nMany users leave the guideline reporting guideline database page without accessing any resources. For example, the STROBE database record was viewed 110,000 times, but the checklist only downloaded 30,000 times (~1 in 3) and the link to the full guidance was only clicked 2,500 times (~1 in 44). Overall, only around 1 in 4 users end up clicking a link that will direct them to a checklist or guideline publication.\n\n\nHow may visitors access publications vs. checklists?\nOf the ten most viewed reporting guideline database pages, all except COREQ and SRQR offer downloadable checklists. For STROBE, STARD, CONSORT, and SQUIRE, the checklist links are clicked more often than links to the full guidance (15:1, 8:1, 6:1, and 3:1 respectively). For PRISMA, CARE, TRIPOD, SPIRIT, link clicks to each resource are more similar (around 1:1).\n\n\nSurvey data suggest some visitors may not understand what the website is about, and may not find it useful\nOur survey received 33 responses in 2 weeks (see Table 6), after being viewed by 25,660 people, (a response rate of 0.1%).\nA few indicated clearly that the user had found what they were looking for; “got what I was looking for!” wrote one user, “found what I needed!” wrote another. Other users hadn’t been so successful, writing “Could not find what I needed”, “I cannot find the guidance that I seek”, “I don’t see what I want”, and “could not find any reporting guidelines for reporting guidelines (specifically, abstracts for reporting guidelines)”. Some visitors voiced frustration with the website (“i did not under stand any thing”, “The site is very complex”, “Too big a mess”).\nSome users hinted at why they were on the website; e.g. “to get reporting guidelines”, “a tool called standard for reporting qualitative research”. One user wrote “word format would be more easy to fulfil”, suggesting they had been looking for a checklist. Others clearly didn’t understand what the EQUATOR website was about. Two authors seemed to be looking for requirements for specific journals: one wrote “format for paper submission to Hindawi”, and another wrote “awful site. I just want to know the requirements in terms of number of words and format for a submission and cannot seem to find this anywhere”. Another author was looking for a “quality of life questionnaire”, another for “scientific research”. More cryptically, one visitor simply wrote “ALGERIA”, another “Germany” (perhaps EQUATOR staff were correct to worry that their website could be mistaken as a geography resource).\nSome users voiced frustration with the survey itself (“do something about your annoying pop up!”). Wary of annoying users and given the poor response rate I decided to take the survey down instead of modifying it.\n\n\nTable 6: Survey responses to the question “Why are you leaving?”\n\n\n\n\n\nSurvey responses\n\n\n\n\nqualitive\n\n\nword format would be more easy to fulfil\n\n\nI’ve just arrived, actually. As I started to look at the page content, this message was thrust in my face - why is it that websites so often ask immediately for feedback on material the viewer hasn’t yet had a chance to explore?\n\n\nI am Research\n\n\nYou are perfect Thanks\n\n\nformat for paper submission in Hindawi\n\n\nI cannot find the guidance that I seek. The site is very complex.\n\n\nI’m not, so you might want to do something about your annoying pop up!\n\n\nI could not find any reporting guidelines for reporting guidelines (specifically, abstracts for reporting guidelines)\n\n\nawful site. I just want to know the requirements in terms of number of words and format for a submission and cannot seem to find this anywhere\n\n\ni did not under stand any thing\n\n\nALGERIA\n\n\nto get reporting guidelines\n\n\nscientific research\n\n\nI just needed the URL to recommend the site to someone else.\n\n\nFound what I needed!\n\n\nbecause I am in the wrong place\n\n\nI am not eligible\n\n\nNo desired\n\n\nI got what I was looking for! Thank you :)\n\n\nCould not find what I needed\n\n\nrelevant titlke is not found\n\n\nDiseño y validación de un nuevo registro clínico de enfermería, para la continuidad de los cuidados y seguridad del paciente en hemodiálisis\n\n\nI just got here?\n\n\nFollowing a link in the website to a different page on the guidelines\n\n\ndont ask that\n\n\nGreat!\n\n\ngermany\n\n\nToo big a mess\n\n\nlink doesn’t work\n\n\nnot relevant\n\n\nquality of life questionnaire\n\n\nthere is another tool called standard for reporting qualitative research. I want that tool because it is development is better than this."
  },
  {
    "objectID": "chapters/5_website_audit/index.html#discussion",
    "href": "chapters/5_website_audit/index.html#discussion",
    "title": "A service evaluation of equator-network.org",
    "section": "Discussion",
    "text": "Discussion\nSummary:\nA core objective of the EQUATOR Network website is to help the global research community to find and access reporting guidelines. Web analytics show that only 1 in 4 users who visited the website ended up clicking a link that would take them to a checklist or to a guideline’s PubMed page. Fewer users would end up actually reaching the guideline, as users then have to navigate from PubMed, to the published article (which may be paywalled), and then find the guidance within the publication itself which can be hidden in a table or supplement.\nOverall, web analytics and survey responses suggest that engagement could be improved. Visitors stay for less than 10 seconds (63%) often not interacting with the site at all (53% bounce rate), almost none return, and some may not understand what the site is about or how to find what they need.\nThere are many possible reasons for poor engagement. Perhaps the website’s content or structure is too complex, perhaps the design puts people off. These reasons could be explored qualitatively using interviews or think aloud and this work should also include non-native english speakers; 40% of users had their browsers set to something other than English. Non-native english speakers may struggle to understand the website content. This could also explain why some countries are under-represented: authors may be less likely to be aware of, discover, or use, resources that are not in their own language.\nTo help non-native english speakers, it may be appropriate to manually translate key parts of the website (like landing pages) and popular guidelines into some languages. But manual translations are expensive, difficult to update, and cannot be scaled. Automatic machine translation is more scalable, updates automatically, and inaccuracies can be refined with custom glossaries and language models. EQUATOR has since integration automatic translation of their website. They could also consider using search engine optimisation to reach non-english speaking authors. For example, they could create landing pages in Chinese or Spanish, or add foreign language words to reporting guideline meta-data so that search engines index them.\nSearch engine optimisation could also help EQUATOR reach more authors. EQUATOR could add metadata, optimization their website for mobile phones, and take advantage of Google Search’s featured snippets and description features. EQUATOR should consider what keywords to optimise for, and consider that some keywords ay help catch authors at earlier stages of writing. Google Search Console (a Google product that allows website owners to view how their site performs in Google searches) shows that when users search for “STROBE guidelines,” EQUATOR’s site appears at the top of search results and 36% click this result. However, if a user searches for “how to write an epidemiological report,” EQUATOR drops to 29th place with a click rate of 0%. EQUATOR should ensure the site is optimized for naïve users at an early stage of writing who may not know guideline acronyms.\nIncreasing traffic from search engines would be useful, but so would understanding where current traffic comes from and, at the moment, half of the traffic comes from “unknown” sources. This is how traffic is labelled when Google has no information about where that traffic comes from. Although it’s possible that some of this traffic comes from links within offline documents, it’s far more likely that this traffic represents referral traffic from websites that are linking to EQUATOR using links that start with http instead of https. When a secure website (with an address that starts with https) links to a less secure one (whose address starts with http) address, no referral data gets sent. EQUATOR upgraded its website to use https years ago, but journals have continued to link to EQUATOR using the old, less secure, http address.\nEQUATOR could ask journals and submission systems to update how they link to EQUATOR’s website. This would result in more correct referral data.This data would tell EQUATOR which journals are successfully recommending reporting guidelines and it would allow EQUATOR to infer visitors’ intentions. For example, traffic from submission systems may signify authors who are in the very late stages of writing, and may be seeking a checklist.\nIndeed, many authors do appear to be accessing checklists over and above full guidance. This could be their intention (if I’m right that “unknown” traffic is actually coming from journal and submission systems), or it could be because EQUATOR places checklists at the very top of reporting guideline database record pages. Either way, guideline developers should be aware that, for many authors, checklists may be the primary way they interact with the guidance. Some authors may even think that the checklist is the guidance, and never discover that full guidance exists (a possibility that first appeared in my thematic synthesis 3 and was confirmed, later, in my pilot 11). Hence guideline developers should ensure checklists link to the full guidance, and should include key information about the guideline like its aim, scope, and how it is intended to be used.\nSome readers may point out that only a minority of the 500+ guidelines in EQUATOR’s database have downloadable checklists. However, very few guidelines receive any meaningful traffic. Only 13 guideline database records were viewed in more than 1% of visits. The remaining guidelines form a long tail, with some guidelines receiving no traffic at all. Of course, some guidelines will naturally be accessed more often (most people would expect general guidance for systematic reviews to be accessed more than, say, guidance for eye-tracking studies in dentistry). But the severity of the skew suggests that authors may not be discovering guidance that is most appropriate for them. Alternatively, a cynic may interpret this skew as evidence that some reporting guidelines simply aren’t useful. Possible improvements may be to make the search function easier to find and use, and to have more links between related guidelines along with clear instructions of when each reporting guideline should or should not be used.\nSimilarly, besides the 13 popular reporting guidelines, the majority of content on EQUATOR’s website (e.g. its toolkits, blog, and library network) is hardly ever accessed. EQUATOR could try to better understand who is using their website and why, and consider reorganising content to make it easier to find, or pruning dead-weight to simplify the website.\n\nLimitations\nGoogle Analytics uses cookies to track users over time. If a user clears their cookies between visits or uses multiple devices or browsers, the user will appear as multiple users. Cookies expire after 2 years by default. The proportion of new vs. returning users is thus an overestimation but, nevertheless, is still high.\nUltimately, numbers can only tell you so much. Counting bounces is useful, but only qualitative research will explain why users bounce from the EQUATOR website with the frequency that they do.\nThe data presented here cannot be used to draw comparisons with other websites. For example, I was tempted to compare EQUATOR’s bounce rate of 53% with eCommerce benchmarks (around 40%, from an analysis of 1068 european eCommerce websites [14]). But this comparison isn’t useful. EQUATOR’s website is unusual in that it is a free learning resource, not a shop, and its users are different in terms of who they are, what they are trying to do, why they are doing it, and how they get to the website. Consequently it’s not clear whether EQUATOR’s bounce rate should ideally be lower than, higher than, or equivalent to eCommerce websites. Perhaps it could be useful to compare EQUATOR’s website with similar resources but, as yet, I’ve not found any with publicly available analytics.\n\n\nConclusions\nThis chapter presents a first step in evaluating the EQUATOR Network’s website. These data will help EQUATOR UK understand how the website could be improved in order to further help the global research community learn about and access reporting guidelines. If EQUATOR decides to make any changes, the data here will provide useful benchmarks against which to measure progress.\n\n\n\n\n1. Fuller T, Pearson M, Peters J, Anderson R (2015) What affects authors’ and editors’ use of reporting guidelines? Findings from an online survey and qualitative interviews. PLoS ONE 10:e0121585\n\n\n2. Gi̇ray E, Coskun OK, Karacaatli M, Gunduz OH, Yagci İ (2020) Assessment of the knowledge and awareness of a sample of young researcher physicians on reporting guidelines and the EQUATOR network: A single center cross-sectional study. Marmara Medical Journal 33:1–6\n\n\n3. Plume A, Weijen D van (2014) Publish or perish? The rise of the fractional author…. Research Trends 1:\n\n\n4. STM Association STM Global Brief 2021 Economics & Market Size. \n\n\n5. Allison R, Hayes C, McNulty CAM, Young V (2019) A Comprehensive Framework to Evaluate Websites: Literature Review and Development of GoodWeb. JMIR Formative Research 3:e14372\n\n\n6. Analytics Tools & Solutions for Your Business Google Analytics. Google Marketing Platform \n\n\n7. Usage Statistics and Market Share of Traffic Analysis Tools for Websites, July 2023. \n\n\n8. [GA4] Automatically collected events - Firebase Help. \n\n\n9. [GA4] Predefined user dimensions - Firebase Help. \n\n\n10. Web & Mobile Tag Management Solutions Google Tag Manager. Google Marketing Platform \n\n\n11. The EQUATOR Network | Enhancing the QUAlity and Transparency Of Health Research. \n\n\n12. Popup Builder That Boosts Sales. Popup Smart \n\n\n13. Scimago Journal & Country Rank. \n\n\n14. Ecommerce Foundation Ecommerce Benchmark Retail Report 2016. Ecommerce Europe"
  },
  {
    "objectID": "chapters/appendix/index.html#app-search-strategies",
    "href": "chapters/appendix/index.html#app-search-strategies",
    "title": "References",
    "section": "Search strategies",
    "text": "Search strategies\nI did not seek any external peer review of my search. I did not record the database versions at the time of searching due to an oversight. I performed forward and backwards citation searching but found no additional records. I did not set up email alerts.\n\nOvid search strategy\nDatabases: Medline, Embase, AMED, PsycINFO.\nSearch date: 08/12/2021\nI used a federated search. The kw field does not exist in PsycINFO or AMED and so was ignored by these databases. The tw field does not exist in AMED either and was mapped to af instead.\n\n((reporting or writ$ or author$) adj2 (checklist$ or statement$ or guid$ or template$ or standard$ or recommendation$)).ti,kw.\n((consort$ or strobe$ or stard$ or prisma$ or moose$ or squire$ or arrive$ or remark$ or tripod$ or cheers$ or spirit$ or srqr$ or coreq$) adj3 (guid$ or statement$ or checklist$)).ti,kw.\n(experience$ or interview$ or survey$ or questionnaire$ or \"focus group$\" or facilitat$ or barrier$).af.\nqualitative.tw.\n1 or 2\n3 or 4\n5 and 6\n\nPlatform-specific filter applied: 1996 – current year\n\n\nGlobal Index Medicus & SciELO search strategy\nDatabases: Latin American and Caribbean Health Sciences Literature, African Index Medicus, Western Pacific Region Index Medicus, Index Medicus for South-East Asia Region, and Index Medicus for the Eastern Mediterranean Region, searched using Global Index Medicus (https://www.globalindexmedicus.net/); Scientific Electronic Library Online (https://scielo.org/en/).\nSearch date: 08/12/2021\nti:(((reporting OR writ* OR author*) AND (checklist* OR statement* OR guid* OR template* OR standard* OR recommendation* OR experience* OR interview* OR survey* OR questionnaire* OR \"focus group*\" OR facilitat* OR barrier* OR qualitative*)))\nPlatform-specific filter applied: 1996 – current year\n\n\nChinese Biomedical Literature Database\nDatabase URL: https://www.imicams.ac.cn/\nSearch Date: 25/10/2021\n1. 报告 OR 撰写 OR 作者\n2. 清单 OR 声明 OR 指导 OR 规范 OR 指南 OR 共识 OR 模板 OR 标准 OR 推荐意见\n3. CONSORT OR PRISMA OR STROBE OR SPIRIT OR STARD OR SRQR OR ARRIVE OR SQUIRE OR CHEERS OR TRIPOD OR COREQ\n4. 经历 OR 体验 OR 访谈OR 调查 OR 问卷调查 OR 焦点小组 OR 焦点群众\n5. 促进 OR 阻碍\n6. 质性研究 OR 定性研究\n7. (#2) AND (#1)\n8. (#7) OR (#3)\n9. (#6) OR (#5) OR (#4)\n10. (#9) AND (#8)\n11. ((#9) AND (#8)) AND (\"循证文献\"[文献类型] OR \"临床试验\"[文献类型])\n\n\nChina National Knowledge Infrastructure\nDatabase URL: https://www.cnki.net/\nSearch Date: 25/10/2021\n( ( ( TI = '报告' OR TI = '撰写' OR TI = '作者') AND (TI = '清单' OR TI = '声明' OR TI = '指导' OR TI = '规范' OR TI = '指南' OR TI = '共识' OR TI = '模板' OR TI = '标准' OR TI = '推荐意见' ) ) OR ( TI = 'CONSORT' OR TI = 'STROBE' OR TI = 'PRISMA' OR TI = 'SPIRIT' OR TI = 'STARD' OR TI = 'SRQR' OR TI = 'ARRIVE' OR TI = 'SQUIRE' OR TI = 'CHEERS' OR TI = 'TRIPOD' OR TI = 'COREQ' ) ) AND (TI = '经历' OR TI = '体验' OR TI = '访谈' OR TI = '调查' OR TI = '问卷调查' OR TI = '焦点群众' OR TI = '焦点小组' OR TI = '促进' OR TI = '阻碍' OR TI = '质性研究' OR TI = '定性研究' )\n\n\nWanfang Data\nDatabase URL: http://www.wanfangdata.com/\n(((题名或关键词:(报告 or 撰写 or 作者)) and (题名或关键词:(清单 or 声明 or 指导 or 规范 or 指南 or 共识 or 模板 or 标准 or 推荐意见))) or (题名或关键词:(CONSORT or STROBE or STARD or PRISMA or MOOSE or SQUIRE or ARRIVE or REMARK or TRIPOD or CHEERS or SPIRIT or SRQR or COREQ))) and (题名或关键词:(经历 or 体验 or 访谈 or 访问 or 采访 or 调查 or 问卷调查 or 焦点小组 or 焦点群众 or 促进 or 阻碍 or 质性研究 or 定性研究))\n\n\nVIP Chinese Medical Journal Database\nDatabase URL: http://www.cqvip.com/\n(((M=(报告 OR 撰写 OR 作者)) AND (M=(清单 OR 声明 OR 指导 OR 规范 OR 指南 OR 共识 OR 模板 OR 标准 OR 推荐意见))) OR (M=(CONSORT OR PRISMA OR STROBE OR SPIRIT OR STARD OR SRQR OR ARRIVE OR SQUIRE OR CHEERS OR TRIPOD OR COREQ))) AND (M=(体验 OR 访谈 OR 调查 OR 问卷调查 OR 焦点群众 OR 焦点小组 OR 质性研究 OR 定性研究))\n\n\nOSF\nURL: https://osf.io/\nSearch Date: 15/12/2021\ntitle:(((reporting OR writ* OR author*) AND (checklist* OR statement* OR guid* OR template* OR standard* OR recommendation* OR experience* OR interview* OR survey* OR questionnaire* OR \"focus group*\" OR facilitat* OR barrier* OR qualitative*)))\n\n\nMethods in Research on Research\nURL: http://miror-ejd.eu/publications/\nSearch Date: 14/12/2021\nI manually searched the list of publications."
  },
  {
    "objectID": "chapters/appendix/index.html#app-barriers",
    "href": "chapters/appendix/index.html#app-barriers",
    "title": "References",
    "section": "Barriers",
    "text": "Barriers\n\n1: Researchers may not know what reporting guidelines are\nResearchers may have never heard the term “reporting guideline” or may misunderstand it. Researchers may more commonly use terms like “writing” or “writing up” and the word “reporting” may get interpreted as a formal task (such as reporting progress to a funder). The word “guideline” may be interpreted by some as rules (as per journal “author guidelines”) and others as recommendations. Some researchers may perceive reporting guidelines as a set of design requirements, especially if they only use checklists, which typically lack the instructions and nuances included in the full guidance.\n\nIdeas to address this barrier:\n\nDescribe reporting guidelines where they are encountered\n\n\nKeep reporting guidelines agnostic to design choices\n\n\nPromote reporting guidelines\n\n\nInstall reporting champions\n\n\n\n\n2: Researchers may not know what reporting guidelines exist\nResearchers may not be aware of which reporting guidelines exist. Most guidelines on the EQUATOR site are hardly ever accessed\n\nIdeas to address this barrier:\n\nShow and encourage citations\n\n\nAvoid confusing authors with too many reporting guidelines\n\n\nMake resources easy to discover and find\n\n\nEndorse and enforce reporting guidelines\n\n\nPromote reporting guidelines\n\n\nDescribe each reporting guideline fully\n\n\n\n\n3: Researchers may not know whether a reporting guideline applies to them\nIf the scope of a reporting guideline is undefined or unclear, then researchers won’t know whether the guidance applies to them. Researchers may not understand study designs, making it difficult for them to identify which guidance applies.\n\nIdeas to address this barrier:\n\nDescribe reporting items fully\n\n\nDescribe each reporting guideline fully\n\n\n\n\n4: Researchers may not know what reporting guideline is their best fit\nResearchers may not know when more specific guidance exists. An author’s “perfect fit” guideline may not exist, in which case they may not know know when to stop searching, and they may try to use an “imperfect fit” guideline without understanding which items are applicable.\n\nIdeas to address this barrier:\n\nAvoid confusing authors with too many reporting guidelines\n\n\nMake resources easy to discover and find\n\n\nDescribe each reporting guideline fully\n\n\n\n\n5: Researchers may not know what resources exist for a reporting guideline\nResources include the guidance itself, checklists, E&E files, templates, and web tools (e.g. PRISMA flow chart maker). Not all resources exist for each reporting guideline and researchers may be unaware of the ones that do. Many researchers may only use the checklist. Sometimes this is purposeful, but other times it may be because researchers don’t know that full guidance and examples exist.\n\nIdeas to address this barrier:\n\nMake resources easy to discover and find\n\n\n\n\n6: Researchers may not know when reporting guidelines should be used\nResearchers may not know when they should use reporting guidelines in their research workflow. Guideline developers may want researchers to use guidance as early as possible, but this is may not be obvious to researchers who may only ever receive instruction to complete a checklist as part of journal submission and may never discover the full guidance. Consequently, researchers may assume that reporting guidelines are supposed to be used by single authors as pre-submission checklists to demonstrate adherence. It may not occur to them that reporting guidelines can be used earlier, or by teams. Some researchers, having come to this realisation themselves, report wanting to be told to use reporting guidelines earlier in their research.\n\nIdeas to address this barrier:\n\nDescribe reporting guidelines where they are encountered\n\n\nCreate ways to catch authors earlier\n\n\nInstall reporting champions\n\n\nDescribe each reporting guideline fully\n\n\n\n\n7: Researchers may misunderstand\nResearchers may not understand concepts, terms or words within the guidance, or they may understand them differently to how the developers intended. Some items (or entire guidelines) might be new concepts. E.g. SQUIRE guidelines written at a time where Quality Improvement was still a new concept to many people, and some items (e.g. Context, Study of the intervention) were less familiar than others. Researchers may have nowhere to turn for help should they not understand something.\n\nIdeas to address this barrier:\n\nMake reporting guidelines easy to understand\n\n\nCreate discussion spaces\n\n\nInstall reporting champions\n\n\nProvide additional teaching\n\n\nMake updating guidelines easier\n\n\n\n\n8: Researchers may not know what benefits to expect\nResearchers may not know what benefits to expect from using a reporting guideline. These benefits may include:\n\nimproved completeness of reporting which helps readers use research and reduces research waste.\nimproved flow and less “waffle” in writing\nfacilitated discussions between collaborators, especially at the design or protocol stage\npublishing and passing peer review more efficiently\nincreased publisher acceptance rates\nefficient, confident writing\nincreased impact of manuscript, as the article is easier to search for and information within the article is easier to find.\n\n\nIdeas to address this barrier:\n\nDescribe reporting guidelines where they are encountered\n\n\nInstall reporting champions\n\n\nProvide testimonials\n\n\n\n\n9: Researchers may not know why items are important\nResearchers may not know why an item is important, or who it is important to.\n\nIdeas to address this barrier:\n\nDescribe reporting guidelines where they are encountered\n\n\nDescribe reporting items fully\n\n\nInstall reporting champions\n\n\nProvide additional teaching\n\n\n\n\n10: Researchers may not know how to do an item\nResearchers might not know how to do something (e.g., a sample size calculation)\n\nIdeas to address this barrier:\n\nCreate discussion spaces\n\n\nDescribe reporting items fully\n\n\n\n\n11: Researchers may not know how to report an item in practice\nResearchers may not understand how to report a particular item in practice\n\nIdeas to address this barrier:\n\nCreate discussion spaces\n\n\nDescribe reporting items fully\n\n\nProvide additional teaching\n\n\n\n\n12: Researchers may not know what to write when they cannot report an item\nResearchers may not know how to report an item that they did not do (deliberately or as an oversight), or an item that they are unable to report for external reasons (e.g., IP, or data was missing from primary studies).\n\nIdeas to address this barrier:\n\nDescribe reporting items fully\n\n\n\n\n13: Researchers have limited time\nGuidelines take time to find, read, understand, and apply. Sometimes they may require time and work from multiple co-authors. Researchers & guideline developers may underestimate the time required for writing, and time is often most limited at the point of submission as grant funding may have run out.\nChecklists take time to complete, and completing them with page numbers or pasted content can be annoying if future edits necessitate updating the checklist too. Checklists also generate work for editors and peer-reviewers who must cross check page numbers or pasted content with manuscript content.\n\nIdeas to address this barrier:\n\nMake resources ready-to-use\n\n\nBudget for reporting\n\n\nCreate ways to catch authors earlier\n\n\nMake information digestible\n\n\nDescribe reporting items fully\n\n\nDescribe each reporting guideline fully\n\n\nKeep guidance short\n\n\n\n\n14: Researchers may not encounter reporting guidelines early enough to act on them\nSome reporting guideline items require work that has to be done within a certain time windows such as:\n\nduring planning or designing\nbefore or during data collection\nwhen other colleagues are available\nduring the duration of a grant\n\n\nIdeas to address this barrier:\n\nCreate reporting guidance for early stages of research\n\n\nCreate ways to catch authors earlier\n\n\nCreate additional tools\n\n\nProvide additional teaching\n\n\n\n\n15: Researchers may not understand the language\nResearchers may not understand the language guidance is written in. A lot of research comes from countries where English is not the first language, as do a lot of EQUATOR website visitors. Even if a researcher speaks English as a second language, language may be an additional barrier.\n\nIdeas to address this barrier:\n\nMake reporting guidelines easy to understand\n\n\n\n\n16: Researchers may struggle to keep writing concise\n\nFollowing a guideline can result in lengthy, bloated reports which are unpleasant to read and breach journals’ word limits. Researchers may not know how to keep writing fluid and concise or where they can report an item (e.g., what section, in the text or in a table or figure, in the manuscript or in supplementary material).\n\nIdeas to address this barrier:\n\nAvoid prescribing structure\n\n\nDescribe reporting items fully\n\n\nDescribe reporting items fully\n\n\n\n\n17: Researchers may not have tools for the job at hand\nResearchers use reporting guidelines for different tasks and want tools to make that job easier. Researchers report using reporting guidelines for:\n\nPlanning research\nDesigning research\n\nResearchers report wanting items presented in the order in which decisions need to be made\nResearchers report wanting links to resources\n\nWhilst collecting data\n\nResearchers report wanting items ordered in the order they are done\nResaerchers report wanting items embedded into data collection tools\n\nDrafting manuscript\n\nResearchers report wanting templates\n\nChecking manuscripts\nDemonstrating compliance\n\nResearchers report wanting checklists embedded into submission workflows\n\nReviewing the reporting of other people’s manuscripts\nAppraising the quality of other people’s manuscripts\n\n\nIdeas to address this barrier:\n\nCreate reporting guidance for early stages of research\n\n\nCreate additional tools\n\n\n\n\n18: reporting guidelines can become outdated\nGuidelines can become out of date compared to other guidance or compared to current research standards.\n\nIdeas to address this barrier:\n\nMake updating guidelines easier\n\n\n\n\n19: Researchers may struggle to reconcile multiple sets of guidance\nResearchers must adhere to journal guidelines, multiple reporting guidelines (e.g., PRISMA + PRISMA-Abstracts + PRISMA-S) and other best practice guidelines (like NIH principles). Using multiple guidelines increases complexity and costs, and guidelines can contradict each other.\n\nIdeas to address this barrier:\n\nAvoid prescribing structure\n\n\nAvoid confusing authors with too many reporting guidelines\n\n\n\n\n20: Researchers may be asked to remove reporting guideline content\nResearchers may be asked to remove guideline content by co-researchers, editors or reviewers.\n\nIdeas to address this barrier:\n\nDescribe reporting items fully\n\n\n\n\n21: Researchers may forget to use reporting guidelines at earlier research stages\nHaving been told to complete a checklist upon journal submission, researchers may forget to use a reporting guideline earlier next time.\nNB forgetting is different to not realising that reporting guidelines can be used early.\n\nIdeas to address this barrier:\n\nCreate ways to catch authors earlier\n\n\n\n\n22: Guidance may be difficult to find\nResearcher should be able to easily find guidance and resources that they believe to exist. However:\n\nsearch functions can be hard to find or use,\nresearchers may not know which search terms to use,\nwebsites may be hard to navigate,\nguidance can be buried within articles,\nresources may not be optimised for search engines,\nand resources may not be in the same place.\n\n\nIdeas to address this barrier:\n\nMake resources easy to discover and find\n\n\nMake information digestible\n\n\n\n\n23: reporting guidelines may be difficult to access\nResearchers may be unable to access guidance published in subscription journals. Journal websites can feature broken links.\n\nIdeas to address this barrier:\n\nMake resources accessible\n\n\n\n\n24: reporting guideline resources may not be in usable formats\nResources differ in how easy or readily usable they are. For example, some checklists are published as PDF tables that cannot be filled or copied. Some guidance can be dense, unstructured text that is hard to digest or navigate; whereas some researchers will read the guidance sequentially, others may dip in and out whilst writing, and unstructured text can make information harder to find.\n\nIdeas to address this barrier:\n\nMake resources ready-to-use\n\n\n\n\n25: Researchers may feel afraid to report transparently\nResearchers may feel afraid or uncertain when trying to report something that they didn’t (or couldn’t) do.\n\nIdeas to address this barrier:\n\nKeep reporting guidelines agnostic to design choices\n\n\nUse persuasive language and design\n\n\nProvide testimonials\n\n\n\n\n26: Researchers may feel restricted if reporting guidelines prescribe design\nAdvice or assumptions about design choices narrow the scope of the guidance and can make checklists appear prescriptive. Sometimes design assumptions can be implicit. For example, in requiring authors to report the method used to assess risk of bias, PRISMA is implying that authors should have designed their review to assess risk of bias.\n\nIdeas to address this barrier:\n\nKeep reporting guidelines agnostic to design choices\n\n\n\n\n27: Researchers may feel patronized\nResearchers can feel patronized by checklists.\n\nIdeas to address this barrier:\n\nCreate discussion spaces\n\n\nUse persuasive language and design\n\n\nDescribe each reporting guideline fully\n\n\n\n\n28: Researchers may not believe stated benefits\nResearchers may not believe that using a reporting guideline will affect their acceptance rate or publication speed, that using a reporting guideline will help them write, or improve the quality of their manuscript.\n\nIdeas to address this barrier:\n\nShow and encourage citations\n\n\nCreate discussion spaces\n\n\nUse persuasive language and design\n\n\nEvidence the benefits\n\n\nMake reporting guidelines appear as a priority\n\n\nProvide testimonials\n\n\n\n\n29: Researchers may not care about the benefits of using a reporting guideline\nResearchers may understand that reporting guidelines aim to reduce poor reporting, but may not feel that poor reporting matters. Instead of hypothetical benefits or benefits to others, researchers report caring more about personal, immediate benefits like feeling confident, efficiency, and job performance.\n\nIdeas to address this barrier:\n\nCreate rewards\n\n\nDescribe reporting items fully\n\n\nMake reporting guidelines appear as a priority\n\n\nProvide testimonials\n\n\nProvide additional teaching\n\n\n\n\n30: Researchers may expect the costs to outweigh benefits\nResearchers may feel that the costs of using a reporting guideline - the time and work required and the added manuscript length - outweigh the benefits.\n\nIdeas to address this barrier:\n\nKeep reporting guidelines agnostic to design choices\n\n\nEndorse and enforce reporting guidelines\n\n\nMake information digestible\n\n\nKeep guidance short\n\n\nProvide testimonials\n\n\n\n\n31: Researchers may feel that checking reporting is someone else’s job.\nResearchers report feeling that completing a reporting checklist should be the job of the editor or peer reviewer, not the author. Editors and reviewers may also disagree about whose role it is.\n(NB. researchers, editors and reviewers could all check for reporting quality, but this research focusses only on researchers).\n\nIdeas to address this barrier:\n\nDescribe reporting guidelines where they are encountered\n\n\nUse persuasive language and design\n\n\nDescribe each reporting guideline fully\n\n\n\n\n32: Researchers may not consider writing as reporting\nResearchers may need to change their approach to writing or what they consider writing to be.Researchers differ in their writing process. Authors that follow a structured approach to writing may find it easier to incorporate reporting guidelines into their workflow. Some experienced researchers may be used to a way of working and reluctant to change, and some inexperienced researchers may be unaware of alternative writing processes.\n\nIdeas to address this barrier:\n\nBudget for reporting\n\n\nProvide additional teaching"
  },
  {
    "objectID": "chapters/appendix/index.html#app-ideas",
    "href": "chapters/appendix/index.html#app-ideas",
    "title": "References",
    "section": "Ideas generated from workshops and focus groups",
    "text": "Ideas generated from workshops and focus groups"
  },
  {
    "objectID": "chapters/appendix/index.html#before-developing-guidance",
    "href": "chapters/appendix/index.html#before-developing-guidance",
    "title": "References",
    "section": "Before developing guidance",
    "text": "Before developing guidance\n\n1: Create reporting guidance for early stages of research\nConsider creating reporting guidance to help authors write protocols, funding applications, and ethics applications.\n\nWho could do this: Guideline developers\n\n\nBarriers addressed:\n\nResearchers may not have tools for the job at hand\n\n\nResearchers may not encounter reporting guidelines early enough to act on them\n\n\n\n\n2: Avoid confusing authors with too many reporting guidelines\n\nTo avoid duplicating resources, before commencing a new reporting guideline:\n\nconsult EQUATOR’s register of reporting guidelines under development;\n\nEQUATOR could make this register easier to find and search;\n\ncontact the developers of related reporting guidelines;\njournals could ask reporting guideline developers to prove that they have registered their guideline with EQUATOR (like they do for clinical trials).\n\nWhen a new reporting guideline is justified, build upon existing reporting guidelines instead of starting from scratch. This could mean extending or replacing subsets of items instead of publishing a totally new reporting guideline.\nConsider making modular guidance. For instance, the Journal Article Reporting Standards (JARS) are a set of reporting guidelines for psychology. JARS has a main guideline for quantitative studies. The Design item can be extended by one of three modules depending on whether the study involved experimental manipulation or was conducted on a single individual. the experimental manipulation module can be further extended by modules for random assignment, non-random assignment, and clinical trials. Other extension modules exist for longitudinal and replication studies.\n\n\nWho could do this: Guideline developers, EQUATOR Network, Publishers\n\n\nBarriers addressed:\n\nResearchers may not know what reporting guidelines exist\n\n\nResearchers may struggle to reconcile multiple sets of guidance\n\n\nResearchers may not know what reporting guideline is their best fit"
  },
  {
    "objectID": "chapters/appendix/index.html#when-developing-guidance",
    "href": "chapters/appendix/index.html#when-developing-guidance",
    "title": "References",
    "section": "When developing guidance",
    "text": "When developing guidance\n\n1: Avoid prescribing structure\n\nAvoid prescribing structure of a journal article as it may clash with journal requirements or other reporting guidelines.\nInstead, give options for where items can be reported.\nInclude options beyond the article body where authors can report information, like tables, figures, or appendices be.\n\n\n\nWho could do this: Guideline developers\n\n\nBarriers addressed:\n\nResearchers may struggle to reconcile multiple sets of guidance\n\n\nResearchers may struggle to keep writing concise\n\n\n\n\n2: Keep reporting guidelines agnostic to design choices\n\nAsk authors to describe methods transparently without making assumptions about, or prescribing, methods or design choices. For example, an instruction to “describe how you determined your sample size” may be more helpful than “report your sample size calculation” for authors who encounter checklists at submission and did not perform a sample size calculation before collecting data.\nAvoid recommending or admonishing design choices within the reporting guidance because:\n\ndoing so may make authors feel nervous or ashamed, and therefore less likely to report transparently;\ndesign advice elongates reporting guidelines;\nincluding design advice may give the impression that the reporting guideline is for designing or appraising design.\n\nConsider linking to external design or appraisal tools instead.\n\nWho could do this: Guideline developers\n\n\nBarriers addressed:\n\nResearchers may feel restricted if reporting guidelines prescribe design\n\n\nResearchers may feel afraid to report transparently\n\n\nResearchers may expect the costs to outweigh benefits\n\n\nResearchers may not know what reporting guidelines are\n\n\n\n\n3: Describe reporting items fully\nFor each item, authors may need to know the following:\n\nWhat needs to be reported – a brief description could go in all resources (checklists, templates etc) with a longer description in the full guideline document.\nWhy the information is important, and to whom\nAny circumstances where the item is not applicable and what to write\nIndicate priority, and any circumstances that modify importance\nWhere the item can be reported, including beyond the main article body (e.g., section, table, figure, appendix)\nWhat to write if an item wasn’t, or couldn’t be done\nWhat to write if an item cannot be reported for external reasons. For example, if items cannot be reported because of intellectual property restrictions.\nExamples, which could be real or generated, including:\n\nexamples of good and bad reporting with explanations.\nexamples of concise or word-count-friendly reporting, perhaps in alternative formats like tables and figures.JH\nexamples of well reported “imperfect” items (items that were not done)\nexamples from different research contexts\n\nLinks to external design or appraisal advice\n\n\nWho could do this: Guideline developers\n\n\nBarriers addressed:\n\nResearchers may not know whether a reporting guideline applies to them\n\n\nResearchers may not know how to report an item in practice\n\n\nResearchers may not know how to do an item\n\n\nResearchers may not know what to write when they cannot report an item\n\n\nResearchers may struggle to keep writing concise\n\n\nResearchers may not know why items are important\n\n\nResearchers may not care about the benefits of using a reporting guideline\n\n\nResearchers may be asked to remove reporting guideline content\n\n\nResearchers have limited time\n\n\nResearchers may struggle to keep writing concise\n\n\n\n\n4: Describe each reporting guideline fully\nFor each reporting guideline, authors may need the following information:\n\nA clear definition of the reporting guideline’s intended scope in plain language.\nIf-then rules to direct authors to other, more appropriate reporting guidelines. For example, CONSORT could point authors writing protocols to SPIRIT.\nIf no better guidance exists then indicate which items do/do not apply. For example, no guideline exists for authors writing protocols for observational epidemiology. Their best option currently is to use STROBE, but only some items will be required in a protocol.\nWhat tasks the reporting guideline can and cannot be used for\nHow long the resource will take to use\nWhy the guidance should be trusted and link to how it was developed\n\n\nWho could do this: Guideline developers, EQUATOR Network\n\n\nBarriers addressed:\n\nResearchers may not know whether a reporting guideline applies to them\n\n\nResearchers may not know what reporting guidelines exist\n\n\nResearchers may not know what reporting guideline is their best fit\n\n\nResearchers may not know when reporting guidelines should be used\n\n\nResearchers may feel that checking reporting is someone else’s job.\n\n\nResearchers have limited time\n\n\nResearchers may feel patronized\n\n\n\n\n5: Keep guidance short\nKeep guidance as a short as possible:\n\nBe concise but clear.\nBe realistic about what to expect from authors as each additional item increases the chances an author will be put off\nLink to other guidance elsewhere if desired.\nConsider splitting broad guidance that tries to cater for different options into shorter, modular guidance (modularity avoids duplication).\n\n\nWho could do this: Guideline developers\n\n\nBarriers addressed:\n\nResearchers have limited time\n\n\nResearchers may expect the costs to outweigh benefits"
  },
  {
    "objectID": "chapters/appendix/index.html#when-writing-guidance-down-and-creating-resources",
    "href": "chapters/appendix/index.html#when-writing-guidance-down-and-creating-resources",
    "title": "References",
    "section": "When writing guidance down and creating resources",
    "text": "When writing guidance down and creating resources\n\n1: Make resources ready-to-use\nEnsure resources are ready-to-use e.g., checklists as Word files, not as tables within published articles.\n\nWho could do this: Guideline developers, EQUATOR Network\n\n\nBarriers addressed:\n\nreporting guideline resources may not be in usable formats\n\n\nResearchers have limited time\n\n\n\n\n2: Make reporting guidelines easy to understand\n\nUse plain language.\nDefine key terms.\nUse consistent terms across related resources.\nProvide translations.\nUpdate guidance in response to user feedback.\n\n\nWho could do this: Guideline developers\n\n\nBarriers addressed:\n\nResearchers may misunderstand\n\n\nResearchers may not understand the language\n\n\n\n\n3: Use persuasive language and design\n\nUse language and design to communicate confidence and simplicity as opposed to judgement and complexity.\nEncourage explanation even when choices were unusual or sub-optimal.\nReassure authors that most research has limitations that can be addressed in Discussion sections.\nReassure authors that reporting guidelines are just guidelines.\nAvoid patronizing authors.\nConsider wording instructions directly at the intended user.JH\n\n\nWho could do this: Guideline developers, EQUATOR Network, Publishers, Funders, Ethics committees, Institutions, Conference organisers, Registries, Preprint servers\n\n\nBarriers addressed:\n\nResearchers may feel afraid to report transparently\n\n\nResearchers may feel patronized\n\n\nResearchers may not believe stated benefits\n\n\nResearchers may feel that checking reporting is someone else’s job.\n\n\n\n\n4: Create additional tools\nCreate tools for different tasks:\n\ndiscussion points for planning research in the order decisions are made\nto-do lists for conducting research in the order data is collected\ntemplates for drafting\nwriting assistance tools (e.g., COBWEB)\nchecklists for checking manuscripts that are easy to fill out, update, and cross-check\ntools for co-researchers to check each other’s work\ntools for generating tables and figures\nresources for peer reviewers who wish to review reporting quality including:\n\nguidance specifically for peer reviewers.\ncommonly-used words that reviewers can search for to quickly find relevant text.JH\nsuggested text that peer reviewers can copy to request information\ntools to generate feedback reports\n\njournal articles where reporting guideline items are annotated/highlighted\n\n\nWho could do this: Guideline developers, EQUATOR Network, Funders, Ethics committees, Publishers\n\n\nBarriers addressed:\n\nResearchers may not have tools for the job at hand\n\n\nResearchers may not encounter reporting guidelines early enough to act on them\n\n\n\n\n5: Make resources easy to discover and find\nLink resources:\n\nEnsure all resources link to each other. For example, checklists should link to example and elaboration documents and vice versa.\nRelated reporting guidelines should link to each other.\nReporting guidelines and resources should link to translations\nLinks should be permanent (e.g. DOIs) where possible and old links should be maintained or redirected. Broken links should be replaced.\n\nMake searching easy:\n\nHost resources somewhere consistent, like the EQUATOR Network website and database.\nProvide easy-to-use website search functions\nWeb pages should be optimized for search engines JH\nCreated curated collections for study types\nCreate decision tools for identifying reporting guidelines\n\nNames reporting guidelines to make them easy to discover and find:\n\n\nReporting guideline names could be descriptive, as acronyms may be meaningless to novice users.\nRelated reporting guidelines should use consistent names to show relationships (e.g. PRISMA and PRISMA-P appear more related than CONSORT and SPRIT).\n\n\nWho could do this: Guideline developers, EQUATOR Network\n\n\nBarriers addressed:\n\nResearchers may not know what reporting guidelines exist\n\n\nResearchers may not know what resources exist for a reporting guideline\n\n\nGuidance may be difficult to find\n\n\nResearchers may not know what reporting guideline is their best fit\n\n\n\n\n6: Make information digestible\nOrganise information so it is easy to navigate and not overwhelming.\n\nCater to users that read from start to finish, and those that dip in and out.\nStructure text with headings.\nUse section URLs to send authors directly to relevant parts of guidance.\nConsider hyperlinking related resources\nConsider embedding reporting guidelines that “fit together”, like PRISMA and PRISMA-Abstracts\nFor information presented online, consider showing/hiding information as required. For example, if PRISMA-Abstracts were embedded into PRISMA, users could choose to expand or collapse it. Or you could show/hide guidance depending on whether the author is writing a funding application, protocol, manuscript.\n\n\nWho could do this: Guideline developers, EQUATOR Network\n\n\nBarriers addressed:\n\nResearchers have limited time\n\n\nResearchers may expect the costs to outweigh benefits\n\n\nGuidance may be difficult to find"
  },
  {
    "objectID": "chapters/appendix/index.html#when-disseminating-resources",
    "href": "chapters/appendix/index.html#when-disseminating-resources",
    "title": "References",
    "section": "When disseminating resources",
    "text": "When disseminating resources\n\n1: Describe reporting guidelines where they are encountered\n\nWhen authors first encounter reporting guidelines they may need to know:\n\nwhat reporting guidelines are\nhow and when to use them\nwhy authors should use them, including:\n\nwhat personal benefits to expect\nthe importance to others.\n\n\nDescriptions could be succinct (e.g. on journal instruction pages) or long (e.g. in publications) JH\nA generalised description can go where authors first encounter reporting guidelines e.g., journal author guidelines, EQUATOR’s home page.\nA reporting guideline-specific description could go at the top of guidance documents, checklists, and templates.\n\nConsider specifying whether the reporting guideline is also a design guideline.\nSpecify whether the reporting guidelines are just guidelines, or whether they are intended to be requirements. Name the resource appropriately - words like guideline, standards, criteria, recommended, preferred, and templates, have different meanings.\n\n\n\nWho could do this: Publishers, EQUATOR Network, Guideline developers, Funders, Ethics committees, Institutions, Registries, Preprint servers, Conference organisers\n\n\nBarriers addressed:\n\nResearchers may not know what reporting guidelines are\n\n\nResearchers may not know when reporting guidelines should be used\n\n\nResearchers may not know what benefits to expect\n\n\nResearchers may not know why items are important\n\n\nResearchers may feel that checking reporting is someone else’s job.\n\n\n\n\n2: Make resources accessible\nEnsure resources are open access. This allows access to authors without journal subscriptions and allows others to build upon the guidance.\n\nWho could do this: Guideline developers, EQUATOR Network\n\n\nBarriers addressed:\n\nreporting guidelines may be difficult to access\n\n\n\n\n3: Show and encourage citations\n\nDisplay usage data (like citations or downloads) alongside the guidelines as a form of social proof.\nEncourage authors to cite the reporting guideline so readers discover it.\n\n\nWho could do this: Guideline developers, EQUATOR Network, Publishers, Conference organisers, Preprint servers\n\n\nBarriers addressed:\n\nResearchers may not know what reporting guidelines exist\n\n\nResearchers may not believe stated benefits\n\n\n\n\n4: Provide testimonials\nTestimonials can be short quotes or longer case studies. They could come from:\n\nresearchers who have had positive experiences using reporting guidelines, including researchers that were nervous about transparency,\ndecision makers (e.g., editors/grant managers) that value good reporting and/or check for reporting as part of their evaluation,\npeer reviewers that use reporting guidelines to check for good reporting,\npatients who are affected by research waste,\nand researchers who need to understand, synthesise, or apply research articles.\n\n\nWho could do this: Guideline developers, EQUATOR Network\n\n\nBarriers addressed:\n\nResearchers may not know what benefits to expect\n\n\nResearchers may not believe stated benefits\n\n\nResearchers may expect the costs to outweigh benefits\n\n\nResearchers may not care about the benefits of using a reporting guideline\n\n\nResearchers may feel afraid to report transparently"
  },
  {
    "objectID": "chapters/appendix/index.html#on-an-ongoing-basis",
    "href": "chapters/appendix/index.html#on-an-ongoing-basis",
    "title": "References",
    "section": "On an ongoing basis",
    "text": "On an ongoing basis\n\n1: Budget for reporting\nFunders and research supervisors could encourage researchers to allocate sufficient time and money for documenting and reporting results of their research.\n\nWho could do this: Funders, Institutions\n\n\nBarriers addressed:\n\nResearchers have limited time\n\n\nResearchers may not consider writing as reporting\n\n\n\n\n2: Create rewards\nStakeholders could create new rewards:\n\njournals could fast-track submissions or review for papers that followed a reporting guideline,\njournals could offer discounts on article processing charges for papers that followed a reporting guideline,\njournals, preprint servers, or peer review platforms could badge well reported articles,\nEQUATOR could offer a certification service,\nfunders could reward good reporting financially,\ninstitutions could offer prizes for good reporting.\n\n\nWho could do this: Guideline developers, EQUATOR Network, Publishers, Funders, Institutions, Preprint servers, Registries\n\n\nBarriers addressed:\n\nResearchers may not care about the benefits of using a reporting guideline\n\n\n\n\n3: Create discussion spaces\nCreate spaces for authors to discuss reporting and reporting guidelines. These could be:\n\nonline (forums, social media, email),\nor offline (meet-ups, clubs).\n\nTry to make spaces accessible to researchers from all nationalities, professional disciplines and other demographics. Spaces will allow authors to:\n\nsolicit help,\nshare experiences,\nprovide feedback to guideline developers,\nand cultivate a feeling of inclusivity and community ownership.\n\n\nWho could do this: EQUATOR Network, Guideline developers, Institutions\n\n\nBarriers addressed:\n\nResearchers may misunderstand\n\n\nResearchers may feel patronized\n\n\nResearchers may not believe stated benefits\n\n\nResearchers may not know how to report an item in practice\n\n\nResearchers may not know how to do an item\n\n\n\n\n4: Create ways to catch authors earlier\n\nConsider creating email campaigns to prompt researchers at early stages.\nThe EQUATOR website could encourage visitors to use reporting guidelines for planning and drafting research.\nWebsites could be optimised for search terms like “how to write [study type]”, “protocol”, “research plan” or “funding application”. For example, reporting guideline pages on EQUATOR’s website rank highly in Google searches for “STROBE checklist” but not “How to write an observational epidemiology study”.JH\nWriting clubs and writing training could flag reporting guidelines.\n\n\nWho could do this: EQUATOR Network, Guideline developers, Publishers, Funders, Ethics committees, Institutions, Conference organisers, Preprint servers, Registries\n\n\nBarriers addressed:\n\nResearchers may forget to use reporting guidelines at earlier research stages\n\n\nResearchers may not encounter reporting guidelines early enough to act on them\n\n\nResearchers have limited time\n\n\nResearchers may not know when reporting guidelines should be used\n\n\n\n\n5: Endorse and enforce reporting guidelines\nStakeholders could:\n\nendorse reporting guidelines\nenforce their use by mandating checklists or (preferably) checking adherence to items.\nFunders could ask about reporting guidelines or checklists when collecting updates from grant recipients.\n\n\nWho could do this: Publishers, Institutions, Ethics committees, Funders, Registries, Conference organisers, Preprint servers\n\n\nBarriers addressed:\n\nResearchers may not know what reporting guidelines exist\n\n\nResearchers may expect the costs to outweigh benefits\n\n\n\n\n6: Evidence the benefits\nEvidence any stated benefits:\n\nQuantifiable benefits could be evidenced with data (e.g., acceptance rates, publishing speed, writing speed).\nExperiential benefits could be evidenced by collecting case studies from authors who find that reporting guidelines help them feel confident and write more easily, and from readers who value well-reported research.\n\n\nWho could do this: Guideline developers, EQUATOR Network, Publishers\n\n\nBarriers addressed:\n\nResearchers may not believe stated benefits\n\n\n\n\n7: Make reporting guidelines appear as a priority\nJournals, funders and ethics committees could make reporting guidelines appear as a priority:\n\nMake them prominent in author instructions.\nPlacing checklists earlier in the PDFs that are automatically created by journal submission systems.\nPublicize when reporting guidelines are used by reviewers.\n\n\nWho could do this: Publishers, Funders, Ethics committees, Institutions, Preprint servers, Conference organisers, Registries\n\n\nBarriers addressed:\n\nResearchers may not believe stated benefits\n\n\nResearchers may not care about the benefits of using a reporting guideline\n\n\n\n\n8: Promote reporting guidelines\n\nPromote reporting guidelines on and offline.\n\nOnline may include websites, email campaigns, social media, and blogs.\nOffline may include appearing at conferences, seminars, and workshops.\n\nInstitutions could promote reporting guidelines in their curricula, learning materials, or through reporting champions. Reporting guideline developers or EQUATOR could push for reporting guidelines to be included in text books.\nPromotion can begin before a reporting guideline has been published so that researchers know about guidelines being developed.\n\nNB. Promotion is different to endorsement; a journal could run an email campaign to promote reporting guidelines without having an endorsement policy.\n\nWho could do this: Institutions, Publishers, Guideline developers, EQUATOR Network, Ethics committees, Funders, Societies, Registries, Conference organisers, Preprint servers\n\n\nBarriers addressed:\n\nResearchers may not know what reporting guidelines are\n\n\nResearchers may not know what reporting guidelines exist\n\n\n\n\n9: Install reporting champions\nAll stakeholders could have members to promote and facilitate the usage of reporting guidelines.\n\nThis could follow a local network model with EQUATOR as the central organiser.\nCould make use of existing networks, like regional reproducibility networks.\n\n\nWho could do this: EQUATOR Network, Guideline developers, Institutions, Funders, Ethics committees, Publishers, Conference organisers, Preprint servers, Registries\n\n\nBarriers addressed:\n\nResearchers may not know what reporting guidelines are\n\n\nResearchers may not know what benefits to expect\n\n\nResearchers may misunderstand\n\n\nResearchers may not know when reporting guidelines should be used\n\n\nResearchers may not know why items are important\n\n\n\n\n10: Provide additional teaching\nProvide education or training (e.g., courses, videos) specific to particular reporting guidelines.\nMore generally, students could:\n\nlearn about writing as a process and workflows for documenting and communicating research,\nlearn about research waste from poor reporting,JH\nattempt a replication to learn about the importance of complete reporting,\nand use a reporting guideline as part of their studies.\n\n\nWho could do this: Guideline developers, EQUATOR Network, Publishers, Institutions, Funders, Ethics committees\n\n\nBarriers addressed:\n\nResearchers may not consider writing as reporting\n\n\nResearchers may misunderstand\n\n\nResearchers may not know why items are important\n\n\nResearchers may not know how to report an item in practice\n\n\nResearchers may not encounter reporting guidelines early enough to act on them\n\n\nResearchers may not care about the benefits of using a reporting guideline\n\n\n\n\n11: Make updating guidelines easier\nUpdate guidance in response to user feedback or changes in the field. This would be easier if:\n\nreporting guideline developers could easily collect feedback from authors.\nsmall updates or refinements could be made without publishing a new article.\nreporting guideline developers had funding to evaluate, refine, and update their resources.JH\n\n\nWho could do this: EQUATOR Network, Funders\n\n\nBarriers addressed:\n\nreporting guidelines can become outdated\n\n\nResearchers may misunderstand"
  },
  {
    "objectID": "chapters/appendix/barriers.html",
    "href": "chapters/appendix/barriers.html",
    "title": "Barriers",
    "section": "",
    "text": "1: Researchers may not know what reporting guidelines are\nResearchers may have never heard the term “reporting guideline” or may misunderstand it. Researchers may more commonly use terms like “writing” or “writing up” and the word “reporting” may get interpreted as a formal task (such as reporting progress to a funder). The word “guideline” may be interpreted by some as rules (as per journal “author guidelines”) and others as recommendations. Some researchers may perceive reporting guidelines as a set of design requirements, especially if they only use checklists, which typically lack the instructions and nuances included in the full guidance.\n\nIdeas to address this barrier:\n\nDescribe reporting guidelines where they are encountered\n\n\nKeep reporting guidelines agnostic to design choices\n\n\nPromote reporting guidelines\n\n\nInstall reporting champions\n\n\n\n\n2: Researchers may not know what reporting guidelines exist\nResearchers may not be aware of which reporting guidelines exist. Most guidelines on the EQUATOR site are hardly ever accessed\n\nIdeas to address this barrier:\n\nShow and encourage citations\n\n\nAvoid confusing authors with too many reporting guidelines\n\n\nMake resources easy to discover and find\n\n\nEndorse and enforce reporting guidelines\n\n\nPromote reporting guidelines\n\n\nDescribe each reporting guideline fully\n\n\n\n\n3: Researchers may not know whether a reporting guideline applies to them\nIf the scope of a reporting guideline is undefined or unclear, then researchers won’t know whether the guidance applies to them. Researchers may not understand study designs, making it difficult for them to identify which guidance applies.\n\nIdeas to address this barrier:\n\nDescribe reporting items fully\n\n\nDescribe each reporting guideline fully\n\n\n\n\n4: Researchers may not know what reporting guideline is their best fit\nResearchers may not know when more specific guidance exists. An author’s “perfect fit” guideline may not exist, in which case they may not know know when to stop searching, and they may try to use an “imperfect fit” guideline without understanding which items are applicable.\n\nIdeas to address this barrier:\n\nAvoid confusing authors with too many reporting guidelines\n\n\nMake resources easy to discover and find\n\n\nDescribe each reporting guideline fully\n\n\n\n\n5: Researchers may not know what resources exist for a reporting guideline\nResources include the guidance itself, checklists, E&E files, templates, and web tools (e.g. PRISMA flow chart maker). Not all resources exist for each reporting guideline and researchers may be unaware of the ones that do. Many researchers may only use the checklist. Sometimes this is purposeful, but other times it may be because researchers don’t know that full guidance and examples exist.\n\nIdeas to address this barrier:\n\nMake resources easy to discover and find\n\n\n\n\n6: Researchers may not know when reporting guidelines should be used\nResearchers may not know when they should use reporting guidelines in their research workflow. Guideline developers may want researchers to use guidance as early as possible, but this is may not be obvious to researchers who may only ever receive instruction to complete a checklist as part of journal submission and may never discover the full guidance. Consequently, researchers may assume that reporting guidelines are supposed to be used by single authors as pre-submission checklists to demonstrate adherence. It may not occur to them that reporting guidelines can be used earlier, or by teams. Some researchers, having come to this realisation themselves, report wanting to be told to use reporting guidelines earlier in their research.\n\nIdeas to address this barrier:\n\nDescribe reporting guidelines where they are encountered\n\n\nCreate ways to catch authors earlier\n\n\nInstall reporting champions\n\n\nDescribe each reporting guideline fully\n\n\n\n\n7: Researchers may misunderstand\nResearchers may not understand concepts, terms or words within the guidance, or they may understand them differently to how the developers intended. Some items (or entire guidelines) might be new concepts. E.g. SQUIRE guidelines written at a time where Quality Improvement was still a new concept to many people, and some items (e.g. Context, Study of the intervention) were less familiar than others. Researchers may have nowhere to turn for help should they not understand something.\n\nIdeas to address this barrier:\n\nMake reporting guidelines easy to understand\n\n\nCreate discussion spaces\n\n\nInstall reporting champions\n\n\nProvide additional teaching\n\n\nMake updating guidelines easier\n\n\n\n\n8: Researchers may not know what benefits to expect\nResearchers may not know what benefits to expect from using a reporting guideline. These benefits may include:\n\nimproved completeness of reporting which helps readers use research and reduces research waste.\nimproved flow and less “waffle” in writing\nfacilitated discussions between collaborators, especially at the design or protocol stage\npublishing and passing peer review more efficiently\nincreased publisher acceptance rates\nefficient, confident writing\nincreased impact of manuscript, as the article is easier to search for and information within the article is easier to find.\n\n\nIdeas to address this barrier:\n\nDescribe reporting guidelines where they are encountered\n\n\nInstall reporting champions\n\n\nProvide testimonials\n\n\n\n\n9: Researchers may not know why items are important\nResearchers may not know why an item is important, or who it is important to.\n\nIdeas to address this barrier:\n\nDescribe reporting guidelines where they are encountered\n\n\nDescribe reporting items fully\n\n\nInstall reporting champions\n\n\nProvide additional teaching\n\n\n\n\n10: Researchers may not know how to do an item\nResearchers might not know how to do something (e.g., a sample size calculation)\n\nIdeas to address this barrier:\n\nCreate discussion spaces\n\n\nDescribe reporting items fully\n\n\n\n\n11: Researchers may not know how to report an item in practice\nResearchers may not understand how to report a particular item in practice\n\nIdeas to address this barrier:\n\nCreate discussion spaces\n\n\nDescribe reporting items fully\n\n\nProvide additional teaching\n\n\n\n\n12: Researchers may not know what to write when they cannot report an item\nResearchers may not know how to report an item that they did not do (deliberately or as an oversight), or an item that they are unable to report for external reasons (e.g., IP, or data was missing from primary studies).\n\nIdeas to address this barrier:\n\nDescribe reporting items fully\n\n\n\n\n13: Researchers have limited time\nGuidelines take time to find, read, understand, and apply. Sometimes they may require time and work from multiple co-authors. Researchers & guideline developers may underestimate the time required for writing, and time is often most limited at the point of submission as grant funding may have run out.\nChecklists take time to complete, and completing them with page numbers or pasted content can be annoying if future edits necessitate updating the checklist too. Checklists also generate work for editors and peer-reviewers who must cross check page numbers or pasted content with manuscript content.\n\nIdeas to address this barrier:\n\nMake resources ready-to-use\n\n\nBudget for reporting\n\n\nCreate ways to catch authors earlier\n\n\nMake information digestible\n\n\nDescribe reporting items fully\n\n\nDescribe each reporting guideline fully\n\n\nKeep guidance short\n\n\n\n\n14: Researchers may not encounter reporting guidelines early enough to act on them\nSome reporting guideline items require work that has to be done within a certain time windows such as:\n\nduring planning or designing\nbefore or during data collection\nwhen other colleagues are available\nduring the duration of a grant\n\n\nIdeas to address this barrier:\n\nCreate reporting guidance for early stages of research\n\n\nCreate ways to catch authors earlier\n\n\nCreate additional tools\n\n\nProvide additional teaching\n\n\n\n\n15: Researchers may not understand the language\nResearchers may not understand the language guidance is written in. A lot of research comes from countries where English is not the first language, as do a lot of EQUATOR website visitors. Even if a researcher speaks English as a second language, language may be an additional barrier.\n\nIdeas to address this barrier:\n\nMake reporting guidelines easy to understand\n\n\n\n\n16: Researchers may struggle to keep writing concise\n\nFollowing a guideline can result in lengthy, bloated reports which are unpleasant to read and breach journals’ word limits. Researchers may not know how to keep writing fluid and concise or where they can report an item (e.g., what section, in the text or in a table or figure, in the manuscript or in supplementary material).\n\nIdeas to address this barrier:\n\nAvoid prescribing structure\n\n\nDescribe reporting items fully\n\n\nDescribe reporting items fully\n\n\n\n\n17: Researchers may not have tools for the job at hand\nResearchers use reporting guidelines for different tasks and want tools to make that job easier. Researchers report using reporting guidelines for:\n\nPlanning research\nDesigning research\n\nResearchers report wanting items presented in the order in which decisions need to be made\nResearchers report wanting links to resources\n\nWhilst collecting data\n\nResearchers report wanting items ordered in the order they are done\nResaerchers report wanting items embedded into data collection tools\n\nDrafting manuscript\n\nResearchers report wanting templates\n\nChecking manuscripts\nDemonstrating compliance\n\nResearchers report wanting checklists embedded into submission workflows\n\nReviewing the reporting of other people’s manuscripts\nAppraising the quality of other people’s manuscripts\n\n\nIdeas to address this barrier:\n\nCreate reporting guidance for early stages of research\n\n\nCreate additional tools\n\n\n\n\n18: reporting guidelines can become outdated\nGuidelines can become out of date compared to other guidance or compared to current research standards.\n\nIdeas to address this barrier:\n\nMake updating guidelines easier\n\n\n\n\n19: Researchers may struggle to reconcile multiple sets of guidance\nResearchers must adhere to journal guidelines, multiple reporting guidelines (e.g., PRISMA + PRISMA-Abstracts + PRISMA-S) and other best practice guidelines (like NIH principles). Using multiple guidelines increases complexity and costs, and guidelines can contradict each other.\n\nIdeas to address this barrier:\n\nAvoid prescribing structure\n\n\nAvoid confusing authors with too many reporting guidelines\n\n\n\n\n20: Researchers may be asked to remove reporting guideline content\nResearchers may be asked to remove guideline content by co-researchers, editors or reviewers.\n\nIdeas to address this barrier:\n\nDescribe reporting items fully\n\n\n\n\n21: Researchers may forget to use reporting guidelines at earlier research stages\nHaving been told to complete a checklist upon journal submission, researchers may forget to use a reporting guideline earlier next time.\nNB forgetting is different to not realising that reporting guidelines can be used early.\n\nIdeas to address this barrier:\n\nCreate ways to catch authors earlier\n\n\n\n\n22: Guidance may be difficult to find\nResearcher should be able to easily find guidance and resources that they believe to exist. However:\n\nsearch functions can be hard to find or use,\nresearchers may not know which search terms to use,\nwebsites may be hard to navigate,\nguidance can be buried within articles,\nresources may not be optimised for search engines,\nand resources may not be in the same place.\n\n\nIdeas to address this barrier:\n\nMake resources easy to discover and find\n\n\nMake information digestible\n\n\n\n\n23: reporting guidelines may be difficult to access\nResearchers may be unable to access guidance published in subscription journals. Journal websites can feature broken links.\n\nIdeas to address this barrier:\n\nMake resources accessible\n\n\n\n\n24: reporting guideline resources may not be in usable formats\nResources differ in how easy or readily usable they are. For example, some checklists are published as PDF tables that cannot be filled or copied. Some guidance can be dense, unstructured text that is hard to digest or navigate; whereas some researchers will read the guidance sequentially, others may dip in and out whilst writing, and unstructured text can make information harder to find.\n\nIdeas to address this barrier:\n\nMake resources ready-to-use\n\n\n\n\n25: Researchers may feel afraid to report transparently\nResearchers may feel afraid or uncertain when trying to report something that they didn’t (or couldn’t) do.\n\nIdeas to address this barrier:\n\nKeep reporting guidelines agnostic to design choices\n\n\nUse persuasive language and design\n\n\nProvide testimonials\n\n\n\n\n26: Researchers may feel restricted if reporting guidelines prescribe design\nAdvice or assumptions about design choices narrow the scope of the guidance and can make checklists appear prescriptive. Sometimes design assumptions can be implicit. For example, in requiring authors to report the method used to assess risk of bias, PRISMA is implying that authors should have designed their review to assess risk of bias.\n\nIdeas to address this barrier:\n\nKeep reporting guidelines agnostic to design choices\n\n\n\n\n27: Researchers may feel patronized\nResearchers can feel patronized by checklists.\n\nIdeas to address this barrier:\n\nCreate discussion spaces\n\n\nUse persuasive language and design\n\n\nDescribe each reporting guideline fully\n\n\n\n\n28: Researchers may not believe stated benefits\nResearchers may not believe that using a reporting guideline will affect their acceptance rate or publication speed, that using a reporting guideline will help them write, or improve the quality of their manuscript.\n\nIdeas to address this barrier:\n\nShow and encourage citations\n\n\nCreate discussion spaces\n\n\nUse persuasive language and design\n\n\nEvidence the benefits\n\n\nMake reporting guidelines appear as a priority\n\n\nProvide testimonials\n\n\n\n\n29: Researchers may not care about the benefits of using a reporting guideline\nResearchers may understand that reporting guidelines aim to reduce poor reporting, but may not feel that poor reporting matters. Instead of hypothetical benefits or benefits to others, researchers report caring more about personal, immediate benefits like feeling confident, efficiency, and job performance.\n\nIdeas to address this barrier:\n\nCreate rewards\n\n\nDescribe reporting items fully\n\n\nMake reporting guidelines appear as a priority\n\n\nProvide testimonials\n\n\nProvide additional teaching\n\n\n\n\n30: Researchers may expect the costs to outweigh benefits\nResearchers may feel that the costs of using a reporting guideline - the time and work required and the added manuscript length - outweigh the benefits.\n\nIdeas to address this barrier:\n\nKeep reporting guidelines agnostic to design choices\n\n\nEndorse and enforce reporting guidelines\n\n\nMake information digestible\n\n\nKeep guidance short\n\n\nProvide testimonials\n\n\n\n\n31: Researchers may feel that checking reporting is someone else’s job.\nResearchers report feeling that completing a reporting checklist should be the job of the editor or peer reviewer, not the author. Editors and reviewers may also disagree about whose role it is.\n(NB. researchers, editors and reviewers could all check for reporting quality, but this research focusses only on researchers).\n\nIdeas to address this barrier:\n\nDescribe reporting guidelines where they are encountered\n\n\nUse persuasive language and design\n\n\nDescribe each reporting guideline fully\n\n\n\n\n32: Researchers may not consider writing as reporting\nResearchers may need to change their approach to writing or what they consider writing to be.Researchers differ in their writing process. Authors that follow a structured approach to writing may find it easier to incorporate reporting guidelines into their workflow. Some experienced researchers may be used to a way of working and reluctant to change, and some inexperienced researchers may be unaware of alternative writing processes.\n\nIdeas to address this barrier:\n\nBudget for reporting\n\n\nProvide additional teaching"
  },
  {
    "objectID": "chapters/11_pilot/index.html",
    "href": "chapters/11_pilot/index.html",
    "title": "Refining the intervention: qualitative study with authors",
    "section": "",
    "text": "Having defined intervention components and built a prototype (chapter 10) I wanted to refine the home page and SRQR guidance page (“the website”) by getting feedback from authors.\nThe MRC guidance on complex interventions, the Person Based Approach, and the Behaviour Change Wheel all stress the importance of including service users in the design of complex interventions [1]; [2]; [3]. Service users can help identify deficiencies which, if addressed, would make the intervention more successful [1]. Involving service users can also help researchers better understand barriers, and whether intervention components are functioning as intended.\nIn this study, I wanted to address limitations I had identified in my thematic synthesis (chapter 3), where I found testing reporting guidelines with users is rarely done, and often suffers from thin description and unrepresentative participants lacking diversity. In contrast, I wanted to obtain rich data from a diverse group of authors. In chapter 10 I positioned SRQR as a good guideline to test with users because its wide scope includes qualitative methods used by many researchers, from many fields, with varying levels of experience. Thus SRQR offered potential to recruit a diverse sample.\nSTRETCH Can you ground this is some web design literature perhaps too, showing your an always improve things and it can’t be perfect? (CA)\nI did not aspire to perfect the website, as I did not believe an optimal design exists. Instead, I viewed the design process as iterative, and the purpose of this study was to collect evidence to inform future iterations. Instead of asking authors to suggest improvements (I suspect this strategy would have led to superficial suggestions or blank faces), I was more interested in identifying deficiencies. I defined a deficiency as any website element that, if modified, could better facilitate the website’s intended target behaviour (successful application of reporting guidance). I used the term deficiency over and above barrier or facilitator because it includes both (if a facilitator could be improved, it is deficient). In the future, EQUATOR and I will be able to identify modifications to address these deficiencies."
  },
  {
    "objectID": "chapters/11_pilot/index.html#introduction",
    "href": "chapters/11_pilot/index.html#introduction",
    "title": "Refining the intervention: qualitative study with authors",
    "section": "",
    "text": "Having defined intervention components and built a prototype (chapter 10) I wanted to refine the home page and SRQR guidance page (“the website”) by getting feedback from authors.\nThe MRC guidance on complex interventions, the Person Based Approach, and the Behaviour Change Wheel all stress the importance of including service users in the design of complex interventions [1]; [2]; [3]. Service users can help identify deficiencies which, if addressed, would make the intervention more successful [1]. Involving service users can also help researchers better understand barriers, and whether intervention components are functioning as intended.\nIn this study, I wanted to address limitations I had identified in my thematic synthesis (chapter 3), where I found testing reporting guidelines with users is rarely done, and often suffers from thin description and unrepresentative participants lacking diversity. In contrast, I wanted to obtain rich data from a diverse group of authors. In chapter 10 I positioned SRQR as a good guideline to test with users because its wide scope includes qualitative methods used by many researchers, from many fields, with varying levels of experience. Thus SRQR offered potential to recruit a diverse sample.\nSTRETCH Can you ground this is some web design literature perhaps too, showing your an always improve things and it can’t be perfect? (CA)\nI did not aspire to perfect the website, as I did not believe an optimal design exists. Instead, I viewed the design process as iterative, and the purpose of this study was to collect evidence to inform future iterations. Instead of asking authors to suggest improvements (I suspect this strategy would have led to superficial suggestions or blank faces), I was more interested in identifying deficiencies. I defined a deficiency as any website element that, if modified, could better facilitate the website’s intended target behaviour (successful application of reporting guidance). I used the term deficiency over and above barrier or facilitator because it includes both (if a facilitator could be improved, it is deficient). In the future, EQUATOR and I will be able to identify modifications to address these deficiencies."
  },
  {
    "objectID": "chapters/11_pilot/index.html#methods",
    "href": "chapters/11_pilot/index.html#methods",
    "title": "Refining the intervention: qualitative study with authors",
    "section": "Methods",
    "text": "Methods\nThe purpose of this study was to identify deficiencies in a redesigned reporting guideline and EQUATOR Network home page (“the website”).\nMy objectives were to:\n\nexplore the experience of a diverse sample of authors,\nand to identify and understand deficiencies.\n\n\nSampling strategy\nMy purposive sample of authors varied in their:\n\nyears of academic experience,\nsubject area,\ntheir first language,\nand their country of residence.\n\nThis variation was important because my thematic synthesis (chapter 3) suggested inexperienced authors may have the most to benefit from reporting guidelines, but also face the most hurdles. Inexperience may be due to early career stage, being new to a field or study design, or new to academic writing. My synthesis also suggested language barriers could hinder adherence, and my service evaluation of EQUATOR’s existing website (chapter 5) revealed a highly international userbase.\nAuthors were eligible to participate if they were currently engaged in research utilizing qualitative methods, and if they were able to attend an online interview conducted in English.\nI recruited through four channels:\n\nI posted on X (called Twitter at the time).\nI advertised through Penelope.ai [4], the manuscript checker I created before starting my PhD. Many medical journals offer the tool to submitting authors. BMJ Open is the largest of these journals, and enjoys a large, international, author base.\nI invited researchers from a research consultancy in the Philippines.\nI wrote to Chinese researchers who had published qualitative research and about the experience of doing qualitative research in China, and I asked them to share my recruitment advert. One of the researchers I contacted posted the advert on internet forums used by Chinese students.\n\nAll channels invited authors to signal their interest by email. To check applicants’ eligibility as qualitative researchers, I asked them to describe their research methods in a few sentences over email. I excluded applicants if their descriptions made no reference of a qualitative method.\nI sent all eligible applicants the participant information sheet and consent form. I used JISC Online Surveys #REF to obtain consent and ask the following demographic questions:\n\nHow many years have you done research?\nPlease describe your research in a couple of sentences\nWhat is your first language?\nWhat country do you work and live in?\n\nI offered participants £50 reimbursement in return for an expected 2 hour commitment. This was a delivered as an Amazon voucher to UK participants, and a bank transfer to international participants. My recruitment advert, information sheet, consent form, and email templates are in Appendix #TODO.\nTime and money limited my target sample size to 10 participants. As argued by Nielsen and Launder [5], small samples (fewer than 10) are often sufficient to identify the majority of deficiencies. In chapter 8 I introduced information power [6] as a concept to guide sample size in qualitative research, and I drew upon it again in this study. I maximised information power firstly by using methods to elicit rich information from each participant. Secondly, I used my table of intervention components (9) as an analysis framework. Hence I anticipated a sample of 10 to sufficiently inform at least one design iteration at the end of data collection.\n\n\nProcedures\nASK are data collection sessions called interviews, even if the semi structured interview was just one component?\nI wanted my study to resemble the ways authors will experience the website in real life. Firstly, interview sessions took place online using Microsoft Teams. This meant participants could view the website on their own computer, using their normal browser, in their usual place of work. This would allow me to identify problems with slow-loading over bad internet connections and display problems on different screen resolutions, whilst avoiding difficulties of asking a participant to use an unfamiliar computer or browser.\nSecondly, I wanted to replicate the experience of encountering a new website as a naïve user, gradually exploring content, and then reading and applying guidance to one’s own writing. I did this by using a variety of methods:\n\n5 second test to capture initial reactions\nThink aloud to capture exploration\nPlus-minus task\nA writing evaluation\nSemi structured interviews throughout.\n\nI have outlined the order of data collection methods in Table 1.\n\n\nTable 1: Data collection methods and when they occurred.\n\n\n\n\n\n\nStage\nMethod\n\n\n\n\nSession 1\n\nFive second test\nSemi-structured interview 1 - prior experience of reporting guidelines\nThink aloud 1 - the home page\nSemi-structured interview 2 - the home page\nThink aloud 2 - the top of the SRQR page\nSemi-structured interview 3 - the SRQR page\n\n\n\nAt home, between sessions\n\nPlus-minus task\nWriting using SRQR\n\n\n\nSession 2\n\nInterview covering the plus/minus annotations\nInterview covering the writing sample\nSemi-structured interview 4 - closing thoughts\n\n\n\n\n\nMy interview schedule (appendix #TODO) included the verbal instructions for each task and topic guides for each semi-structured interview. I tested the interview schedule by doing a mock interview with a student at Oxford University.\nI began sessions by introducing myself as part of team creating a new website. To encourage open and truthful feedback I reassured participants the best way they could help was by being honest, not to worry about critiquing the website or offending me, and to share positive and negative feedback. I asked participants to tell me a little about themselves to relax into the interview and help them feel comfortable talking, before moving on to the first task: the 5 second test.\n\nFive second test\nUntil this point, participants had no idea what the website was about. My recruitment materials and interview introduction made no mention of writing nor reporting guidelines, and so participants were blind to the website’s purpose.\nThe five second test is an “in the moment” survey method [7]. By sharing my screen, I showed participants the top of the home page for five seconds before removing it and asking questions. The test limits exposure to five seconds because although a participant can absorb much information (colours, words, shapes), five seconds is rarely sufficient to make sense of everything as a whole. The aim is to capture participant’s immediate reactions to salient design elements (like images, large words) before they have a change to consider the content more critically. Furthermore, this five second limit was relevant because my website service evaluation found many authors leave EQUATOR’s website within five seconds without interacting with it.\nThis test was appropriate for the top of the home page because, as per best practice, the area has little text, all relevant content is visible in one frame, and I asked few questions:\n\nWhat do you think the website is about?\nHow do you think this website may affect your work?\n\nIf participants answered the first question with “reporting guidelines”, I asked “what do you think reporting guidelines are?”. If participant’s answers did not mention writing, I asked what stages of research they might use the website.\nI designed these questions to explore three intervention ingredients: describe what reporting guidelines are, how they can be best used, and their benefits. These are the main ingredients featured at the top of the home page.\n\n\nSemi structured interview 1 - prior experience with reporting guidelines\nAfter the five second test it was no longer necessary to keep participants blinded, and so I asked participants about their prior awareness of, or experience with, reporting guidelines. I asked which guidelines they had used and what they had used them for.\n\n\nThink aloud 1 - home screen\nWebsite designers often ask participant’s to “think aloud” as they complete a task or view a website as a way of exploring participants’ thought processes (e.g. [8]; [9]). Think aloud as a method was first described by cognitive psychologists Ericsson and Simon [10]. Their strict approach viewed verbalizations as “indicators of what information was heeded and in what order, a sort of time stamp of the contents of short-term memory” [11]. As user experience testers adopted the method, they used it more flexibly to additionally capture participants’ thoughts, feelings, and expectations [11]. Whereas cognitive psychologists use the method to understand cognitive processes, usability testers use it to “support the development of usable systems by identifying system deficiencies”. Because “building robust models of human cognition is not a central concern”, Ericsson and Simon’s fixed approach is less appropriate, and testers use a more flexible and pragmatic approach to data collection and interpretation [11].\nAs per best practice [11] I began by explaining the task, and giving instruction to continually verbalize a train of thought. I then demonstrated by sharing my screen, opening up a different website, and “thinking aloud” for a minute. Participants then shared their screen as they explored the home page. Whenever participants stopped talking, I would prompt them openly to continue by asking “What are you thinking?”. I acknowledged participants’ verbalizations with neutral sounds like “uh-hu” and “mmm”, which encourage further talk, but do not show agreement or disgareement. These verbalizations and prompts are also considered best practice [11].\n\n\nSemi-structured interview 2 - home page\nOnce participants had explored the entire home page, I asked participants about any intervention components they had not talked about in the think aloud, about their overall opinions, and whether their understanding had changed since first viewing it.\n\n\nThink aloud 2 - SRQR guideline page\nI asked participants to find the relevant guideline for reporting qualitative research, and then to continue thinking aloud as they explored it. The top of the SRQR page included information about the guideline, such as its scope, and the number of journals endorsing it.\n\n\nSemi-structured interview 3 - SRQR guideline page\nBecause the SRQR guidance is long, I stopped participants from thinking aloud once they reached the guidance itself. I then used semi-structured interview questions to explore any intervention components missed by the think aloud, and to explore participants’ expectations of four key features within in guidance: defined words (signified by a dotted underlines), footnotes (signified by a superscript number), links to discussion boards (signified by an icon), and drop down content (signified by a chevron icon). I pointed to an example of each and asked participants what they expected to happen if they clicked on it.\nThis marked the end of the first interview session. I then explained the plus-minus and writing tasks participants needed to complete before the second session.\n\n\nPlus-Minus task:\nIn their review of methods to solicit text evaluations from readers, de Jong and Schellens [12] distinguish between evaluation goals: selection (whether readers will engage with the text), comprehension, application (being able to apply information in a real world setting), acceptance (including credibility), appreciation, and relevance and completeness. I was not interested in selection (participants had no option other than to engage with the text), and my study scope did not extend to SRQR’s relevence nor completeness. Instead, I was interested in participant’s experience of the design decisions I had taken in presenting the SRQR guideline, including the layout, structure, optional content, definitions, and tone of voice.\nde Jong and Schellens describe methods to target comprehension, acceptance, appreciation in isolation, but because my interest included all three, I chose a nonspecific method, the Plus-Minus task. In this task, readers are asked to annotate a document with plus and minus signs to signify positive and negative reading experiences and then discuss annotations retrospectively.\nI asked participants to select and annotate 2 or 3 reporting items relevant to whatever they happened to writing-up in the time between interviews. I created duplicates of the SRQR guidance page and gave participants unique URLs so they did not see each other’s annotations. I used a web annotation tool called Hypothes.is #REF. Participants could optionally add comments. Participants explained their annotations in the second interview.\nAs de Jong and Schellens note [12], the plus-minus method is advantageous over other nonspecific methods (reading think aloud #REF, and signalled stopping technique #REF) because it collect data without disturbing participants’ natural reading process. Additionally, it was useful in this study as participants could make annotations in their own time, as part of their normal work pattern.\nAlthough the plus-minus task will detect text that participants consider incomprehensible, it cannot detect whether participants comprehend guidance correctly or whether they are able to apply it to their writing. To address this, I used a writing evaluation.\n\n\nWriting Evaluation\nIn the plus-minus task described above, I asked participants to select a few reporting items relevent to what they happened to be writing at the time. For the writing evaluation, I asked participants to send me the paragraphs they had written before our next interview. I read the excerpts and noted reporting items (and sub items) as present or missing.\nIn the second interview, inspired by Davies et al.’s SQUIRE guidelines evaluation [13], I asked participants to identify parts of their writing pertaining to reporting items. When I considered an item (or sub item) to be missing, I asked the participant whether they had reported this information. If they felt they had, I asked them to point out where, and then explored any misinterpretations. If they had not reported information, I asked why.\n\n\nSemi-structured interview 4 - closing thoughts\nTo end the second interview session I asked participants to describe their experience of using the reporting guideline, and to share any final thoughts.\n\n\nMethods explored different intervention components.\nEach method targeted multiple intervention components. For example, in the 5 second test, participants could only see top of the home page. The text, images, and design in this section are there to communicate what reporting guidelines are, when they can be used, and that they will benefit authors. These functions come from three intervention components defined in chapter 9:\n\nDescribe what reporting guidelines are where they are first encountered,\nClarify what tasks (e.g., writing, designing, or appraising research) guidelines and resources are designed for, and\nDescribe personal benefits and benefits to others where reporting guidelines are introduced (home page, on resources, in communications).\n\nComponents that required participants to read text could be best explored in the plus minus-task, and I hoped the writing evaluation would reveal how participants interpreted and applied instruction. I hoped the think aloud would capture opinions on salient features, and the semi structured interviews would allow me to explore remaining, un-noticed, features.\nI did not attempt to explore three intervention components. I did not expect participants to know about or comment on search engine optimization, especially as a large amount of optimization occurs in the website meta data and is thus invisible. Although I built the website so as to allow guideline developers to make incremental updates, I did not expect participants to comment on this either. Finally, because I did not want to edit the meaning of the SRQR guidelines (just its layout), I did not want to add instructions aboutwhat to report when an item was not done, could not be done, or does not apply.\nIn table Table 2 I detail the intervention components I expected each method to explore. The intervention componetns are defined in chapter 9, where I also list the web elements related to each component.\n\n\n\nTable 2: Methods used and the intervention components they explore. Intervention components are defined in chapter 9\n\n\n\n\n\n\nMethod\nIntervention Components (defined in chapter 9)\n\n\n\n\n5 Second Test\n\nDescribe what reporting guidelines are where they are first encountered,\nClarify what tasks (e.g., writing, designing, or appraising research) guidelines and resources are designed for,\nDescribe personal benefits and benefits to others where reporting guidelines are introduced (home page, on resources, in communications),\nInclude design, features, and language to foster trust\n\n\n\nThink Aloud\n\nClarify what tasks (e.g., writing, designing, or appraising research) guidelines and resources are designed for,\nInstruct authors to cite reporting guidelines so readers may learn about them,\nLinks between related guidelines,\nCentralised hosting,\nSearch function on website,\nDescribe the scope of a reporting guideline at the top of every resource,\nUse if-then rules to direct authors to more appropriate and up-to-date guidance when available,\nExplicitly state when no better guidance exists for a use case,\nProvide translations,\nMake guidance appear shorter by removing superfluous information, hiding optional content, splitting long guidelines, using concise language, and separating design advice,\nCater to different kinds of user (readers vs dippers) by structuring guidance with headings, itemisation, hyperlinking to particular sections, and with optional content,\nInclude testimonials from researchers who were nervous about being punished for reporting transparently,\nRemove branding and messaging that may invoke feelings of judgement, complexity, or administrative red-tape,\nReassure that all research has limitations to encourage explanation over perfect design,\nEducate authors about writing as a process,\nlink all resources to each other,\nGather and communicate evidence for benefits,\nInclude design, features, and language to foster trust,\nCreate spaces for authors to discuss reporting guidelines with others,\nUse tone of voice and design to communicate personal benefits; confidence and simplicity,\nInclude testimonials from research users who benefit from complete reporting,\nExplain importance of complete reporting to the scientific community,\nProvide links to other resources that explain how an item can be done,\nStructure guideline items to make them quicker to digest,\nTell authors how long the guidance will take to read,\nProvide advice regarding how to respond if asked to remove reporting guideline content by a colleague, editor, or reviewer,\nReassure when guidelines are just guidelines,\nExplain how the guidance was developed and why it can be trusted\n\n\n\nInterview\n\nDescribe what reporting guidelines are where they are first encountered,\nClarify what tasks (e.g., writing, designing, or appraising research) guidelines and resources are designed for,\nInstruct authors to cite reporting guidelines so readers may learn about them,\nDescribe the scope of a reporting guideline at the top of every resource,\nInclude testimonials from researchers who were nervous about being punished for reporting transparently,\nAddress communications to authors,\nDescribe personal benefits and benefits to others where reporting guidelines are introduced (home page, on resources, in communications),\nCreate spaces for authors to discuss reporting guidelines with others,\nUse tone of voice and design to communicate personal benefits; confidence and simplicity,\nInclude testimonials from research users who benefit from complete reporting,\nDefine key terms,\nFor each item, explain why the information is important and to whom (not just what constitutes “good” design),\nFor each item, provide clear instruction of what needs to be described,\nFor each item, provide examples of reporting in different contexts,\nStructure guideline items to make them quicker to digest,\nTell authors when to use reporting guidelines, or that reporting guidelines are best used as early as possible,\nCreate tools to be used for early writing tasks,\nProvide instruction as to how and where information can be reported without breaching word count limits or making articles bloated.,\nExplain when reporting guidelines do not intended to prescribe structure,\nProvide advice regarding how to respond if asked to remove reporting guideline content by a colleague, editor, or reviewer,\nEncourage explanation even when choices are unusual or not optimal,\nAvoid patronizing language,\nExplain how the guidance was developed and why it can be trusted\n\n\n\n+/- test\n\nDecrease fear of judgement by making reporting guidelines design agnostic,\nUse plain language,\nDefine key terms,\nProvide links to other resources that explain how an item can be done,\nFor each item, provide clear instruction of what needs to be described,\nFor each item, provide examples of reporting in different contexts,\nStructure guideline items to make them quicker to digest\n\n\n\nWriting Evaluation\n\nUse plain language,\nFor each item, provide clear instruction of what needs to be described\n\n\n\nNot Explored\n\nSearch Engine Optimization,\nProvide clear instruction of what needs to be described when an item was not done, could not be done, or does not apply,\nMake it possible for guideline developers to make small edits without having to publish new articles\n\n\n\n\n\n\n\n\n\n\nData processing and analysis\nI recorded video and audio transcriptions using Microsoft Teams. Because automatic audio transcription was not always accurate, I corrected them by rewatching the videos. I de-identified transcripts by replacing names with participant codes, before uploading them to NVivo for coding [14].\nI used my intervention ingredient table (see chapter 9) as a framework to code transcripts line by line. I did this deductively; whenever a participant said anything about a component, I coded text to that component. Because some website features implemented multiple components (for example, an image can both educate and persuade), I sometimes coded text to multiple components. In this way, I created categories of codes, and each category was an intervention component.\nOnce all transcripts were coded, I grouped my categorised codes into deficiencies. If a single component was deficient in multiple ways, I created a code group for each deficiency. If there was disagreement about a deficiency (e.g. some people disliked a component, but others liked it), then I created sub-groups within each deficiency. Although positive feedback did not directly address my objective of identifying deficiencies, I kept these codes because they provided context and counter-evidence to deficiencies.\nSome participants spontaneously suggested modifications. In these instances, I coded the proposed modification and the underlying deficiency. Because some participants spontaneously shared prior experiences using reporting guidelines I coded these using my list of barriers from 7 as a framework. I decided to create new codes for any barriers not previously identified.\nIn this way, I ended up with a list of deficiencies (my primary unit of analysis), and incidental lists of barriers and possible modifications.\n\n\nTrustworthiness\nAs with previous chapters, I used a number of techniques to ensure credibility, transferability, dependability, and confirmability [15]. I describe these in Table 3.\n\n\nTable 3: Techniques I used in this study for establishing trustworthiness. Based on Lincoln and Guba’s Evaluative Criteria [15]\n\n\n\n\n\n\nTechnique\nImplementation\n\n\n\n\nTechniques for establishing transferability\n\n\n\nThick description\nI aspired to report my results with context by indicating when ideas were common or rare, and who they originated from when I felt this was particularly relevant. I reported disagreements, provide quotes, and relationships between ideas.\nInterview sessions were long and used multiple techniques to elicit lots of data. I reported findings alongside relevant context, including participant demographics, and used context to reconcile disagreements.\n\n\nTechniques for establishing confirmability\n\n\n\nAudit trail\nI referred to video recordings when I needed to clarify parts of the transcript. I kept all raw data, and a record of modifications made.\n\n\nReflexivity\nI kept a diary during data collection to note of ideas and my own feelings during. Because I created the website being tested, I felt it was important to reflect on any feedback that made me feel defensive, frustrated, or that I did not understand. In my previous experience, these moments of conflict are important as they often hint at a latent misunderstanding or deficiency.\nReceiving negative feedback does not always hurt me. For example, sometimes I expect negative feedback because it reflects a limitation or trade-off that I already know about. Other times negative feedback can feel like an “aha” moment, as I discover a problem I immediately understand, agree with, and can see a solution to. In contrast, when feedback feels bad, in my experience that is because I have misunderstood something, and my internal model of the situation is off. Although I found it easy to remain professional and neutral within the interviews, having a “thick skin” is not enough. In these moments of friction I made sure to delve deeper into the issue with the same participant or future ones.\n\n\nTechniques for establishing credibility\n\n\n\nNegative case analysis\nI purposefully explored negative feedback that I found unexpected or challenging (see reflexivity). \n\n\nMember-checking\nI invited participants to comment on my synthesised results, asking for feedback on my interpretation and conclusions. Lincoln and Guba argue that member checking is the most important way to the establish validity [15]. \n\n\nPeer debriefing\nCA acted as a disinterested peer throughout design, data collection, analysis and reporting . She questioned my reasoning she helped me become aware of biases, potential flaws, and assumptions I was making.\n\n\n\n\n\n\nEthics\nOxford University’s Medical Sciences Interdivisional Research Ethics Committee deemed this study to be a service evaluation, and so judged ethical approval unnecessary.\n\n\nReporting\nI used SRQR [16] when outlining this chapter, and again to check my reporting during revision."
  },
  {
    "objectID": "chapters/11_pilot/index.html#results",
    "href": "chapters/11_pilot/index.html#results",
    "title": "Refining the intervention: qualitative study with authors",
    "section": "Results",
    "text": "Results\n\nRecruitment\nI recruited participants between 21/03/2023 until 9/08/2023. The number of people expressing interest, eligible, consenting, and participating and shown in Table 4, as are the reasons for drop out. Eleven people participated. Two dropped out before the second interview, without giving a reason. Participants’ characteristics are summarized in Table 5 and included variety in research experience (from 1 to 10+ years), subject area, country of origin, first language, and experience in using reporting guidelines. Six participants had never heard of reporting guidelines before. One had, but did not remember which one they had used. Three participants had used a reporting guideline before, and one had used many reporting guidelines before. The first interview lasted between 45 minutes - 1.5 hours, and the second interview lasted 30-45 minutes.\n\n\n\nTable 4: Recruitment and drop out of participants through different channels\n\n\n\n\n\n\n\n\n\n\n\n\nChannel\nTold about the study\nExpressed Interest\nEligible\nInvited to consent\nConsented\nCompleted first interview\nCompleted second interview\n\n\n\n\nPenelope.ai\nUnknown\n144\n(23 were excluded because they did not want to attend an online interview conducted in English. A further 78 did not describe using qualitative methods when asked to describe their research).\n43\n43\n(30 did not reply)\n13\n(3 could not find a time for interview because of work commitments, 2 did not reply)\n8\n(1 lost to follow up)\n7\n\n\nX\nUnknown\n1\n1\n1\n(1 did not reply)\n0\n0\n0\n\n\nEmail invitation\nUnknown\n(2 emails sent, but were forwarded to an unknown number of recipients)\n4\n3\n3\n3\n3\n(1 lost to follow up)\n2\n\n\nTotal\nUnknown\n149\n47\n47\n16\n11\n9\n\n\n\n\n\n\n\nTable 5: Participant characteristics\n\n\n\n\n\n\n\n\n\n\n\nID\nJob title\nSubject area\nResearch experience (years)\nFirst language\nCountry of origin\nPrevious experience with reporting guidelines\n\n\n\n\n1\nResearch consultancy\nHealth policy\n4\nEnglish\nPhilippines\nNone\n\n\n2\nMedical student\nGeneral qualitative medical research\n1\nEnglish\nGhana\nHad used PRISMA\n\n\n3\nAcademic researcher\nClinical psychology and public health\n7\nSpanish\nEcuador\nHad used COREQ\n\n\n4\nAcademic researcher\nPhysiotherapy\n10+\nEnglish\nUK\nCould not remember\n\n\n5\nAcademic researcher\nMedical ethics\n7\nEnglish\nIndia\nNone\n\n\n6\nMidwifery student\nSexual and reproductive health\n3\nLango\nUganda\nNone\n\n\n7\nAcademic researcher\nEnvironmental Health\n10+\nEnglish\nSouth Africa\nHad used JARS\n\n\n8\nAcademic researcher\nPhysiotherapy\n10+\nEnglish\nAustralia\nHad used many reporting guidelines before\n\n\n9\nPre PhD student\nPublic health\n1\nChichewa\nMalawi\nNone\n\n\n10\nPhD student\nChild development\n7\nChinese\nChina\nNone\n\n\n11\nPhD student\nChild development\n4\nChinese\nChina\nNone\n\n\n\n\n\n\nDesign Iterations\nI had originally planned to finish data collection before making any changes to the website. However, the first five participants consistently mentioned similar deficiencies. After reflecting and discussing with UK EQUATOR staff, we agreed these deficiencies would likely affect many authors and diminish the website’s success, and so we decided to iterate our design.\nBriefly, the changes we made included:\n\nEditing the text at the top of the home page for clarity and to emphasise benefits.\nAdding images to home page to make the page more attractive and to convey meaning of accompany text.\nAddition of publisher logos to foster trust\nReorganisation of the introduction to SRQR to make it appear shorter.\n\nThe results below include quotes and discussion pertaining to these changes.\n\n\nMain findings\nI identified deficiencies. Table Table 6 shows deficiencies, quotes, and codes, for each intervention component. The table also lists seven intervention components that received no mention. Three of these were purposefully not tested, three others were perhaps too subtle, and one was about removing aversive stimulus, so it was good that no participants commented on the presence of ugly or judgemental design.\nI have chosen to describe a few deficiencies in more detail, either because I deemed them important, they were frequently mentioned, or involved interactions between components.\n\nDescribe what reporting guidelines are where they are first encountered\nRelevant website features: Prominent definition on home page and guideline page.\nBarriers addressed: Researchers may not know what reporting guidelines are\nBecause it is important that future website visitor website visitors quickly realise that the site contains resources for writing up research articles, as opposed to designing or appraising studies, I used the 5 second test to explore what participants understood the website to be about upon first impression. This was the first time participants saw the website. Before then, they had no idea what it would be about.\nOn immediate impression, some participants quickly realised the website was about writing, but some did not, or thought it was about methodological guidelines\nAll participants realised the website was about research. Some researchers realised the website was about writing within 5 seconds:\n\n“From what I have seen, I think probably the website should be about, uh, helping you try to discover….identify the guidelines that you will use for writing your study quickly.” (value)\n\n\n“how to go about writing something” (value)\n\nHowever, other participants gleaned only vague understandings like “support for doing research” ((value)) or “guidelines of some sorts, I think in relation to research” ((value)), and one expected the website to be about methodological guidance:\n\n“methodology guidelines one can use or follow when conducting research” (value)\n\nParticipants with previous experience using reporting checklists recognized that the website might be about reporting guidelines:\n\n“Well, I hadn’t seen the the guide that I’m familiar with (that is the COREQ). But I think the other guides also are like COREQ. So that’s what comes to my mind. So I think it’s a [website] where you are going to find all the checklists or the guides for all the […] final stages of the research when we are, like, writing the paper, just to […] double check that everything has been included.” (value)\n\n\n“That’s the EQUATOR guidelines, so that’s… consensus… expert consensus-developed guidelines for the reporting of different research.” (value)\n\n\n“[I read that] you can use reporting guidelines to try and help you [write research articles] more efficiently or quickly. And then I was thinking about, what the heck are reporting guidelines? And then I think it might be stuff like STROBE or like those checklists things or PRISMA, if you’re doing a systematic review or something. And that’s all I got” (value)\n\nGiven a few more seconds to explore the website on their own (during the think aloud task), all participants realised that the guidelines were for writing and others gained more insight into what to expect from reporting guidelines.\n\n“after reading this sentence I think you want to give me a framework, a framework about writing. Is that right?”\n\n\n“I think the guideline would, uh, would clearly state the different sections of the research report or the manuscript and then whatever is required under a section like maybe under methods. Like what are the nitty gritties required under method.” (value)\n\nSome participants found later, longer form descriptions more informative. Referring to content half way down the landing page, one participant said:\n\n“Why couldn’t this be further up? Why can’t that be at the top and then the that stuff here follow? Because then I’d have a better idea of what this is about.[…] I would have liked to have read this at the top and I would have known straight away what this whole website was about.” (value)\n\n\n\nInclude design, features, and language to foster trust\nRelevant website features: Professional design. EQUATOR’s Logo remains prominent. Citation metrics are presented at the top the reporting guidance. Information about who developed the guidelines, how they developed it, and why the guidance is credible is still provided, and easily findable from the top of the guidance.\nBarriers addressed: Researchers may not believe stated benefits\nWhen participants talked about trust, they mentioned whether the website appeared professional, credible, and believable. This intervention component is complex because content and design throughout the entire website influenced judgements regarding trust. In particular, participants wanted to know who made the website and why they could be trusted, and identified some design elements that could look more professional.\nEQUATOR’s introduction could be more prominent\nApart from its logo, EQUATOR is not mentioned at the top of the page. Participants who did know about EQUATOR said that its brand lent credibility:\n\n“and then I picked up the top my top left hand corner with the EQUATOR logo so it seemed from reputable source.” (value)\n\n\n“I already trust the website because I saw that… like this is legit and I see the credentials from EQUATOR network” (value)\n\nParticipants unfamiliar with EQUATOR expressed wanting to know who developed the website. Whilst looking at the top of the home page, one participant said:\n\n“I really don’t get an idea of […] who’s responsible for the website […] would I trust the the developers of the website?” (value)\n\nThe EQUATOR Network is introduced towards the bottom of the home page. Participants recommended moving this introduction (or parts of it) up to the top, using an updated photo, and adding EQUATOR’s affiliations and awards.\n\n“So most places put the the about stuff at the bottom and I would have liked to have seen what [EQUATOR stands for] explained right at the top.” (value)\n\n\n“And then this is the thing on the bottom that I want to look at on every website… because I want to see if they have an actual office. So usually I click this”about us” first” (value)\n\n\n“Participant: Oh, so this is your mission. So here my question is What is this group? What is EQUATOR? So this is my first question because from my background, which is not from medicine, I’m not very familiar with this group. So I want to know what is this and the second feeling I have, I think I might want to see this information at the very first beginning…I want to know who this website is created by? Because I want to use a professional website, so I think the reason I read this paragraph is I want to find some evidence to prove that your group is very professional. Interviewer: Mmm. Participant: Well, maybe you can put [the section about EQUATOR] at the very first beginning and provide a little description about what the group is about, what it is, and what are those people where do they come from something like this.” (value)\n\n\n“I also did some research with my Chinese colleagues and sometimes some prestigious university’s logo will add trust, yeah.” (value)\n\n\n“I think sometimes it is useful […] to put some awards here. Just to make people think you are trustworthy.” (value)\n\nThe site’s design could be more professional\nA design intention was to make the website appear simple. The first iteration was apparently too simple and one participant explained how its simplicity made it less trustworthy:\n“it looks kind of like a blog […] it’s a basic website” (value)\n“I wouldn’t say [it looks] partically trustworthy, but not particularly suspicious either. Kind of in the middle […] something that will be more trustworthy will be something which is more sophisticated because I know, ‘OK, this is someone who actually took his time to do a lot of work… put a lot of work in designing it’. Most of the time if it is a fake website, it’s usually much more simple.” (value)\nOne participant viewed both the first and second iterations of the home page (their second interview session occurred after the iteration), and they described the second iteration as better because “It’s like more trustable […] Scientific. Evidence based. Yeah, of course, legit.” Others decribed the second iteration as “clean” and “uncluttered” (value), “straightforward” and “open” (value), and even as “calming” (value).\nHowever, one participant still questioned the second iteration simplicity and trustworthiness, and drew a comparison with another website that she did trust:\n\n“So I’m saying it’s kind of basic, that the format itself is kind of basic […] [When] I’m looking for information on PUB Med, just the outlet itself gives you the picture that, you know, somehow you can trust it. You know it looks as if there was more work put in it.” (value)\n\nLogos lend credibility and could be more prominent\nParticipants noticed that the first iteration had no logos:\n\n“I don’t know if this is just me, but I kinda want some logos. So I know who will vouch for [the website] right away. Like, usually […] there’s some, like, other medical societies that are, like,”We we are on the EQUATOR network” (value)\n\nInspired by these comments, we added logos to the second iteration home page to show publishers endorsing reporting guidelines. All participants liked these, but some suggested they could appear at the top of the home page so they are immediately visible.\n\n“Leading publishers….Wow, this is good…Nature. Really? Elsevier, BMJ. Yes, this is good. And this brings some sense of trust and authenticity in the website.” (value)\n\n\n“[The publishers’ logos are] encouraging, because these are all publishing houses with mostly reputable journals, probably all reputable journals. […] You know, if these were higher up, then […] that would have made me feel a little bit more like ohh, this is good.” (value)\n\nReporting guideline endorsements and citations lend credibility, but may not be intuitive\nThe top of the SRQR guideline page included widgets displaying the number of journal endorsements, and the number of times the reporting guideline had been cited. Some people commented that this information lent credibility:\n\n“I think it is authentic. It’s robust. If it was endorsed by many journals and developed by experienced researchers, if I use it, maybe I’ll get a better quality work.” (value)\n\n\n“I think citations here might be some people or some people’s work who has cited this page. So this this button […] might show people how many other words use this page.” (value)”\n\n\n“…understand […] that the SRQR guidelines is something that’s already widely used.”\n\n\n“I didn’t pay attention before, but I think I like it (the citation information) […] if it is more cited, I think, like, I will believe it. I will believe it, like, much better, and also like the journal endorsements” (value)\n\nHowever, not everybody understood what these numbers meant.\n\n“Is this [widget] telling me [something], or is it what I am supposed to click on?” (value)\n\n\n“I was a bit confused there, OK” (value)\n\n\n“I think the citation tab over here… What is the relevance of it? I mean, why I’m seeing that?” (value)\n\nOthers were not sure whether the citation information pertained to the website or an underlying article.\n\n“I’m not sure if [it is about], you know, the website or connected paper.” (value)\n\n\n“Has it been cited 4000 times? I don’t understand that.” (value)\n\nNot everybody considered the image at the top of the home page to be trustworthy\nFor the second iteration, I added an image to the top of the home page comprising of three icons to represent the process of writing a manuscript. On participant described this image as a “bit naff […] I actually think this [image] reduces [the website’s] score on the first impressions of trustworthiness kind of thing. Just because [the icons making up the image] are so, umm, ubiquitous, and, uh cheap?” (value)\n\n\nDescribe personal benefits and benefits to others where reporting guidelines are introduced (home page, on resources, in communications)\nRelevant website features: Benefits are prominently and consistently displayed across the home page and guidance pages. Descriptions prioritize personal benefits to the authors above hypothetical benefits to others.\nBarriers addressed: Researchers may not know what benefits to expect\nBenefits are clear, but could be communicated quicker\nWe wanted website visitors to immediately expect the website to benefit them as researchers and authors. The website headline is one of the first things visitors see, and so was an important feature for communicating benefits. In the first iteration the heading was “Research articles, made simple”, but participants thought this was about reading or explaining research articles. In the second iteration, we changed this heading to “Want help writing up research?”.\nAll participants talked about benefits or help. None talked about the opposite (e.g., rules, requirements, or red tape). In the 5 second test, some of these descriptions were vague:\n\n“I see research and writing. So i’m thinking. This is to help me with something with my job.” (value)\n\n\n“so it’s going to assist me in research. It’s going to help me somehow. Make things easier for me.” (value)\n\nUnder the headline, we included a short statement: ‘reporting guidelines help you describe research quickly, confidently in completely’. Two participants did not find this brief text meaningful in the five second test:\n\n“I tried to read the sub headline just below the biggest one and it says help you blah blah confidently and blah blah. […] So I think this information maybe be meaningless for me […] because it sounds like it didn’t provide some concrete information. It’s just a sentence that tried to cheer me up.” (value)\n\n\n“And then there’s something to do with ‘reporting guidelines help you describe research quickly, confidently in completely’. This information is not really telling me much” (value)\n\nHowever, they seemed to understand the reported benefits after reading the top of the home page in more detail, and after vieweing the section below where benefits are stated more clearly (e.g.,). Similarly, other participants began to understand the benefits and link them to their own experiences as authors.\n\n“OK, I think this part is very great because when I when I see something like ”easy writing”, ”smoother publishing” I think ”Ohh, that’s great. That’s what I want.”” (value)\n\n\n“Participant: Now I’m getting a sense of what the website is about. Now looking at the things down here…\nInterviewer: OK.\nParticipant: I’m getting that might be a useful resource. That actually, umm, because these areas I think for… early career researchers like me, I’d say I’d be very interested to come in and see this.” (value)\n\nParticipants seemed to understand how reporting guidelines might make publishing “smoother”.\n\n“it gives me an impression that maybe this website will help me write my work easily and it will also help me increase the chance of my work getting published” - (value)\n\n\n“Umm, just aligning myself to this standard that already many people use. And hopefully, In doing that, I’ll be up to standard and then I won’t stress myself too much later.” (value)\n\nMaking a distinction between benefits to authors and readers may lead to confusion about the intended user\nFurther down the home page, the section title ‘Helping authors and readers’ made one participant expect the website to also host resources for readers.\n“So this is a bit weird. So is the point here that this one is for the writers. And now it’s saying, OK, but we can also help readers. OK, I suppose that’s interesting.” PPT-CW\nThe images depicting benefits could be clearer\nOne participant said the icons describing writing and impact were appropriate (a blank page and an award, respectively), but the image depicting “smoother publishing” was not intuitive.\n\n“looking at that icon, it doesn’t really tell me anything about smoother [publishing].” (value)\n\n\n\nClarify what tasks (e.g., writing, designing, or appraising research) guidelines and resources are designed for\nRelevant website features: Clear instruction and differentiation of resources\nBarriers addressed: Researchers may not know what reporting guidelines are; Researchers may not know when reporting guidelines should be used\nTools for drafting and checking were mostly intuitive, but could be more prominent.\nThe home page included a section describing how templates and checklists could be used to draft and check manuscripts. Participants seemed to find these intuitive and appealing:\n\n“I like that: different stages and different tools” (value)\n\n\n“Yeah, writing templates is something I’ve recently come across, and I think that might be useful. I’ve tried it a bit when writing abstracts. And I guess, yeah, that would be something I’d be interested in looking into further.” (value)\n\nEven though participants could not download templates or checklists, they had expectations of what these resources might look like.\n\n“The checklist could be a pre-populated document that I can go through..it may have a table that I could go through as a tick box exercise ticking which of the [guideline reporting items] my study includes.” (value)\n\n\n“[Regarding templates] I would like to adjust this template by myself. Just like a semi structured interview. I don’t want this template be a structured interview. I want it to be semi structured so I can have the space to adjust it.” (value)\n\nPutting this information further up the page might help visitors “get” what RGs are about\n\n“would actually be very good to appear [higher up the page] because then it would now start opening up one’s understanding as to exactly where this kind of guidelines might be applied.” (value)\n\n\n“If any of these things: writing research, checking manuscripts and planning research, if these can be consolidated on [the top of] your landing page somewhere […] it might might be beneficial because my thought process is that I need to know what I’m doing and only then reporting guidelines can help me, right? So if I know that this website gonna help me with writing the manuscripts, checking […] I think then, reporting guidelines can make a logical progression in that particular case?” (value)\n\nUsing a reporting guideline “for planning” was not intuitive\nThe home page instructed that reporting guidelines can be used when planning research, but will not dictate design decisions. The SRQR page had a button to download a “log book” where researchers could document the decisions and data they would later need to report. However, no participants understood what this log book might be:\n\n“it’s not immediately intuitive what a log book might be” (value)\n\n\n“OK, what’s a log book? Don’t know.” (value)\n\n\n“nothing has been mentioned about the log book overhead. How [am I] gonna use the log book? Maybe you have mentioned about the template checklist, but uh, maybe we can add [something about] the log book.” (value)\n\nThe word “planning” was not intuitive either. One thought this meant planning a manuscript, and so confused the purpose of the log book with that of the template. Another interpreted it as guidance for writing a research proposal.\n\n“When I when I’m reading planning research, I think maybe I have already done this when I design my own outline.” (value)\n\n\n“To write can help you plan a study. Yeah, I guess it’s… I I would have thought this might be useful if you’re writing a research grant or a research proposal.” (value)\n\nI had ordered the tasks and tools as “drafting”, “checking” and then “planning”. I put planning at the end because it is the least conventional way to use a reporting guidelines. Some participants questioned this ordering:\n\n“Why is planning at the end? You have to plan first before you write.” (value)\n\n\n\nFor each item, provide examples of reporting in different contexts\nMany participants said they wanted more, varied examples\nEach reporting item in SRQR comes with one or more examples from published literature. All participants stressed the usefulness of these examples. The attention examples received was notable because I did not ask about them; all comments about examples came spontaneously from the participant in the think aloud and plus minus tasks.\n\n“Oh, and you have examples that could be really helpful, yeah.” (value)\n\n\n“the most important thing I would say is the examples” (value)\n\n\n“I found [this section] very useful as they give more detailed explanations on each specific section, and particularly the examples.” (value)\n\nHowever, many participants said they wanted more examples, and greater variation in style, length, and conciseness.\n\n“need further explanation and examples” (value)\n\n\n“it would would have been useful or helpful for me to to have more than just one example.” (value)\n\n\n“illustrations of how to [report] this information in a concise manner, that would be very helpful as well.” (value)\n\n\n“you may as well put a whole discussion in there, or at least just sort of three or four paragraph discussion” (value)\n\n\n“And here if there could be more examples, because when I […] started to read through and understand the the PRISMA guidelines and use the official explanation file to try to understand exactly what I’m required to write about and the examples particularly helped me a lot.” (value)\n\nMany participants wanted examples from their own field, or even entire publications that have used the guideline:\n\n“So I want something more relatable, [because] when I was reading these examples they were not relevant to my work.” (value)\n\n\n“…if you can list some […] papers who use the SRQR, you can put it here.” (value)\n\n\n“I’ve searched and pubmed to find an example of a research article that’s used these standards so I could copy or check how they’ve laid it out, which subheadings they’ve used.” (value)\n\nCitations may be less credible if they are old or not referenced\nIn the original SRQR publication, all examples are referenced. I had not included references for the examples when putting them onto the website because of time constraints. This bothered one participant.\n\n“Well, there’s no references there, so is that a very good example? No, I don’t know the source of these things. […] You know, I don’t know, where did it come from? Where’s the reference?” (value)\n\nWhen I asked about hypothetically labelling examples as fake if they were made up, the participant said “Yeah, I guess that would be alright”.\nThe same participant also noted that an example was quite old (10 years).\nCitations could be more useful if explained or annotated\nBecause some reporting items contain multiple sub-items, one participant said annotating examples may be helpful. Taking a discussion item about transferability and integrating findings, they suggested “if you could underline or maybe indicate [in the example] that this [sentence] is now how they are trying to say the result can be transferable, umm, this is another [sentence] trying to say how they’re trying to integrate…. something like that.” (value)\n\n\n\nOther findings\n\n\n\n\n\nTable 6: Intervention components, relevant website features, the barriers they address, and deficiencies identified with supportive quotes.\n\n\n\n\n\n\nIntervention component and their relevant features and barriers\nDeficiencies\n\n\n\n\nIntervention component: Include design, features, and language to foster trust\nRelevant website features: Professional design. EQUATOR’s Logo remains prominent. Citation metrics are presented at the top the reporting guidance. Information about who developed the guidelines, how they developed it, and why the guidance is credible is still provided, and easily findable from the top of the guidance.\nBarriers addressed: Researchers may not believe stated benefits\nSee text\n\n\nIntervention component: Describe what reporting guidelines are where they are first encountered\nRelevant website features: Prominent definition on home page and guideline page.\nBarriers addressed: Researchers may not know what reporting guidelines are\nSee text\n\n\nIntervention component: Use tone of voice and design to communicate personal benefits; confidence and simplicity\nRelevant website features: A clean, simple interface for the home page and guidance pages. Text uses phrases like “confidence”, “quick”, “maximum impact”.\nBarriers addressed: Researchers may not believe stated benefits\nSee text\n\n\nIntervention component: Describe personal benefits and benefits to others where reporting guidelines are introduced (home page, on resources, in communications)\nRelevant website features: Benefits are prominently and consistently displayed across the home page and guidance pages. Descriptions prioritize personal benefits to the authors above hypothetical benefits to others.\nBarriers addressed: Researchers may not know what benefits to expect\nSee text\n\n\nIntervention component: Clarify what tasks (e.g., writing, designing, or appraising research) guidelines and resources are designed for\nRelevant website features: Clear instruction and differentiation of resources\nBarriers addressed: Researchers may not know what reporting guidelines are; Researchers may not know when reporting guidelines should be used\nSee text\n\n\nIntervention component: For each item, provide examples of reporting in different contexts\nRelevant website features: SRQR already had some examples. No more examples added\nBarriers addressed: Researchers may not know how to report an item in practice\nSee text\n\n\nIntervention component: Define key terms\nRelevant website features: SRQR now has a glossary, and text is marked-up with definitions that appear upon click.\nBarriers addressed: Researchers may misunderstand\nParticipants wanted more definitions within the guidance and on the rest of the website\nSome complex words in the redesigned SRQR guideline had blue dotted lines underneath. When participants clicked them, a definition would pop up. Participants stressed usefulness of this feature, and everybody liked that the definitions appeared in a popup, and not on a new page.\n\n“I got great help there for me as a writer.” (value)\n\n\n“One feature that I really found to be very useful was where you [described] different kind of kinds of terms there […] When I’m not quite sure about the term there was. If I clicked on it, there was a description and that was very clearly written out and it made it quite easy to use the information that was there.” (value)\n\nParticipants wanted more definitions within the guidance (some struggled with words like “transferability” or “generalizability”) and other areas of the website. For example, when reading about the scope of SRQR and related guidelines, participants struggled to understand terms like “qualitative evidence synthesis”\n\n“A qualitative evidence synthesis. What’s that? Is that like a review?” (value)\n\nWhen reading a list of guidelines for different study types on the home page, one participant explained how not understanding terms like “cohort study” led her to feel anxious and worried.\n\n“Actually, I feel a little anxious because I’m not sure whether it shows that I am not good at something or I am ignorant or something. So when I saw all of these and, like half of them, I didn’t know. I may feel anxious and maybe, like, lose confidence sometimes.” (value)\n\nUser experience might be clearer if definitions were signified by a different colour (not blue)\nBecause I wanted to know whether future users would discover this functionality by themselves, I asked participants what they expected the dotted lines to signify. Most participants guessed correctly.\n\n“Interviewer: Like what do you expect those dotted lines to mean? Participant: Umm, I think dotted lines would mean that it will give me a definition.” (value)\n\n\n“It’s not exactly a hyperlink, but it might give you a definition or something like that” (value)\n\nHowever, a couple of participants thought the lines might signify hyperlinks, and another thought it was a misspelt word because it looked like Microsoft Word’s autocorrect feature.\n\n“I’m honestly not sure whether I see just a highlight or it’s a link. Or what it is Usually links are in blue completely in blue.” (value)\n\n\n“I think uh, there are some misspelled words” (value)\n\n\n\nIntervention component: Cater to different kinds of user (readers vs dippers) by structuring guidance with headings, itemisation, hyperlinking to particular sections, and with optional content\nRelevant website features: SRQR items are structured consistently, making information easier to find. Itemisation is used consistently, content is hyperlinked when useful.\nBarriers addressed: Researchers may expect the costs to outweigh benefits\nOverall, participants welcomed the structure and navigation features, including headings, consistent subheadings for each guideline item, navigation menus, and links to sections.\n\n“I find the guideline more interactive than simply, you know, a PDF document. So I really appreciate that it was more structured and the guide was more easy to follow. And somehow having all these other, umm options, you know that the web offers, yeah […] For me it was very useful for moving around more easily and more quickly” (value)\n\nParticipants liked having extra content as notes, but footnotes might not be optimal implementation\nWhen an SRQR item contained extra context or explanation not relevant to all users, I moved this into a note. I wa not sure of the best way to implement notes.\nI tried footnotes, but participants expected the footnote identifiers (superscript numbers) to be references, and did not like how reading the footnote took them to the bottom of the page, away from where they had been reading:\n\n“I think it’s a citation. So uh linked to a reference. But I think it may also be foot note.” ?var:pilot.participants.PPT-\n\n\n“[Superscript numbers are] always a link to, like, a reference. If not a reference like some…[clicks it]…okay, so it’s telling me things. It’s probably not a reference like in Wikipedia. It shows you it’s a link. This one is telling me more things. So it’s a footnote.” \n\n\n“I worked hard to scroll down. I’m not gonna go back up. So, whatever, i’m not reading [it].” ?var:pilot.participants.PPT-\n\nMenu navigation was useful but could be more prominent and go one level deeper\nThe redesigned SRQR guideline has a side navigation menu so users can easily navigate to the Introduction, Methods, Results, and Discussion guidance sections, or to the FAQs and citation information. A few participants did not notice the menu, or suggested it could be more prominent, and one suggested it would be useful if the menu listed all reporting items.\n\n“when I scrolling I forgot to pay attention to the menu on the side. So […] I suddenly realized it was a menu there […] Maybe make it much more, like, the characters larger or, like, the colour, maybe change it, because now it is so light and I didn’t pay attention to that. But this menu is useful if I pay attention to this.” (value)\n\nEach reporting item has its own menu so users can jump to the item’s justification, examples, or related resources. The same participant described these item menus as a useful way to help “find the detailed part” they desired.\nSection URLs could be more intuitive\nWhen the mouse hovers above a reporting item heading, a button would appear to allow users to a copy the URL to that specific section. One participant did not find this intuitive, and thought the button would take them to more information instead.\nParticipants may desire a summary of the guidance\nIn the think aloud, one participant said they would prefer to browse the checklist above the full guidance because “in my head, like the checklist probably has these things already” (value), and expected the checklist to act as a summary. This expectation may explain why another participant said “I just want to see what this thing is and at the moment I don’t know. Should I read the guidelines or checklist?”, perhaps because they desired an overview (value).\n“So maybe not the full guideline, but I’m not sure how how you can present it. Maybe one paragraph, or, uh, or you could present a mind map. Maybe you can present a mind map and you can put the keywords of this guideline. So if people want to see, for example, […] introduction, literature review, methodology, blah blah. Something like this.” (value)\nParticipants found item titles hard to spot when scrolling\n\n“The names of the sections are, like, black and a little plain. So I think […] the name of each section should be somehow highlighted” (value)\n\n\n“And they don’t really stand out in my opinion, like, just about the layout of the page themself. You see, this [heading] doesn’t stand out so much from the rest, just the font is a little bigger. For example, if I’m scrolling through it very fast, it may be I might miss it. So if those headings or subheadings could be made more visible…” (value)\n\nSome participants did not expect / want all guidance on one page\n\n“I didn’t expect it to be part of the same page because, just looking at the side [menu] here, it makes it a very long page to scroll through. So I guess what I’m a bit more familiar with is, you know, you click on something, it takes you to the guidance and then each of [the items] might be on a separate page […] So it’s less of a scroll and more of a click.” (value)\n\n\n\nIntervention component: Explain how the guidance was developed and why it can be trusted\nRelevant website features: Brief description included on home page and at top of reporting guideline, links to full to development information\nBarriers addressed: Researchers may feel patronized\nThe relationship between the website and publication could be clearer\nThe top of the guideline page references the original SRQR publication. Some participants said this helped them trust the website, but others were confused about the relationship between the publication and the content of the webpage, whether the guidance matched, and which one they should cite.\n\n“I know I have the old information to look for the paper and I could find it. So yeah, it is been inspires trust and it’s amazing to have it here.” (value)\n\n\n“Is this [content on the website] now this article? Because if it is the article then I’m not clear on that […] [After reading through the item titles] Do they match?…Yes, they match.” (value)\n\nThe FAQs helped clear up this relationship, and some participants recommended they be signposted (or briefly explained) at the top of the guidance page.\n\n“[The FAQ section about development] gave me more information about how [the guideline] came about. You know, I was asking [for] that when we first spoke, and here it was. It was nice to be able to see [that] somebody put this together. […] I like that I could get more information about the background. So in a way it’s not really a frequently asked question. It is actually the background.” (value)\n\n\n“I think it’s very fine to put it here [in the FAQs], but I’m just thinking about, like, could we put a link or a summary at the very beginning of the website? Because it may make me trust it much more quickly.” (value)\n\n\n\nIntervention component: Make guidance appear shorter by removing superfluous information, hiding optional content, splitting long guidelines, using concise language, and separating design advice\nRelevant website features: SRQR has been edited. The only text presented immediately is instruction on what the author needs to describe. Additional information is hidden at first and can be expanded. Text is shortened through editing and by using active voice. In the case of SRQR, this reduced the text length by 60%.\nBarriers addressed: Researchers may expect the costs to outweigh benefits\nOn first impression, the guidance may appear too long\nParticipants spoke positively of the various features to hide optional content (like collapsible sections and pop-ups). Nobody expressed wanting all information up front, and all preferred to have it only when needed.\n\n“If you want to, you can extend and read through [the extra information]. I think that’s very useful.” (value)\n\n\n“but if you don’t need [the extra information], uh, you can’t see it. But if you need it, you may click on it and just show it very directly.” (value)\n\nHowever, some participants still felt the guidance appeared very long, and this could be off-putting upon first impression.\n\n“So I kind of know how long this page is based on seeing [the scroll bar] move. I don’t want to read the whole thing […] I don’t want to spend 17 min on that” (value)\n\nIn the first iteration, the poage was even longer because of content in the introduction before the guidance began (e.g. information about the guideline’s scope, development, or how to use it).\n\n“There’s so much going on [at the top of the page, before the guidance]. […] Yo, this is getting annoying. All I want is the guideline or the document” (value)\n\nIn the second iteration, I moved some of this introduction information into a collapsible box, but some participants still described the page as very long.\n\n\nIntervention component: Create spaces for authors to discuss reporting guidelines with others\nRelevant website features: Each reporting item has its own discussion board.\nBarriers addressed: Researchers may not believe stated benefits; Researchers may misunderstand; Researchers may not know how to report an item in practice; reporting guidelines can become outdated; Researchers may feel patronized\nEach reporting item had a link to its own discussion page. Participants welcomed the opportunity to post comments, ask questions, and suggest edits on this page.\n\n“So you do not want to just show people how to do it, but you want to provide a platform for the readers to discuss and to create their own ideas. I think that’s great.” (value)\n\n\n“Maybe what helps is if you can, say, share your thoughts here, […] you feel a little bit more [that] I’ve got a voice instead of I’m just being told that it’s a great place to be and it’s got all the answers.” (value)\n\n\n“Maybe if I identify something that I know is not useful for me. Yeah, in that case, maybe I would write something, you know, to other researchers to read it and maybe give feedback about it, but also for developers of the guideline.” (value)\n\nThe button could be clearer\nThe link to the discussion page was a button with just an icon of a speech bubble, no text, and not all participants knew what to expect this button to do or where it might take them. One participant said this button would be easier to understand if it used text, and recommended it be placed at the end of the expandable content for each item.\n\n“Maybe the blog? No, I don’t know.” (value)\n\n\n“I thought it might be, uh, like a frequently asked questions or a or a further explanation” \n\n\n“I think you can just put [a button] after all the [extra] content you have seen [in the expandable box], because if if people haven’t seen this content they will have nothing [to discuss].” (value)\n\nThe discussion page could be easier to use\nThe discussion page requires users to sign in before they can post. This deterred some participants.\n\n“Ohh I see I need to sign in. Ah, usually that is something that discourages me immediately.” ?var:pilot.participants.PPT-\n\n\n“I would comment, if I didn’t have to log in” (value)\n\nAdditionally, one participant did not understand the discussion page’s purpose.\n\n“Am I doing this to myself? Is it comments about my research that I’m completing the guidance for or is it for the website for other people to see these comments that I’ve made perhaps?” (value)\n\n\n\nIntervention component: Address communications to authors\nRelevant website features: All resources and website copy are directed predominantly at authors.\nBarriers addressed: Researchers may feel that checking reporting is someone else’s job.\nParticipants expected the target audience was researchers, but the target could be more specific\nEveryone recognised the website was for researchers, as opposed to editors. Most realised it was for health researchers.\n\n“It can be useful for academics. It can be useful for people who do research on daily basis or as a regular part of their job” (value)\n\nSome participants specified the target might be early career researchers, but not absolute beginners nor younger students.\n\n“those who are just getting started in the career journey in research….[pause]…definitely students?…umm…. people who don’t really have that much experience in writing.” (value)\n\n\n“Maybe like master research or higher degrees and also for some junior scholars” (value)\n\n\n“But I think your your target audience is completely different than the very basic student…because it’s more for the expert researchers, right… for the expert researchers” (value)\n\nSome participants hinted that seeing terms specific to their field signalled that the website was for them:\n\n“If I know any of these terms, then clearly I’m gonna make use of it” (value)\n\n\nOhh then maybe if it sounded like “Want help writing up Qualitative research or something like that, so that I know that this is actually addressing me. It’s relevant to me. Yeah, but [the way it is written now] way is kind of open. So it’s not kind of, it’s not relating to me. (value)\n\n\n\nIntervention component: Structure guideline items to make them quicker to digest\nRelevant website features: Items have consistent structure and use bullet points consistently\nBarriers addressed: Researchers have limited time\nParticipants welcomed item’s was structure and said they preferred it to unstructured text.\n\n“It was more structured and and the guide was more easy to follow.” (value)\n\nOne participant questioned whether they might prefer full sentences over bullet points. They used SRQR’s item 12 as an example, where the text says:\n\n“If the actual sample differs from the target sample, describe:\n\nthe difference,\nwhy these differences may have occurred,\nhow this might affect the findings.”\n\n\nThe participant suggested changing this to “Why the difference has occurred and how it might affect the findings. So one line and you’re done with it”, instead of taking up four lines of text. (value)\n\n\nIntervention component: Describe the scope of a reporting guideline at the top of every resource\nRelevant website features: The intended scope of a guideline is clearly & prominently described. This definition includes contexts in which the guidance should not be used.\nBarriers addressed: Researchers may not know whether a reporting guideline applies to them\nNot all participants found the scope clear\nOne participant was not clear whether SRQR applied to survey studies, and said they wanted a definition of what qualitative research meant. Another participant wondered why the guidance discussed patient outcomes when the scope had not specified health research.\n\n\nIntervention component: Search function on website\nRelevant website features: Search function is easier to find as a recognizable icon in the navigation bar of every page. The home page includes additional ways to access search functionality.\nBarriers addressed: Guidance may be difficult to find\nThe website featured search buttons (one in the navigation menu, one at the top of the landing page), but the search buttons did not work, so participants could not explore the search functionality. However, many participants commented on the search buttons and instinctively knew what they were.\n\n\nIntervention component: Include testimonials from research users who benefit from complete reporting\nRelevant website features: SRQR includes dummy testimonials and quotes from research users\nBarriers addressed: Researchers may not care about the benefits of using a reporting guideline\nQuotes helped participants believe items were important, but some participants questioned their credibility\nThe redesigned SRQR guidance featured quotes in the margin of some items from people that use research, like evidence synthesisers, editors, or other researchers. These quotes were made up, as I didn’t have time to collect real ones just for testing. Some participants said they liked these quotes:\n\n“I think it makes this….how can I say?…practical from a different point of view. Like why exactly you need this [reporting item]. So now this person [in the quote] is telling from her own perspective how useful it is that you have [the item] described clearly, so it makes it such that if I’m trying to describe [this item], I’ll try to keep that in mind that.” (value)\n\n\n“I like it because each of them tells me the why…umm…you know, kind of gives a plain language reason for […] why it’s a useful thing. […] That’s gives it, you know, humanity.” (value)\n\n\n“This is encouraging because it’s a feedback from a person working at the journal with responsibility for publishing. The editor.” (value)\n\nHowever, some participants questioned whether these quotes were from real people.\n\n“Maybe they’re real…maybe it’s legit” (value)\n\nQuotes may not be valuable enough to be prominent\nSome participants didn’t find the quotes useful, or described them as distracting.\n\n“I don’t care what people think.” (value)\n\n\n“I didn’t really pay much attention to it. I saw a few quotes and read through a few quotes, but I was like, OK, so what does that add to me?” (value)\n\n\n“sometimes when I have to read [something complicated], but I don’t want to, [distraction] may be a big problem for me because, like, these comments are what people say, so it’s, like, more easy to read than the [reporting item]. So it may attract a lot of attention from me” (value)\n\n\n\nIntervention component: Include testimonials from researchers who were nervous about being punished for reporting transparently\nRelevant website features: Quotes included alongside guideline\nBarriers addressed: Researchers may expect the costs to outweigh benefits\nSome of the fictitious quotes contained reassurance from researchers who had felt nervous when using a reporting guidelines becuase they felt unsure about being so transparent about parts of their work they knew were not perfect. There were only a couple of quotes of this kind, and few authors noticed or commented on them.\n\n“That makes it relatable to a user, particularly a new user, because we can see that all of these people are, you know, they were first time users once.” \n\n\n\nIntervention component: Instruct authors to cite reporting guidelines so readers may learn about them\nRelevant website features: Consistent instruction to cite reporting guidelines\nBarriers addressed: Researchers may not know what reporting guidelines exist\nThe title How to cite was misleading to one participant\nParticipants expected citation instructions and were not surprised to see them in a section called How to cite. However, one participant questioned whether the section would tell them how to cite the guidance, or how to cite resources in general.\n\n“I want to see, like, if it says right away, a certain citation style like, are you talking about Chicago?” (value)\n\nSome participants did not know whether to cite the website or the publication\nOne participant said they had sought the original paper because they “wanted to add the citation” ((value)) to their article. Another asked “Do you reference the EQUATOR network or do you reference the original article?” ((value))\nSome participants recommended the citation instruction be more prominent\n\n“I think you can put citation in a in a box in a [coloured] box to make people notice it.” (value)\n\n\n“It is helpful of course, umm, but I think it got lost in the page.” (value)\n\n\n\nIntervention component: Explain when reporting guidelines do not intended to prescribe structure\nRelevant website features: Explained at top of guidance\nBarriers addressed: Researchers may struggle to reconcile multiple sets of guidance\nSome participants included sub headings for each reporting item in their writing sample\nMany participants noticed and welcomed the instruction that SRQR does not prescribe structure. Nobody objected to it.\n\n“I think this is very clear. If you say that did not prescribe order nor structure. […] Yes, that’s great. […] I like it very much because I think it helps me understand that the guideline is not a standardized one, but it depends on the user. To use this kind guide in their own context. So I think this one this sentence is is very important.” (value)\n\nHowever, in the writing task, two participants used subheadings for the items they wrote. When asked about these subheadings in the second interview, both said they intended to remove at least of the subheadings. One participant replied “One of our reviewers wanted more structure, so of course we leave some [subheadings] in, but not all” (value). Another said “I’ll probably not include them [in my final article], but the information, that’s what I’m I’m going to maintain, yeah.” (value)\n\n\nIntervention component: Provide links to other resources that explain how an item can be done\nRelevant website features: Links included when relevant.\nBarriers addressed: Researchers may not know how to do an item\nParticipants mentioned needs that are not addressed by the current links\nMost participants voiced support for links to helpful resources. None were against them.\n\n“I wasn’t really expecting this here. But then it’s useful. These two [links] seem useful.” \n\nHowever, participants wanted links to other resources, including resources to help them with “flow charts” (value), “writ[ing] in a concise manner” (value), “sample size calculations” (value), and more item-specific “training” (value), possibly including “videos” (value).\n\n\nIntervention component: Links between related guidelines\nRelevant website features: Guidelines prominently link to other relevant guidelines and explain when they should be used.\nBarriers addressed: Researchers may not know what reporting guidelines exist\nThe top of the guideline page included information on SRQR’s scope and links to other, related guidelines. Participants said they liked these links. Most found the explanations clear, including the instructions of when not to use SRQR.\n\n“This part, when it should do not use, I think it is much useful for me to understand whether I use the right guideline or not. Yeah. […] I need this judgment.” (value)\n\n\n“[Reading out loud] Do not use it for writing a qualitative evidence synthesis. use this instead. OK, that could be useful.” (value)\n\nNot all participants understood the term ‘related guidelines’ or differences between similar reporting guidelines\n\n“why would you put related if if you’re saying use this one, but then you could also use this one… I think that really starts to get confusing.” (value)\n\n\n“Initially it seemed confusing which guidelines to use when doing mixed methods research” \n\n\n\nIntervention component: Educate authors about writing as a process\nRelevant website features: Some SRQR items now link to relevant EQUATOR materials and courses.\nBarriers addressed: Researchers may not consider writing as reporting\nThe website did not sufficiently address participants’ need for writing training\nThe top of the SRQR guideline page had some very short instruction about how to apply the reporting guideline whilst writing but for many participants, this was not enough. Many participants expressed desire for training on how to write an article (as opposed to what to write).\nOne participant wanted training on how to “write in a concise manner” (value)\nAnother talked about their struggle to apply training and guidance to their own writing process.\n\n“When I try to look at your guidance on your website, I really want to use it in my own writing, but it is very strange because when I try to, uh, connect the information on the website with my own writing, I found there there might be a great gap because I think everything on your website is very clear (actually they are very specific, those suggestions), but when I try to connect those information with my own writing, I found it just a little bit difficult to to generate some some specific ideas to start my writing.\n\nSo I’m thinking if that’s because the problem of my writing is not the lack of specific guidance but some other thing like my motivation or, I don’t know…it’s just a little bit strange.\nAnd I also talk about this with my friends because lots of my friends, they are also PhD students and they are struggling at writing too. So ask them if they have some guidance, uh, if they have looked at some guidance and if [they] have put those guidelines in [their] own writing and and their answers were, like, quite similar with me and and they all talk about that, “yes, we we look at lots of guidance we try to look at lots of those writing books to teach you how to write, to teach you how to structure your writing.” But it’s still very hard, and when you, when you really, when you really sit down and start writing, actually you couldn’t, uh, call up [the information].” (value)\n\n\nIntervention component: For each item, explain why the information is important and to whom (not just what constitutes “good” design)\nRelevant website features: Information added when necessary\nBarriers addressed: Researchers may not know why items are important\nThe labels Justification and Why readers need this information were ambiguous to some participants\nEach reporting item included a section within its expandable content called Why readers need this information alongside examples of writing and links to other resources. Participants found the information useful but were less enthusiastic than they were for the examples.\n\n“So yeah, this this section, justification, examples and resources. I found them very useful as they give more detailed explanations on each specific section, and particularly the examples” (value)\n\nHowever, a few readers interpreted the title of this subsection differently.\n\n“I thought it might give you an example of how to justify [the choices you made in your study]” ?var:pilot.participants.PPT-\n\n\n“I was asking myself: which readers?…. the one like me who is using the platform or the readers of my paper?” (value)\n\nNot all participants found the justification compelling\nWhereas many participants spontaneously said how useful examples were, none said the same about the justification section.\n\n“[It was] one of those things that you just read and go, like, ohh OK. But not, like, something which I requested or you sit down and think through.” (value)\nInterviewer: Maybe this section could do a better job of reminding authors who is going to be reading their research, and how different those people might be. Or their different perspectives or experiences. Do you think if it had done that you would have found it a bit more convincing and motivating?\nParticipant: Umm. Definitely, definitely. From your explanation, I think it kind of puts it in context, [as to] why this section is more important. So I didn’t read it at first, but I think it did not come up very clearly as compared to how you’ve explained it. And so, after hearing that explanation, I’d say that this the way the readers need information.” (value)\n\n\n\nIntervention component: For each item, provide clear instruction of what needs to be described\nRelevant website features: Writing instruction occurs first for each item.\nBarriers addressed: Researchers may not know how to report an item in practice\nOne participant felt the instructions could be a little longer and less ambiguous\nEach reporting item began with instructions of what to write. Participants welcomed these instructions, as articulated by one participant when describing how they used the guideline for the writing task, “I used it specially for understanding what kind of information, you know, apart from the obvious ones, every section wants me to to write, so it it was really helpful in that respect” (value).\nHowever, one participant suggested the information could be a little longer: “I guess just a few more words to kind of, articulate [the instruction] better or give it a bit more detail [because it may be ambiguous to] somebody less experienced” (value).\n\n\nIntervention component: Gather and communicate evidence for benefits\nRelevant website features: Dummy quotes provides evidence for experienced benefits.\nBarriers addressed: Researchers may not believe stated benefits\nParticipants questioned whether quotes were credible\nThe website included (fake) quotes from authors exalting the benefit reporting guidelines have brought to their job. However, some participants worried these quotes were bias, fake, or vague.\n\n“If you’re using quotes on a website, my first thought is you could be biasing, you know, how useful the website is, because you’ve picked out great quotes so, you know, until I’ve looked at this and used it myself, I might not agree” (value)\n\n\n“Maybe you can put some concrete stories, because for me I like to read stories instead of just reading some uh, broad words like I like it. Yeah.” - (value)\n\n\n“Yes, I think it’s much more useful [to] have some timelines to show when people say [things], yeah, because if I could see the time, I may also check like how long the company or the website may [have] last[ed] for […], how many people use that and how they are feeling like from one year ago or two years ago to now.” (value)\n\n\n\nIntervention component: Provide advice regarding how to respond if asked to remove reporting guideline content by a colleague, editor, or reviewer\nRelevant website features: Advice given in FAQ\nBarriers addressed: Researchers may be asked to remove reporting guideline content\nMany participants did not notice the advice\nThe FAQ section included some advice on what to do if a colleague or editor asked you to remove content from your manuscript pertaining to one or more SRQR items. Only one participant noticed this advice. When talking about what they would do if asked to remove content by “a reviewer or, even a co-author, it was amazing for me that you give these tips, you know, to what to do in those cases and what you can do to, umm, make it or to highlight the importance of have each section within your paper.” (value)\nAnother participant, who had not noticed the advice, nevertheless said they would return to the website should they be asked to remove content:\n\n“I think first I will ask his or her reason why he or she want me to delete that if I think, I don’t agree with that. I may try to make some formal explanation to explain that and also I may come back to this website to double check some reliable things which can support my view to let them believe that I should also I I need this paragraph.” (value)\n\n\n\nIntervention component: Use plain language\nRelevant website features: SRQR is edited to use plainer language.\nBarriers addressed: Researchers may misunderstand\nParticipants did not understand all words\nI had tried to use plain language on the home page and introduction to SRQR, but participants still questioned the meaning of some words (e.g. “synthesis”), including words commonly used by EQUATOR staff (e.g. “transparency”). Even the term “reporting guidelines” confused some participants, especially when used at the start of a sentence, where some participants interpreted the word “reporting” to be a verb, instead of part of a compound noun.\n\n\nIntervention component: link all resources to each other\nRelevant website features: Guidance links to all tools and development article\nBarriers addressed: Researchers may not know what resources exist for a reporting guideline\nParticipants noticed links to resources, such as the buttons to download checklists and templates.\n\n\nIntervention component: Reassure that all research has limitations to encourage explanation over perfect design\nRelevant website features: This reassurance appears on the home page and all guidance pages\nBarriers addressed: Researchers may expect the costs to outweigh benefits; Researchers may feel afraid to report transparently\nTwo participants spoke about content that reassured authors to be honest about limitations. After noticing a reassuring quote from an editor, one participant agreed, saying:\n\n“sometimes when we don’t report limitations and the reviewer identifies the limitations [instead] then then we lose credibility. So it’s better we report [limitations] so that the the reviewers say ohh this person has acknowledged this limitation, then then this is a good study. So I just liked it.” (value)\n\nAnother voiced support for the SRQR reporting item about limitations, saying “no study is ever done perfectly or done in a way that the next person would do it. And so we’ve become really, really good, I think, in my team, at making quite long limitation sections to try and avoid peer reviewers from finding everything that’s wrong”. (value)\n\n\nIntervention component: Tell authors when to use reporting guidelines, or that reporting guidelines are best used as early as possible\nRelevant website features: Stated prominently\nBarriers addressed: Researchers may not know when reporting guidelines should be used\nUpon realising reporting guidelines are for writing articles, all participants naturally thought about using them in the drafting/writing process (as opposed to retrospectively check manuscripts that are already written). Sometimes this was immediate:\n\n“From what I have seen [after the 5 second test], I think probably the website should be about, uh, helping you try to discover… identify the guidelines that you will use for writing your study quickly.” (value)\n\nFor some participants, using the guidance for the writing task reaffirmed their opinion that the guidelines would help them in early stages of writing.\n\n“I saw it as very important for me at this stage that I’m at. I’m writing a manuscript. But I’d say that it can be usable at any level, only that I found it very important at my level, which is at my paper writing stage.” \n\nNo participants talked about consulting reporting guidelines when planning a study\nPerhaps because participants did not intuitively understand how they could use a reporting guideline to plan research, none talked about using them in that way.\n\n\nIntervention component: Use consistent terms\nRelevant website features: SRQR uses consistent terms across items.\nBarriers addressed: Researchers may misunderstand\nParticipants were confused when different words referred to the same thing\nTwo participants questioned why the home page referred to “guidelines” and “reporting guidelines” and asked whether there was a difference. Another asked whether “guidance” and “guideline” were the same. Another participant noticed that an example used a word differently to the SRQR guideline.\n\n\nIntervention component: Avoid patronizing language\nRelevant website features: Continue to avoid using patronizing language\nBarriers addressed: Researchers may feel patronized\nNo participants mentioned feeling patronized.\nWhen asked, one participant described the tone as “suitable” and said “simple comments” and “explaining like this, [is not patronizing] because I also need a lot to learn.” (value)\n\n\nIntervention component: Centralised hosting\nRelevant website features: A core set of frequently accessed guidelines are now presented on a single website.\nBarriers addressed: Guidance may be difficult to find\nThat the website contains many different reporting guidelines was rarely mentioned, but one participant said “to have a repository where all those things are, instead of having to go and search for them. That makes sense.” (value)\n\n\nIntervention component: Provide translations\nRelevant website features: Translations are prominently listed above the guidance\nBarriers addressed: Researchers may not understand the language; Researchers may misunderstand\nOne participant noticed that their language was missing\n\n“So what about, you know, all the researchers that speak Spanish?” (value)\n\n\n\nIntervention component: Tell authors how long the guidance will take to read\nRelevant website features: Estimated reading time given\nBarriers addressed: Researchers have limited time\nNot all participants liked being told how long the guidance would take to read\nThe guidance advises readers that it may take 16 minutes to read. One participant liked this: “Oh, I know how it’s going to take. It’s helpful when you are performing research you have like tight deadlines and not much time.” (value)\nBut another felt like 16 minutes was too long, especially if you only found the guideline as part of manuscript submission “Uh, it will be frustrating. Now again, you waste a lot of time.” (value).\nAnd a third worried they would feel bad if the guidance took them longer than the stated time:\n\n“I think this is, like, useful, but I’m not sure whether some… when some people read it, they may feel stressful because, from my teaching experience before, some of my students may say to me if, like, there is guidance saying that you may need, like, 60 minute to read it, sometimes, if they take longer, they may feel confused, or lose a little bit [of] confidence [and worry that] they read it so slow or they are not, like, normal one. I think this [estimated time] may be a good [information] because, like, nowadays I found, like, many websites use it. But actually, for me sometimes this part is useful, but sometimes it is not.” (value)\n\n\n\nIntervention component: Encourage explanation even when choices are unusual or not optimal\nRelevant website features: Added to items\nBarriers addressed: Researchers may feel afraid to report transparently\nOnly one participant noticed this instruction\nSome reporting items in SRQR ask authors to explain their reasoning behind design choices. Only one participant noticed one of these sentences. They reflected that researchers often “follow this path of only mentioning [what we did] but not explaining how we did it and why it was important to apply this strategy” ((value)).\n\n\nIntervention component: Explicitly state when no better guidance exists for a use case\nRelevant website features: Reporting guidelines warn authors when no better guidance exists for a use case, and how the current guidance can be adapted instead\nBarriers addressed: Researchers may not know what reporting guideline is their best fit\nOnly one participant commented on this instruction\nIn the introduction to SRQR, where its scope is explained, the instruction mentions that there are no better reporting guidelines for writing protocols for qualitative research, and instead recommends authors use certain items from SRQR. The participant annotated this explanation and said they liked it ((value))\n\n\nIntervention component: Provide instruction as to how and where information can be reported without breaching word count limits or making articles bloated.\nRelevant website features: Added instruction at top of reporting guideline and in some items where most useful\nBarriers addressed: Researchers may struggle to keep writing concise\nOnly one participant commented on instructions of where content can be reported\nVery little of the guideline text deals with alternative reporting locations, and so it was not surprising that only one participant noticed it and said “I like the reminder” (value).\n\n\nIntervention component: Reassure when guidelines are just guidelines\nRelevant website features: Prominently displayed at top of reporting guideline\nBarriers addressed: Researchers may feel restricted if reporting guidelines prescribe design\nOnly one participant drew the distinction that reporting guidelines are not rules\nThroughout the homepage and SRQR page I had tried to convey that reporting guidelines are recommendations, and I took care not to use words like rules or standards. Only one participant commented explicitly about this, but no participants talked about the guideline as if it were a set of rules.\n\n“So I think this explanation here is very clear that it helps searches to know that definitely they can have their own ideas and this guideline is it is kind of like a a supporting one but not the but not a rule, not a standardized rule.” (value)\n\n\n\nIntervention component: Ensure all resources and tools (e.g., checklists and templates) are in ready-to-use formats\nRelevant website features: No changes made\nBarriers addressed: Researchers have limited time; reporting guideline resources may not be in usable formats\nEven though the links to the checklist and template were not live, participants expected the resources to be in ready-to-use formats.\n\n\nIntervention component: Communicate why reporting is primarily the responsibility of the author\nRelevant website features: Clear explanation of why guidelines and tools should be used by authors primarily, although can also be used by others.\nBarriers addressed: Researchers may feel that checking reporting is someone else’s job.\nNo participants commented on this.\n\n\nIntervention component: Decrease fear of judgement by making reporting guidelines design agnostic\nRelevant website features: SRQR explicitly states that it makes no assumptions about design. Inadvertent design assumptions were edited.\nBarriers addressed: Researchers may expect the costs to outweigh benefits; Researchers may feel afraid to report transparently; Researchers may feel restricted if reporting guidelines prescribe design\nNo participants commented on this.\n\n\nIntervention component: Explain importance of complete reporting to the scientific community\nRelevant website features: Continue to do this\nBarriers addressed: Researchers may not know why items are important\nThe website explains the societal and community importance of complete reporting in a few places: the justification subsections of each item, quotes in the margin, and briefly on the home page.\nAlthough participants commented on the quotes from research consumers, and on the Justification sections within each reporting item, nobody talked about why complete reporting to the scientific community in general. One participant seemed confused by the benefits to readers section on the home page:\n\n“So this is a bit weird. So is the point here that this one is for the writers. And now it’s saying, OK, but we can also help readers.OK, I suppose that’s interesting” (value)\n\n\n\nIntervention component: Make it possible for guideline developers to make small edits without having to publish new articles\nRelevant website features: Developers can make small updates any time\nBarriers addressed: reporting guidelines can become outdated\nThis component was not tested and no participants commented on this.\n\n\nIntervention component: Provide clear instruction of what needs to be described when an item was not done, could not be done, or does not apply\nRelevant website features: Instructed where relevant\nBarriers addressed: Researchers may not know what to write when they cannot report an item\nThis component was not tested and no participants commented on this.\n\n\nIntervention component: Remove branding and messaging that may invoke feelings of judgement, complexity, or administrative red- tape\nRelevant website features: A clean, simple interface for the home page and guidance pages. Text makes less use of to judgemental phrases and fewer references to the negative consequences of poor reporting.\nBarriers addressed: Researchers may expect the costs to outweigh benefits\nNo participants described the design as unpleasant or judgemental.\n\n\nIntervention component: Search Engine Optimization\nRelevant website features: The site has additional meta-data. Each reporting guideline page has its own meta-data. The site is optimized for mobiles.\nBarriers addressed: Guidance may be difficult to find; Researchers may not encounter reporting guidelines early enough to act on them\nThis component was not tested and no participants commented on this.\n\n\nIntervention component: Use if- then rules to direct authors to more appropriate and up-to-date guidance when available\nRelevant website features: Reporting guidelines clearly and consistently point authors to more appropriate guidance when appropriate, using if-then rules. These links can be updated any time.\nBarriers addressed: Researchers may not know what reporting guideline is their best fit\nAlthough participants commented on the links to related guidelines, they did not comment on the “if…then…” structure of these links.\n\n\n\n\n\n\n\n\nBarriers\nParticipants naturally discussed barriers and facilitators they encountered when applying guidance, either during this study or in their previous experience. These factors were external to the website being tested, and I had identified many of them in my previous work (chapters . Participants mentioned:\n\nNot having known what reporting guidelines were earlier in their career\nFeeling frustrated when asked to use a reporting guideline or checklist when they have already finalised their manuscript and are submitting to a journal.\n(Previously) finding the checklist, but not the full guidance\nBeing limited by journal requirements and word limits\nStruggling to keep writing concise and fluid\nNeeding more guidance\nBeing unable to report an item because it is their colleague’s responsibility, or because they hadn’t done what was being asked when designing their study or collecting data.\nPaywalls\nUsing reporting guidelines for teaching students\nWanting guidance for funding applications\nFunders enforcing reporting guidelines\n\nParticipants also mentioned barriers I had not identified previously. These included:\n\nWhen guideline author names appear Western, some (non western) participants expected the guidance to be less relevant to them.\nDescribing a guideline as “version 1.0” might make people feel like the guidance is (too) new, and therefore less trustable.\nThe loading speed of websites (thankfully, this was not an issue for the website being tested)\nNot understanding reviewer feedback\nNot wanting to read on a screen\nNot understanding the relationship between the EQUATOR Network and the guidelines or guideline developers.\n\n\n\nComparisons between the website being tested and the old EQUATOR website & guideline publications.\nA few participants ended up exploring the original EQUATOR website and the original SRQR publication during their interviews. Participant’s drove these unplanned explorations and comparisons. One wanted to retrace their steps to show me the guideline they had used previously. Two others wanted to continue using reporting guidelines in the future and asked me where the original SRQR guidance could be found. Some others spontaneously reflected on their previous experience.\nRecounting their experience of seeing the original EQUATOR website for the first time between interview sessions, one participant (value) said “Ohh no I didn’t like it. The [new] one is much, much better” because it looked more “trustworthy, more organised” and they preferred the font and colours. Another participant described the original website as “boring”, “outdated” and “text heavy” before recounting their experience of using it:\n\n“Not that long ago I went on to the site because I was looking to complete a reporting checklist and it seemed clear to find the checklist that I wanted. But when I went through the checklist, it wasn’t appropriate. And then I just ended up feeling a bit unsure about what it is, which was the best one to go for.” (value)\n\nI witnessed another participant (value) experience similar confusion. They wanted to find the original SRQR guidance to continue using it after the study finished. Sharing their screen and thinking aloud, they started on the EQUATOR Network home page and tried to find the original SRQR guidance without my help. Although at first they thought EQUATOR’s home page looked “full” and “rich”, they were quickly “confused” by both EQUATOR’s website and the SRQR publication. After eight minutes and giving up three times, they eventually found the checklist but not the supplement containing the full guidance.\nA second participant (value) achieved the same outcome a few minutes faster. Because examples only appear in the full guidance (which they hadn’t found) they instead looked through “the reference list to see if there was potentially an example paper” and then planned to “go back to PubMed and search for an article that used these guidelines”. The participant appeared to have little interest in the article’s text, saying they did not “care what [the guideline developers] did to come up with it”.\nAnother participant (value) echoed this opinion when comparing the original SRQR publication with the redesigned version. They said they “don’t need” to know how SRQR was made when they are trying to use it, and they felt the redesigned guidance is “a bit more precise and to the point”. When I showed them the original SRQR full guidance (the supplement), they said:\n\n“Participant: That’s too heavy on the content.\nInterviewer: So if if the option was between this this version that you’re looking at now [the supplement] and the the website that you saw first, which do you think you prefer to use?\nParticipant: I think [the website] is far better than the [supplement]. Yeah, this one [the website] is far better.”\n\nAnother participant reflected on their previous experience of using the PRISMA guidelines and explanation document. They said:\n\n“I rather prefer this form [the website] of guidance than the other one. There can be a lot more information that are being presented in this way. […] That’s better because it it’s more (how can I say?) well presented, well laid out, so that where I need to go deeper, I can go easily. Where I need just surface information or the parts that I’m already familiar with, I can just scroll through […] So I think I’ll prefer something presented in this way than the than the document that I read” (value).\n\n\n\n\n\n\n\nResults and Discussion\n\n\n\nWork in progress…\n\n\n\n\n\n\n\n\n\n1. Skivington K, Matthews L, Simpson SA, et al (2021) A new framework for developing and evaluating complex interventions: Update of Medical Research Council guidance. BMJ n2061\n\n\n2. Michie S, van Stralen MM, West R (2011) The behaviour change wheel: A new method for characterising and designing behaviour change interventions. Implementation Science 6:42\n\n\n3. Yardley L, Morrison L, Bradbury K, Muller I (2015) The Person-Based Approach to Intervention Development: Application to Digital Health-Related Behavior Change Interventions. Journal of Medical Internet Research 17:e4055\n\n\n4. Penelope.ai. Penelope.ai \n\n\n5. Nielsen J, Landauer TK (1993) A mathematical model of the finding of usability problems. In: Proceedings of the INTERACT ’93 and CHI ’93 Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, USA, pp 206–213\n\n\n6. Malterud K, Siersma VD, Guassora AD (2016) Sample Size in Qualitative Interview Studies: Guided by Information Power. Qualitative Health Research 26:1753–1760\n\n\n7. (2014) The UX Five-Second Rules. 19–76\n\n\n8. Szinay D, Perski O, Jones A, Chadborn T, Brown J, Naughton F (2021) Influences on the Uptake of Health and Well-being Apps and Curated App Portals: Think-Aloud and Interview Study. JMIR mHealth and uHealth 9:e27173\n\n\n9. Yardley L, Morrison LG, Andreou P, Joseph J, Little P (2010) Understanding reactions to an internet-delivered health-care intervention: Accommodating user preferences for information provision. BMC Medical Informatics and Decision Making 10:52\n\n\n10. Laffal J (1985) Ericsson, K. Anders, and Simon, Herbert A. Protocol Analysis: Verbal Reports as Data. MIT Press, Cambridge, MA, 1984. Viii + 426 pp. $27.50. The Journal of Nervous and Mental Disease 173:703\n\n\n11. Boren T, Ramey J (2000) Thinking aloud: Reconciling theory and practice. IEEE Transactions on Professional Communication 43:261–278\n\n\n12. DEJONG M, SCHELLENS PJ (1997) Reader-Focused Text Evaluation: An Overview of Goals and Methods. Journal of Business and Technical Communication 11:402–432\n\n\n13. Davies L, Donnelly KZ, Goodman DJ, Ogrinc G (2016) Findings from a novel approach to publication guideline revision: User road testing of a draft version of SQUIRE 2.0. BMJ quality & safety 25:265–272\n\n\n14. QSR International Pty Ltd. (2020) NVivo. \n\n\n15. Lincoln YS, Guba EG (1985) Naturalistic Inquiry. SAGE\n\n\n16. O’Brien BC, Harris IB, Beckman TJ, Reed DA, Cook DA (2014) Standards for Reporting Qualitative Research: A Synthesis of Recommendations. Academic Medicine 89:1245–1251"
  },
  {
    "objectID": "chapters/12_discussion/index.html",
    "href": "chapters/12_discussion/index.html",
    "title": "Discussion",
    "section": "",
    "text": "Work in progress…"
  },
  {
    "objectID": "chapters/4_survey_content/tbl_studies.html",
    "href": "chapters/4_survey_content/tbl_studies.html",
    "title": "Thesis",
    "section": "",
    "text": "Table 1: Studies that collected quantitative data to explore researcher’s experiences of reporting guidelines\n\n\n\n\n\n\n\n\n\n\nCitation\nTitle\nGuidelines studied\nSample geographics\nSample size\nQuantitative or mixed methods\n\n\n\n\nBrouwers et al. 2016 [1]\nThe AGREE Reporting Checklist: a tool to improve reporting of clinical practice guidelines\nAGREE Reporting Checklist\nNot reported\n15\nQuantitative\n\n\nBurford, Welch, Waters et al., 2013 [2]\nTesting the PRISMA-Equity 2012 reporting guideline: the perspectives of systematic review authors\nPRISMA-Equity Checklist items embedded into survey\nNot reported\n151\nMixed methods\n\n\nDavies, Donnelly, Goodman, Ogrinc 2016 [3]\nFindings from a novel approach to publication guideline revision: user road testing of a draft version of SQUIRE 2.0\nSQUIRE Guidelines, which are presented as a checklist\nNot reported but invited participants were from USA, UK Lebanon, Sweden.\n44\nMixed methods\n\n\nDewey, Levine, Bossuyt et al., 2019 [4]\nImpact and perceived value of journal reporting guidelines among Radiology authors and reviewers\nCONSORT, STROBE, PRISMA, STARD checklists\nUSA, Canada, China, South Korea, Japan, Germany, France , Italy, UK, Other European countries, Middle East, Latin America and ‘Other’.\n831\nMixed methods\n\n\nEysenbach, 2013 [5]\nCONSORT-EHEALTH: Implementation of a Checklist for Authors and editors to improve reporting of web-based and mobile randomized controlled trials\nCONSORT-Ehealth checklist\nNot reported\n61\nMixed methods\n\n\nFang, Xi, Liu et al. 2016 [6]\nA survey on awareness of the ARRIVE Guideline and GSPC in researchers field in animal experiments field in Lanzhou City\nARRIVE Guidelines and Gold Standard Publication Checklist\nChina\n287\nQuantitative\n\n\nFuller, Pearson, Peters, Anderson, 2015 [7]\nWhat affects authors’ and editors’ use of reporting guidelines? Findings from an online survey and qualitative interviews\nTREND and reporting guidelines in general\nPredominantly North America\n56\nMixed methods\n\n\nGiray et al. 2020 [8]\nAssessment of the knowledge and awareness of a sample of young researcher physicians on reporting guidelines and the EQUATOR network: A single center cross-sectional study\nCONSORT, PRISMA, CARE, GRASS, STARD, STROBE, ARRIVE, SAMPL guidelines\nTurkey\n100\nQuantitative\n\n\nGuo, Qi, Yang et al., 2018 [9]\nRecognition status of quality assessment and standards for reporting randomized controlled trials of traditional Chinese medicine researchers\nCONSORT Statement, STRICTA guidelines and CONSORT extension for Traditional Chinese Medicine\nChina\n180\nQuantitative\n\n\nKorevaar, Cohen, Reitsma, et al, 2016 [10]\nUpdating standards for reporting diagnostic accuracy: the development of STARD 2015\nSTARD checklist\nNot reported for quantitative survey\n12\nMixed methods\n\n\nMa et al. 2017 [11]\nSurvey of basic medical researchers on the awareness of animal experimental designs and reporting standards in China\nARRIVE guidelines and Gold Standard Publication Checklist\nChina\n266\nQuantitative\n\n\nMacleod, Collings, Graf et al. 2021 [12]\nThe MDAR (Materials Design Analysis Reporting) Framework for transparent reporting in the life sciences\nMDAR checklist\nUSA, China, Japan, Germany, Other EU, ‘Other’\n211\nMixed methods\n\n\nMcDonough et al. 2011 [13]\nFamiliarity of non-industry authors with good publication practice and clinical data reporting guidelines\nCONSORT guidelines\nUSA, UK, Canada, South Africa, Israel, China\n23\nQuantitative\n\n\nÖncel et al. 2018 [14]\nKnowledge and awareness of optimal use of reporting guidelines in paediatricians: A cross-sectional study\nCONSORT guidelines, STROBE, PRISMA, CARE, SRQR, STARD, SQUIRE, CHEERS, SPIRIT, ARRIVE, TREND, STREGA, the Conference on Guideline Standardization (COGS), Outbreak Reports and Intervention Studies Of Nosocomial infection (ORION)\nTurkey\n244\nQuantitative\n\n\nPage, McKenzie, Bossuyt et al. 2021 [15]\nUpdating guidance for reporting systematic reviews: development of the PRISMA 2020 statement\nPRISMA statement\nNot reported\n110\nMixed methods\n\n\nPrady & MacPherson 2007 [16]\nAssessing the Utility of the Standards for Reporting Trials of Acupuncture (STRICTA): A survey of authors\nSTRICTA\nNot reported\n28\nMixed methods\n\n\nPrager, Gannon, Bowdridge et al. 2021 [17]\nBarriers to reporting guideline adherence in point-of care ultrasound research: a cross- sectional survey of authors and journal editors\nSTARD\nNot reported\n18\nMixed methods\n\n\nPhillips et al 2015 [18]\nPilot testing of the Guideline for Reporting of Evidence-Based Practice Educational Interventions and Teaching (GREET)\nGREET checklist and E&E\nNot reported\n31\nQuantitative\n\n\nRader, Mann, Stransfield et al., 2014 [19]\nMethods for documenting systematic review searches: a discussion of common issues\nPRISMA statement\nNot reported\n263\nMixed methods\n\n\nSharp, Glonti, Hren, 2020 [20]\nUsing the STROBE statement: survey ﬁndings emphasized the role of journals in enforcing reporting guidelines\nSTROBE statement\nThe full survey was answered by participants in Africa, Asia, Europe, North and South America, Middle East, and Pacific Region. It is unclear who answered the free text question.\n1015\nMixed methods\n\n\nStruthers, Harwood, de Beyer et al., 2021 [21]\nGoodReports: developing a website to help health researchers find and use reporting guidelines\nReporting guidelines in general\nNot reported\n274\nMixed methods\n\n\nTam, Tang, Woo, Goh 2019 [22]\nPerception of the Preferred Reporting Items for Systematic Reviews and Meta Analyses (PRISMA) statement of authors publishing reviews in nursing journals: a cross-sectional online survey\nPRISMA statement\nNot reported\n230\nMixed methods\n\n\n\n\n\n\n\n\n1. Brouwers MC, Kerkvliet K, Spithoff K, Consortium ANS (2016) The AGREE Reporting Checklist: A tool to improve reporting of clinical practice guidelines. BMJ 352:i1152\n\n\n2. Burford BJ, Welch V, Waters E, Tugwell P, Moher D, O’Neill J, Koehlmoos T, Petticrew M (2013) Testing the PRISMA-Equity 2012 reporting guideline: The perspectives of systematic review authors. PloS one 8:e75122\n\n\n3. Davies L, Donnelly KZ, Goodman DJ, Ogrinc G (2016) Findings from a novel approach to publication guideline revision: User road testing of a draft version of SQUIRE 2.0. BMJ quality & safety 25:265–272\n\n\n4. Dewey M, Levine D, Bossuyt PM, Kressel HY (2019) Impact and perceived value of journal reporting guidelines among Radiology authors and reviewers. European Radiology 29:3986–3995\n\n\n5. Eysenbach G. (2013) CONSORT-EHEALTH: Implementation of a checklist for authors and editors to improve reporting of web-based and mobile randomized controlled trials. Studies in health technology and informatics 192:657–661\n\n\n6. Fang Z.-P., Leng X., Liu Y.-L., Liu W.-B., Hu W.-J., Zhang Z.-J., Ma B., Li D.-M. (2015) A survey on awareness of the ARRIVE guideline and GSPC in researchers field in animal experiments field in Lanzhou city. Chinese Journal of Evidence-Based Medicine 15:797–801\n\n\n7. Fuller T, Pearson M, Peters J, Anderson R (2015) What affects authors’ and editors’ use of reporting guidelines? Findings from an online survey and qualitative interviews. PLoS ONE 10:e0121585\n\n\n8. Gi̇ray E, Coskun OK, Karacaatli M, Gunduz OH, Yagci İ (2020) Assessment of the knowledge and awareness of a sample of young researcher physicians on reporting guidelines and the EQUATOR network: A single center cross-sectional study. Marmara Medical Journal 33:1–6\n\n\n9. Guo S, Qi S, Yang L, Wang X, Zhu Q, Meng X, Zeng Y, Institute M and A of, China SMC of A (2018) Recognition status of quality assessment and standards for reporting randomized controlled trials of traditional Chinese medicine researchers. China Journal of Traditional Chinese Medicine and Pharmacy 1077–1081\n\n\n10. Korevaar DA, Cohen JF, Reitsma JB, et al (2016) Updating standards for reporting diagnostic accuracy: The development of STARD 2015. Research integrity and peer review 1:7\n\n\n11. Ma B, Xu J, Wu W, Liu H, Kou C, Liu N, Zhao L (2017) Survey of basic medical researchers on the awareness of animal experimental designs and reporting standards in China. PLoS ONE 12:e0174530\n\n\n12. Macleod M, Collings AM, Graf C, Kiermer V, Mellor D, Swaminathan S, Sweet D, Vinson V (2021) The MDAR (Materials Design Analysis Reporting) Framework for transparent reporting in the life sciences. Proceedings of the National Academy of Sciences 118:e2103238118\n\n\n13. McDonough J., O’Dunne A., B. C, Margerum B., Sutton D. (2011) Familiarity of non-industry authors with good publication practice and clinical data reporting guidelines. Current Medical Research and Opinion 27:S9\n\n\n14. Karadağ Öncel E, Başaranoğlu ST, Aykaç K, Kömürlüoğlu A, Akman AÖ, Kıran S (2018) Knowledge and awareness of optimal use of reporting guidelines in paediatricians: A cross-sectional study. Turk Pediatri Arsivi 53:163–168\n\n\n15. Page MJ, McKenzie JE, Bossuyt PM, Boutron I, Hoffmann TC, Mulrow CD, Shamseer L, Tetzlaff JM, Moher D (2021) Updating guidance for reporting systematic reviews: Development of the PRISMA 2020 statement. Journal of Clinical Epidemiology 134:103–112\n\n\n16. Prady SL, MacPherson H (2007) Assessing the utility of the standards for reporting trials of acupuncture (STRICTA): A survey of authors. The Journal of Alternative and Complementary Medicine 13:939–943\n\n\n17. Prager R, Gagnon L, Bowdridge J, Unni RR, McGrath TA, Cobey K, Bossuyt PM, McInnes MDF (2021) Barriers to reporting guideline adherence in point-of-care ultrasound research: A cross-sectional survey of authors and journal editors. BMJ Evidence-Based Medicine bmjebm-2020-111604\n\n\n18. Phillips A., Lewis L.K., McEvoy M.P., Galipeau J., Glasziou P., Moher D., Tilson J.K., Williams M.T. (2015) Pilot testing of the guideline for reporting of evidence-based practice educational interventions and teaching (greet). Physiotherapy (United Kingdom) 101:eS1203–eS1204\n\n\n19. Rader T., Mann M., Stansfield C., Cooper C., Sampson M. (2014) Methods for documenting systematic review searches: A discussion of common issues. Research synthesis methods 5:98–115\n\n\n20. Sharp MK, Bertizzolo L, Rius R, Wager E, Gómez G, Hren D (2019) Using the STROBE statement: Survey findings emphasized the role of journals in enforcing reporting guidelines. Journal of Clinical Epidemiology 116:26–35\n\n\n21. Struthers C, Harwood J, de Beyer JA, Dhiman P, Logullo P, Schlüssel M (2021) GoodReports: Developing a website to help health researchers find and use reporting guidelines. BMC medical research methodology 21:217\n\n\n22. Tam WWS, Tang A, Woo B, Goh SYS (2019) Perception of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement of authors publishing reviews in nursing journals: A cross-sectional online survey. BMJ Open 9:e026271"
  },
  {
    "objectID": "chapters/4_survey_content/tbl_codes_categories.html",
    "href": "chapters/4_survey_content/tbl_codes_categories.html",
    "title": "Thesis",
    "section": "",
    "text": "Table 1: Codes describing the focus of questions asked and their code categories. Items in bold did not appear in the qualitative data.\n\n\n\n\n\n\nCode**\nCategory\n\n\n\n\nParticipant’s experience [1–13]\nParticipant’s speciality [1, 2, 4–6, 9, 10, 12–15]\nParticipant’s age [4, 6–8, 10, 12, 13, 15]\nParticipant’s gender [4, 6–8, 12, 13, 15]\nParticipant’s geography [3, 5, 11, 13]\nParticipant’s stage of current research project [1]\nDemographics\n\n\nAwareness of a particular guideline [1, 4–6, 8, 10–13, 15, 16]\nAwareness of EQUATOR [5, 6]\nHow did they first hear about guidelines or EQUATOR? [5, 8, 13]\nWhen did they first learn about a guideline? [5]\nAwareness\n\n\nHow frequently do they use guidelines? [1–3, 5, 6, 8, 12, 13, 15]\nWhen should guidelines be used? [3, 5, 6, 8, 12, 13]\nWould they use a guideline, hypothetically [1, 13, 17]\nUsage\n\n\nDid the guidance impact subsequent behaviour? [1–3, 16–19]\nImpact on behaviour\n\n\nIs the guidance usable? [12, 13, 20]\nIs the guidance easy to understand? [2, 7, 9, 12, 13, 19]\nUsability\n\n\nIs the guidance useful? [3, 5, 10, 13, 17, 19, 21]\nUsefulness\n\n\nIs the guidance important? [2, 5, 7, 10, 15, 18]\nImportance\n\n\nAre time and length barriers? [5, 12, 13, 18, 19]\nIs language of guidance a barrier? [12]\nAre guidelines lacking for study type? [5]\nBarriers\n\n\nIs the layout OK? [9, 13, 17]\nShould the content be modified? [9, 17, 22]\nIs the guidance relevant? [19]\nAre guidelines prescriptive? [5]\nOpinions on content\n\n\nWill using a guideline benefit the manuscript? [1, 12, 13, 17]\nProductivity benefits of using guidelines [13]\nUsing guidelines because of journal requirements [5, 13]\nUsing guidelines because of funder requirements [5]\nUsing guidelines because of employment requirements [5]\nUsing guidelines because of other researchers expecting it [13]\nReasons for using a guideline\n\n\nOpinions on reporting quality of the literature [4, 5, 10, 12]\nOpinions on reporting quality\n\n\nAre guidelines easy to find and access [5, 10, 12]\nAccessibility\n\n\nWho should complete the checklist? [5, 12]\nRoles\n\n\nAre endorsements a facilitator? [5]\nIs evidence of increased chance of publication a facilitator? [5]\nIs evidence of improved reporting quality a facilitator? [5]\nIs explanatory information a facilitator? [5]\nIs training a facilitator? [5]\nIs the behaviour of peers a facilitator or motivator? [5]\nIs the evidence base underlying a reporting guideline a motivator? [5]\nIs transparency in guideline development a motivator? [5]\nFacilitators and motivators\n\n\nIs the aim of the guidance clear? [9]\nAim of guidance\n\n\n\n\n\n\n\n\n1. Burford BJ, Welch V, Waters E, Tugwell P, Moher D, O’Neill J, Koehlmoos T, Petticrew M (2013) Testing the PRISMA-Equity 2012 reporting guideline: The perspectives of systematic review authors. PloS one 8:e75122\n\n\n2. Davies L, Donnelly KZ, Goodman DJ, Ogrinc G (2016) Findings from a novel approach to publication guideline revision: User road testing of a draft version of SQUIRE 2.0. BMJ quality & safety 25:265–272\n\n\n3. Dewey M, Levine D, Bossuyt PM, Kressel HY (2019) Impact and perceived value of journal reporting guidelines among Radiology authors and reviewers. European Radiology 29:3986–3995\n\n\n4. Fang Z.-P., Leng X., Liu Y.-L., Liu W.-B., Hu W.-J., Zhang Z.-J., Ma B., Li D.-M. (2015) A survey on awareness of the ARRIVE guideline and GSPC in researchers field in animal experiments field in Lanzhou city. Chinese Journal of Evidence-Based Medicine 15:797–801\n\n\n5. Fuller T, Pearson M, Peters J, Anderson R (2015) What affects authors’ and editors’ use of reporting guidelines? Findings from an online survey and qualitative interviews. PLoS ONE 10:e0121585\n\n\n6. Gi̇ray E, Coskun OK, Karacaatli M, Gunduz OH, Yagci İ (2020) Assessment of the knowledge and awareness of a sample of young researcher physicians on reporting guidelines and the EQUATOR network: A single center cross-sectional study. Marmara Medical Journal 33:1–6\n\n\n7. Guo S, Qi S, Yang L, Wang X, Zhu Q, Meng X, Zeng Y, Institute M and A of, China SMC of A (2018) Recognition status of quality assessment and standards for reporting randomized controlled trials of traditional Chinese medicine researchers. China Journal of Traditional Chinese Medicine and Pharmacy 1077–1081\n\n\n8. Karadağ Öncel E, Başaranoğlu ST, Aykaç K, Kömürlüoğlu A, Akman AÖ, Kıran S (2018) Knowledge and awareness of optimal use of reporting guidelines in paediatricians: A cross-sectional study. Turk Pediatri Arsivi 53:163–168\n\n\n9. Korevaar DA, Cohen JF, Reitsma JB, et al (2016) Updating standards for reporting diagnostic accuracy: The development of STARD 2015. Research integrity and peer review 1:7\n\n\n10. Ma B, Xu J, Wu W, Liu H, Kou C, Liu N, Zhao L (2017) Survey of basic medical researchers on the awareness of animal experimental designs and reporting standards in China. PLoS ONE 12:e0174530\n\n\n11. McDonough J., O’Dunne A., B. C, Margerum B., Sutton D. (2011) Familiarity of non-industry authors with good publication practice and clinical data reporting guidelines. Current Medical Research and Opinion 27:S9\n\n\n12. Prager R, Gagnon L, Bowdridge J, Unni RR, McGrath TA, Cobey K, Bossuyt PM, McInnes MDF (2021) Barriers to reporting guideline adherence in point-of-care ultrasound research: A cross-sectional survey of authors and journal editors. BMJ Evidence-Based Medicine bmjebm-2020-111604\n\n\n13. Sharp MK, Bertizzolo L, Rius R, Wager E, Gómez G, Hren D (2019) Using the STROBE statement: Survey findings emphasized the role of journals in enforcing reporting guidelines. Journal of Clinical Epidemiology 116:26–35\n\n\n14. Rader T., Mann M., Stansfield C., Cooper C., Sampson M. (2014) Methods for documenting systematic review searches: A discussion of common issues. Research synthesis methods 5:98–115\n\n\n15. Tam WWS, Tang A, Woo B, Goh SYS (2019) Perception of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement of authors publishing reviews in nursing journals: A cross-sectional online survey. BMJ Open 9:e026271\n\n\n16. Prady SL, MacPherson H (2007) Assessing the utility of the standards for reporting trials of acupuncture (STRICTA): A survey of authors. The Journal of Alternative and Complementary Medicine 13:939–943\n\n\n17. Brouwers MC, Kerkvliet K, Spithoff K, Consortium ANS (2016) The AGREE Reporting Checklist: A tool to improve reporting of clinical practice guidelines. BMJ 352:i1152\n\n\n18. Eysenbach G. (2013) CONSORT-EHEALTH: Implementation of a checklist for authors and editors to improve reporting of web-based and mobile randomized controlled trials. Studies in health technology and informatics 192:657–661\n\n\n19. Struthers C, Harwood J, de Beyer JA, Dhiman P, Logullo P, Schlüssel M (2021) GoodReports: Developing a website to help health researchers find and use reporting guidelines. BMC medical research methodology 21:217\n\n\n20. Phillips A., Lewis L.K., McEvoy M.P., Galipeau J., Glasziou P., Moher D., Tilson J.K., Williams M.T. (2015) Pilot testing of the guideline for reporting of evidence-based practice educational interventions and teaching (greet). Physiotherapy (United Kingdom) 101:eS1203–eS1204\n\n\n21. Macleod M, Collings AM, Graf C, Kiermer V, Mellor D, Swaminathan S, Sweet D, Vinson V (2021) The MDAR (Materials Design Analysis Reporting) Framework for transparent reporting in the life sciences. Proceedings of the National Academy of Sciences 118:e2103238118\n\n\n22. Page MJ, McKenzie JE, Bossuyt PM, Boutron I, Hoffmann TC, Mulrow CD, Shamseer L, Tetzlaff JM, Moher D (2021) Updating guidance for reporting systematic reviews: Development of the PRISMA 2020 statement. Journal of Clinical Epidemiology 134:103–112"
  },
  {
    "objectID": "chapters/7_workshops/index.html",
    "href": "chapters/7_workshops/index.html",
    "title": "Following the BCW Guide: Workshops with EQUATOR",
    "section": "",
    "text": "Having identified factors that may limit the impact of reporting guidelines (chapters 3 - 5) and selected the Behaviour Change Wheel as a framework [1] (chapter 6), my next step was to use the framework to turn reporting guidelines into a behaviour change intervention. In my introduction I describe how the reporting guideline system grew organically and I justified why I wanted to redesign this system and the guidelines themselves using evidence and behaviour change theory.\n\nIn “The Behaviour Change Wheel - A Guide To Designing Interventions”[2], Michie et al. suggest eight steps to help intervention designers understand behaviour and identify intervention options, content, and implementation options (see Figure 1). The guide is aimed at intervention designers who are not behaviour science specialists, and so offered a practical way to include other stakeholders from reporting guideline ecosystem. I wanted to do this because I expected that input from experts with intimate knowledge of reporting guidelines would lead to more ideas, and that these ideas to be more likely to gain traction and have impact.\n\n\n\nFigure 1: The three stages and eight steps recommended in The Behaviour Change Wheel - A Guide To Designing Interventions [2]\n\n\nHowever, I had to be mindful of how much time I could expect stakeholders to give to my project. The eight stages outlined by Michie et al. would take many hours and require background familiarity with the COM-B model, intervention functions and policy categories. This seemed like too much to ask of strangers, and so I decided to begin by involving members of the UK EQUATOR Center, with whom I already had a relationship, and who were already invested in the project. See chapter 2 for an introduction to the EQUATOR Network, and see chapters 8 and 11 for how I sought input from wider stakeholders and authors.\nIn this chapter, I describe how I led members of the UK EQUATOR Centre through the intervention design process outlined in “The Behaviour Change Wheel - A Guide To Designing Interventions”[2]. I go through each stage in turn, and describe the methods and results for each."
  },
  {
    "objectID": "chapters/7_workshops/index.html#introduction",
    "href": "chapters/7_workshops/index.html#introduction",
    "title": "Following the BCW Guide: Workshops with EQUATOR",
    "section": "",
    "text": "Having identified factors that may limit the impact of reporting guidelines (chapters 3 - 5) and selected the Behaviour Change Wheel as a framework [1] (chapter 6), my next step was to use the framework to turn reporting guidelines into a behaviour change intervention. In my introduction I describe how the reporting guideline system grew organically and I justified why I wanted to redesign this system and the guidelines themselves using evidence and behaviour change theory.\n\nIn “The Behaviour Change Wheel - A Guide To Designing Interventions”[2], Michie et al. suggest eight steps to help intervention designers understand behaviour and identify intervention options, content, and implementation options (see Figure 1). The guide is aimed at intervention designers who are not behaviour science specialists, and so offered a practical way to include other stakeholders from reporting guideline ecosystem. I wanted to do this because I expected that input from experts with intimate knowledge of reporting guidelines would lead to more ideas, and that these ideas to be more likely to gain traction and have impact.\n\n\n\nFigure 1: The three stages and eight steps recommended in The Behaviour Change Wheel - A Guide To Designing Interventions [2]\n\n\nHowever, I had to be mindful of how much time I could expect stakeholders to give to my project. The eight stages outlined by Michie et al. would take many hours and require background familiarity with the COM-B model, intervention functions and policy categories. This seemed like too much to ask of strangers, and so I decided to begin by involving members of the UK EQUATOR Center, with whom I already had a relationship, and who were already invested in the project. See chapter 2 for an introduction to the EQUATOR Network, and see chapters 8 and 11 for how I sought input from wider stakeholders and authors.\nIn this chapter, I describe how I led members of the UK EQUATOR Centre through the intervention design process outlined in “The Behaviour Change Wheel - A Guide To Designing Interventions”[2]. I go through each stage in turn, and describe the methods and results for each."
  },
  {
    "objectID": "chapters/7_workshops/index.html#methods",
    "href": "chapters/7_workshops/index.html#methods",
    "title": "Following the BCW Guide: Workshops with EQUATOR",
    "section": "Methods",
    "text": "Methods\nI invited 7 members of the UK EQUATOR Center to take part in the workshops, of whom 6 took part.\nI took an active part in the workshops. I felt justified drawing on my own experience from my earlier PhD work, as an author, and developer of tools to help authors (see chapter 2). Together, we completed exercises through discussion. Hence my paradigm was constructivist in that knowledge was “constructed between inquirer and participant through the inquiry process itself”[3]. Constructivism “rejects the idea that there is objective knowledge in some external reality for the researcher to retrieve mechanistically” and instead, “the researcher’s values and dispositions influence the knowledge that is constructed through interaction with the phenomenon and participants in the inquiry”[3].\nOn one hand I expected that my experience would be an asset, and would contribute to our aim of understanding and addressing the phenomena we were interested in. On the other hand, I wanted to ensure that my opinions didn’t bias the group, that I remained open-minded, and that I captured the thoughts of other workshop participants accurately. And so I used a number of established techniques to enhance trustworthiness and facilitate discussion.\nTo enhance trustworthiness, I used Lincoln and Guba’s criteria for trustworthiness [4], which asserts that for for a study to be trustworthy, the researcher must show that the findings are credible (‘true’), transferable (applicable to other contexts), dependable (consistent and repeatable), and confirmable (shaped by participants, not by the researcher’s bias or motivation). I describe the techniques I used to achieve each criteria in Table 1.\n\n\n\n\n\n\nTable 1: Techniques for establishing trustworthiness. Based on Lincoln and Guba’s Evaluative Criteria [4]\n\n\n\n\n\n\nTechnique\nImplementation\n\n\n\n\nTechniques for establishing credibility\n\n\n\nMember-checking\nLincoln and Guba argue that member checking is the most important way to the establish validity of an account [4]. All workshop participants could edit the worksheets during and after each workshop. At the end of each session, I would invite workshop participants to confirm that their thoughts were reflected in the file, and I invited participants to comment on my written account of the workshops.\n\n\nTechniques for establishing transferability\n\n\n\nThick description\nAlthough perhaps most relevant to ethnographic studies, I nevertheless drew on relevant aspects of ‘thick description’. I encouraged participants to describe ideas in detail and to document disagreement and context. Our aim was to document ideas in sufficient detail so they could be transferable across different reporting guidelines and stakeholders.\n\n\nTechniques for establishing confirmability\n\n\n\nReflexivity\nI wrote down my own ideas before each workshop, and made notes at the end of each workshop to reflect on the process. I would invite participants to consider the worksheet on their own or in pairs. I did this so that everybody would engage with the task and have an opportunity to think before being influenced by others. We then discussed ideas as a group, with members agreeing, disagreeing, and bouncing off each other. I withheld my own ideas until the end of a session or task, whereupon I would invite discussion on any ideas that I felt hadn’t been covered already. This allowed me to contribute my experience from previous PhD work, as an author, and as a software developer and give others a chance to discuss my thoughts, without biasing or narrowing discussion. I also actively acknowledge, and reflect on, my own role in knowledge creation throughout this chapter.\n\n\nAudit trail\nAll worksheets were stored on the University One Drive account which performs automatic saves and versioning. This created an audit trail which meant I could look at how a worksheet changed over time as participants added and edited content.\n\n\n\n\n\n\nTo encourage open discussion, I encouraged participants to rise above their own preconceptions and reassured them that there were no wrong answers, that all ideas were valid and should be documented (#TODO: name and cite). To facilitate rich discussion I used open-ended questioning (#TODO: name and cite), left space for participants to talk (#TODO: name and cite), and followed Michie et al.’s worksheets which structure inquiry around frameworks, models, and taxonomies (#TODO: name and cite).\nTODO: name all biases. Perhaps using https://www.ideatovalue.com/crea/nickskillicorn/2021/06/a-list-of-all-of-the-biases-which-affect-creativity-and-innovation-performance/ or https://catalogofbias.org/about/\nWe met 7 times between December 2021 and May 2022, and each online meeting lasted around 2 hours. All participants had access to the Behaviour Change Wheel book. We established some ground rules which were that no idea was a bad idea, we should favour evidence over preconceptions, and that we should aspire to challenge our own assumptions and be open minded as far as possible. We didn’t seek consensus. Instead, we kept note of any disagreements that could not be resolved by discussion.\nI followed the eight steps recommended by Michie et al (Figure 1) faithfully, and I would explain the objectives and any background theory at the start of each step. I will now summarise each step and our discussions. Our co-edited worksheets are included in the Appendix. I purposefully use “we” in this chapter instead of “participants” to reflect that my voice is included."
  },
  {
    "objectID": "chapters/7_workshops/index.html#results",
    "href": "chapters/7_workshops/index.html#results",
    "title": "Following the BCW Guide: Workshops with EQUATOR",
    "section": "Results",
    "text": "Results\n\n1. Defining the problem in behavioural terms\nMichie et al.’s first step is to define the problem in terms of who needs to do what. For example, weight loss is not a behaviour, but increasing physical activity is, and could be specified further as walking 10,000 steps a day. We were all in alignment here: we want researchers to include important details in their articles, in line with the relevant reporting guidelines.\n\n\n2. Select target behaviour\n“Behaviours do not exist in a vacuum but occur within the context of other behaviours of the same or other individuals” write Michie et al. [2] when explaining that the desired behaviour needn’t be the behaviour that you target. For instance, if you want a child to eat more fruit (desired behaviour), you may seek to influence what food their parent buys (target behaviour).\nStep 2 involves generating a longlist of candidate target behaviours that could bring about the desired behaviour before then selecting which behaviour(s) to focus on.\nWe generated a longlist of 18 ideas, many of which targeted authors directly such as “reading the guidance in full” or “studying guidance (in an abstract sense)”. We also considered targeting people other than authors. For instance, “peer reviewers” and “editors” could “check articles against guidelines [and] tell authors what is missing”, and supervisors could “encourage” the use of guidelines.\nThis exercise helped us break down things that we hadn’t questioned previously. For instance, we were forced to define what we actually meant by a “reporting guideline”, noting that “guidance” could be distributed across publications, checklists, or supplements. We decided that we wanted people to “use the full guidance (often reported in an Example & Elaboration document)” and “not just the checklist”.\nSimilarly, where we may previously have thought of “writing” as a single task, we began to consider how authors could “use guidance when planning”, “drafting”, “editing [their own work]”, or “checking”. We discussed how these behaviours required researchers’ to be open-minded to assistance and wondered if we could encourage them to “ask for help when writing”.\nThe next step was to select a behaviour from our longlist by considering:\n\nThe expected impact if the behaviour were to be performed\nHow easy we expected behaviour change to be\nThe centrality of behaviour - how close it was to our desired behaviour\nHow easy the behaviour will be to measure\n\nCriteria 2 and 3 lead us to prioritise the behaviour of authors above that of editors or peer reviewers. When considering criteria 1, we felt that authors were most likely to act on guidance if they used it early when writing. Just how early depended on the guideline and discipline; for example, protocols are common in some disciplines and some guidelines cater for this (e.g. SPIRIT, PRISMA-P). But other disciplines don’t typically have a protocol culture, and some guidelines are harder to use for writing protocols than others. Because we wanted our intervention to be transferable between reporting guidelines, we decided not to specify a stage of work and to use the term “early as possible” instead.\n\n\n3. Specify the target behaviour\nHaving decided that we wanted “authors to use the full guidance as early as possible” it was time to specify this behaviour in more detail. Michie et al. suggest defining who needs to perform the behaviour, what do they need to do, when will they do it, where will they do it, how often will they do it, and with whom they will do it.\nOur final definition of our target behaviour was: Researchers should use reporting guidance as early as possible in their research pipeline. They will do this for every piece of research, at their place of work, on their own but in the context of collaboration\nWe broke this key behaviour into two sub behaviours:\n\nEngage with reporting guidelines as early as possible (ie access and read them) and,\nApply the guidance to their writing as intended by the guideline developer.\n\nBy “research pipeline” we mean the many steps involved in a typical project which may include ideation, obtaining funding and ethics permission, designing, writing a protocol, drafting a manuscript, editing a manuscript, submitting a manuscript. Instead of specifying which stage reporting guidance should be used, we decided to specify that we want authors to use guidance as “early as possible”. We did this for a few reasons. Firstly, guidelines differ in how easily they can be used for writing protocols or applications. Secondly, researchers will naturally come across reporting guidelines at different stages of their work. Should a researcher discover a guideline at the point of journal submission, then we would still want them to apply the guidance then, even though this is a relatively late stage. But by specifying “as early as possible”, we declare our hope that next time that same researcher may decide to use a guideline at an earlier point. And finally, not all research projects will begin with a written funding application, ethics application, or protocol.\nBy “apply”, we refer to using guidance to plan, write or edit a written description of research (e.g. within a manuscript or application). Applying guidance may include the use of tools like templates or checklists. Participants discussed specifying “completing a reporting checklist” as a target behaviour, but decided against it as previous research showed that authors who complete checklists upon submission don’t necessarily edit their manuscript or comply with guidelines. Participants also recognised that focussing on checklists may be problematic because checklists appear administrative, are used after a manuscript has been written, at which point authors are least able or motivated to edit their work. Nevertheless, participants recognised that checklists will continue to be an important part of how reporting guidance are disseminated, and so they are included within the term “apply the guidance”, without being named.\nWe felt it important to specify that guidance should be applied “as intended by the guideline developer” as my previous research showed that sometimes authors who misinterpret guidelines may believe that they are adhering when they are not.\nHence our target behaviour definition specifies the who, what, when, where, how often, and with whom but is broad enough to account for differences between researchers’ working practices and reporting guidelines.\n\n\n4. Identify what needs to change\nThis step involved identifying what needs to change in the person and/or environment to achieve our target behaviour. Michie et al provide a questionnaire to facilitate this step, called the COM-B Questionnaire, which asks you to consider each COM-B domain in turn and to consider 23 items like “To perform the target behaviour, authors would…have to know more about why it was important”, “…have more time to do it”, and “…have more support from others” etc.\nMichie et al. emphasise the importance of evidence in this step, recommending that data should be “collected from as many relevant sources as possible” and “triangulated”, as a consistent picture of behaviour from multiple sources will “increase confidence in the analysis”[2]. Consequently, we included all of the factors that I identified from my previous PhD work (see chapters 3) and used the COM-B questionnaire to label each as being driven by capability, opportunity, or motivation.\nThe result was a set of 32 factors that we felt needed to change for our target behaviour to occur. These can be seen in Appendix A.\n\n\n5 & 6. Identify Intervention Functions and Policy Categories\nHaving defined our target behaviour and identifying what needs to change for that behaviour to occur, the next step was to consider how to achieve those changes. Michie et al suggest that it is important “considering the full range” of possible intervention functions and policy categories available. See chapter 6 for a fuller introduction to these terms but, briefly, an intervention function is a “category of means by which an intervention can change behaviour” and policy categories are options for delivering those functions. For example, the function modelling could be delivered through a communication campaign or a service.\nMichie et al. [2] recommend using the APEASE criteria to prioritze options, which stands for Affordability, Practicability, Efficacy, Acceptability, Side-effects, and Equity. They note that whilst effectiveness is key, the other criteria must also be considered as “behaviour change interventions operate within a social context”.\nWe saw Enablement, Education, Training, Persuasion, Modelling, and Environmental Restructuring as ranking favourably on all APEASE criteria. Of these, Enablement was viewed as a particularly low-hanging fruit that would likely be effective and welcomed by the research community.\nWe found the remaining intervention functions problematic. Incentivization and restriction (e.g. rewarding guideline adherence with funding or reduced article processing, or punishing non-adherence by withholding these benefits) were seen as inequitable because as long as guideline adherence is harder for some researchers than others, less-experienced, poorly resourced researchers would be at a disadvantage, as would those working in disciplines where reporting guidelines are poorly designed or harder to apply. Conversely, we felt that enablement has potential to reduce existing inequalities. In addition to being inequitable, participants voiced that restriction or punishment would be unacceptable to researchers, as would coercion (the threat of punishment). Furthermore, threats without enforcement may become known as pointless administration, lose effectiveness, and erode trust.\nRegarding Policy Categories, we saw environmental planning as the most affordable, effective, acceptable, safe, and equitable. When talking about the environment, we were really talking about the digital environment, within which EQUATOR has control over its own website but not websites or digital services run by others. Improving and extending the website was viewed as an affordable investment, that would be welcomed by authors (thus acceptable), and a practical way to increase the proportion of authors engaging with reporting guidelines that would increase equity without side effects.\nWe recognised communication as a favourable policy category that is already utilised. Communication channels included the website, mailing lists, social media, conferences, and publications. Whilst communication was rated as affordable, practicable, acceptable, safe, and equitable, its effectiveness may be limited. Although communication could promote awareness of reporting guidelines are available, what they are, and how they can be used, EQUATOR members noted that the efficacy of a communications campaign may be undermined if the campaign is directing authors to a website that is difficult to use, or guidelines that are difficult to access or understand. Thus communication on its own might not be sufficient, and members suggested that it should come after improving the digital environment.\nService provision was also favoured, as long as the service was financially sustainable. So too where guidelines, specifically guidance to help reporting guideline developers create and disseminate resources. Participants felt that Legislation, Regulation, and Fiscal Measures would not be acceptable to researchers and were not not practical options for EQUATOR to use.\n\n\n7. Identify behaviour change techniques\nThe aim of step 7 is to identify possible behaviour change techniques by systematically considering items from a taxonomy of 93 techniques [5] for each intervention function chosen in step 5. Michie et al suggest doing it this way because “the process of designing behaviour change interventions usually involves first of all determining the broad approach that will be adopted and then working on the specifics of the intervention design. For example, when attempting to reduce excessive antibiotic prescribing one may decide that an educational intervention is the appropriate approach. Alternatively, one may seek to incentivise appropriate prescribing or in some way penalise inappropriate prescribing. Once one has done this, one would decide on the specific intervention components.”. So Michie et al. suggest that intervention designers will select only one or two intervention functions, and then consider all behaviour change techniques relevant to that function.\nBut in our case, picking a “broad approach” didn’t feel helpful. Given that a system already exists for disseminating reporting guidance, and we found ourselves considering how we could use multiple intervention functions to refine the existing system. Asking EQUATOR to consider the full behaviour change technique taxonomy for multiple intervention functions was impractical.\nAdditionally, although the taxonomy is designed to be applicable to a range of contexts and intervention types, it didn’t always feel like a perfect fit for our needs. Some techniques are explicitly health-focussed (e.g. Body changes, Pharmacological support, and Information about health consequences) whereas for others the link with health interventions came from the examples provided. For example, the technique practical social support lists the example of “Ask the partner of the patient to put their tablet on the breakfast tray so that the patient remembers to take it”. It’s not immediately obvious how this technique could generalise to our target behaviour. So before choosing techniques, we first had to “translate” the taxonomy to work out how it might be applied to reporting guidelines. The taxonomy developers perhaps acknowledge this limitation, suggesting in their discussion that the list can be viewed as a “core” taxonomy that can be modified or extended according to context [5].\nHence, given that we didn’t want to choose a “broad approach”, and given that this step required familiarity with the taxonomy and how it can be adapted to our context, doing this step with EQUATOR staff would have been very time consuming, as others have also found (e.g., [6]).\nInstead, I did this step on my own. I was well placed to do this because I was familiar with the taxonomy and the Behaviour Change Wheel Guide, I had lead all workshops, and I was most familiar with the barriers we were trying to address. I went through each intervention function that the group had favoured in step 5 (Enablement, Education, Training, Persuasion, Modelling, and Environmental Restructuring) and considered all relevant behaviour change techniques.\nTODO: I could better explain that this was iterative. I did it before the focus groups and then again afterwards where I went “backwards” from ideas back to BCTs.\n\n\n\n\n\n\n\nIntervention Function\nPossible Behaviour Change Techniques\n\n\n\n\nEnablement\nSocial support (practical)Adding objects to the environmentRestructuring the physical environment (which I took to include the digital environment)Reduce negative emotionsSelf-monitoring of outcome(s) of behaviourFraming/reframingIdentification of self as role modelSalience of consequencesAnticipated regretVicarious consequences\n\n\nEducation\nInformation about social and environmental consequencesInformation about health consequences (which I took to mean other people’s health)Feedback on behaviour (e.g. feedback on guideline use)Feedback on outcomes of the behaviour (e.g. feedback regarding quality of reporting from editors, reviewers, or colleagues)Self monitoring of outcome(s) of behaviour (e.g. checking one’s own work against guidance)Information about emotional consequences (e.g. telling authors they how they will feel)Information about others’ approval\n\n\nTraining\nInstruction on how to perform the behaviourDemonstration of the behaviourFeedback on the behaviourFeedback on the outcome(s) of the behaviourSelf-monitoring of outcome(s) of the behaviour\n\n\nPersuasion\nCredible sourceInformation about social and environmental consequencesInformation about health consequencesFeedback on behaviourFeedback on outcome(s) of the behaviourMonitoring outcome(s) of behaviour by others without feedbackIdentification as self as role modelInformation about emotional consequencesSalience of consequencesInformation about others’ approvalSocial comparisonFraming/reframingRemove aversive stimulus\n\n\nModelling\nDemonstration of the behaviour\n\n\nEnvironmental Restructuring\nAdding objects to the environmentPrompts/cuesRestructuring physical environment\n\n\n\n\n\n\n\n8. Identifying delivery options\nI then had to decide how to deliver our favoured intervention functions and policy options. Michie et al.’s guidance for this step is more open ended [2]; although they offer delivery options for communication as an example intervention function, there is no framework or systematic approach to this step and instead they recommend that designers consider the delivery options that they have at their disposal. For EQUATOR, that meant developing a new service or training programme, running a communication campaign, developing new guidance, or improving their website. It was important that my choice was doable within my funding window, and I wanted it to have a lasting impact after my DPhil was finished.\nCreating a new service felt unsustainable as it would likely stop once my funding ran out. Developing training or guidance for guideline developers could be useful and acceptable but not achievable within my time constraint. I could have developed a communications campaign, but EQUATOR members felt this was something they could do independently and that it should be done after addressing other barriers.\nInstead, redesigning reporting guidelines and key parts of EQUATOR’s website felt like the perfect choice for multiple reasons. We decided that displaying redesigned guidelines as a webpage would make them easy to disseminate, easy to access, and would offer us functions not available in traditional publications. These redesigned guidelines could become part of the existing EQUATOR Network website. We also felt that the home page was particularly important to focus on, as it is the most visited page of the website where the highest proportion of visitors disappear.\nImproving guidelines and the website aligned with our prioritised options. Firstly, planning the digital environment was our highest ranked policy category. Secondly, we could use the website to deliver many of our favoured intervention functions (enablement, education, persuasion, modelling). Thirdly, it spoke to my skills as a software developer. Finally, these changes could be made within the time limit of my PhD and would have a lasting impact."
  },
  {
    "objectID": "chapters/7_workshops/index.html#discussion",
    "href": "chapters/7_workshops/index.html#discussion",
    "title": "Following the BCW Guide: Workshops with EQUATOR",
    "section": "Discussion",
    "text": "Discussion\nFollowing Michie et al.’s guide helped us to specify our target behaviour, understand the behavioural drivers behind 32 barriers to our target behaviour, select intervention functions, policy categories, behaviour change techniques, and delivery options.\nMore fundamentally, the process helped the workshop participants to begin thinking about reporting guidelines as a behaviour change intervention. The process helped us break down the differences between tasks (e.g., writing vs. editing vs. reviewing research articles), users (e.g., inexperienced vs. experienced researchers, editors, reviewers), resources (e.g., example and elaboration documents vs. checklists). It helped workshops participants to look at the current system objectively and it helped us challenge our preconceptions.\nOne of the most interesting parts of this process was witnessing an unexpected change of opinion amongst EQUATOR staff. Before beginning this study, a common refrain heard around the office was that in order for reporting guidelines to be successful editors had to start enforcing them and refuse to publish research that didn’t adhere. So it was fascinating to see that workshop participants unanimously rated restriction and coercion as their least favourite options.\nI think two things happened here. Firstly, having discussed the barriers that authors face when trying to use reporting guidelines, participants felt that forcing authors to use them would be unacceptable to authors, impractical for editors, and inequitable as some authors would face larger hurdles than others. Secondly, participants reassessed things they had taken for granted, and realised that there are many low-hanging fruit that could make guidelines easier to find and use, and that these fruit were growing in their own orchard.\nUsing a framework helped participants to systematically consider a full range of options, many of which may not have come to mind naturally. However, sometimes it was difficult to get participants to think “outside of the box”. The default was to think about how the existing system could be improved, and it was difficult to imagine a world where we could be starting from scratch. In one sense this was an opportunity, as improving a system that already exists is easier than creating something totally new. But it could also be seen as a limitation, as our imagination may have been limited by what already exists.\nWe may also have been limited by group-think #REF. All workshop participants had worked for the EQUATOR Center for many years and had many shared opinions and experiences. Including other stakeholders in these workshops would have helped address this but would have been impractical to coordinate. To mitigate this, I decided to gather input from guideline developers, publishers, and authors through separate pieces of work which I describe in chapters 8 and 11.\nIn conclusion, by working through Michie et al’s suggested approach to applying the Behaviour Change Wheel [2] in a series of workshops with members of the EQUATOR Network, we defined our target behaviour as “Researchers should use reporting guidance as early as possible in their research pipeline. They will do this for every piece of research, at their place of work, on their own but in the context of collaboration”. We broke this down into two sub behaviours: 1) engage with reporting guidelines as early as possible (ie access and read them) and, 2) apply the guidance to their writing as intended by the guideline developer. We identified 32 factors that could affect this target behaviour. We favoured Enablement, Education, Training, Persuasion, Modelling, and Environmental Restructuring as intervention functions, and we favoured Environmental Planning, Communication, Service Provision, and Guidelines as policy categories. I identified #TODO behaviour change techniques that could be used, and we decided that our focus (and the focus of the rest of my thesis) should be on redesigning reporting guidelines and the EQUATOR Network home page.\n\n\n\n\n\n\n\n1. Michie S, van Stralen MM, West R (2011) The behaviour change wheel: A new method for characterising and designing behaviour change interventions. Implementation Science 6:42\n\n\n2. Michie S, Atkins L, West R (2014) The Behaviour Change Wheel: A Guide to Designing Interventions. Silverback Publishing, London\n\n\n3. M.Given L (2008) The SAGE Encyclopedia of Qualitative Research Methods. https://doi.org/10.4135/9781412963909\n\n\n4. Lincoln YS, Guba EG (1985) Naturalistic Inquiry. SAGE\n\n\n5. Michie S, Richardson M, Johnston M, Abraham C, Francis J, Hardeman W, Eccles MP, Cane J, Wood CE (2013) The behavior change technique taxonomy (v1) of 93 hierarchically clustered techniques: Building an international consensus for the reporting of behavior change interventions. Annals of Behavioral Medicine: A Publication of the Society of Behavioral Medicine 46:81–95\n\n\n6. Carvalho F (2020) Participatory Design for Behaviour Change: An Integrative Approach to Improving Healthcare Practice Focused on Staff Participation. PhD thesis"
  },
  {
    "objectID": "chapters/8_focus_groups/index.html",
    "href": "chapters/8_focus_groups/index.html",
    "title": "Generating ideas to address factors limiting reporting guideline impact: workshops with EQUATOR and focus groups with developers, publishers, and experts",
    "section": "",
    "text": "Known todos:\n\nMulti-panel figure not rendering nicely in docx format\nI need to name and cite bias in methods section"
  },
  {
    "objectID": "chapters/8_focus_groups/index.html#introduction",
    "href": "chapters/8_focus_groups/index.html#introduction",
    "title": "Generating ideas to address factors limiting reporting guideline impact: workshops with EQUATOR and focus groups with developers, publishers, and experts",
    "section": "Introduction",
    "text": "Introduction\nIn chapter 7 I described how I led EQUATOR UK staff members through the stages recommended in Michie et al.’s book “The Behaviour Change Wheel - A Guide to Designing Interventions” [1]. This guide helped us define our target behaviour, identify 32 factors that may influence this behaviour, and to prioritise lists of intervention functions, policy categories, possible behaviour change techniques, and delivery options. However, these lists were abstract that still had to be actualised. For instance, although we had prioritised Restructuring the physical environment as a possible behaviour change technique, how could the environment be restructured? Similarly, we prioritised Enablement, Persuasion, or Education, but how and where could these functions be applied?\nThus my next objective was to gather concrete ideas on how these abstract concepts could be realised to address reporting guideline limitations. I did this with EQUATOR as part of the workshops described in the previous chapter. However, this was also an opportunity to invite input from broader stakeholders which was important because, although EQUATOR is a key part of the reporting guideline landscape, they are only a part of it. Ultimately, all stakeholders within the academic system influence the impact of reporting guidelines. Guideline developers and publishers arguably do so most directly, and so it was important to draw on their experience and opinions. It hadn’t been feasible to include these stakeholders through all stages of the BCW approach as a) the time commitment was too great, b) it would have required stakeholders to become familiar with the framework and its terminologies which was too big an ask. In contrast, brainstorming ideas was a convenient and important stage to include them in. I expected that seeking input from a more diverse group would lead to more ideas and that those ideas would be more likely to gain traction.\nIn this chapter I explain how I went through this brainstorming process 1) by running workshops with EQUATOR staff members and 2) by running focus groups with other stakeholders before describing the combined results."
  },
  {
    "objectID": "chapters/8_focus_groups/index.html#methods",
    "href": "chapters/8_focus_groups/index.html#methods",
    "title": "Generating ideas to address factors limiting reporting guideline impact: workshops with EQUATOR and focus groups with developers, publishers, and experts",
    "section": "Methods",
    "text": "Methods\nThe purpose of this study was to elicit ideas from experts familiar with RG dissemination, on how to address factors that may influence the application of (and thereby adherence to) reporting guidance. My methods had two parts: 1) brainstorming ideas with EQUATOR during workshops and 2) extending this list through focus-groups with guideline developers, publishers, and guideline advocates. I then describe the combined results.\nBefore describing my workshop and focus group methods in detail, I will briefly outline the key differences. Whereas I actively contributed during the EQUATOR workshops, I tried to remain a passive facilitator during the focus groups - I wanted to capture the thoughts of participants whilst minimising my influence as far as possible. Whereas in the workshops, EQUATOR staff and I spent many hours, across multiple sessions, working through behavioural influences one at a time, focus group participants had limited time and so selected their own influences.\n\nWorkshopping Ideas with EQUATOR\nI described the workshops I ran with EQUATOR in chapter , including the techniques I used to encourage rich discussion and to navigate my position as a participating researcher, contributing ideas myself whilst facilitating others’ to share their voices. For this particular step, I asked workshop participants to consider each intervention function in turn and suggest ideas that would employ that function. After the workshop, I then labelled each idea with the influence(s) it would be addressing to create an “ideas document” with two columns: influences were described in the left hand column, and ideas in the right hand column. I invited workshop participants to review and edit this document, thereby co-producing lists of influences and ideas. The file can be seen in Appendix #TODO.\n\n\nFocus groups with external stakeholders\nFocus groups are researcher-led group discussions that use conversation as a form of data collection [2]. A key element of focus groups is interactions between participants as they agree, disagree, challenge, and ‘feed off’ of each other. I chose focus groups because I expected this interaction to lead to more ideas being generated than if I interviewed participants in isolation. Focus groups are also a practical way to collect data from larger groups of people. This is in contrast to in-depth interviews which are more useful in eliciting detail about individuals’ perspectives.\n\n\n\nSampling\nTo seek variation and ideas from broad range of stakeholders, I invited a purposive sample including the developers of popular reporting guidelines, publishing professionals, and academics that have studied reporting guidelines. I asked participants to extend the invite to others they felt would be appropriate. Because the BCW requires input from experts with insight into the intervention, I decided to elicit the opinions of authors in a separate study (see chapter 11).\nFollowing best-practice, I used information power [3] to guide my estimated sample size. Malterud et al. posit that the more relevant information a sample holds, the fewer participants are needed. They argue that sample size sufficiency depends on five factors: 1) whether the study’s aim is narrow or broad, 2) whether samples are considered dense (they have a lot of relevant experience or knowledge of the phenomena) or sparse, 3) whether the study is well supported by theory, 4) the quality of dialogue, and 5) whether data will between compared between participants/groups.\nMy aim was narrow and well defined. My sample was dense in that participants knew a lot about how reporting guidance is disseminated but also showed variance in terms of which guidelines they work on and which parts of the academic system they represented. I used the BCW as an applied theory. I used open questioning and to encourage strong dialogue (I elaborate on this later), and I was not planning a cross-case analysis. Therefore, I deemed my information power sufficient to justify initially recruiting 15-20 participants in 4-5 groups.\nI used the dialogue criteria from Information Power to decide when to stop recruiting. By monitoring the number of edits to the co-produced file I could be confident that my information power was good. Once groups began to add fewer and fewer comments, I judged that the benefit of continued recruitment was insufficient given time constraints.\n\n\nMaterials\nI used the ideas document generated in the workshops with EQUATOR to prompt discussion in the focus groups. However, because ideas generated by previous participants could bias or limit the creativity of current participants I initially hid them by turning the text in the ideas column white before sharing. I would reveal the text only after participants had exhausted their own imagination (see Figure 1 for an example). All participants could edit this file to record their own ideas or elaborate on other people’s ideas. At the end of each focus group, I would then turn the ideas column white again, ready for the next group to continue the process. In this way, each group built upon the output of the previous groups.\n\n\n\n\n\n\n\nIdeas initially hidden\n\n\n\n\n\n\n\nIdeas showing\n\n\n\n\nFigure 1: An example entry in the Ideas Document that was co-edited by participants. Existing ideas were initially hidden (by turning the text white), and only made visible once participants had discussed their own ideas.\n\n\n\n\nFocus Groups\nI conducted focus groups between May-July 2022 online using Zoom. Before each focus group, I asked participants to spend some time thinking about barriers and facilitators. I did this because I wanted participants to get into the frame of mind and come to the focus group “armed” with influences they were ready to discuss. I was also interested to see whether participants would contribute influences that I did not identify in chapters 3.\nASK CA Is this combatting recall bias? https://catalogofbias.org/biases/recall-bias/\nEach focus group lasted 2 hours. Following standard practice, I began by introducing myself and the project in a way that I hoped would help participants relax and to think open-mindedly, not defensively. I explained where the list of influences had come from, and that the influences were in reference to reporting guidelines in general, and not necessarily a comment on their guideline. I encouraged participants to think beyond the guideline documents themselves, and to consider all stakeholders and resources involved. I explained the goal was to brainstorm as many ideas as possible, and not worry about whether ideas were good or bad.\nASK CA I say that I wanted to put participants at ease. Is that addressing apprehension bias and hawthorne effect? https://catalogofbias.org/biases/apprehension-bias/ https://catalogofbias.org/biases/hawthorne-effect/\nIt was not possible for a single focus group to cover all influences within a reasonable amount of time, so I allowed participants to select which items they wanted to discuss. I did this by giving them a few minutes to read through a list of influences, raise any additional influences they felt were missing, and mark those that they wanted to talk about. I occasionally selected influences to discuss myself, either because they had been neglected by previous groups or because I expected participants to have insight into it.\nFor each influence discussed, I would explain it and allow participants to ask questions. I then asked participants to spend a couple of minutes reflecting on the influence and brainstorming solutions on their own before discussing them as a group. I encouraged this solo reflection because I wanted all participants to engage with the problem. (#REF)\nASK CA: What is a reference for this technique?\nTo facilitate discussion, I would ask open ended questions, often drawing on intervention functions from the BCW by asking questions like “how could this be easier to do?” or “how could we change how people feel about this?”. I did this when participants ran out of ideas, or when they got fixated on a particular type of intervention, in which case I would reassure participants that their fixated solution was already documented and that it would be useful to think of alternatives.\nOnce participants had discussed all of their own ideas, I would reveal the ideas identified by previous groups by changing the colour of text from white to black. Participants could then edit and extend the text until it reflected all of their thoughts too. Ideas were never removed from the document, but participants could add concerns or disagreements if they wanted to. Editing the file in this way allowed participants to document their thoughts in their own words.\nAfter each focus groups I made notes on how the session went and reflected on what I could have done differently. I made a copy of the ideas document and then turned the text in the ideas column white again, ready for the next group. Taking copies after each group created a paper trail of how the document had evolved after each session, and counted the number of additions so that I could monitor how many new ideas had been added and, therefore, whether I could stop data collection. \n\n\nData processing and analysis\nI used qualitative description for my analysis ([4]; [5]), which involved aggregating and summarising ideas. I imported the final ideas document into NVivo and applied descriptive codes to ideas. If a sentence contained multiple ideas, I would code each idea separately. I also coded the barriers and stakeholders that were related to each idea. I did not interpret data as doing so would erase the views captured during co-production.\nI grouped ideas inductively in ways that felt cohesive and made the results easy for my intended audience (the reporting guideline community) to understand and act upon. For example, I aggregated “ask authors to cite reporting guidelines” and “display citation metrics on reporting guideline resources” into a group about “Citations”, even though they target different barriers (discoverability and perceived trustworthiness) and employ different intervention functions (education and persuasion). \nI discussed and refined my coding, aggregating and summarising with JdB. I sent the aggregated, summarised ideas to focus group participants and EQUATOR members, inviting them to check that it reflected their ideas faithfully. I also invited feedback from guideline developers who had shown an interest in the study but had been unable to attend a focus group.\n\n\nReflexivity & Trust\nIn chapter 7 I described my active role within my workshops with EQUATOR and I argued that my subjectivity was an asset within the workshops. In contrast, I tried to remain objective when running focus groups with external stakeholders, in order to capture the perspectives of participants without influencing them. My research paradigm for the focus groups was post-positivist, in that I considered that ideas were “out there”, but that differences in context, experience, and opinion would affect what I (and participants) observed, understood, and concluded.\nI wanted to ensure my results could be trusted as an account of participants’ views. Lincoln and Guba [6] argue that for a study to be trustworthy, the researcher must show that the findings are credible (‘true’), transferable (applicable to other contexts), dependable (consistent and repeatable), and confirmable (shaped by participants, not by the researcher’s bias or motivation). Lincon and Guba propose a number of techniques to achieve these criteria, and I describe the techniques I used in Table 1.\nASK CA: I haven’t named an approach. I’ve tried to be clear with what I did, but I’ve struggled to find the correct label for it. We talked previously about Active Research but I don’t think this is correct for this chapter.\n\n\nTable 1: Techniques for establishing trustworthiness. Based on Lincoln and Guba’s Evaluative Criteria [6]\n\n\n\n\n\n\nTechnique\nImplementation\n\n\n\n\nTechniques for establishing credibility\n\n\n\nMember-checking\nLincoln and Guba argue that member checking is the most important way to the establish validity of an account [6]. Accordingly, I invited participants to comment on my synthesised results, asking for feedback on the structure of categories, my interpretation of their data, and my findings and conclusions. I also 3 invited participants to comment on the product of my data analysis in the form of itemized information and condensed notes.\n\n\nPeer debriefing\nThroughout the design, data collection, analysis and reporting, CA acted as a disinterested peer. By questioning my reasoning and exploring my assumptions, she helped me become aware of biases, perspectives that I was taking for granted, and assumptions I was making. JdB acted as a disinterested peer during data analysis.\n\n\nTechniques for establishing transferability\n\n\n\nThick description\nI aspired to report my results with context by indicating when ideas were common or rare, and who they originated from when I felt this was particularly relevant. I reported disagreements, provide quotes, and relationships between ideas.\n\n\nTechniques for establishing confirmability\n\n\n\nAudit trail\nI referred to audio recordings of the focus groups whenever I needed to clarify parts of the document. I kept versions of raw data collected from all stages. I made a note of my own ideas before commencing data collection, documented all stages of the workshops I held internally with the EQUATOR Network, and kept copies of the co-produced file after every focus group. I kept a copy of my coding in NVivo, and versions of the unitized information and summaries that I sent to participants before and after member checking. This audit trail meant that I could be certain of which stages of research ideas originated from.\n\n\nReflexivity\nI wrote down my own ideas before commencing the study (see chapter 7), along with my beliefs and experiences of reporting guidelines. I continued to keep personal notes throughout planning, data collection, analysis and reporting, in an attempt to remain aware of my own perspectives and positions, and how they may influence my research.\n\n\n\n\n\n\n\n\nEthics & Data Management\nThe study was approved by the Medical Sciences Interdivisional Research Ethics Committee (R80414/RE001). Participants gave informed consent by completing an online form. Participant’s edits to the co-produced file were anonymous. I recorded the audio of focus groups so that I could refer to them during analysis, if necessary. All data and recordings were kept on secure university storage."
  },
  {
    "objectID": "chapters/8_focus_groups/index.html#results",
    "href": "chapters/8_focus_groups/index.html#results",
    "title": "Generating ideas to address factors limiting reporting guideline impact: workshops with EQUATOR and focus groups with developers, publishers, and experts",
    "section": "Results",
    "text": "Results\n\nUnits of study\nI held 7 focus groups involving 16 participants in total. Participants included guideline developers (n=11), publishing professionals (n=3), and academics that study reporting guidelines (n=2). Although I had intended to include 4 or 5 participants per focus group, in practice it was difficult to coordinate participants across time zones, and so sessions only had 2 or 3 participants.\nBecause I invited people to share the invitation I have no way of knowing my recruitment rate. Of the 23 invitations that I sent in total, 7 received no response. Of the 15 guideline groups I invited, 5 took part. Of the remainder, 4 guideline groups wanted to participate but were unable to coordinate a time, 5 groups did not respond, and 1 group refused to participate; they felt that their guideline didn’t need updating because it was highly cited.\nBefore the focus groups, my workshops with EQUATOR had generated a list of TODO ideas, which formed the initial “ideas document” presented to the first focus group. After the final focus group, participants had extended this list further to include 128 ideas to address 32 barriers. Participants identified 10 stakeholders that could enact these ideas including funders, ethics committees, institutions, publishers, equator network, guideline developers, registries, preprint servers, conference organisers, and societies. I grouped these ideas into 28 broader ideas, which I categorised according to whether they could be considered before developing guidance, when developing guidance, when writing guidance down and creating resources, when disseminating resources, or on an ongoing basis.\nIn the summary below I have occasionally mentioned which stakeholder group an idea came from, but only when I felt like it added useful context. I have chosen not to label ideas according to the stakeholder it came from for three reasons. Firstly, stakeholders were all editing the same file, so some ideas would be revisited multiple times by different stakeholders who would build upon the thoughts of previous contributors, editing and extending ideas. Consequently, it wasn’t always possible to definitively say who any particular idea came from, as it may have been the product of multiple stakeholders. Secondly, just because a stakeholder didn’t edit an idea in the document didn’t mean that the idea hadn’t also occurred to them. Hence allocating an idea to a stakeholder just because they were the one that wrote it down may be misleading. Thirdly, I didn’t consider labelling the origin of an idea to be useful because I didn’t see it as an indication of that idea’s quality; to the contrary, I had explicitly encouraged participants not to worry about whether an idea was “good”.\n\n\nSynthesis and summary\nHere I describe the aggregated ideas. I use the term “stakeholders” instead of “participants” to clarify that ideas came from the focus group participants and the workshop participants.\n\nBefore developing guidance\n\nCreate reporting guidance for protocols and applications\nStakeholders suggested “developing [reporting guidance] for protocols”, funding, and ethics applications as a way to encourage authors to consult reporting guidance earlier in their work when they are more likely to have the time, motivation, and ability to reflect and act on it.\n\n\nAvoid confusing authors with too many reporting guidelines\n“We need fewer, better, reporting guidelines” wrote one stakeholder, when discussing how authors may struggle to identify which reporting guidelines apply to their work.\nAcknowledging that reporting guideline developers may duplicate each other’s work unwittingly, stakeholders wrote that developers should consult EQUATOR’s register of reporting guidelines under development before creating a new guideline. Stakeholders noted that “EQUATOR cannot prevent guideline developers from creating duplicate guidelines” but could “improve the registration process for reporting guidelines that are under development”, better “highlight [reporting guidelines] that are under development in the main search results”, and could “create options and instructions to encourage developers to extend existing guidance instead of duplicating. This could go in the new guidance for guideline developers”.\nStakeholders had ideas of how this “extend”ing could be done. They suggested that developers could tailor existing guidance to a particular niche by making different “versions” or “extensions” of existing guidance “e.g., STROBE split into STROBE Cohort, Case-control etc”. If only a few items need to be edited or added, stakeholders suggested creating “modular” guidance instead of duplicating an entire guideline. Stakeholders spoke of modules in two different ways. Firstly, new reporting guidelines can be created to substitute for particular items in existing reporting guidelines. Stakeholders named TIDIER as an example of this strategy, which can be substituted for item 5 in CONSORT. Stakeholders identified a second kind of modularity in the JARS guidelines, where a general reporting guideline covering all quantitative psychology research can be mixed-and-matched by modules for specific designs (non-experimental and experimental designs, with or without random assignment, or special modules for longitudinal, n-of-1, or replication studies [7]).\nWhen discussing how modules or extensions could be “harmonized” so that they “speak to another”, participants suggested resources could have compatible structure, and should not “use different wording for what is essentially the same item”. Participant’s recommendation to “use similar terminology [across] related guidelines” extended to the title; STROBE-nut, STROBE-ME and STROBE-RDS are more readily identifiable as STROBE extensions than STREGA, ROSES-I, or STROME-ID are. Naming can also indicate when a guideline has been revised: “ARRIVE 2.0 is recognisable as a replacement of ARRIVE”, whereas “TIDIER Placebo appears to be an extension but should be called TIDIER 2.0”.\n\n\nAvoid prescribing structure\nStakeholders suggested that “[reporting] guidelines should avoid prescribing structure as [it] may clash with journal guidelines”.\n\n\nKeep reporting guidelines agnostic to design choices\nStakeholders discussed how designing research is a separate task to reporting it, and that many authors will only encounter reporting guidelines after the manuscript has been written, at which point design advice is less useful.\nNevertheless, stakeholders cited multiple reasons for including design opinions in reporting guidance. Some suggested it should be included so that authors can “learn for next time”, and others wrote that consequences of design choices justify why an item was important to report, but acknowledged that that justifying items in this way can be problematic if developers do not consider all contexts or study types in which their guidance may be used.\nSome participants argued that design advice could be removed from reporting guidelines entirely. This was proposed as a solution when considering how authors may feel afraid to report transparently if what they did goes against the design recommendation, or may feel restricted if forced to use a reporting guideline that prescribes design choices.\nInstead, developers could encourage authors to “explain reasons for methods choices, [which may] be legitimate”, noting that “the consequence of not choosing one [design] option over another, even if the choice is a rarely used option, may not have major consequence on the results of the study”. One stakeholder wrote that authors may feel reassured if told that “editors and peer-reviewers may not judge as harshly when they understand the rationale for the choice”. Stakeholders wrote that developers should “encourage transparent reporting over and above good design”, and that authors could be encouraged to “describe what they did in plain language to make it clear - if what they did doesn’t quite fit with standard terminology (e.g., if they didn’t really do theoretical sampling or aren’t sure if they did theoretical sampling) then just describe what they did, how they made sampling decisions”.\nDiscussions about removing design advice also arose when considering how including design advice elongates guidance, potentially deterring authors from reading it. To solve this, stakeholders suggested reporting guidelines could link to design resources “elsewhere”. For example, if reporting guidance were presented on a website, authors could be given options to “display or hide” design advice by choice, or depending on whether the author was “designing [research], applying for funding, or drafting” a manuscript.\n\n\nDescribe reporting items fully\nMany barriers prompted stakeholders to suggest that specific content should be included for every reporting item.\nFirstly, stakeholders noted that authors need to know what to write, and that a brief description could go in checklists and a longer description in the full guidance. Stakeholders suggested this description should include what to write if they didn’t or couldn’t do something to make it “easy for researchers to report things they are embarrassed about”. In these instances, developers could suggest authors “explain reasons” for their choices or “consider the item in their discussion section as a possible limitation” if necessary. Stakeholders also wrote that developers could suggest what to write when an item doesn’t apply.\nStakeholders suggested explaining “why [an item] is important and who it is important to” as this isn’t always obvious to authors. To make guidelines faster to use, stakeholders suggested indicating which items are most important, perhaps “prioritise[ing] certain items as essential vs. recommended, like ARRIVE 2.0 essential 10” and indicating conditions that make items less or more important, including circumstances that make the item non-applicable.\nTo help authors who want to keep writing concise or need to reduce word counts, stakeholders suggested advising “when an item can be put into a table, figure, box, supplement, or appendix etc” and to “explain the pros and cons of different options e.g., whether content will be peer reviewed or indexed” by search tools.\nMany stakeholders suggested including examples to help authors understand and apply guidance. These examples could include both “good and bad reporting” with an explanation of “how [bad reporting] could be improved”. To help authors who are afraid to transparently report limitations, stakeholders suggested including examples of imperfect research that is perfectly-reported. Examples could also include “reporting in different contexts”, from different disciplines, “in multiple languages”, and concise reporting e.g. reporting “TIDIER items nicely in a table”.\nBecause examples from the published literature can be “difficult to […] find and list”, stakeholders suggested that examples could be “real or generated” by developers. Examples should be “easy to find” from within the guidance resources. Others suggested a searchable “bank” of examples, or ways to showcase “exemplar papers (e.g., badges)”.\nFinally, stakeholders discussed the pros and cons of including item-specific design advice, procedural instructions, and appraisal advice, and the different ways of doing it (see Keep reporting guidelines agnostic to design choices).\n\n\n?var:IFs.rg-introductions\nStakeholders identified that many of the barriers they discussed could be addressed by editing the text that authors might read before using a guideline or checklist.\nFor example, stakeholders suggested that reporting guidelines and resources should “clearly state what kind of research the guidance applies to” when considering how authors may struggle to identify which reporting guideline they should use. Additionally, stakeholders suggested guidelines could “point researchers to other guidelines if more relevant, perhaps using ‘if, then’ rules e.g., ‘if you did X then use Y instead of this guideline’”. If no better guideline exists for a particular type of study, stakeholders suggested to “warn the user that they can still use this guideline but that they may need to ignore certain questions e.g. ‘This guideline is for studies that did X. You can still use it if you did Y, but you will need to ignore items 4, 6, and 8-10’”.\nTo ensure that a reporting guideline for writing manuscripts can also be used for writing protocols, developers could specify “which items need to be considered at protocol/planning stage, and which at results reporting stage.”\nStakeholders warned that developers should be mindful when using words like standard instead of guideline when introducing a reporting guideline, as these words may influence how prescriptive the user expects the resource to be. In choosing their wording, stakeholders suggested developers should be honest and clear about a guideline’s “aim”. Developers could also “be clear about what reporting guidelines are not”. For instance, “when they are not design guidelines or critical appraisal tools” this could be specified and authors could be directed to other tools instead.\nNoting that it might not be obvious how or when to use guidance, participants suggested being explicit about this. For example, “tell authors that they don’t need to fill out templates [or checklists] sequentially but can use an order that matches their workflow or decision making. Put this instruction on the template, checklists, and other tools. Example from PRISMA – population and subgroup items are separated in the checklist but go together when thinking/making decisions.”. Similarly, authors could benefit from explicit suggestions of how to use reporting guidelines as a team, perhaps by asking “their co-researchers to check their reporting”. Finally, stakeholders suggested telling authors how much time the guideline is expected to take them to read and use, why it can be trusted, and where they can read about its development.\n\n\nKeep guidance short\n“Make guidance shorter” wrote one stakeholder, and “Make checklists shorter”, wrote another, when considering how “guideline length […] may put researchers off” as “long guidelines appear more complex and time consuming” and can challenge word limits.\nOne stakeholder posited that guidelines could be shorter if developers were “realistic about what they ask for”. Another challenged developers to “try using a guideline from start to finish and see how the manuscript ends up”. Others wrote that guidelines could be shortened by linking out to content that wasn’t directly related to reporting, like design or appraisal advice (see Keep reporting guidelines agnostic to design choices). Guidelines that are “very long or [have] lots of optional items” could be split into multiple versions (see STROBE as an example in Avoid confusing authors with too many reporting guidelines).\nOthers suggested presenting guidance online with non-essential content collapsed so that guidance appeared short but authors could choose to “display or hide” content. Stakeholders noted that “itemisation can make guidance more digestible, but can also make it harder to get the bigger picture” and makes text appear longer.\n\n\n\nWhen developing guidance\n\nMake resources ready-to-use\nStakeholders suggested that resources should be in ready-to-use formats. For instance, “checklists should be editable (not PDFs)”.\n\n\nMake reporting guidelines easy to understand\n“Make guidance easier to understand” was written as a solution to help authors who misinterpret, or can’t understand, guidance. Stakeholders suggested developers could “make guidance as ‘plain language’ as possible” os “create plain language versions of existing guidance”, whilst being “mindful of language that may appear patronizing”.\nStakeholders suggested defining “key words or phrases in a glossary or tooltips”, using consistent terms across related resources, translating guidelines and examples, and ensuring these translations are easy to find by making them “searchable”, “linked properly” to each other, and “more evident on [the] EQUATOR site”.\nStakeholders recognised a need to collect and respond to feedback from “international researchers” representative of the user base across disciplines and institutions, and that this could be sought when developing guidance to “test” it, but also on an ongoing basis to “continually revisit the items in the guidelines that may be confusing or difficult to implement”. As an example, one guideline developer contributed that collecting feedback had lead them to remove “the word ‘context’ because users struggled to understand it”.\n\n\nUse persuasive language and design\nWhen discussing how authors feel about reporting guidelines and checklists, stakeholders reflected on how the wording and presentation of resources can influence authors’ perceptions of it.\nStakeholders noted that a poorly formatted checklist, lacking “visual appeal/graphic design”, can “appear outdated” or “larger than it is”. Stakeholders suggested that developers could use design to “foster feelings of simplicity”, and “engage graphic designers” if necessary.\nReassurance seemed especially important to stakeholders when considering how to motivate authors to transparently report items that aren’t perfect. Stakeholders suggested that developers could use language and tone of voice to “foster a feeling of confidence, not judgement”, perhaps reminding “researchers that all research has limitations”. Stakeholders also wrote that reassurance could also be given by including notes aimed at reviewers, and cited JARS as an example “which explains to reviewers why an author may make certain choices”, thereby educating reviewers whilst reassuring authors that they won’t be penalised for transparency. Stakeholders suggested rewording text and tone of voice to make reporting guidelines and checklists “appear less like ‘red tape’”, and to reassure authors that “reporting guidelines are just that: guidelines!”.\n\n\nCreate additional tools\nStakeholders noted that checklists may be easier to use if they are editable (see Make resources ready-to-use), and if authors could complete them with “the relevant text, rather than [the] page/section number” which can be frustrating to keep updated. Although one stakeholder, a publisher, noted that checklists completed with text are difficult to double check, and therefore it would be most useful to include the text and the page number.\nTo aid writing, stakeholders suggested “templates for drafting” manuscripts, interactive forms and “writing tools (e.g., COBWEB)”, “tools for creating figures and tables like PRISMA’s flowchart generator”, and tools for generating text, like TIDIER’s tool for generating intervention description (#REF). However, one stakeholder warned that these kinds of structured writing “provide opportunities for inclusion [of reporting items] but there is always the risk that they exclude more [important items] that are outside the boundaries of the template”.\nTo encourage authors to consider reporting guidance earlier in their research, stakeholders considered to-do lists with items “in the order they are done”, or “embed[ing] items into data collection tools” like software for systematic reviewers.\nTo help reviewers check reporting, stakeholders suggested creating “tools for co-researchers to check each others’ work”, creating extra guidance for peer reviewers, providing text that can be pasted in reviewer feedback forms to “request additional information” for poorly reported items, or even building a “reviewer tool that generates a report”.\nStakeholders noted that tools should presented in ways that “better differentiate” how and when they should be used “e.g., resources for writing vs. checking vs. reviewing”.\n\n\nMake resources easy to discover and find\nStakeholders had ideas for how to help authors discover and find resources. Stakeholders wrote about hyperlinks being an important way for authors to discover related guidelines and tools. These could be “horizontal links between related guidelines” and between guideline “documents and tools” like checklists. All resources should “link to one another”, ideally “item by item” so that checklist items could link directly to the relevant section of full guidance. Stakeholders noted that hyperlinks can become out of date, and so stakeholders should “fix broken links”.\nStakeholders also wrote that resources could be hosted on “a convenient place, such as a unified website”. EQUATOR’s website is one such place, and participants suggested making it “easier to navigate” and its “search tool more prominent and easier to use”. Although some authors will use such search tools, stakeholders recognised that those who browse may benefit from “curated” collections of reporting guidelines or a “manageable list of related, commonly used guidelines”. As an example, an author writing a review article may be helped by a page listing “PRISMA, MOOSE, ENTREQ, PRISMA-SRc etc.” and that “this page could be kept up to date as guidelines are revised”. Another stakeholder (a publisher) warned, however, that journals may not want to link to these pages if the collection has “a much broader scope” and includes guidelines that the journal doesn’t endorse.\nWhen discussing how to help authors who are less familiar with study designs, one stakeholder suggested creating “tools to help researchers identify study designs (e.g., a questionnaire)”, and another, already familiar with such a tool previously developed by EQUATOR suggested it should “use plain language”.\n\n\nMake information digestible\nStakeholders acknowledged that authors’ needs may differ between tasks (e.g. drafting an article vs demonstrating compliance), and that authors may use guidance in different ways; some will read the whole thing from start to finish, whilst others will dip in and out as-and-when they need. Consequently, stakeholders wrote that “having different options available that meet the needs of different users is vital” and that authors should be able to consult guidance in ways that “work for them”.\nOne suggested way of doing this may be to “structure guidance using navigation menus and subheadings” so that “it is easy to find the information you need”, making reporting guidelines faster to use and less overwhelming. Another noted that checklists can also be designed, citing TIDIER as “a nice example” that has “integrated the intervention and placebo into one table” with the active intervention and placebo in adjacent columns.\nDynamically hiding and showing content was floated here again (see Keep guidance short), with one stakeholder suggesting that users could “filter out” irrelevant content, to only see instructions for their task (e.g. planning, writing, reviewing) or specific to their study. This could be done with a “decision tree” or “branching questions” to determine specific features of the study (“e.g., a systematic review with network meta-analysis of individual participant data”). Answers to these questions could then be used to to “modify” items to create “personalised guidelines”, or to generate a “customised reporting checklist” that includes all “main and relevant extension items”.\nDynamic content was also seen as a favourable way to embed guideline extensions, with the aim of making them easier to discover without overwhelming the author. For example, noting that “some guidelines ‘fit together’…e.g., PRISMA and PRISMA-Abstracts”, stakeholders wrote that PRISMA-Abstracts could be “embedded” as collapsed content that interested authors could expand.\n\n\n\nWhen disseminating resources\n\nDescribe reporting guidelines where they are encountered\nWhen first introducing reporting guidelines stakeholders suggested telling authors what reporting guidelines are, “when and how best to use” them, and what benefits to expect. This information could go wherever authors are advised to use reporting guidelines (like like journal instruction pages, registries), EQUATOR’s website, social media campaigns, at the start of the guidelines, and could go at the beginning of checklists too “in case people don’t read the whole [guideline] paper”. This introductory text could be “short, sweet, and to the point”. Benefits could be even more prominent by putting them “in a box, or [by using] font or positioning”.\n\n\n\nMake resources accessible\nStakeholders wrote “Ensure guidance is open access” so that all authors can it. Stakeholders also noted that if guidance is published under a permissive license then others can reuse the content to extend the guidance or build new tools.\n\n\nShow and encourage citations\nDisplaying citation counts on the EQUATOR Network website (or other websites where authors search for reporting guidelines) was described as a way to “provide social proof” and convince authors that guidelines are credible.\nTo generate these citations, stakeholders suggested explicitly asking “researchers to cite the guideline they used”. Stakeholders wrote that if an author cites a guideline they have used, then readers may discover the guideline from that authors’ article.\n\n\nProvide testimonials\nStakeholders suggested providing “testimonials” as a way to tackle a few different barriers using education and persuasion. Stakeholders suggested providing “quotes from authors/researchers who felt that reporting guidelines helped their work and who have had positive experiences” such as making “writing easier” or helping “with co-authorship communications”. Stakeholders proposed that testimonials could bring benefits to life, thereby making them more believable.\nTo make authors care more about research waste caused by poor reporting, stakeholders suggested testimonials from “research consumers for whom an item is important”, or quotes that illustrate “how detrimental poor reporting is for end users”.\nStakeholders wrote that testimonials from decision makers (like editors, reviewers, and grant-givers) could communicate their “preference for transparent reporting” and convince authors that reporting will be checked. If these testimonials conveyed that transparency is valued above perfectionism, participants suggested this could reassure authors. Stakeholders also suggested collecting positive testimonials from such “nervous researchers”.\nFinally, stakeholders suggested collecting testimonials from researchers “with a range of experience”, including “experienced researchers who have benefited by changing their practices”. Diverse case studies would help engage a diverse user base, and challenge assumptions that reporting guidelines are too patronizing for experienced researchers or too complicated for inexperienced ones.\n\n\n\nOn an ongoing basis\n\nBudget for reporting\nStakeholders noted that “researchers need budget to allocate time to writing” and that “funders could encourage proper financial/time budgeting for writing”, as could research supervisors.\n\n\nCreate rewards\nStakeholders suggested “offer[ing] some sort of tangible reward/benefit” to motivate guideline use, creating new rewards when necessary. Ideas included “publishers offering a fast-track review/discount”, “badges on published articles” or platforms “like publons”, or “a certificate after completing training”.\n\n\nCreate discussion spaces\nMultiple barriers lead stakeholders to suggest “create[ing] spaces for researchers to connect with other researchers to celebrate and share experiences”. These spaces could include “forums, meetings, tea clubs, [and] clinics both in real life and virtual”. Such spaces could help authors solicit help and could act as social proof, as seeing “others using and talking about the guidance” may be motivational.\nOnline discussion spaces were also considered a useful way to gather feedback from users directly (by asking for it) and indirectly (by monitoring discussions). Stakeholders wrote that feedback channels “could be useful to guideline developers”, and may also “cultivate a feeling of community ownership” by “communicating an invitational attitude”, thereby making guidelines appear less bureaucratic.\n\n\n\nCreate ways to catch authors earlier\nStakeholders thought of ways to “try and shift the time at which researchers discover or use guidelines”, hypothesising that “it’s more likely that guidelines will save them time” if used earlier or “at the right time” and “not just upon submission”.\nMost simply, stakeholders suggested “telling” or “encouraging” authors to use reporting guidelines for planning or drafting research (and not just for demonstrating compliance upon submission). Building upon this, stakeholders suggested organising the EQUATOR Network website to make it obvious which stages of work resources can be used for.\nStakeholders suggested “including [reporting guidelines] in the university teaching and training curriculum and text books” so that students learn about them before running or writing up their first study.\nStakeholders suggested creating reporting guidance for early research outputs like funding applications and protocols (as previously described in Create reporting guidance for early stages of research), and advertising resources through funders, ethics committees, and writing training programmes.\nWhen considering whether authors may need reminders to use a reporting guideline for their next study, stakeholders suggested publishers and EQUATOR could use “email reminders” or strategies used by e-commerce sites “like when you buy something from an online business…then they work hard to gain your custom again”.\n\n\nEndorse and enforce reporting guidelines\nStakeholders suggested “encouraging more journals to endorse guidelines” and drew a distinction between endorsing reporting guidelines and promoting them on a website, social media, or email (see Promote reporting guidelines). Endorsement was described as a long term commitment to recommend or encourage guideline use, requiring buy-in from organisational leaders, and possibly changes to policies, instructions, infrastructure, and workflows. Promotion, conversely, was described as ephemeral and does not require organisational changes.\nStakeholders drew another distinction between endorsement and enforcement, whereby enforcement meant reporting guidelines are “a requirement” or “condition”. Enforcement was further divided into enforcing checklist completion or, noting that checklists may not always accurately reflect manuscript content, checking text for adherence to guidance.\nWhen considering who could enforce guidelines, stakeholders noted that reporting guidelines could be “a requirement for publication”, “for registration (where applicable) (e.g. clinical trial registries, PROSPERO)”, or when submitting conference abstracts. “Ethics committees and funding organisations” could require that adhere to guidelines, “for example, completion of SPIRIT for clinical trial submissions”, or to declare that they will use a guideline when writing their results. To facilitate enforcement, stakeholders suggested that the software academics use to provide funders with updates could ask for completed checklists. One stakeholder suggested that reporting guideline adherence should be a condition of university employment and that a “digital dashboard [may] help audit[ing] and monitoring”. Noting that enforcement requires resources, stakeholders suggested to “focus on main RGs and being compliant with them”.\n\n\nEvidence the benefits\nStakeholders suggested that benefits may be more believable if there were evidenced. This could be “evidence that [reporting guidelines] improve the completeness and transparency of the output”. For quantifiable benefits, the suggestion was to collect and report data on “acceptance rates, publishing speed, writing speed”. One stakeholder posited that “more transparent reporting / structured reporting may lead to faster editorial processes as it becomes easy for peer reviewers and editors to review papers about their study” and another suggested to “provide statistics about processing times of articles that follow / don’t follow reporting standards” would help evidence this claim and “emphasise to researchers that clear reporting will minimise the number of times others contact them for clarification”. However, some stakeholders were sceptical whether data on acceptance rates would show any benefit at all: “Likelihood of being accepted might not be heavily influenced – bad research well reported would still be rejected”.\nFor experiential benefits that cannot be quantified, stakeholders suggested providing case studies or testimonials (see Provide testimonials).\n\n\nMake reporting guidelines appear as a priority\nWhen a journal endorses or enforces reporting guidelines, a few stakeholders suggested making reporting guidelines more prominent within the journal’s workflow to make them appear more important. Notes included that “reporting guidelines could be more prominent on journal author guideline pages”, or that “if the journal uses any sort of structured peer review (e.g., specific questions related to methodology) to tell authors this explicitly [on author guideline pages and within review feedback] and link it to the reporting guideline content”. A third suggestion was that “when journals ‘stitch’ or ‘build’ together the manuscript pdf (including cover letter, manuscript main text, appendices, etc.), prioritise the reporting guideline or move it earlier in the pdf”.\nHowever, a few stakeholders (publishers) warned that prioritising guidelines on author instruction pages “is complicated as these [pages] already have to do a lot”, although guidelines could be more prominent if the pages “were better organised and/or filterable”.\n\n\nPromote reporting guidelines\nStakeholders noted may bodies that could help spread the word that reporting guidelines exist. For example, “professional societies” could “advertise” reporting guidelines, despite not having a role in the funding, regulation or dissemination of research.\nPromotion could occur online. Most obviously, on stakeholder guidance web pages. “Email campaigns, social media, blogs” could be useful channels to “share and connect with others [and] drive traffic to guideline website[s]”, but “these require time and energy” from the reporting guideline community to set up and manage.\n“Conferences and workshops at institutions” were cited as channels to promote reporting guideline offline, as were ” seminars, webinars, and presentations” especially in “hard to reach countries/fields”. These events were described as useful ways “to assist in the interpretation and use of the guidelines” and could be opportunities for “universities/funders/journals [to speak] together about the importance of reporting guidelines”.\nTo reach students, participants suggested that universities could include reporting guidelines in their curricula, learning materials, or through reporting champions (see Install reporting champions).\nStakeholders wrote that “promotion can begin before a guideline has been published so that researchers know about guidelines being developed” and suggested “the provision of a time buffer/phasing period for updating new reporting guidelines which would allow researchers to have information about these new guidelines”.\n\n\nInstall reporting champions\nStakeholders wrote that publishers, universities, funders, and ethics committees could have members to promote and facilitate the usage of reporting guidelines, centring on the terms “champions” and “EQUATOR ambassadors”. Within publishers, funders, or ethics board, a champion’s responsibility may be to “expand knowledge/awareness of guidelines”. Within institutions, champions could also “help researchers” by “providing feedback on writing” and that ECRs may feel most comfortable talking with a champion from “an accessible level (e.g., post-doc, library staff)”. This could follow a local network model (UKRN was cited as an example) with EQUATOR as the central organiser, and could utilize existing reproducibility networks.\n\n\nProvide additional teaching\nStakeholders proposed additional teaching as a way to promote reporting guidelines, make them easier to use, and a way to communicate the impacts of poor reporting. Education and training could be general (EQUATOR’s publication school was cited as an example) of could be “guideline-specific”, and could be delivered in person or online, as courses, videos, or text.\nIn addition to learning about a particular guideline, stakeholders suggested that students could learn about “writing as a process” and “workflows for documenting and communicating research”. This was considered useful as “researchers don’t necessarily understand that reporting is a stage in the research process”. Curricula could include “methods studies that indicate the research waste” to teach students why reporting matters, or students could learn for themselves by attempting “to replicate a study or do a systematic review to discover how poorly research is currently reported”.\nTo gain experience in using reporting guidelines, stakeholders suggested students could develop “research protocols (as Bachelor or Master Theses) using reporting guidelines” and that these could “be assessed based on the compliance with the appropriate reporting guideline” in addition to “other criteria more related with methodology”. To make this easier, stakeholders suggested structuring courses around reporting guideline items “for example: a course on [randomized controlled trials] covering every single SPIRIT or CONSORT item”.\n\n\n\nMake updating guidelines easier\nStakeholders acknowledged that “researchers have opinions on how the guidance could be improved including how to make it clearer, and whether items should be rearranged, separated, combined, added or removed” and that “this information could be useful to guideline developers”. Some stakeholders went further, expressing that guidance and websites should be updated “in response to user feedback or changes in the field”. Others suggested that “developers could consult different user groups when creating guidance” and “engage as many health professions as possible” so that “professional cultural issues can be usefully accommodated.”\nHowever, stakeholders were not forthcoming on how to go about gathering this feedback. One wrote “provide ways for researchers to give feedback to guideline developers” without suggesting any ways to do this. Another simply asked “how can we enable users to give feedback on guidance?”. Many guideline developers noted that they required access to extra funding to evaluate, refine, and update their resources. One developer suggested that “minor updates could be made without publishing a new article” if the guidance were disseminated on a website."
  },
  {
    "objectID": "chapters/8_focus_groups/index.html#discussion",
    "href": "chapters/8_focus_groups/index.html#discussion",
    "title": "Generating ideas to address factors limiting reporting guideline impact: workshops with EQUATOR and focus groups with developers, publishers, and experts",
    "section": "Discussion",
    "text": "Discussion\n\nDescription of findings\n\n\n\nSummary of results\nMy objective was to identify ideas to address factors that may influence adherence to reporting guidelines by running workshops with the EQUATOR Network and focus groups with guideline developers, publishers, and other experts. Stakeholders identified 128 ideas employing all intervention functions. There were ideas to consider before, during, and after creating guidance, ideas to consider when writing the guidance down, ideas about tools to help guidance application, ideas about ongoing activities to support or promote guidance use, and ideas about refining guidance over time in response to feedback. Many of these ideas could be enacted by guideline developers, publishers, and EQUATOR, but participants also saw opportunities for ethics committees, funders, academics, registries, and syllabus writers; stakeholders who are typically less frequently considered.\nI believed that including perspectives from a range of stakeholders would lead to more ideas. This seems to be the case: EQUATOR staff thought of TODO which was then expanded to 128 through the focus groups.\nThis is the first time that guideline developers from different groups have come together with publishers and academics to consider reporting guidance dissemination as a system. I had a fair response rate from guideline groups and of their responses were generally supportive, even if they were unable to take part. Multiple guideline developers volunteered their guideline to be a “guinneapig” and expressed support for my work. All stakeholders were open minded to the barriers I presented except for one developer who expressed scepticism that reporting guidelines were anything but perfect, and requested additional evidence of authors’ negative experiences.\n\n\n\n\n\n\nImplications\nThese results will also be of interest to the reporting guideline community. Publishers, funders, and institutions will find food for thought on how to create effective policies to support reporting guidelines. Many of the ideas could be enacted by the EQUATOR Network. Guideline developers may find inspiration here when writing or revising guidance. At the time of writing, the EQUATOR Network was in the process of updating its advice for guideline developers, which I hope will be informed by these results. The ideas generated may also be of interest to publishers, funders, and Universities.\nTODO: elaborate on what the guidance for guideline developers covers currently, how that could be extended by the ideas here, and perhaps contrast that with what happens in reality.\n\nAlthough only a few reporting guideline development groups took part in this study, most ideas identified were abstract enough to generalise to most reporting guidance, which tend to be developed and distributed in similar ways; (e.g., all development groups will have to consider what guidance to create, its scope, how to communicate it clearly and how to disseminate it). Some ideas may even generalise to other interventions to encourage good research practices (e.g., to communicate personal benefits, not to patronize researchers).\n\n\nLimitations\nThe study may have benefited from more diversity in participants’ backgrounds and expertise. We could have recruited participants from funders, ethics committees, or registries, behaviour change experts or experts familiar with user experience of websites or written documents. All participants were western and proficient in English. Although this is partly a limitation of who writes reporting guideline, we could have sought input from publishing houses that cater to non-western authors. Broadening the participant pool in this way would have led to more ideas.\nMy focus groups were smaller than I had planned. Some would consider these group sizes too small to be called focus groups, and may instead call them paired interviews, dyads, or triads (#REF). A hallmark of focus groups is that they use “group interaction to produce data” [8], and these interactions may include sharing experiences and challenging each other. However, I did not feel the small group sizes to be a limitation in this study for two reasons. Firstly, because participants were co-editing a file and building upon the thoughts of previous groups, participants could react and respond to to participants from previous groups. Secondly, participants had deep understandings of the topic (evidenced by sessions overrunning and participants dwelling on a single topic) which meant that even pairs of participants had plenty to discuss, share, and debate. If I had condensed participants into, say, 3 groups of 5-6 participants, each participants would have had less time to speak and I anticipate that many ideas would have gone un-spoken.\n\n\nFuture work\nI purposefully did not seek input from authors as this study required input from experts familiar with reporting guideline dissemination. Instead, I describe how I sought input from authors in chapter 11. I also purposefully did not ask stakeholders to prioritize or rank ideas. In chapter 7 I explained that prioritization is subjective and, therefore, should be done by stakeholder separately. I also described how EQUATOR began to prioritize intervention options. In the next chapter I describe how I decided which ideas to act on and how I turned them into intervention components.\n\n\n\n\n\n1. Michie S, Atkins L, West R (2014) The Behaviour Change Wheel: A Guide to Designing Interventions. Silverback Publishing, London\n\n\n2. Given L (2008) Focus Groups. In: The SAGE Encyclopedia of Qualitative Research Methods. SAGE Publications, Inc., Thousand Oaks, pp 353–354\n\n\n3. Malterud K, Siersma VD, Guassora AD (2016) Sample Size in Qualitative Interview Studies: Guided by Information Power. Qualitative Health Research 26:1753–1760\n\n\n4. Bradshaw C, Atkinson S, Doody O (2017) Employing a Qualitative Description Approach in Health Care Research. Global Qualitative Nursing Research 4:2333393617742282\n\n\n5. Kim H, Sefcik JS, Bradway C (2017) Characteristics of Qualitative Descriptive Studies: A Systematic Review. Research in nursing & health 40:23–42\n\n\n6. Lincoln YS, Guba EG (1985) Naturalistic Inquiry. SAGE\n\n\n7. Appelbaum M, Cooper H, Kline RB, Mayo-Wilson E, Nezu AM, Rao SM (2018) Journal article reporting standards for quantitative research in psychology: The APA Publications and Communications Board task force report. American Psychologist 73:3–25\n\n\n8. L.Morgan D (1997) Focus Groups as Qualitative Research. https://doi.org/10.4135/9781412984287"
  },
  {
    "objectID": "chapters/1_introduction/7_summary.html",
    "href": "chapters/1_introduction/7_summary.html",
    "title": "Thesis",
    "section": "",
    "text": "In summary, I’ve explained that a lot of medical research is poorly reported, and that this makes it difficult for other researchers to understand, appraise, synthesize, or replicate studies. This, in turn, makes research less useful to patients.\nI’ve introduced reporting guidelines, created by the research community with the aim to improve reporting quality. I’ve described the system of tools, websites, people, and policy that has organically grown around reporting guidelines, and I have argued that this system forms a complex behaviour change intervention with the goal of altering what authors write.\nI’ve discussed how this system has had only a modest effect on reporting quality, at best. I’ve described how studies exploring modifications to this system are limited because they did not explore barriers thoroughly, and lacked a systematic method to identify options to address those barriers."
  },
  {
    "objectID": "chapters/1_introduction/7_summary.html#summary",
    "href": "chapters/1_introduction/7_summary.html#summary",
    "title": "Thesis",
    "section": "",
    "text": "In summary, I’ve explained that a lot of medical research is poorly reported, and that this makes it difficult for other researchers to understand, appraise, synthesize, or replicate studies. This, in turn, makes research less useful to patients.\nI’ve introduced reporting guidelines, created by the research community with the aim to improve reporting quality. I’ve described the system of tools, websites, people, and policy that has organically grown around reporting guidelines, and I have argued that this system forms a complex behaviour change intervention with the goal of altering what authors write.\nI’ve discussed how this system has had only a modest effect on reporting quality, at best. I’ve described how studies exploring modifications to this system are limited because they did not explore barriers thoroughly, and lacked a systematic method to identify options to address those barriers."
  },
  {
    "objectID": "chapters/1_introduction/3_2_complex_interventions.html",
    "href": "chapters/1_introduction/3_2_complex_interventions.html",
    "title": "Thesis",
    "section": "",
    "text": "The publishing community began to take note. The International Committee for Medical Journal Editors encouraged journals “to ask authors to follow [reporting] guidelines” [1]. Concerned editors sought ways to adopt reporting guidelines, and more and more journals [2]; [3] have since found a range of strategies to introduce reporting guidelines into their policies, outlined in Table 1. There is variation in the degree of enforcement (from passive recommendation through to compulsory enforcement) and variation in the guidelines subject to the policy; PRISMA and CONSORT and more commonly enforced than STROBE, say, and many reporting guidelines in EQUATOR’s library are not enforced nor endorsed by any journals. Instead of listing reporting guidelines by name, many journals keep their instructions vague and merely recommend authors find an appropriate guideline on EQUATOR’s website.\n\n\nTable 1: Examples of how journals have introduced reporting guidelines into their policies. Journals also differ in the reporting guidelines they enforce. For example, some journals may only have policies for randomised trials or systematic reviews, whereas other journals may enforce guidelines for other study types. To my knowledge, no journal explicitly advises against using a reporting guideline.\n\n\n\n\n\n\nEnforcement type\nExample\n\n\n\n\nEnforcing adherence\nAn editor or peer reviewer checks the article body for reporting guideline adherence and asks the author to add missing items.\n\n\nRequesting peer reviewers use reporting guidelines\nEditors ask peer reviewers to consider reporting guideline adherence as part of their review. Some editors may supply the reviewer with the relevant checklist. The reviewer can choose whether to review reporting.\n\n\nEnforcing checklist submission\nEditorial staff require authors to submit a completed reporting checklist as part of manuscript submission. Some journals may refuse to process a submission when the checklist is missing. Some journal submission systems may include fields for authors to upload their checklists, whereas other journals may expect authors to upload checklists as a supplementary file.\n\n\nJournal endorsement\nThe journal’s instructions to authors recommends authors follow reporting guidelines. Guidelines may be specified, in which case journals may link to guideline specific websites, to the guideline publications, or to the EQUATOR Network website. Sometimes journals include a general statement but do not name guidelines, instead referring authors to the EQUATOR website with an instruction to follow “relevant guidance”.\n\n\nPublisher endorsement\nSometimes reporting guideline policies are set at the level of the publisher, as is commonly done for editorial policies. Individual journals may point authors to their publisher policies.\n\n\nNo policy\nJournals have no policies regarding reporting guidelines.\n\n\n\n\nOther stakeholders have begun incorporating reporting guidelines into their policies. Conferences like the Peer Review Congress ask applicants to use reporting guidelines when writing their abstracts. MedRxiv, a large preprint server, asks authors to declare they have “followed all appropriate research reporting guidelines, such as any relevant EQUATOR Network research reporting checklist(s)” [4].\nEQUATOR have developed training programmes based on reporting guidelines. The training covers different ways to use reporting guidelines, including drafting manuscripts, checking manuscripts you have written, and appraising the reporting of someone else’s manuscript. Researchers have developed writing software to help authors apply reporting guidelines when drafting [5, 6], online applications to facilitate checklist and flow diagram completion [7, 8], and resources for reviewers to check compliance [9].\nHence over the years, a system has organically grown around reporting guidelines, driven by separate groups of people doing what they felt was sensible. This system includes the guidance resources themselves (the publications, checklists, flow diagrams), the websites that host those resources (guideline websites, the EQUATOR website, publisher’s websites), organisations that promote or enforse their use (EQUATOR, publishers, ICMJE), and staff at those institutions (researchers, editors, reviewers). These components all have the same aim: to influence what information researchers include in their articles.\nAs the Medical Research Council notes, a system with multiple components (like those listed above) is one of many hallmarks of a complex intervention: “an intervention might be considered complex because of properties of the intervention itself, such as the number of components involved; the range of behaviours targeted; expertise and skills required by those delivering and receiving the intervention; the number of groups, settings, or levels targeted; or the permitted level of flexibility of the intervention or its components.”. The reporting guideline system exhibits these sources of complexity, as described in Table 2.\n\n\nTable 2: Sources of complexity within reporting guidelines and the system drives their use.\n\n\n\n\n\n\nSource of complexity\nExample\n\n\n\n\nNumber of components involved\nReporting guidelines often consist of guidance documents, checklists, and flow diagrams, and other tools. These are disseminated through websites, publishing platforms, submission systems, and they are endorsed and enforced by staff at stakeholders including publishers, the EQUATOR Network, conference organisers, and pre-print platforms.\n\n\nRange of behaviours targeted\nGuidelines comprise “reporting items”. Some items are relatively simple, like asking the author to specify their study design in the title. Others are harder, perhaps because they require time, expertise, or prerequisite tasks. For instance, some items may require authors to have conducted their study or analysis in a certain way, or to have collected particular information.\n\n\nExpertise and skills required by those delivering and receiving the intervention\nAcademics from a particular field write reporting guidelines for their peers (as opposed to a lay audience), and so authors, editors, and reviewers must have sufficient expertise to use them.\n\n\nThe number of groups, settings, or levels targeted\nGroups: Users of reporting guidelines differ in their field of expertise, their experience, place of work.\n\nSettings: Although mostly written with authoring in mind, most guideline developers may also hope their resources are used by editors or peer reviewers for checking or appraising research articles.\n\nGuidelines are written with individuals in mind, but their efficacy is generally measured at group level (e.g. articles from a particular field published in a period time).\n\n\nFlexibility of the intervention or its components\nThere is variation between guideline content, resources, and the implementation strategies that development groups, publishers, and other stakeholders employ.\n\n\n\n\nViewing reporting guidelines as part of a complex behaviour change intervention may seem novel. Researchers often call reporting guidelines “tools” or “strategies” (#REF). In “A history of the EQUATOR Network”, Doug Altman refers to reporting guidelines as “resources” that “influence” reporting, but he does not call them interventions. However, I believe my perspective is not radical. I will now outline studies exploring the efficacy of reporting guidelines, and argue that these studies take a systems perspective too although they seldom acknowledge it explicitly.\n\n\n\n\n1. ICMJE | Recommendations | Preparing a Manuscript for Submission to a Medical Journal. \n\n\n2. Koch M, Riss P, Umek W, Hanzal E (2016) The explicit mentioning of reporting guidelines in urogynecology journals in 2013: A bibliometric study. Neurourology and Urodynamics 35:412–416\n\n\n3. Sharp MK, Tokalić R, Gómez G, Wager E, Altman DG, Hren D (2019) A cross-sectional bibliometric study showed suboptimal journal endorsement rates of STROBE and its extensions. Journal of clinical epidemiology 107:42–50\n\n\n4. medRxiv.org - the preprint server for Health Sciences. \n\n\n5. Barnes C, Boutron I, Giraudeau B, Porcher R, Altman DG, Ravaud P (2015) Impact of an online writing aid tool for writing a randomized trial report: The COBWEB (Consort-based WEB tool) randomized controlled trial. BMC Medicine 13:221\n\n\n6. Hawwash D, Sharp MK, Argaw A, Kolsteren P, Lachat C (2019) Usefulness of applying research reporting guidelines as Writing Aid software: A crossover randomised controlled trial. BMJ Open. https://doi.org/10.1136/bmjopen-2019-030943\n\n\n7. Haddaway NR, Page MJ, Pritchard CC, McGuinness LA (2022) PRISMA2020: An R package and Shiny app for producing PRISMA 2020-compliant flow diagrams, with interactivity for optimised digital transparency and Open Synthesis. Campbell Systematic Reviews 18:e1230\n\n\n8. Struthers C, Harwood J, de Beyer JA, Dhiman P, Logullo P, Schlüssel M (2021) GoodReports: Developing a website to help health researchers find and use reporting guidelines. BMC medical research methodology 21:217\n\n\n9. Compliance Questionnaire | ARRIVE Guidelines."
  },
  {
    "objectID": "chapters/1_introduction/3_2_complex_interventions.html#reporting-guidelines-are-part-of-a-complex-behaviour-intervention",
    "href": "chapters/1_introduction/3_2_complex_interventions.html#reporting-guidelines-are-part-of-a-complex-behaviour-intervention",
    "title": "Thesis",
    "section": "",
    "text": "The publishing community began to take note. The International Committee for Medical Journal Editors encouraged journals “to ask authors to follow [reporting] guidelines” [1]. Concerned editors sought ways to adopt reporting guidelines, and more and more journals [2]; [3] have since found a range of strategies to introduce reporting guidelines into their policies, outlined in Table 1. There is variation in the degree of enforcement (from passive recommendation through to compulsory enforcement) and variation in the guidelines subject to the policy; PRISMA and CONSORT and more commonly enforced than STROBE, say, and many reporting guidelines in EQUATOR’s library are not enforced nor endorsed by any journals. Instead of listing reporting guidelines by name, many journals keep their instructions vague and merely recommend authors find an appropriate guideline on EQUATOR’s website.\n\n\nTable 1: Examples of how journals have introduced reporting guidelines into their policies. Journals also differ in the reporting guidelines they enforce. For example, some journals may only have policies for randomised trials or systematic reviews, whereas other journals may enforce guidelines for other study types. To my knowledge, no journal explicitly advises against using a reporting guideline.\n\n\n\n\n\n\nEnforcement type\nExample\n\n\n\n\nEnforcing adherence\nAn editor or peer reviewer checks the article body for reporting guideline adherence and asks the author to add missing items.\n\n\nRequesting peer reviewers use reporting guidelines\nEditors ask peer reviewers to consider reporting guideline adherence as part of their review. Some editors may supply the reviewer with the relevant checklist. The reviewer can choose whether to review reporting.\n\n\nEnforcing checklist submission\nEditorial staff require authors to submit a completed reporting checklist as part of manuscript submission. Some journals may refuse to process a submission when the checklist is missing. Some journal submission systems may include fields for authors to upload their checklists, whereas other journals may expect authors to upload checklists as a supplementary file.\n\n\nJournal endorsement\nThe journal’s instructions to authors recommends authors follow reporting guidelines. Guidelines may be specified, in which case journals may link to guideline specific websites, to the guideline publications, or to the EQUATOR Network website. Sometimes journals include a general statement but do not name guidelines, instead referring authors to the EQUATOR website with an instruction to follow “relevant guidance”.\n\n\nPublisher endorsement\nSometimes reporting guideline policies are set at the level of the publisher, as is commonly done for editorial policies. Individual journals may point authors to their publisher policies.\n\n\nNo policy\nJournals have no policies regarding reporting guidelines.\n\n\n\n\nOther stakeholders have begun incorporating reporting guidelines into their policies. Conferences like the Peer Review Congress ask applicants to use reporting guidelines when writing their abstracts. MedRxiv, a large preprint server, asks authors to declare they have “followed all appropriate research reporting guidelines, such as any relevant EQUATOR Network research reporting checklist(s)” [4].\nEQUATOR have developed training programmes based on reporting guidelines. The training covers different ways to use reporting guidelines, including drafting manuscripts, checking manuscripts you have written, and appraising the reporting of someone else’s manuscript. Researchers have developed writing software to help authors apply reporting guidelines when drafting [5, 6], online applications to facilitate checklist and flow diagram completion [7, 8], and resources for reviewers to check compliance [9].\nHence over the years, a system has organically grown around reporting guidelines, driven by separate groups of people doing what they felt was sensible. This system includes the guidance resources themselves (the publications, checklists, flow diagrams), the websites that host those resources (guideline websites, the EQUATOR website, publisher’s websites), organisations that promote or enforse their use (EQUATOR, publishers, ICMJE), and staff at those institutions (researchers, editors, reviewers). These components all have the same aim: to influence what information researchers include in their articles.\nAs the Medical Research Council notes, a system with multiple components (like those listed above) is one of many hallmarks of a complex intervention: “an intervention might be considered complex because of properties of the intervention itself, such as the number of components involved; the range of behaviours targeted; expertise and skills required by those delivering and receiving the intervention; the number of groups, settings, or levels targeted; or the permitted level of flexibility of the intervention or its components.”. The reporting guideline system exhibits these sources of complexity, as described in Table 2.\n\n\nTable 2: Sources of complexity within reporting guidelines and the system drives their use.\n\n\n\n\n\n\nSource of complexity\nExample\n\n\n\n\nNumber of components involved\nReporting guidelines often consist of guidance documents, checklists, and flow diagrams, and other tools. These are disseminated through websites, publishing platforms, submission systems, and they are endorsed and enforced by staff at stakeholders including publishers, the EQUATOR Network, conference organisers, and pre-print platforms.\n\n\nRange of behaviours targeted\nGuidelines comprise “reporting items”. Some items are relatively simple, like asking the author to specify their study design in the title. Others are harder, perhaps because they require time, expertise, or prerequisite tasks. For instance, some items may require authors to have conducted their study or analysis in a certain way, or to have collected particular information.\n\n\nExpertise and skills required by those delivering and receiving the intervention\nAcademics from a particular field write reporting guidelines for their peers (as opposed to a lay audience), and so authors, editors, and reviewers must have sufficient expertise to use them.\n\n\nThe number of groups, settings, or levels targeted\nGroups: Users of reporting guidelines differ in their field of expertise, their experience, place of work.\n\nSettings: Although mostly written with authoring in mind, most guideline developers may also hope their resources are used by editors or peer reviewers for checking or appraising research articles.\n\nGuidelines are written with individuals in mind, but their efficacy is generally measured at group level (e.g. articles from a particular field published in a period time).\n\n\nFlexibility of the intervention or its components\nThere is variation between guideline content, resources, and the implementation strategies that development groups, publishers, and other stakeholders employ.\n\n\n\n\nViewing reporting guidelines as part of a complex behaviour change intervention may seem novel. Researchers often call reporting guidelines “tools” or “strategies” (#REF). In “A history of the EQUATOR Network”, Doug Altman refers to reporting guidelines as “resources” that “influence” reporting, but he does not call them interventions. However, I believe my perspective is not radical. I will now outline studies exploring the efficacy of reporting guidelines, and argue that these studies take a systems perspective too although they seldom acknowledge it explicitly.\n\n\n\n\n1. ICMJE | Recommendations | Preparing a Manuscript for Submission to a Medical Journal. \n\n\n2. Koch M, Riss P, Umek W, Hanzal E (2016) The explicit mentioning of reporting guidelines in urogynecology journals in 2013: A bibliometric study. Neurourology and Urodynamics 35:412–416\n\n\n3. Sharp MK, Tokalić R, Gómez G, Wager E, Altman DG, Hren D (2019) A cross-sectional bibliometric study showed suboptimal journal endorsement rates of STROBE and its extensions. Journal of clinical epidemiology 107:42–50\n\n\n4. medRxiv.org - the preprint server for Health Sciences. \n\n\n5. Barnes C, Boutron I, Giraudeau B, Porcher R, Altman DG, Ravaud P (2015) Impact of an online writing aid tool for writing a randomized trial report: The COBWEB (Consort-based WEB tool) randomized controlled trial. BMC Medicine 13:221\n\n\n6. Hawwash D, Sharp MK, Argaw A, Kolsteren P, Lachat C (2019) Usefulness of applying research reporting guidelines as Writing Aid software: A crossover randomised controlled trial. BMJ Open. https://doi.org/10.1136/bmjopen-2019-030943\n\n\n7. Haddaway NR, Page MJ, Pritchard CC, McGuinness LA (2022) PRISMA2020: An R package and Shiny app for producing PRISMA 2020-compliant flow diagrams, with interactivity for optimised digital transparency and Open Synthesis. Campbell Systematic Reviews 18:e1230\n\n\n8. Struthers C, Harwood J, de Beyer JA, Dhiman P, Logullo P, Schlüssel M (2021) GoodReports: Developing a website to help health researchers find and use reporting guidelines. BMC medical research methodology 21:217\n\n\n9. Compliance Questionnaire | ARRIVE Guidelines."
  },
  {
    "objectID": "chapters/1_introduction/4_efficacy.html",
    "href": "chapters/1_introduction/4_efficacy.html",
    "title": "Thesis",
    "section": "",
    "text": "Many studies have compared quality of reporting before and after reporting guidelines were published and/or journals began asking authors to use them. For example, in 2023 Kilicoglu et al. [1] analysed 176,620 randomized trial reports published between 1966 and 2018. They used software to identify sentences pertaining to CONSORT methodology items. They found reporting quality in clinical trials has improved over time but remains sub optimal: articles published before 1990 reported 24% of CONSORT items, whereas articles published between 2010-2018 reported 48%. The fastest improvement occurred in the years immediately after CONSORT was published.\nThis result mirrors similar, manual, efforts to monitor reporting quality over time. For example, Dechartres et al. [2] looked at CONSORT items related to bias in 20,920 trial reports. They found information on sequence generation was missing from 69% of articles published before CONSORT (1986-1990), but only 31% of articles published after CONSORT (2011-2014). The proportion of articles missing information on allocation concealment fell from 70% to 45% over the same periods.\nThese two examples focus on clinical trials, but other research types have seen modest improvements too. De Jong et al. [3] reviewed 284 qualitative evidence syntheses. These syntheses appraised whether their primary studies had adhered to COREQ (a reporting guidelines for qualitative research). Studies published before COREQ reported 16 of COREQ’s 32 items, whereas studies published after reported 18. Similarly, when comparing the reporting quality of genetic association studies, Nedovic et al. [4] found reporting quality was better in generic association articles published after STREGA but only in journals that endorsed STREGA (63% vs 49% showing full adherence).\nNot all studies have found relationships like these. For example, Howell et al. found no improvement in the reporting of quality improvement studies after the SQUIRE guidelines were published [5]. Similarly, Pouwels et al. found no improvement in observational epidemiology following STROBE’s publication [6] and another study [7] found that reporting quality improved before STROBE was published, but not afterwards.\nOther studies have explored the effect of different journal policies. For example, Hopewell et al. [8] assessed the reporting of clinical trial abstracts before and after the publication of CONSORT for Abstracts and compared journals that a) had no reporting guideline policy b) endorsed the guideline and c) actively enforced guideline adherence. They only found an effect in the active enforcement group, where 5.41 items (out of 9) were reported, which was 50% higher than expected. No improvement was seen in journals that endorsed the guideline without enforcing it.\nSome observational studies have focussed on a single time period. In 2018, before my PhD, I collaborated with EQUATOR and BMJ Open to compare manuscript versions before and after authors completed a reporting checklist [9]. Authors made few edits to their work. Of 20 included authors, three added information for a single reporting item, one added information for two items, and one added information for six items. The remaining 15 authors made no changes. On average, manuscripts described 57% of necessary reporting items before the author completed the checklist and 60% afterwards.\nIn 2018 the senior managing editor of the Journal of the National Cancer Institute asked 2000 submitting authors whether they had used a reporting guideline [10]. She then asked peer reviewers to rate manuscripts for their clarity and adherence to reporting guidelines. Declared guideline use was associated with better adherence to guidelines, but not associated with improved clarity nor acceptance rates.\n\n\n\nSome experimental studies have tried to isolate the effect of journal policies by randomising authors. In 2015 PLOS One randomly allocated 1689 incoming submissions reporting in vivo animal research manuscripts to either a) request completion the ARRIVE reporting checklist or b) usual practice [11]. No article achieved full compliance with ARRIVE, and only one sub item (details of animal husbandry) showed improvement between groups.\nIn another study [12], 197 authors submitting to 46 participating journals were randomly allocated to receive either a) access to an online tool (WebCONSORT) to generate customised reporting checklists and flow diagrams based on CONSORT and its extensions or b) a standard CONSORT flow diagram generator without a reporting checklist. There was no difference in reporting quality between groups: authors only reported half of required items.\nIn two earlier studies, Cobo et al. explored the roll of peer review. Simply providing peer reviewers with reporting guidelines had no effect [13]. Adding a reviewer whose task was to check for guideline adherence did lead to improved reporting quality [14], but “the observed effect was smaller than hypothesised and not definitively demonstrated”.\nIt would be unsurprising to find that the stricter the enforcement, the better the adherence. Pandis et al. [15] describe an enforcement strategy used in a small Dentistry journal where the associate editor checked manuscripts for adherence to CONSORT. The associate editor would complete a CONSORT checklist for each manuscript, making note of unclear or unreported items and suggesting ways to improve the manuscript. This would be sent back to the author. Resubmitted manusctripts were subject to the same process, and the manuscript would only be sent out for peer review once the reporting was deemed satisfactory. Over two years, 23 manuscripts were handled in this way. The policy was effective. All studies reported at least 33 of 37 CONSORT items (compared to 15 items before the policy was introduced). However, even with this heavy handed approach, “four items were still unreported in all trials: changes to methods (3b), changes to outcomes (6b), interim analysis (7b), and trial stopping (14b).”.\nTo summarise, reporting standards may have improved over the last two decades. There is some evidence that reporting guidelines may have contributed to this change, but if they have, their effect has only been modest and the bottom line is that most research still does not include the details these guidelines call for. There is huge room for improvement. In a systematic review of 124 studies assessing adherence to one of eight reporting guidelines, Jin et al [16] found 88% of studies reported suboptimal adherence to reporting guidelines. Similarly, Dal Santo et al [17] reviewed 148 studies of reporting quality from the previous few years and almost all described reporting quality as suboptimal.\n\n\n\nBecause reporting guidelines do not exist in a vacuum, it is difficult to separate the guidelines themselves from the policies, people, websites, and tools involved in their implementation. For example, many before-and-after studies use the publication of the guideline as their defining time point. However, it is impossible for these studies to disentangle the effect of guidelines coming into existence with the effect of subsequent journal policies and editorial practices. Similarly, in experimental studies comparing the effect of asking authors to complete a checklist or use a resource, the intervention groups included changes to editorial workflows. These changes were external to the resource being tested, but could be equally important to its success: in the WebCONSORT study, editors’ inability to identify randomised trial reports was an important source of failure external to the tool being tested. Hence, in describing reporting guidelines as being part of a complex behaviour change intervention, I believe I am explicitly articulating a systems perspective that previous studies have hinted at, and I am exploring that system’s scope in more granularity.\n\n\n\n\n1. Kilicoglu H, Jiang L, Hoang L, Mayo-Wilson E, Vinkers CH, Otte WM (2023) Methodology reporting improved over time in 176,469 randomized controlled trials. Journal of Clinical Epidemiology. https://doi.org/10.1016/j.jclinepi.2023.08.004\n\n\n2. Dechartres A, Trinquart L, Atal I, Moher D, Dickersin K, Boutron I, Perrodeau E, Altman DG, Ravaud P (2017) Evolution of poor reporting and inadequate methods over time in 20 920 randomised controlled trials included in Cochrane reviews: Research on research study. BMJ 357:j2490\n\n\n3. de Jong Y, van der Willik EM, Milders J, Voorend CGN, Morton RL, Dekker FW, Meuleman Y, van Diepen M (2021) A meta-review demonstrates improved reporting quality of qualitative reviews following the publication of COREQ- and ENTREQ-checklists, regardless of modest uptake. BMC Medical Research Methodology 21:184\n\n\n4. Nedovic D, Panic N, Pastorino R, Ricciardi W, Boccia S (2016) Evaluation of the Endorsement of the STrengthening the REporting of Genetic Association Studies (STREGA) Statement on the Reporting Quality of Published Genetic Association Studies. Journal of Epidemiology 26:399–404\n\n\n5. Howell V, Schwartz AE, O’Leary JD, Donnell CM (2015) The effect of the SQUIRE (Standards of QUality Improvement Reporting Excellence) guidelines on reporting standards in the quality improvement literature: A before-and-after study. BMJ Quality & Safety 24:400–406\n\n\n6. Pouwels KB, Widyakusuma NN, Groenwold RHH, Hak E (2016) Quality of reporting of confounding remained suboptimal after the STROBE guideline. Journal of Clinical Epidemiology 69:217–224\n\n\n7. Bastuji-Garin S, Sbidian E, Gaudy-Marqueste C, Ferrat E, Roujeau J-C, Richard M-A, Canoui-Poitrine F, European Dermatology Network (EDEN) (2013) Impact of STROBE statement publication on quality of observational study reporting: Interrupted time series versus before-after analysis. PloS One 8:e64733\n\n\n8. Hopewell S, Ravaud P, Baron G, Boutron I (2012) Effect of editors’ implementation of CONSORT guidelines on the reporting of abstracts in high impact medical journals: Interrupted time series analysis. BMJ 344:e4178\n\n\n9. Struthers C, Harwood J, de Beyer JA, Dhiman P, Logullo P, Schlüssel M (2021) GoodReports: Developing a website to help health researchers find and use reporting guidelines. BMC medical research methodology 21:217\n\n\n10. Botos J (2018) Reported use of reporting guidelines among JNCI: Journal of the National Cancer Institute authors, editorial outcomes, and reviewer ratings related to adherence to guidelines and clarity of presentation. Research Integrity and Peer Review 3:7\n\n\n11. Hair K, Macleod MR, Sena ES, et al (2019) A randomised controlled trial of an Intervention to Improve Compliance with the ARRIVE guidelines (IICARus). Research Integrity and Peer Review 4:12\n\n\n12. Hopewell S, Boutron I, Altman DG, et al (2016) Impact of a web-based tool (WebCONSORT) to improve the reporting of randomised trials: Results of a randomised controlled trial. BMC Medicine 14:199\n\n\n13. Cobo E, Selva-O’Callagham A, Ribera J-M, Cardellach F, Dominguez R, Vilardell M (2007) Statistical Reviewers Improve Reporting in Biomedical Articles: A Randomized Trial. PLoS ONE 2:e332\n\n\n14. Cobo E, Cortés J, Ribera JM, et al (2011) Effect of using reporting guidelines during peer review on quality of final manuscripts submitted to a biomedical journal: Masked randomised trial. BMJ 343:d6783\n\n\n15. Pandis N, Shamseer L, Kokich VG, Fleming PS, Moher D (2014) Active implementation strategy of CONSORT adherence by a dental specialty journal improved randomized clinical trial reporting. Journal of Clinical Epidemiology 67:1044–1048\n\n\n16. Jin Y, Sanger N, Shams I, et al (2018) Does the medical literature remain inadequately described despite having reporting guidelines for 21 years? A systematic review of reviews: An update. Journal of Multidisciplinary Healthcare 11:495–510\n\n\n17. Santo TD, Rice DB, Amiri LSN, Tasleem A, Li K, Boruff JT, Geoffroy M-C, Benedetti A, Thombs BD (2023) Methods and results of studies on reporting guideline adherence are poorly reported: A meta-research study. Journal of Clinical Epidemiology 159:225–234"
  },
  {
    "objectID": "chapters/1_introduction/4_efficacy.html#reporting-quality-has-improved-over-time-but-remains-sub-optimal",
    "href": "chapters/1_introduction/4_efficacy.html#reporting-quality-has-improved-over-time-but-remains-sub-optimal",
    "title": "Thesis",
    "section": "",
    "text": "Many studies have compared quality of reporting before and after reporting guidelines were published and/or journals began asking authors to use them. For example, in 2023 Kilicoglu et al. [1] analysed 176,620 randomized trial reports published between 1966 and 2018. They used software to identify sentences pertaining to CONSORT methodology items. They found reporting quality in clinical trials has improved over time but remains sub optimal: articles published before 1990 reported 24% of CONSORT items, whereas articles published between 2010-2018 reported 48%. The fastest improvement occurred in the years immediately after CONSORT was published.\nThis result mirrors similar, manual, efforts to monitor reporting quality over time. For example, Dechartres et al. [2] looked at CONSORT items related to bias in 20,920 trial reports. They found information on sequence generation was missing from 69% of articles published before CONSORT (1986-1990), but only 31% of articles published after CONSORT (2011-2014). The proportion of articles missing information on allocation concealment fell from 70% to 45% over the same periods.\nThese two examples focus on clinical trials, but other research types have seen modest improvements too. De Jong et al. [3] reviewed 284 qualitative evidence syntheses. These syntheses appraised whether their primary studies had adhered to COREQ (a reporting guidelines for qualitative research). Studies published before COREQ reported 16 of COREQ’s 32 items, whereas studies published after reported 18. Similarly, when comparing the reporting quality of genetic association studies, Nedovic et al. [4] found reporting quality was better in generic association articles published after STREGA but only in journals that endorsed STREGA (63% vs 49% showing full adherence).\nNot all studies have found relationships like these. For example, Howell et al. found no improvement in the reporting of quality improvement studies after the SQUIRE guidelines were published [5]. Similarly, Pouwels et al. found no improvement in observational epidemiology following STROBE’s publication [6] and another study [7] found that reporting quality improved before STROBE was published, but not afterwards.\nOther studies have explored the effect of different journal policies. For example, Hopewell et al. [8] assessed the reporting of clinical trial abstracts before and after the publication of CONSORT for Abstracts and compared journals that a) had no reporting guideline policy b) endorsed the guideline and c) actively enforced guideline adherence. They only found an effect in the active enforcement group, where 5.41 items (out of 9) were reported, which was 50% higher than expected. No improvement was seen in journals that endorsed the guideline without enforcing it.\nSome observational studies have focussed on a single time period. In 2018, before my PhD, I collaborated with EQUATOR and BMJ Open to compare manuscript versions before and after authors completed a reporting checklist [9]. Authors made few edits to their work. Of 20 included authors, three added information for a single reporting item, one added information for two items, and one added information for six items. The remaining 15 authors made no changes. On average, manuscripts described 57% of necessary reporting items before the author completed the checklist and 60% afterwards.\nIn 2018 the senior managing editor of the Journal of the National Cancer Institute asked 2000 submitting authors whether they had used a reporting guideline [10]. She then asked peer reviewers to rate manuscripts for their clarity and adherence to reporting guidelines. Declared guideline use was associated with better adherence to guidelines, but not associated with improved clarity nor acceptance rates.\n\n\n\nSome experimental studies have tried to isolate the effect of journal policies by randomising authors. In 2015 PLOS One randomly allocated 1689 incoming submissions reporting in vivo animal research manuscripts to either a) request completion the ARRIVE reporting checklist or b) usual practice [11]. No article achieved full compliance with ARRIVE, and only one sub item (details of animal husbandry) showed improvement between groups.\nIn another study [12], 197 authors submitting to 46 participating journals were randomly allocated to receive either a) access to an online tool (WebCONSORT) to generate customised reporting checklists and flow diagrams based on CONSORT and its extensions or b) a standard CONSORT flow diagram generator without a reporting checklist. There was no difference in reporting quality between groups: authors only reported half of required items.\nIn two earlier studies, Cobo et al. explored the roll of peer review. Simply providing peer reviewers with reporting guidelines had no effect [13]. Adding a reviewer whose task was to check for guideline adherence did lead to improved reporting quality [14], but “the observed effect was smaller than hypothesised and not definitively demonstrated”.\nIt would be unsurprising to find that the stricter the enforcement, the better the adherence. Pandis et al. [15] describe an enforcement strategy used in a small Dentistry journal where the associate editor checked manuscripts for adherence to CONSORT. The associate editor would complete a CONSORT checklist for each manuscript, making note of unclear or unreported items and suggesting ways to improve the manuscript. This would be sent back to the author. Resubmitted manusctripts were subject to the same process, and the manuscript would only be sent out for peer review once the reporting was deemed satisfactory. Over two years, 23 manuscripts were handled in this way. The policy was effective. All studies reported at least 33 of 37 CONSORT items (compared to 15 items before the policy was introduced). However, even with this heavy handed approach, “four items were still unreported in all trials: changes to methods (3b), changes to outcomes (6b), interim analysis (7b), and trial stopping (14b).”.\nTo summarise, reporting standards may have improved over the last two decades. There is some evidence that reporting guidelines may have contributed to this change, but if they have, their effect has only been modest and the bottom line is that most research still does not include the details these guidelines call for. There is huge room for improvement. In a systematic review of 124 studies assessing adherence to one of eight reporting guidelines, Jin et al [16] found 88% of studies reported suboptimal adherence to reporting guidelines. Similarly, Dal Santo et al [17] reviewed 148 studies of reporting quality from the previous few years and almost all described reporting quality as suboptimal.\n\n\n\nBecause reporting guidelines do not exist in a vacuum, it is difficult to separate the guidelines themselves from the policies, people, websites, and tools involved in their implementation. For example, many before-and-after studies use the publication of the guideline as their defining time point. However, it is impossible for these studies to disentangle the effect of guidelines coming into existence with the effect of subsequent journal policies and editorial practices. Similarly, in experimental studies comparing the effect of asking authors to complete a checklist or use a resource, the intervention groups included changes to editorial workflows. These changes were external to the resource being tested, but could be equally important to its success: in the WebCONSORT study, editors’ inability to identify randomised trial reports was an important source of failure external to the tool being tested. Hence, in describing reporting guidelines as being part of a complex behaviour change intervention, I believe I am explicitly articulating a systems perspective that previous studies have hinted at, and I am exploring that system’s scope in more granularity.\n\n\n\n\n1. Kilicoglu H, Jiang L, Hoang L, Mayo-Wilson E, Vinkers CH, Otte WM (2023) Methodology reporting improved over time in 176,469 randomized controlled trials. Journal of Clinical Epidemiology. https://doi.org/10.1016/j.jclinepi.2023.08.004\n\n\n2. Dechartres A, Trinquart L, Atal I, Moher D, Dickersin K, Boutron I, Perrodeau E, Altman DG, Ravaud P (2017) Evolution of poor reporting and inadequate methods over time in 20 920 randomised controlled trials included in Cochrane reviews: Research on research study. BMJ 357:j2490\n\n\n3. de Jong Y, van der Willik EM, Milders J, Voorend CGN, Morton RL, Dekker FW, Meuleman Y, van Diepen M (2021) A meta-review demonstrates improved reporting quality of qualitative reviews following the publication of COREQ- and ENTREQ-checklists, regardless of modest uptake. BMC Medical Research Methodology 21:184\n\n\n4. Nedovic D, Panic N, Pastorino R, Ricciardi W, Boccia S (2016) Evaluation of the Endorsement of the STrengthening the REporting of Genetic Association Studies (STREGA) Statement on the Reporting Quality of Published Genetic Association Studies. Journal of Epidemiology 26:399–404\n\n\n5. Howell V, Schwartz AE, O’Leary JD, Donnell CM (2015) The effect of the SQUIRE (Standards of QUality Improvement Reporting Excellence) guidelines on reporting standards in the quality improvement literature: A before-and-after study. BMJ Quality & Safety 24:400–406\n\n\n6. Pouwels KB, Widyakusuma NN, Groenwold RHH, Hak E (2016) Quality of reporting of confounding remained suboptimal after the STROBE guideline. Journal of Clinical Epidemiology 69:217–224\n\n\n7. Bastuji-Garin S, Sbidian E, Gaudy-Marqueste C, Ferrat E, Roujeau J-C, Richard M-A, Canoui-Poitrine F, European Dermatology Network (EDEN) (2013) Impact of STROBE statement publication on quality of observational study reporting: Interrupted time series versus before-after analysis. PloS One 8:e64733\n\n\n8. Hopewell S, Ravaud P, Baron G, Boutron I (2012) Effect of editors’ implementation of CONSORT guidelines on the reporting of abstracts in high impact medical journals: Interrupted time series analysis. BMJ 344:e4178\n\n\n9. Struthers C, Harwood J, de Beyer JA, Dhiman P, Logullo P, Schlüssel M (2021) GoodReports: Developing a website to help health researchers find and use reporting guidelines. BMC medical research methodology 21:217\n\n\n10. Botos J (2018) Reported use of reporting guidelines among JNCI: Journal of the National Cancer Institute authors, editorial outcomes, and reviewer ratings related to adherence to guidelines and clarity of presentation. Research Integrity and Peer Review 3:7\n\n\n11. Hair K, Macleod MR, Sena ES, et al (2019) A randomised controlled trial of an Intervention to Improve Compliance with the ARRIVE guidelines (IICARus). Research Integrity and Peer Review 4:12\n\n\n12. Hopewell S, Boutron I, Altman DG, et al (2016) Impact of a web-based tool (WebCONSORT) to improve the reporting of randomised trials: Results of a randomised controlled trial. BMC Medicine 14:199\n\n\n13. Cobo E, Selva-O’Callagham A, Ribera J-M, Cardellach F, Dominguez R, Vilardell M (2007) Statistical Reviewers Improve Reporting in Biomedical Articles: A Randomized Trial. PLoS ONE 2:e332\n\n\n14. Cobo E, Cortés J, Ribera JM, et al (2011) Effect of using reporting guidelines during peer review on quality of final manuscripts submitted to a biomedical journal: Masked randomised trial. BMJ 343:d6783\n\n\n15. Pandis N, Shamseer L, Kokich VG, Fleming PS, Moher D (2014) Active implementation strategy of CONSORT adherence by a dental specialty journal improved randomized clinical trial reporting. Journal of Clinical Epidemiology 67:1044–1048\n\n\n16. Jin Y, Sanger N, Shams I, et al (2018) Does the medical literature remain inadequately described despite having reporting guidelines for 21 years? A systematic review of reviews: An update. Journal of Multidisciplinary Healthcare 11:495–510\n\n\n17. Santo TD, Rice DB, Amiri LSN, Tasleem A, Li K, Boruff JT, Geoffroy M-C, Benedetti A, Thombs BD (2023) Methods and results of studies on reporting guideline adherence are poorly reported: A meta-research study. Journal of Clinical Epidemiology 159:225–234"
  },
  {
    "objectID": "chapters/1_introduction/index.html",
    "href": "chapters/1_introduction/index.html",
    "title": "Introduction",
    "section": "",
    "text": "In this chapter I introduce the evidence gap I have addressed and the approach I have taken to address it. I begin by describing the prevalence and consequences of poorly reported medical research. I introduce reporting guidelines and position them as part of a complex behaviour change intervention. This intervention has had a disappointing impact, as medical research is still poorly reported. This brings me to my evidence gap: how can we get more authors to adhere to reporting guidelines? I then outline my aims, objectives, and thesis structure."
  },
  {
    "objectID": "chapters/1_introduction/index.html#the-problem-of-poor-reporting-in-health-research",
    "href": "chapters/1_introduction/index.html#the-problem-of-poor-reporting-in-health-research",
    "title": "Introduction",
    "section": "The problem of poor reporting in health research",
    "text": "The problem of poor reporting in health research\nFacing an uncertain choice during treatment for multiple myeloma, epidemiologist Alessandro Liberati wrote “Why was I forced to make my decision knowing that information was somewhere but not available?”[1]. When I started my PhD, governments may have been asking the same question. The world was in the grip of COVID-19 and decision makers were wading through a deluge of patchy research articles missing important information [2]. In the years since, friends and family have had to make treatment decisions where the evidence is of “low certainty” because key details are missing from research articles.\nA selfish silver-lining of these tumultuous years was that my family and friends finally understood the problem my thesis addresses: when medical researchers inadequately describe what they did or what they found, other people cannot understand, replicate, or use their work. Research costs huge amounts of time, money, and effort, and the written account is typically its sole legacy. When details are omitted they are lost. The remaining gaps are sources of doubt; are they accidental omissions? Oversights? Cover-ups? Whatever their source, the gaps fragment the full picture, and the potential value to patients drains away.\nEarly concern over reporting quality often came from frustrated reviewers unable to find the data they needed within research reports. For example, in 1963, Glick [3] found many reports of psychiatric therapy used ambiguous descriptions of treatment duration like “at least two months” or “from one to several months”. These descriptions were so vague they were “unsuitable for comparative purposes”. More recently, Dechartes found systematic reviewers could not judge the potential for bias in a third of clinical trials because of poorly described methods, thereby limiting the confidence of conclusions [4].\nReviewers are not the only people affected. When interventions are poorly described, researchers cannot appraise or repeat research. Carp [5] described how a third of 241 brain imaging studies missed information necessary to interpret and repeat them, like the number of examinations, examination duration, and the resolution of images. Doctors and service providers also need clear descriptions to replicate interventions [6]. As Feinstein noted in 1974 [7], it is difficult enough for a clinician to understand the value of unfamiliar procedure, but “it is much more difficult when he is not told what that procedure was”. For example, Davidson et al. [8] reviewed trials describing exercise interventions for chronic back pain and found authors often did not describe interventions sufficiently for other healthcare providers to copy them.\nThese are a mere handful of many studies documenting poor reporting in medical literature. A 2023 systematic review found 148 published between 2020-2022 alone [9]. All investigated reporting quality in different medical research disciplines, and almost all concluded reporting was sub-optimal. Hence, poor reporting is a long-standing problem, plagues all disciplines, devalues research, and derails the uptake of new knowledge into clinical practice."
  },
  {
    "objectID": "chapters/1_introduction/index.html#reporting-guidelines-to-the-rescue",
    "href": "chapters/1_introduction/index.html#reporting-guidelines-to-the-rescue",
    "title": "Introduction",
    "section": "Reporting guidelines to the rescue?",
    "text": "Reporting guidelines to the rescue?\nConcern over reporting quality crescendoed through the eighties and early nineties as systematic reviews became more common. Responding to calls for “strategies”, “guides”, and “lists” to help authors prepare their manuscripts, a group of methodologists, trialists, and editors met in 1996 to create the CONsolidated Standards of Reporting Trials (CONSORT) statement [10]. CONSORT is a set of recommendations detailing what information authors should include in clinical trial reports. It comprised an article describing how it was made, a checklist, flow diagram, and (after an update in 2001) and ‘Explanation and Elaboration’ publication [11]; [12].\nCONSORT proved influential, and other groups quickly developed guidelines for different research types. Reporting guidelines are like a theme and variations, where CONSORT forged a path others have followed with varying fidelity (See Table 1). Most have acronym names. Most were first published as a journal article describing their development. Some, but not all, have checklists and elaboration documents. Some guideline developers publish resources as separate documents, others put them all into a single journal article. Guidelines are developed by different groups, with different composition (possibly including methodologists, editors, clinicians etc) and in different ways (e.g., some by delphi consensus). Although most follow CONSORT’s approach of presenting recommendations focussing on reporting above conduct, guidelines differ in how forceful their recommendations are and whether they also seek to influence design.\n\n\n\nTable 1: A selection of highly cited reporting guidelines\n\n\n\n\n\n\n\n\n\n\n\n\n\nGuideline acronym\nDefinition\nApplicable study type\nPublication year\nDevelopment article?\nFillable checklist?\nExplanatory document?\nOther resources\nInfluences design?\n\n\nCONSORT\nConsolidated Standards of Reporting Trials\nRandomised controlled trials\n1996 #REF updated in 2001 #REF and 2010 #REF\nYes\nYes\nYes, as a separate article\nFlow diagram\nWebsite\nCOBWEB writing tool #REF\nNo\n\n\nPRISMA\nPreferred Reporting Items for Systematic Reviews and Meta-Analyses\nSystematic Reviews and Meta-Analyses\n2009 #REF\nUpdated in 2021 #REF\nYes\nYes\nYes\nFlow diagram\nWebsite\nNo\n\n\nARRIVE\nAnimal Research: Reporting of In Vivo Experiments\nPublications describing research involving live animals\n2010 #REF\nUpdated in 2020 #REF\nYes\nYes\nYes\nWebsite\nAction Plans\nCompliance questionnaire\nNot explicitly, but does contain design guidance\n\n\nSRQR\nStandards for Reporting Qualitative Research\nQualitative health research\n2014 [13]\nYes\nNo\nYes, as supplementary material that is hard to find\n\n\n\n\ne.t.c. for all guidelines mentioned on EQUATOR’s home page\n\n#TODO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are now over 500 reporting guidelines, representing the collective work of thousands of academics. The best-known guidelines are endorsed by large numbers of medical journals and the International Committee of Medical Journal Editors, and are amongst the 1% most highly cited publications indexed by Web of Science [14]."
  },
  {
    "objectID": "chapters/1_introduction/index.html#the-equator-network-unites-the-reporting-guideline-movement",
    "href": "chapters/1_introduction/index.html#the-equator-network-unites-the-reporting-guideline-movement",
    "title": "Introduction",
    "section": "The EQUATOR Network unites the reporting guideline movement",
    "text": "The EQUATOR Network unites the reporting guideline movement\nAs reporting guidelines grew in number and the problem of poor reporting gained recognition, Doug Altman saw the need to catalogue reporting guidelines and form a community. He united academics from around the world to form The EQUATOR Network, often simply called EQUATOR, standing for Enhancing the QUAlity and Transparency Of health Research. It was the first coordinated attempt to combat poor reporting systematically and on a global scale. One of EQUATOR’s core objectives was to create a database of reporting guidelines, accessible via their website where researchers will also find training and information about developing guidelines."
  },
  {
    "objectID": "chapters/1_introduction/index.html#reporting-guidelines-are-part-of-a-complex-behaviour-intervention",
    "href": "chapters/1_introduction/index.html#reporting-guidelines-are-part-of-a-complex-behaviour-intervention",
    "title": "Introduction",
    "section": "Reporting guidelines are part of a complex behaviour intervention",
    "text": "Reporting guidelines are part of a complex behaviour intervention\nThe publishing community began to take note. The International Committee for Medical Journal Editors encouraged journals “to ask authors to follow [reporting] guidelines” [15]. Concerned editors sought ways to adopt reporting guidelines, and more and more journals [16]; [17] have since found a range of strategies to introduce reporting guidelines into their policies, outlined in Table 2. There is variation in the degree of enforcement (from passive recommendation through to compulsory enforcement) and variation in the guidelines subject to the policy; PRISMA and CONSORT and more commonly enforced than STROBE, say, and many reporting guidelines in EQUATOR’s library are not enforced nor endorsed by any journals. Instead of listing reporting guidelines by name, many journals keep their instructions vague and merely recommend authors find an appropriate guideline on EQUATOR’s website.\n\n\nTable 2: Examples of how journals have introduced reporting guidelines into their policies. Journals also differ in the reporting guidelines they enforce. For example, some journals may only have policies for randomised trials or systematic reviews, whereas other journals may enforce guidelines for other study types. To my knowledge, no journal explicitly advises against using a reporting guideline.\n\n\n\n\n\n\nEnforcement type\nExample\n\n\n\n\nEnforcing adherence\nAn editor or peer reviewer checks the article body for reporting guideline adherence and asks the author to add missing items.\n\n\nRequesting peer reviewers use reporting guidelines\nEditors ask peer reviewers to consider reporting guideline adherence as part of their review. Some editors may supply the reviewer with the relevant checklist. The reviewer can choose whether to review reporting.\n\n\nEnforcing checklist submission\nEditorial staff require authors to submit a completed reporting checklist as part of manuscript submission. Some journals may refuse to process a submission when the checklist is missing. Some journal submission systems may include fields for authors to upload their checklists, whereas other journals may expect authors to upload checklists as a supplementary file.\n\n\nJournal endorsement\nThe journal’s instructions to authors recommends authors follow reporting guidelines. Guidelines may be specified, in which case journals may link to guideline specific websites, to the guideline publications, or to the EQUATOR Network website. Sometimes journals include a general statement but do not name guidelines, instead referring authors to the EQUATOR website with an instruction to follow “relevant guidance”.\n\n\nPublisher endorsement\nSometimes reporting guideline policies are set at the level of the publisher, as is commonly done for editorial policies. Individual journals may point authors to their publisher policies.\n\n\nNo policy\nJournals have no policies regarding reporting guidelines.\n\n\n\n\nOther stakeholders have begun incorporating reporting guidelines into their policies. Conferences like the Peer Review Congress ask applicants to use reporting guidelines when writing their abstracts. MedRxiv, a large preprint server, asks authors to declare they have “followed all appropriate research reporting guidelines, such as any relevant EQUATOR Network research reporting checklist(s)” [18].\nEQUATOR have developed training programmes based on reporting guidelines. The training covers different ways to use reporting guidelines, including drafting manuscripts, checking manuscripts you have written, and appraising the reporting of someone else’s manuscript. Researchers have developed writing software to help authors apply reporting guidelines when drafting [19, 20], online applications to facilitate checklist and flow diagram completion [21, 22], and resources for reviewers to check compliance [23].\nHence over the years, a system has organically grown around reporting guidelines, driven by separate groups of people doing what they felt was sensible. This system includes the guidance resources themselves (the publications, checklists, flow diagrams), the websites that host those resources (guideline websites, the EQUATOR website, publisher’s websites), organisations that promote or enforse their use (EQUATOR, publishers, ICMJE), and staff at those institutions (researchers, editors, reviewers). These components all have the same aim: to influence what information researchers include in their articles.\nAs the Medical Research Council notes, a system with multiple components (like those listed above) is one of many hallmarks of a complex intervention: “an intervention might be considered complex because of properties of the intervention itself, such as the number of components involved; the range of behaviours targeted; expertise and skills required by those delivering and receiving the intervention; the number of groups, settings, or levels targeted; or the permitted level of flexibility of the intervention or its components.”. The reporting guideline system exhibits these sources of complexity, as described in Table 3.\n\n\nTable 3: Sources of complexity within reporting guidelines and the system drives their use.\n\n\n\n\n\n\nSource of complexity\nExample\n\n\n\n\nNumber of components involved\nReporting guidelines often consist of guidance documents, checklists, and flow diagrams, and other tools. These are disseminated through websites, publishing platforms, submission systems, and they are endorsed and enforced by staff at stakeholders including publishers, the EQUATOR Network, conference organisers, and pre-print platforms.\n\n\nRange of behaviours targeted\nGuidelines comprise “reporting items”. Some items are relatively simple, like asking the author to specify their study design in the title. Others are harder, perhaps because they require time, expertise, or prerequisite tasks. For instance, some items may require authors to have conducted their study or analysis in a certain way, or to have collected particular information.\n\n\nExpertise and skills required by those delivering and receiving the intervention\nAcademics from a particular field write reporting guidelines for their peers (as opposed to a lay audience), and so authors, editors, and reviewers must have sufficient expertise to use them.\n\n\nThe number of groups, settings, or levels targeted\nGroups: Users of reporting guidelines differ in their field of expertise, their experience, place of work.\n\nSettings: Although mostly written with authoring in mind, most guideline developers may also hope their resources are used by editors or peer reviewers for checking or appraising research articles.\n\nGuidelines are written with individuals in mind, but their efficacy is generally measured at group level (e.g. articles from a particular field published in a period time).\n\n\nFlexibility of the intervention or its components\nThere is variation between guideline content, resources, and the implementation strategies that development groups, publishers, and other stakeholders employ.\n\n\n\n\nViewing reporting guidelines as part of a complex behaviour change intervention may seem novel. Researchers often call reporting guidelines “tools” or “strategies” (#REF). In “A history of the EQUATOR Network”, Doug Altman refers to reporting guidelines as “resources” that “influence” reporting, but he does not call them interventions. However, I believe my perspective is not radical. I will now outline studies exploring the efficacy of reporting guidelines, and argue that these studies take a systems perspective too although they seldom acknowledge it explicitly."
  },
  {
    "objectID": "chapters/1_introduction/index.html#reporting-quality-has-improved-over-time-but-remains-sub-optimal",
    "href": "chapters/1_introduction/index.html#reporting-quality-has-improved-over-time-but-remains-sub-optimal",
    "title": "Introduction",
    "section": "Reporting quality has improved over time but remains sub optimal",
    "text": "Reporting quality has improved over time but remains sub optimal\n\nEvidence from observational studies\nMany studies have compared quality of reporting before and after reporting guidelines were published and/or journals began asking authors to use them. For example, in 2023 Kilicoglu et al. [24] analysed 176,620 randomized trial reports published between 1966 and 2018. They used software to identify sentences pertaining to CONSORT methodology items. They found reporting quality in clinical trials has improved over time but remains sub optimal: articles published before 1990 reported 24% of CONSORT items, whereas articles published between 2010-2018 reported 48%. The fastest improvement occurred in the years immediately after CONSORT was published.\nThis result mirrors similar, manual, efforts to monitor reporting quality over time. For example, Dechartres et al. [4] looked at CONSORT items related to bias in 20,920 trial reports. They found information on sequence generation was missing from 69% of articles published before CONSORT (1986-1990), but only 31% of articles published after CONSORT (2011-2014). The proportion of articles missing information on allocation concealment fell from 70% to 45% over the same periods.\nThese two examples focus on clinical trials, but other research types have seen modest improvements too. De Jong et al. [25] reviewed 284 qualitative evidence syntheses. These syntheses appraised whether their primary studies had adhered to COREQ (a reporting guidelines for qualitative research). Studies published before COREQ reported 16 of COREQ’s 32 items, whereas studies published after reported 18. Similarly, when comparing the reporting quality of genetic association studies, Nedovic et al. [26] found reporting quality was better in generic association articles published after STREGA but only in journals that endorsed STREGA (63% vs 49% showing full adherence).\nNot all studies have found relationships like these. For example, Howell et al. found no improvement in the reporting of quality improvement studies after the SQUIRE guidelines were published [27]. Similarly, Pouwels et al. found no improvement in observational epidemiology following STROBE’s publication [28] and another study [29] found that reporting quality improved before STROBE was published, but not afterwards.\nOther studies have explored the effect of different journal policies. For example, Hopewell et al. [30] assessed the reporting of clinical trial abstracts before and after the publication of CONSORT for Abstracts and compared journals that a) had no reporting guideline policy b) endorsed the guideline and c) actively enforced guideline adherence. They only found an effect in the active enforcement group, where 5.41 items (out of 9) were reported, which was 50% higher than expected. No improvement was seen in journals that endorsed the guideline without enforcing it.\nSome observational studies have focussed on a single time period. In 2018, before my PhD, I collaborated with EQUATOR and BMJ Open to compare manuscript versions before and after authors completed a reporting checklist [22]. Authors made few edits to their work. Of 20 included authors, three added information for a single reporting item, one added information for two items, and one added information for six items. The remaining 15 authors made no changes. On average, manuscripts described 57% of necessary reporting items before the author completed the checklist and 60% afterwards.\nIn 2018 the senior managing editor of the Journal of the National Cancer Institute asked 2000 submitting authors whether they had used a reporting guideline [31]. She then asked peer reviewers to rate manuscripts for their clarity and adherence to reporting guidelines. Declared guideline use was associated with better adherence to guidelines, but not associated with improved clarity nor acceptance rates.\n\n\nEvidence from intervention studies\nSome experimental studies have tried to isolate the effect of journal policies by randomising authors. In 2015 PLOS One randomly allocated 1689 incoming submissions reporting in vivo animal research manuscripts to either a) request completion the ARRIVE reporting checklist or b) usual practice [32]. No article achieved full compliance with ARRIVE, and only one sub item (details of animal husbandry) showed improvement between groups.\nIn another study [33], 197 authors submitting to 46 participating journals were randomly allocated to receive either a) access to an online tool (WebCONSORT) to generate customised reporting checklists and flow diagrams based on CONSORT and its extensions or b) a standard CONSORT flow diagram generator without a reporting checklist. There was no difference in reporting quality between groups: authors only reported half of required items.\nIn two earlier studies, Cobo et al. explored the roll of peer review. Simply providing peer reviewers with reporting guidelines had no effect [34]. Adding a reviewer whose task was to check for guideline adherence did lead to improved reporting quality [35], but “the observed effect was smaller than hypothesised and not definitively demonstrated”.\nIt would be unsurprising to find that the stricter the enforcement, the better the adherence. Pandis et al. [36] describe an enforcement strategy used in a small Dentistry journal where the associate editor checked manuscripts for adherence to CONSORT. The associate editor would complete a CONSORT checklist for each manuscript, making note of unclear or unreported items and suggesting ways to improve the manuscript. This would be sent back to the author. Resubmitted manusctripts were subject to the same process, and the manuscript would only be sent out for peer review once the reporting was deemed satisfactory. Over two years, 23 manuscripts were handled in this way. The policy was effective. All studies reported at least 33 of 37 CONSORT items (compared to 15 items before the policy was introduced). However, even with this heavy handed approach, “four items were still unreported in all trials: changes to methods (3b), changes to outcomes (6b), interim analysis (7b), and trial stopping (14b).”.\nTo summarise, reporting standards may have improved over the last two decades. There is some evidence that reporting guidelines may have contributed to this change, but if they have, their effect has only been modest and the bottom line is that most research still does not include the details these guidelines call for. There is huge room for improvement. In a systematic review of 124 studies assessing adherence to one of eight reporting guidelines, Jin et al [37] found 88% of studies reported suboptimal adherence to reporting guidelines. Similarly, Dal Santo et al [38] reviewed 148 studies of reporting quality from the previous few years and almost all described reporting quality as suboptimal.\n\n\nStudies exploring the efficacy of reporting guidelines are actually exploring the efficacy of the reporting guideline system\nBecause reporting guidelines do not exist in a vacuum, it is difficult to separate the guidelines themselves from the policies, people, websites, and tools involved in their implementation. For example, many before-and-after studies use the publication of the guideline as their defining time point. However, it is impossible for these studies to disentangle the effect of guidelines coming into existence with the effect of subsequent journal policies and editorial practices. Similarly, in experimental studies comparing the effect of asking authors to complete a checklist or use a resource, the intervention groups included changes to editorial workflows. These changes were external to the resource being tested, but could be equally important to its success: in the WebCONSORT study, editors’ inability to identify randomised trial reports was an important source of failure external to the tool being tested. Hence, in describing reporting guidelines as being part of a complex behaviour change intervention, I believe I am explicitly articulating a systems perspective that previous studies have hinted at, and I am exploring that system’s scope in more granularity."
  },
  {
    "objectID": "chapters/1_introduction/index.html#evidence-gap-what-more-could-be-done-to-improve-guideline-adherence",
    "href": "chapters/1_introduction/index.html#evidence-gap-what-more-could-be-done-to-improve-guideline-adherence",
    "title": "Introduction",
    "section": "Evidence gap: What more could be done to improve guideline adherence?",
    "text": "Evidence gap: What more could be done to improve guideline adherence?\nSome of the articles I have cited end with rallying cries like “major improvements need active enforcement” [39]. It would be tempting to look at Pandis’ results as support for heavy editorial enforcement being the best option, but this approach may not generalise to other journals and other guidelines. The dentistry journal in this study was small. Only 23 manuscripts underwent this treatment over 2 years, and despite giving “30 to 60 minutes” of editorial attention to each manuscript, not all completely adhered to CONSORT. The study authors admit the benefits should be “considered in the light of the additional time requirement and need for greater editorial input during the peer review process”.\n\nOther articles have called for lighter forms of enforcement; “We need to promote more active implementation, such as submission of the checklist with the manuscript” wrote Dechartres [4], and “it is not sufficient for journals to simply recommend the use of STREGA to authors in the authors’ instructions; instead, journals should require submission of the STREGA checklist together with the manuscript” wrote Nedovic [26]. But the PLOS One [32] and BMJ Open studies [22] found little effect of checklist completion on reporting quality. Additionally, the PLOS One study found that enforcing checklists, although less burdensome than the editorial enforcement described by Pandis, still came with costs to both editors and authors and significantly prolonged publication times. Peer reviews focussing on reporting might help [35] but is relatively unexplored as an option.\nThese studies all focussed on different methods of enforcement. Some include incidental findings hinting at areas-for-improvement unfixable by enforcement alone. For example, because reviewers assessing adherence in the PLOS One study did not always agree or fully understand the guidance, the study authors suggested refining the guideline’s “content” and “perceived clarity”. In the WebCONSORT study, [33] Hopewell et al. made some guesses for why their intervention failed. They had to exclude 39% of manuscripts because editors had incorrectly identified them as randomised trials, and a quarter of authors selected inappropriate extensions. As a solution, they suggest a tool to help authors and editors identify study types. The study authors also raised other hypotheses to explain why their intervention failed: perhaps the custom combined checklists were too long, unclear, or perhaps giving feedback during manuscript revision was too late.\nBoth of these studies may have benefited from a qualitative component to understand why the interventions were not working. In our BMJ Open study we surveyed authors after they completed checklists. Many reported finding the checklist too long, confusing, or irrelevant. However, because we used a multiple choice question with a (small) box for a free text answer, and because we did not survey authors if they did not complete a checklist, authors may have faced other barriers too.\nThese studies suggest authors may face barriers when trying to use reporting guidelines that require solutions beyond enforcement. Noting the outcome assessors’ confusion in the PLOS One study, ARRIVE’s developers took steps to refine its clarity when they revised it (#REF). They also decided to prioritise items to make the guidance quicker to apply. Hopewell et al. created WebCONSORT because they worried combining CONSORT with its extensions may be “cumbersome and difficult” without providing evidence for this claim. In 2018 I worked with EQUATOR as a freelance developer to create GoodReports.org, a website where authors could find and fill out checklists on-line. The website addressed two barriers. Firstly a questionnaire helped authors find the right guideline. Secondly, authors could complete checklists easily (previously, some reporting guidelines came with uneditable PDF checklists).\nThese innovation efforts shared limitations. None took steps to identify barriers thoroughly. By focussing on a few barriers they may have neglected others or introduced new ones. For example, in trying to make combining checklists easier, WebCONSORT may inadvertently made checklists longer, and increased the risk of authors selecting inappropriate guidance. Secondly, these studies did not systematically consider options to solve those barriers. For example, ARRIVE’s development team decided to prioritize items as a way to make the guidance quicker to apply, but this is not the only solution. They could also have considered reducing the number of items, making guidance more concise, providing suggested wording or creating tools to speed up writing. Thirdly, although the studies describe their innovations, they do not always describe changes beyond the tool in question. For example, implementing WebCONSORT and PLOS One’s checklist policy also involved changes in editorial practice. Nor do these studies always clearly explain how changes are expected to alter behaviour."
  },
  {
    "objectID": "chapters/1_introduction/index.html#summary",
    "href": "chapters/1_introduction/index.html#summary",
    "title": "Introduction",
    "section": "Summary",
    "text": "Summary\nIn summary, I’ve explained that a lot of medical research is poorly reported, and that this makes it difficult for other researchers to understand, appraise, synthesize, or replicate studies. This, in turn, makes research less useful to patients.\nI’ve introduced reporting guidelines, created by the research community with the aim to improve reporting quality. I’ve described the system of tools, websites, people, and policy that has organically grown around reporting guidelines, and I have argued that this system forms a complex behaviour change intervention with the goal of altering what authors write.\nI’ve discussed how this system has had only a modest effect on reporting quality, at best. I’ve described how studies exploring modifications to this system are limited because they did not explore barriers thoroughly, and lacked a systematic method to identify options to address those barriers."
  },
  {
    "objectID": "chapters/1_introduction/index.html#aims-and-objectives",
    "href": "chapters/1_introduction/index.html#aims-and-objectives",
    "title": "Introduction",
    "section": "Aims and Objectives",
    "text": "Aims and Objectives\nMy aim was to identify and address barriers preventing authors from adhering to reporting guidelines. I wanted to explore the entire reporting guideline system, and I wanted to be thorough: I wanted to identify as many barriers as possible, and as many solutions as possible, before deciding which to implement.\nMy objectives were:\n\nTo identify factors that may limit reporting guideline impact by synthesising existing research and by evaluating the EQUATOR Network website (addressed in chapters 3)\nTo work with key stakeholders to identify intervention changes to address these limiting factors (addressed in chapters 6)\nTo implement these changes (described in 10)\nTo refine the new intervention in response to feedback from authors (addressed in chapter 11)\n\nMy thesis bares many of the hallmarks of pragmatism. I used both qualitative and quantitative methods. Constraints (like time and access to participants) influenced my decisions. I balanced participants’ views with my own; I sought to remove my views as much as possible in all chapters except for the workshops I conducted with EQUATOR (chapter 7) and when designing the intervention (chapter 10). I balanced inductive and deductive reasoning; my early chapters were exploratory and inductive, and my later chapters became increasingly deductive as my focus narrowed and I relied more heavily on a framework.\nASK Do I need to justify why I did not seek to quantify the system, or to define it using behaviour change techniques?"
  },
  {
    "objectID": "chapters/1_introduction/index.html#thesis-structure",
    "href": "chapters/1_introduction/index.html#thesis-structure",
    "title": "Introduction",
    "section": "Thesis structure",
    "text": "Thesis structure\nChapter 2 - Reflections on starting my DPhil\nI reflect on my background and my prior held opinions, and those of my supervision team, and how these may have influenced the direction of this thesis.\nChapter 3 - What facilitators and barriers might researchers encounter when using reporting guidelines? Part 1: A thematic synthesis\nThe next three chapters pertain to my first objective - to identify possible reasons as to why reporting guidelines have had only a limited impact on reporting quality. This chapter describes a thematic synthesis of studies that qualitatively explored authors’ experiences of using reporting guidelines, where I sought to identify what may influence whether an author successfully adheres to reporting guidance.\nChapter 4 - What facilitators and barriers might researchers encounter when using reporting guidelines? Part 2: Describing the questions asked in quantitative surveys.\nThis chapter builds on the previous one by identifying additional possible influences from the content of quantitative survey questions.\nChapter 5 - A service evaluation of equator-network.org\nThis chapter describes a service evaluation of the EQUATOR Network website which, although an important piece of the reporting guideline infrastructure, was rarely explored by studies reviewed in the previous two chapters. From this evaluation, I then infer possible barriers that authors may encounter when trying to find and access reporting guidelines from EQUATOR’s website.\nChapter 6 - Selecting the Behaviour Change Wheel framework\nThe next 4 chapters pertain to my second objective - identifying intervention changes. This chapter introduces the Behaviour Change Wheel, which is a framework for designing and defining behaviour change interventions. I explain how my thesis gained form at this point in time; my view of reporting guidelines as a system crystallised, and Charlotte Albury joined my supervision team as my plans took an unexpected qualitative turn. I wanted my thesis to accurately reflect the twists and turns of my DPhil, and so I introduce my chosen framework in this middle chapter instead of the introduction which may be more customary.\nChapter 7 - Following the BCW Guide: Workshops with EQUATOR\nThis chapter describes how I lead workshops with UK EQUATOR center staff to identify intervention options using Behaviour Change Wheel framework.\nChapter 8 - Generating ideas to address factors limiting reporting guideline impact: workshops with EQUATOR and focus groups with developers, publishers, and experts\nThis chapter reports focus groups where I collected ideas on how intervention options could be realised.\nChapter 9 - Defining Intervention Content\nIn this chapter, I bring together the outputs of the previous two chapters to create a table of intervention components.\nChapter 10 - Developing intervention components into a prototype\nThis chapter concerns my third objective; implementing the intervention changes by redesigning a reporting guideline (SRQR) and the EQUATOR Network website’s home page.\nChapter 11 - Refining the intervention: qualitative study with authors\nIn this chapter I address my final objective by refining the intervention in response to feedback from authors. I describe a qualitative study where I used observation, think aloud, structured interviews, and a writing evaluation, to gather feedback from an international sample of authors.\n\n\n\n\n1. Liberati A (2004) An unfinished trip through uncertainties. BMJ : British Medical Journal 328:531\n\n\n2. Ziemann S, Paetzolt I, Grüßer L, Coburn M, Rossaint R, Kowark A (2022) Poor reporting quality of observational clinical studies comparing treatments of COVID-19 a retrospective cross-sectional study. BMC Medical Research Methodology 22:23\n\n\n3. Glick BS (1963) Inadequacies in the reporting of clinical drug research. The Psychiatric Quarterly 37:234–244\n\n\n4. Dechartres A, Trinquart L, Atal I, Moher D, Dickersin K, Boutron I, Perrodeau E, Altman DG, Ravaud P (2017) Evolution of poor reporting and inadequate methods over time in 20 920 randomised controlled trials included in Cochrane reviews: Research on research study. BMJ 357:j2490\n\n\n5. Carp J (2012) The secret lives of experiments: Methods reporting in the fMRI literature. NeuroImage 63:289–300\n\n\n6. Glasziou P, Meats E, Heneghan C, Shepperd S (2008) What is missing from descriptions of treatment in trials and reviews? BMJ 336:1472–1474\n\n\n7. Feinstein AR (1974) Clinical biostatistics. XXV. A survey of the statistical procedures in general medical journals. Clinical Pharmacology and Therapeutics 15:97–107\n\n\n8. Davidson SRE, Kamper SJ, Haskins R, Robson E, Gleadhill C, da Silva PV, Williams A, Yu Z, Williams CM (2021) Exercise interventions for low back pain are poorly reported: A systematic review. Journal of Clinical Epidemiology 139:279–286\n\n\n9. Santo TD, Rice DB, Amiri LSN, Tasleem A, Li K, Boruff JT, Geoffroy M-C, Benedetti A, Thombs BD (2023) Methods and results of studies on reporting guideline adherence are poorly reported: A meta-research study. Journal of Clinical Epidemiology 159:225–234\n\n\n10. Begg C, Cho M, Eastwood S, et al (1996) Improving the Quality of Reporting of Randomized Controlled Trials: The CONSORT Statement. JAMA 276:637–639\n\n\n11. Altman DG (1996) Better reporting of randomised controlled trials: The CONSORT statement. BMJ (Clinical research ed) 313:570–571\n\n\n12. Altman DG, Schulz KF, Moher D, Egger M, Davidoff F, Elbourne D, Gøtzsche PC, Lang T (2001) The Revised CONSORT Statement for Reporting Randomized Trials: Explanation and Elaboration. Annals of Internal Medicine 134:663–694\n\n\n13. O’Brien BC, Harris IB, Beckman TJ, Reed DA, Cook DA (2014) Standards for Reporting Qualitative Research: A Synthesis of Recommendations. Academic Medicine 89:1245–1251\n\n\n14. Caulley L, Cheng W, Catalá-López F, Whelan J, Khoury M, Ferraro J, Husereau D, Altman DG, Moher D (2020) Citation impact was highly variable for reporting guidelines of health research: A citation analysis. Journal of Clinical Epidemiology 127:96–104\n\n\n15. ICMJE | Recommendations | Preparing a Manuscript for Submission to a Medical Journal. \n\n\n16. Koch M, Riss P, Umek W, Hanzal E (2016) The explicit mentioning of reporting guidelines in urogynecology journals in 2013: A bibliometric study. Neurourology and Urodynamics 35:412–416\n\n\n17. Sharp MK, Tokalić R, Gómez G, Wager E, Altman DG, Hren D (2019) A cross-sectional bibliometric study showed suboptimal journal endorsement rates of STROBE and its extensions. Journal of clinical epidemiology 107:42–50\n\n\n18. medRxiv.org - the preprint server for Health Sciences. \n\n\n19. Barnes C, Boutron I, Giraudeau B, Porcher R, Altman DG, Ravaud P (2015) Impact of an online writing aid tool for writing a randomized trial report: The COBWEB (Consort-based WEB tool) randomized controlled trial. BMC Medicine 13:221\n\n\n20. Hawwash D, Sharp MK, Argaw A, Kolsteren P, Lachat C (2019) Usefulness of applying research reporting guidelines as Writing Aid software: A crossover randomised controlled trial. BMJ Open. https://doi.org/10.1136/bmjopen-2019-030943\n\n\n21. Haddaway NR, Page MJ, Pritchard CC, McGuinness LA (2022) PRISMA2020: An R package and Shiny app for producing PRISMA 2020-compliant flow diagrams, with interactivity for optimised digital transparency and Open Synthesis. Campbell Systematic Reviews 18:e1230\n\n\n22. Struthers C, Harwood J, de Beyer JA, Dhiman P, Logullo P, Schlüssel M (2021) GoodReports: Developing a website to help health researchers find and use reporting guidelines. BMC medical research methodology 21:217\n\n\n23. Compliance Questionnaire | ARRIVE Guidelines. \n\n\n24. Kilicoglu H, Jiang L, Hoang L, Mayo-Wilson E, Vinkers CH, Otte WM (2023) Methodology reporting improved over time in 176,469 randomized controlled trials. Journal of Clinical Epidemiology. https://doi.org/10.1016/j.jclinepi.2023.08.004\n\n\n25. de Jong Y, van der Willik EM, Milders J, Voorend CGN, Morton RL, Dekker FW, Meuleman Y, van Diepen M (2021) A meta-review demonstrates improved reporting quality of qualitative reviews following the publication of COREQ- and ENTREQ-checklists, regardless of modest uptake. BMC Medical Research Methodology 21:184\n\n\n26. Nedovic D, Panic N, Pastorino R, Ricciardi W, Boccia S (2016) Evaluation of the Endorsement of the STrengthening the REporting of Genetic Association Studies (STREGA) Statement on the Reporting Quality of Published Genetic Association Studies. Journal of Epidemiology 26:399–404\n\n\n27. Howell V, Schwartz AE, O’Leary JD, Donnell CM (2015) The effect of the SQUIRE (Standards of QUality Improvement Reporting Excellence) guidelines on reporting standards in the quality improvement literature: A before-and-after study. BMJ Quality & Safety 24:400–406\n\n\n28. Pouwels KB, Widyakusuma NN, Groenwold RHH, Hak E (2016) Quality of reporting of confounding remained suboptimal after the STROBE guideline. Journal of Clinical Epidemiology 69:217–224\n\n\n29. Bastuji-Garin S, Sbidian E, Gaudy-Marqueste C, Ferrat E, Roujeau J-C, Richard M-A, Canoui-Poitrine F, European Dermatology Network (EDEN) (2013) Impact of STROBE statement publication on quality of observational study reporting: Interrupted time series versus before-after analysis. PloS One 8:e64733\n\n\n30. Hopewell S, Ravaud P, Baron G, Boutron I (2012) Effect of editors’ implementation of CONSORT guidelines on the reporting of abstracts in high impact medical journals: Interrupted time series analysis. BMJ 344:e4178\n\n\n31. Botos J (2018) Reported use of reporting guidelines among JNCI: Journal of the National Cancer Institute authors, editorial outcomes, and reviewer ratings related to adherence to guidelines and clarity of presentation. Research Integrity and Peer Review 3:7\n\n\n32. Hair K, Macleod MR, Sena ES, et al (2019) A randomised controlled trial of an Intervention to Improve Compliance with the ARRIVE guidelines (IICARus). Research Integrity and Peer Review 4:12\n\n\n33. Hopewell S, Boutron I, Altman DG, et al (2016) Impact of a web-based tool (WebCONSORT) to improve the reporting of randomised trials: Results of a randomised controlled trial. BMC Medicine 14:199\n\n\n34. Cobo E, Selva-O’Callagham A, Ribera J-M, Cardellach F, Dominguez R, Vilardell M (2007) Statistical Reviewers Improve Reporting in Biomedical Articles: A Randomized Trial. PLoS ONE 2:e332\n\n\n35. Cobo E, Cortés J, Ribera JM, et al (2011) Effect of using reporting guidelines during peer review on quality of final manuscripts submitted to a biomedical journal: Masked randomised trial. BMJ 343:d6783\n\n\n36. Pandis N, Shamseer L, Kokich VG, Fleming PS, Moher D (2014) Active implementation strategy of CONSORT adherence by a dental specialty journal improved randomized clinical trial reporting. Journal of Clinical Epidemiology 67:1044–1048\n\n\n37. Jin Y, Sanger N, Shams I, et al (2018) Does the medical literature remain inadequately described despite having reporting guidelines for 21 years? A systematic review of reviews: An update. Journal of Multidisciplinary Healthcare 11:495–510\n\n\n38. Santo TD, Rice DB, Amiri LSN, Tasleem A, Li K, Boruff JT, Geoffroy M-C, Benedetti A, Thombs BD (2023) Methods and results of studies on reporting guideline adherence are poorly reported: A meta-research study. Journal of Clinical Epidemiology 159:225–234\n\n\n39. Glasziou P, Altman DG, Bossuyt P, Boutron I, Clarke M, Julious S, Michie S, Moher D, Wager E (2014) Reducing waste from incomplete or unusable reports of biomedical research. Lancet (London, England) 383:267–276"
  },
  {
    "objectID": "chapters/1_introduction/3_1_rgs.html",
    "href": "chapters/1_introduction/3_1_rgs.html",
    "title": "Thesis",
    "section": "",
    "text": "Concern over reporting quality crescendoed through the eighties and early nineties as systematic reviews became more common. Responding to calls for “strategies”, “guides”, and “lists” to help authors prepare their manuscripts, a group of methodologists, trialists, and editors met in 1996 to create the CONsolidated Standards of Reporting Trials (CONSORT) statement [1]. CONSORT is a set of recommendations detailing what information authors should include in clinical trial reports. It comprised an article describing how it was made, a checklist, flow diagram, and (after an update in 2001) and ‘Explanation and Elaboration’ publication [2]; [3].\nCONSORT proved influential, and other groups quickly developed guidelines for different research types. Reporting guidelines are like a theme and variations, where CONSORT forged a path others have followed with varying fidelity (See Table 1). Most have acronym names. Most were first published as a journal article describing their development. Some, but not all, have checklists and elaboration documents. Some guideline developers publish resources as separate documents, others put them all into a single journal article. Guidelines are developed by different groups, with different composition (possibly including methodologists, editors, clinicians etc) and in different ways (e.g., some by delphi consensus). Although most follow CONSORT’s approach of presenting recommendations focussing on reporting above conduct, guidelines differ in how forceful their recommendations are and whether they also seek to influence design.\n\n\n\nTable 1: A selection of highly cited reporting guidelines\n\n\n\n\n\n\n\n\n\n\n\n\n\nGuideline acronym\nDefinition\nApplicable study type\nPublication year\nDevelopment article?\nFillable checklist?\nExplanatory document?\nOther resources\nInfluences design?\n\n\nCONSORT\nConsolidated Standards of Reporting Trials\nRandomised controlled trials\n1996 #REF updated in 2001 #REF and 2010 #REF\nYes\nYes\nYes, as a separate article\nFlow diagram\nWebsite\nCOBWEB writing tool #REF\nNo\n\n\nPRISMA\nPreferred Reporting Items for Systematic Reviews and Meta-Analyses\nSystematic Reviews and Meta-Analyses\n2009 #REF\nUpdated in 2021 #REF\nYes\nYes\nYes\nFlow diagram\nWebsite\nNo\n\n\nARRIVE\nAnimal Research: Reporting of In Vivo Experiments\nPublications describing research involving live animals\n2010 #REF\nUpdated in 2020 #REF\nYes\nYes\nYes\nWebsite\nAction Plans\nCompliance questionnaire\nNot explicitly, but does contain design guidance\n\n\nSRQR\nStandards for Reporting Qualitative Research\nQualitative health research\n2014 [4]\nYes\nNo\nYes, as supplementary material that is hard to find\n\n\n\n\ne.t.c. for all guidelines mentioned on EQUATOR’s home page\n\n#TODO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are now over 500 reporting guidelines, representing the collective work of thousands of academics. The best-known guidelines are endorsed by large numbers of medical journals and the International Committee of Medical Journal Editors, and are amongst the 1% most highly cited publications indexed by Web of Science [5]."
  },
  {
    "objectID": "chapters/1_introduction/3_1_rgs.html#reporting-guidelines-to-the-rescue",
    "href": "chapters/1_introduction/3_1_rgs.html#reporting-guidelines-to-the-rescue",
    "title": "Thesis",
    "section": "",
    "text": "Concern over reporting quality crescendoed through the eighties and early nineties as systematic reviews became more common. Responding to calls for “strategies”, “guides”, and “lists” to help authors prepare their manuscripts, a group of methodologists, trialists, and editors met in 1996 to create the CONsolidated Standards of Reporting Trials (CONSORT) statement [1]. CONSORT is a set of recommendations detailing what information authors should include in clinical trial reports. It comprised an article describing how it was made, a checklist, flow diagram, and (after an update in 2001) and ‘Explanation and Elaboration’ publication [2]; [3].\nCONSORT proved influential, and other groups quickly developed guidelines for different research types. Reporting guidelines are like a theme and variations, where CONSORT forged a path others have followed with varying fidelity (See Table 1). Most have acronym names. Most were first published as a journal article describing their development. Some, but not all, have checklists and elaboration documents. Some guideline developers publish resources as separate documents, others put them all into a single journal article. Guidelines are developed by different groups, with different composition (possibly including methodologists, editors, clinicians etc) and in different ways (e.g., some by delphi consensus). Although most follow CONSORT’s approach of presenting recommendations focussing on reporting above conduct, guidelines differ in how forceful their recommendations are and whether they also seek to influence design.\n\n\n\nTable 1: A selection of highly cited reporting guidelines\n\n\n\n\n\n\n\n\n\n\n\n\n\nGuideline acronym\nDefinition\nApplicable study type\nPublication year\nDevelopment article?\nFillable checklist?\nExplanatory document?\nOther resources\nInfluences design?\n\n\nCONSORT\nConsolidated Standards of Reporting Trials\nRandomised controlled trials\n1996 #REF updated in 2001 #REF and 2010 #REF\nYes\nYes\nYes, as a separate article\nFlow diagram\nWebsite\nCOBWEB writing tool #REF\nNo\n\n\nPRISMA\nPreferred Reporting Items for Systematic Reviews and Meta-Analyses\nSystematic Reviews and Meta-Analyses\n2009 #REF\nUpdated in 2021 #REF\nYes\nYes\nYes\nFlow diagram\nWebsite\nNo\n\n\nARRIVE\nAnimal Research: Reporting of In Vivo Experiments\nPublications describing research involving live animals\n2010 #REF\nUpdated in 2020 #REF\nYes\nYes\nYes\nWebsite\nAction Plans\nCompliance questionnaire\nNot explicitly, but does contain design guidance\n\n\nSRQR\nStandards for Reporting Qualitative Research\nQualitative health research\n2014 [4]\nYes\nNo\nYes, as supplementary material that is hard to find\n\n\n\n\ne.t.c. for all guidelines mentioned on EQUATOR’s home page\n\n#TODO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are now over 500 reporting guidelines, representing the collective work of thousands of academics. The best-known guidelines are endorsed by large numbers of medical journals and the International Committee of Medical Journal Editors, and are amongst the 1% most highly cited publications indexed by Web of Science [5]."
  },
  {
    "objectID": "chapters/1_introduction/3_1_rgs.html#the-equator-network-unites-the-reporting-guideline-movement",
    "href": "chapters/1_introduction/3_1_rgs.html#the-equator-network-unites-the-reporting-guideline-movement",
    "title": "Thesis",
    "section": "The EQUATOR Network unites the reporting guideline movement",
    "text": "The EQUATOR Network unites the reporting guideline movement\nAs reporting guidelines grew in number and the problem of poor reporting gained recognition, Doug Altman saw the need to catalogue reporting guidelines and form a community. He united academics from around the world to form The EQUATOR Network, often simply called EQUATOR, standing for Enhancing the QUAlity and Transparency Of health Research. It was the first coordinated attempt to combat poor reporting systematically and on a global scale. One of EQUATOR’s core objectives was to create a database of reporting guidelines, accessible via their website where researchers will also find training and information about developing guidelines.\n\n\n\n\n1. Begg C, Cho M, Eastwood S, et al (1996) Improving the Quality of Reporting of Randomized Controlled Trials: The CONSORT Statement. JAMA 276:637–639\n\n\n2. Altman DG (1996) Better reporting of randomised controlled trials: The CONSORT statement. BMJ (Clinical research ed) 313:570–571\n\n\n3. Altman DG, Schulz KF, Moher D, Egger M, Davidoff F, Elbourne D, Gøtzsche PC, Lang T (2001) The Revised CONSORT Statement for Reporting Randomized Trials: Explanation and Elaboration. Annals of Internal Medicine 134:663–694\n\n\n4. O’Brien BC, Harris IB, Beckman TJ, Reed DA, Cook DA (2014) Standards for Reporting Qualitative Research: A Synthesis of Recommendations. Academic Medicine 89:1245–1251\n\n\n5. Caulley L, Cheng W, Catalá-López F, Whelan J, Khoury M, Ferraro J, Husereau D, Altman DG, Moher D (2020) Citation impact was highly variable for reporting guidelines of health research: A citation analysis. Journal of Clinical Epidemiology 127:96–104"
  },
  {
    "objectID": "chapters/1_introduction/5_evidence_gap.html",
    "href": "chapters/1_introduction/5_evidence_gap.html",
    "title": "Thesis",
    "section": "",
    "text": "Some of the articles I have cited end with rallying cries like “major improvements need active enforcement” [1]. It would be tempting to look at Pandis’ results as support for heavy editorial enforcement being the best option, but this approach may not generalise to other journals and other guidelines. The dentistry journal in this study was small. Only 23 manuscripts underwent this treatment over 2 years, and despite giving “30 to 60 minutes” of editorial attention to each manuscript, not all completely adhered to CONSORT. The study authors admit the benefits should be “considered in the light of the additional time requirement and need for greater editorial input during the peer review process”.\n\nOther articles have called for lighter forms of enforcement; “We need to promote more active implementation, such as submission of the checklist with the manuscript” wrote Dechartres [2], and “it is not sufficient for journals to simply recommend the use of STREGA to authors in the authors’ instructions; instead, journals should require submission of the STREGA checklist together with the manuscript” wrote Nedovic [3]. But the PLOS One [4] and BMJ Open studies [5] found little effect of checklist completion on reporting quality. Additionally, the PLOS One study found that enforcing checklists, although less burdensome than the editorial enforcement described by Pandis, still came with costs to both editors and authors and significantly prolonged publication times. Peer reviews focussing on reporting might help [6] but is relatively unexplored as an option.\nThese studies all focussed on different methods of enforcement. Some include incidental findings hinting at areas-for-improvement unfixable by enforcement alone. For example, because reviewers assessing adherence in the PLOS One study did not always agree or fully understand the guidance, the study authors suggested refining the guideline’s “content” and “perceived clarity”. In the WebCONSORT study, [7] Hopewell et al. made some guesses for why their intervention failed. They had to exclude 39% of manuscripts because editors had incorrectly identified them as randomised trials, and a quarter of authors selected inappropriate extensions. As a solution, they suggest a tool to help authors and editors identify study types. The study authors also raised other hypotheses to explain why their intervention failed: perhaps the custom combined checklists were too long, unclear, or perhaps giving feedback during manuscript revision was too late.\nBoth of these studies may have benefited from a qualitative component to understand why the interventions were not working. In our BMJ Open study we surveyed authors after they completed checklists. Many reported finding the checklist too long, confusing, or irrelevant. However, because we used a multiple choice question with a (small) box for a free text answer, and because we did not survey authors if they did not complete a checklist, authors may have faced other barriers too.\nThese studies suggest authors may face barriers when trying to use reporting guidelines that require solutions beyond enforcement. Noting the outcome assessors’ confusion in the PLOS One study, ARRIVE’s developers took steps to refine its clarity when they revised it (#REF). They also decided to prioritise items to make the guidance quicker to apply. Hopewell et al. created WebCONSORT because they worried combining CONSORT with its extensions may be “cumbersome and difficult” without providing evidence for this claim. In 2018 I worked with EQUATOR as a freelance developer to create GoodReports.org, a website where authors could find and fill out checklists on-line. The website addressed two barriers. Firstly a questionnaire helped authors find the right guideline. Secondly, authors could complete checklists easily (previously, some reporting guidelines came with uneditable PDF checklists).\nThese innovation efforts shared limitations. None took steps to identify barriers thoroughly. By focussing on a few barriers they may have neglected others or introduced new ones. For example, in trying to make combining checklists easier, WebCONSORT may inadvertently made checklists longer, and increased the risk of authors selecting inappropriate guidance. Secondly, these studies did not systematically consider options to solve those barriers. For example, ARRIVE’s development team decided to prioritize items as a way to make the guidance quicker to apply, but this is not the only solution. They could also have considered reducing the number of items, making guidance more concise, providing suggested wording or creating tools to speed up writing. Thirdly, although the studies describe their innovations, they do not always describe changes beyond the tool in question. For example, implementing WebCONSORT and PLOS One’s checklist policy also involved changes in editorial practice. Nor do these studies always clearly explain how changes are expected to alter behaviour.\n\n\n\n\n1. Glasziou P, Altman DG, Bossuyt P, Boutron I, Clarke M, Julious S, Michie S, Moher D, Wager E (2014) Reducing waste from incomplete or unusable reports of biomedical research. Lancet (London, England) 383:267–276\n\n\n2. Dechartres A, Trinquart L, Atal I, Moher D, Dickersin K, Boutron I, Perrodeau E, Altman DG, Ravaud P (2017) Evolution of poor reporting and inadequate methods over time in 20 920 randomised controlled trials included in Cochrane reviews: Research on research study. BMJ 357:j2490\n\n\n3. Nedovic D, Panic N, Pastorino R, Ricciardi W, Boccia S (2016) Evaluation of the Endorsement of the STrengthening the REporting of Genetic Association Studies (STREGA) Statement on the Reporting Quality of Published Genetic Association Studies. Journal of Epidemiology 26:399–404\n\n\n4. Hair K, Macleod MR, Sena ES, et al (2019) A randomised controlled trial of an Intervention to Improve Compliance with the ARRIVE guidelines (IICARus). Research Integrity and Peer Review 4:12\n\n\n5. Struthers C, Harwood J, de Beyer JA, Dhiman P, Logullo P, Schlüssel M (2021) GoodReports: Developing a website to help health researchers find and use reporting guidelines. BMC medical research methodology 21:217\n\n\n6. Cobo E, Cortés J, Ribera JM, et al (2011) Effect of using reporting guidelines during peer review on quality of final manuscripts submitted to a biomedical journal: Masked randomised trial. BMJ 343:d6783\n\n\n7. Hopewell S, Boutron I, Altman DG, et al (2016) Impact of a web-based tool (WebCONSORT) to improve the reporting of randomised trials: Results of a randomised controlled trial. BMC Medicine 14:199"
  },
  {
    "objectID": "chapters/1_introduction/5_evidence_gap.html#evidence-gap-what-more-could-be-done-to-improve-guideline-adherence",
    "href": "chapters/1_introduction/5_evidence_gap.html#evidence-gap-what-more-could-be-done-to-improve-guideline-adherence",
    "title": "Thesis",
    "section": "",
    "text": "Some of the articles I have cited end with rallying cries like “major improvements need active enforcement” [1]. It would be tempting to look at Pandis’ results as support for heavy editorial enforcement being the best option, but this approach may not generalise to other journals and other guidelines. The dentistry journal in this study was small. Only 23 manuscripts underwent this treatment over 2 years, and despite giving “30 to 60 minutes” of editorial attention to each manuscript, not all completely adhered to CONSORT. The study authors admit the benefits should be “considered in the light of the additional time requirement and need for greater editorial input during the peer review process”.\n\nOther articles have called for lighter forms of enforcement; “We need to promote more active implementation, such as submission of the checklist with the manuscript” wrote Dechartres [2], and “it is not sufficient for journals to simply recommend the use of STREGA to authors in the authors’ instructions; instead, journals should require submission of the STREGA checklist together with the manuscript” wrote Nedovic [3]. But the PLOS One [4] and BMJ Open studies [5] found little effect of checklist completion on reporting quality. Additionally, the PLOS One study found that enforcing checklists, although less burdensome than the editorial enforcement described by Pandis, still came with costs to both editors and authors and significantly prolonged publication times. Peer reviews focussing on reporting might help [6] but is relatively unexplored as an option.\nThese studies all focussed on different methods of enforcement. Some include incidental findings hinting at areas-for-improvement unfixable by enforcement alone. For example, because reviewers assessing adherence in the PLOS One study did not always agree or fully understand the guidance, the study authors suggested refining the guideline’s “content” and “perceived clarity”. In the WebCONSORT study, [7] Hopewell et al. made some guesses for why their intervention failed. They had to exclude 39% of manuscripts because editors had incorrectly identified them as randomised trials, and a quarter of authors selected inappropriate extensions. As a solution, they suggest a tool to help authors and editors identify study types. The study authors also raised other hypotheses to explain why their intervention failed: perhaps the custom combined checklists were too long, unclear, or perhaps giving feedback during manuscript revision was too late.\nBoth of these studies may have benefited from a qualitative component to understand why the interventions were not working. In our BMJ Open study we surveyed authors after they completed checklists. Many reported finding the checklist too long, confusing, or irrelevant. However, because we used a multiple choice question with a (small) box for a free text answer, and because we did not survey authors if they did not complete a checklist, authors may have faced other barriers too.\nThese studies suggest authors may face barriers when trying to use reporting guidelines that require solutions beyond enforcement. Noting the outcome assessors’ confusion in the PLOS One study, ARRIVE’s developers took steps to refine its clarity when they revised it (#REF). They also decided to prioritise items to make the guidance quicker to apply. Hopewell et al. created WebCONSORT because they worried combining CONSORT with its extensions may be “cumbersome and difficult” without providing evidence for this claim. In 2018 I worked with EQUATOR as a freelance developer to create GoodReports.org, a website where authors could find and fill out checklists on-line. The website addressed two barriers. Firstly a questionnaire helped authors find the right guideline. Secondly, authors could complete checklists easily (previously, some reporting guidelines came with uneditable PDF checklists).\nThese innovation efforts shared limitations. None took steps to identify barriers thoroughly. By focussing on a few barriers they may have neglected others or introduced new ones. For example, in trying to make combining checklists easier, WebCONSORT may inadvertently made checklists longer, and increased the risk of authors selecting inappropriate guidance. Secondly, these studies did not systematically consider options to solve those barriers. For example, ARRIVE’s development team decided to prioritize items as a way to make the guidance quicker to apply, but this is not the only solution. They could also have considered reducing the number of items, making guidance more concise, providing suggested wording or creating tools to speed up writing. Thirdly, although the studies describe their innovations, they do not always describe changes beyond the tool in question. For example, implementing WebCONSORT and PLOS One’s checklist policy also involved changes in editorial practice. Nor do these studies always clearly explain how changes are expected to alter behaviour.\n\n\n\n\n1. Glasziou P, Altman DG, Bossuyt P, Boutron I, Clarke M, Julious S, Michie S, Moher D, Wager E (2014) Reducing waste from incomplete or unusable reports of biomedical research. Lancet (London, England) 383:267–276\n\n\n2. Dechartres A, Trinquart L, Atal I, Moher D, Dickersin K, Boutron I, Perrodeau E, Altman DG, Ravaud P (2017) Evolution of poor reporting and inadequate methods over time in 20 920 randomised controlled trials included in Cochrane reviews: Research on research study. BMJ 357:j2490\n\n\n3. Nedovic D, Panic N, Pastorino R, Ricciardi W, Boccia S (2016) Evaluation of the Endorsement of the STrengthening the REporting of Genetic Association Studies (STREGA) Statement on the Reporting Quality of Published Genetic Association Studies. Journal of Epidemiology 26:399–404\n\n\n4. Hair K, Macleod MR, Sena ES, et al (2019) A randomised controlled trial of an Intervention to Improve Compliance with the ARRIVE guidelines (IICARus). Research Integrity and Peer Review 4:12\n\n\n5. Struthers C, Harwood J, de Beyer JA, Dhiman P, Logullo P, Schlüssel M (2021) GoodReports: Developing a website to help health researchers find and use reporting guidelines. BMC medical research methodology 21:217\n\n\n6. Cobo E, Cortés J, Ribera JM, et al (2011) Effect of using reporting guidelines during peer review on quality of final manuscripts submitted to a biomedical journal: Masked randomised trial. BMJ 343:d6783\n\n\n7. Hopewell S, Boutron I, Altman DG, et al (2016) Impact of a web-based tool (WebCONSORT) to improve the reporting of randomised trials: Results of a randomised controlled trial. BMC Medicine 14:199"
  },
  {
    "objectID": "chapters/3_synthesis/tbl_codes_broken.html",
    "href": "chapters/3_synthesis/tbl_codes_broken.html",
    "title": "Thesis",
    "section": "",
    "text": "TODO: This table needs fixing. The markdown format doesn’t work with merged cells\n\n\nTable 1: Codes, descriptive themes, and analytic themes\n\n\n\n\n\n\n\n\n\nAnalytic theme\n\n\n\n\nWhat does this term mean? [1–5] | What does this mean? | Researchers may not understand the guidance as intended, or what reporting guidelines are, even if they think they do | | | What does this item mean? [1–6] | Why is this item important? | | | | How are these items different?[2, 4, 6, 7] | Does this apply to me? | | | | Have I understood this as intended? [1, 2] | I don’t understand what reporting guidelines are | | | | Examples help me understand items [4, 8, 9] | | | | | Why is this item important? [2–4, 10] | | | | | Who is this item important to? [2, 4, 11] | | | | | Have I understood the guideline’s scope as intended? [4, 5] | | | | | Does this item apply to me? [2, 4–7] | | | | | Is this item optional? [2, 6] | | | | | What are reporting guidelines? [11, 12] | | | | | How should I use a reporting guideline? [13] | | |\n\n\nI find guidelines useful in general [5, 14] | Guidelines benefit me | Researchers report a variety of reasons for using reporting guidelines, and that some are more important than others | | | Guidelines make me feel confident [11] | I use guidelines because of other people | | | | Guidelines help me develop as a researcher [11, 15] | Guidelines benefit others | | | | Guidelines may help me improve my manuscript [2, 7, 11, 14, 15] | Some benefits are more important than others | | | | I believe guidelines may help me publish more easily [16] | | | | | I may use guidelines because journals and editors tell me to [11, 13, 15, 16] | | | | | I may use guidelines because other researchers expect it [13, 16] | | | | | Standardized reporting benefits the community [11, 16, 17] | | | | | Immediate benefits are more important than hypothetical ones [11, 16] | | | | | Personal benefits are more important than benefits to others [16] | | |\n\n\nI use reporting guidelines for planning research [2, 11] | Researchers use reporting guidelines for different tasks | Researchers report using reporting guidelines for different tasks and wanting guidance to be delivered in ways that better fit their needs | | | I use reporting guidelines for designing research [6, 11, 12, 14] | I want guidance presented in formats that are better suited to the task I am doing | | | | I use reporting guidelines for writing [2, 6, 11, 14] | | | | | I use reporting guidelines for checking my own or other people’s writing [11, 12] | | | | | I use reporting guidelines to appraise the quality of other people’s reporting [3] | | | | | I use reporting guidelines for peer reviewing [11] | | | | | I want items presented in the order in which I must do them [; [17]; [9]] | | | | | I want design or methods advice [2, 4, 11] | | | | | I want templates for writing [14] | | | | | I want checklists that are easy to fill in [5, 18] | | | | | I want checklists embedded into journal submission workflows [14] | | | | | I want items embedded into data collection tools [15] | | |\n\n\nGuidelines take time to read, understand and apply [13, 15, 16] | Guidelines take time | Using reporting guidelines has costs, and researchers may not feel that benefits outweigh the costs | | | Some items require extra work which takes time and effort [1, 2, 19] | Itemization may decrease costs | | | | I want an indication of which items to prioritize [2, 6] | Itemization may increase perceived costs | | | | Perceived complexity [2, 14, 16, 18] | I think guidelines make my manuscripts long and bloated | | | | Long guidelines are off-putting [5, 7, 11, 15] | The benefits of using a reporting guideline may not outweigh the costs | | | | Itemization helps me navigate guidance[4] | The balance of benefits vs costs may be more favourable when guidelines are used early | | | | Itemization summarizes the guidance[14] | | | | | Itemization makes guidance appear longer[4] | | | | | Itemization blocks the bigger picture[2] | | | | | Following reporting guidance can result in long, bloated articles [2, 6, 7, 15] | | | | | Long, bloated articles may exceed journal word limits [6, 7, 13, 18] | | | | | I want options for where to report this item [1, 2, 4, 7, 11, 13] | | | | | The benefits of using a reporting guideline may not outweigh the costs [7, 11, 13] | | | | | Guidelines are more valuable when used early [2, 5, 11, 14] | | |\n\n\nI would clarify this item [4, 6] | I think the guidance could be improved | Reporting guidelines may need to be revised and updated for different reasons | | | I would move this item [1, 2] | Guidelines need to be kept updated | | | | I would split this item into two [2, 4, 9] | | | | | I would add or remove items from this guideline [2–4, 6] | | | | | I would add or remove requirements from this item [4, 6, 8, 10, 11] | | | | | Guidelines can become out of date [2] | | | | | Guidelines need to be updated [4] | | |\n\n\nI cannot report this because I didn’t do it | I feel unable to report this | Researchers may not be able to report all items which can leave them feeling uncertain or worried | | [2, 4, 6, 7] | I feel nervous or uncertain if I am unable to report an item | | | | I cannot report this because of intellectual property issues | | | | [7] | | | | | I cannot report this because it clashes with journal guidelines [4] | | | | | I cannot report this because data was missing from my primary studies [15] | | | | | Editors, reviewers or co-authors asked me to remove this item [6, 19] | | | | | I feel uncertain because I don’t know how to say that I didn’t do it [4] | | | | | I feel worried that I will be judged for transparently reporting something I didn’t do [4, 11] | | |\n\n\nI may not know that reporting guidelines exist, or what guidance exists [3, 5, 13, 14, 16] | I can only use what I know about and have | Awareness and accessibility may limit reporting guideline usage | | | I may not be able to easily access guidance [5, 16] | | |\n\n\nReporting guidelines may be less valuable to experienced researchers [7, 11, 14] | Reporting guidelines are more valuable to inexperienced researchers | Reporting guidelines may be more useful to less experienced researchers, but less experienced researchers may find them harder to use | | | Experienced researchers feel that they already know how to report [2, 11, 14] | Reporting guidelines can be hard to use at first but get easier with experience | | | | Experienced researchers find guidance patronizing and feel untrusted [4, 7, 13, 18] | | | | | Reporting guidelines can be hard to use at first but get easier with experience [2, 13, 16] | | |\n\n\nI want design or methodological advice [4, 11, 18] | I want or need design advice | Researchers want or need design advice, but reporting guidelines may not be the right place | | | I don’t know how to do this item [2, 4, 6] | I think this guidance prescribes how research should be designed | | | | Guidelines are procedural straightjackets [11] | | | | | This guideline is too prescriptive [4, 10, 11] | | |\n\n\nThe guideline’s applicability criteria are not clear [3, 5, 14] | A guideline’s scope can be unclear | Reporting guidelines can be harder to use if their scope is too broad, too narrow, or poorly defined | | | This guideline isn’t a perfect fit for me [5] | A guideline can be too narrow | | | | This guideline doesn’t generalise [4, 10, 11, 14, 18] | A guideline’s scope can be too broad | | | | This guideline is too prescriptive [4, 10, 11] | | | | | I don’t want to see optional items that only apply to other types of study [5, 6] | | |\n\n\nI need to adhere to journal guidelines or other research guidelines [4, 6, 13, 14] | Authors often need to adhere to multiple sets of guidance | Researchers may have to use multiple sets of reporting guidelines, multiplying complexity and costs | | | I might need to use multiple reporting guidelines [11] | I want guidelines to harmonize | | | | I want reporting guidelines to be linked or embedded [3, 4] | | | | | I want reporting guidelines to use similar structure [4] | | | | | I want reporting guidelines to use similar terms [4] | | |\n\n\nI don’t like checklists[5, 7, 11, 14] | I experience reporting guidelines primarily as, or through, checklists | Researchers may use checklists but never read the full guidance | | | I may use the checklist instead of the full guidance [8] | | | | | I may use the checklist before I read the full guidance [8] | | |\n\n\n\n\n\n\n\n\n1. Davies L, Donnelly KZ, Goodman DJ, Ogrinc G (2016) Findings from a novel approach to publication guideline revision: User road testing of a draft version of SQUIRE 2.0. BMJ quality & safety 25:265–272\n\n\n2. Davies L, Batalden P, Davidoff F, Stevens D, Ogrinc G (2015) The SQUIRE Guidelines: An evaluation from the field, 5years post release. BMJ quality & safety 24:769–775\n\n\n3. Korevaar DA, Cohen JF, Reitsma JB, et al (2016) Updating standards for reporting diagnostic accuracy: The development of STARD 2015. Research integrity and peer review 1:7\n\n\n4. Page MJ, McKenzie JE, Bossuyt PM, Boutron I, Hoffmann TC, Mulrow CD, Shamseer L, Tetzlaff JM, Moher D (2021) Updating guidance for reporting systematic reviews: Development of the PRISMA 2020 statement. Journal of Clinical Epidemiology 134:103–112\n\n\n5. Struthers C, Harwood J, de Beyer JA, Dhiman P, Logullo P, Schlüssel M (2021) GoodReports: Developing a website to help health researchers find and use reporting guidelines. BMC medical research methodology 21:217\n\n\n6. Prady SL, MacPherson H (2007) Assessing the utility of the standards for reporting trials of acupuncture (STRICTA): A survey of authors. The Journal of Alternative and Complementary Medicine 13:939–943\n\n\n7. Eysenbach G. (2013) CONSORT-EHEALTH: Implementation of a checklist for authors and editors to improve reporting of web-based and mobile randomized controlled trials. Studies in health technology and informatics 192:657–661\n\n\n8. Sert NP du, Hurst V, Ahluwalia A, et al (2020) The ARRIVE guidelines 2.0: Updated guidelines for reporting animal research. PLOS Biology 18:e3000410\n\n\n9. de Vries RBM, Hooijmans CR, Langendam MW, van Luijk J, Leenaars M, Ritskes-Hoitinga M, Wever KE (2015) A protocol format for the preparation, registration and publication of systematic reviews of animal intervention studies. Evidence-based Preclinical Medicine 2:e00007\n\n\n10. Tam WWS, Tang A, Woo B, Goh SYS (2019) Perception of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement of authors publishing reviews in nursing journals: A cross-sectional online survey. BMJ Open 9:e026271\n\n\n11. Sharp MK, Glonti K, Hren D (2020) Online survey about the STROBE statement highlighted diverging views about its content, purpose, and value. Journal of clinical epidemiology 123:100–106\n\n\n12. Prager R, Gagnon L, Bowdridge J, Unni RR, McGrath TA, Cobey K, Bossuyt PM, McInnes MDF (2021) Barriers to reporting guideline adherence in point-of-care ultrasound research: A cross-sectional survey of authors and journal editors. BMJ Evidence-Based Medicine bmjebm-2020-111604\n\n\n13. Fuller T, Pearson M, Peters J, Anderson R (2015) What affects authors’ and editors’ use of reporting guidelines? Findings from an online survey and qualitative interviews. PLoS ONE 10:e0121585\n\n\n14. Dewey M, Levine D, Bossuyt PM, Kressel HY (2019) Impact and perceived value of journal reporting guidelines among Radiology authors and reviewers. European Radiology 29:3986–3995\n\n\n15. Burford BJ, Welch V, Waters E, Tugwell P, Moher D, O’Neill J, Koehlmoos T, Petticrew M (2013) Testing the PRISMA-Equity 2012 reporting guideline: The perspectives of systematic review authors. PloS one 8:e75122\n\n\n16. Svensøy JN, Nilsson H, Rimstad R (2021) A qualitative study on researchers’ experiences after publishing scientific reports on major incidents, mass-casualty incidents, and disasters. Prehospital and Disaster Medicine 36:536–542\n\n\n17. Page MJ, McKenzie JE, Bossuyt PM, et al (2021) The PRISMA 2020 statement: An updated guideline for reporting systematic reviews. BMJ (Clinical research ed) 372:n71\n\n\n18. Macleod M, Collings AM, Graf C, Kiermer V, Mellor D, Swaminathan S, Sweet D, Vinson V (2021) The MDAR (Materials Design Analysis Reporting) Framework for transparent reporting in the life sciences. Proceedings of the National Academy of Sciences 118:e2103238118\n\n\n19. Rader T., Mann M., Stansfield C., Cooper C., Sampson M. (2014) Methods for documenting systematic review searches: A discussion of common issues. Research synthesis methods 5:98–115"
  },
  {
    "objectID": "chapters/3_synthesis/tbl_study_characteristics.html",
    "href": "chapters/3_synthesis/tbl_study_characteristics.html",
    "title": "Thesis",
    "section": "",
    "text": "Table 1: Study characteristics\n\n\n\n\n\n\n\n\n\n\n\nAuthors\nParticipants\nParticipant’s country\nGuideline(s) considered\nMethods\nPhenomena of interest\nCASP rating\n\n\n\n\nBurford et al., 2013 [1]\n151 systematic review authors\nNot reported\nPRISMA Equity\nMixed methods survey\nPerceived utility, facilitators, and barriers\nFairly valuable\n\n\nDavies et al., 2015 [2]\n18 experts and 29 end users\nUSA, Canada, Sweden, the UK, and Norway\nSQUIRE\nFocus groups and interviews\nExperiences with and impressions of the SQUIRE Guidelines\nValuable\n\n\nDavies et al., 2016 [3]\n44 graduates faculty, and directors of healthcare\nNot reported\nSQUIRE\nMixed methods survey and written exercise\n“whether SQUIRE 1.6 was understood and implemented as intended by the developers”\nValuable\n\n\nDe Vries et al. 2015 [4]\n7 researchers\nNot reported\nSystematic review protocols of animal intervention studies\n\nFeedback on usability, missing items, possibilities for improvement and clarity\nNot very valuable\n\n\nDewey et al., 2019 [5]\n74 out of 831 survey respondents that provided (optional) free text comments\nThe full survey was answered by respondents in the USA, Canada, China, South Korea, Japan, Germany, France, Italy, UK, Other European countries, Middle East, Latin America, Other. It's unclear where respondents for the free text answers came from.\nCONSORT, STROBE, PRISMA, STARD\nMixed methods survey\n\"(1) When and how are reporting guidelines and checklists used by authors and reviewers? (2) What is their impact on the content of final manuscript drafts according to authors? and (3) How do authors and reviewers perceive the value of reporting guidelines and related checklists?\"\nFairly valuable\n\n\nEysenbach 2013 [6]\n61 authors\nNot reported\nCONSORT Ehealth\nMixed methods survey\nViews on completing the checklist as part of submission\nFairly valuable\n\n\nFuller et al., 2015 [7]\n5 authors\nUSA and Australia\nTREND and Reporting Guidelines in general\nSemi structured interviews\nFactors that affected authors’ use of TREND and other reporting guidelines\nValuable\n\n\nKorevaar et al., 2016 [8]\n4 radiology residents, 8 laboratory medicine experts\nRadiology residents were from the Netherlands. No geographical details provided for experts\nSTARD\nInterview (residents) and mixed methods survey (experts)\nTo identify items that were vague, ambiguous, difficult to interpret, or missing\nFairly valuable\n\n\nMacleod et al., 2021 [9]\n211 authors, but only some answered the free text question\nThe full survey was answered by participants in the USA, China, Japan, Germany, EU, and “Other” areas. It is unclear who answered the free text question.\nMaterials Design Analysis Reporting framework\nMixed methods survey\nWhether the checklist was clear and useful\nFairly valuable\n\n\nPage et al., 2021 [10]\n110 systematic review authors and experts\nNot reported\nPRISMA\nMixed methods survey\nOpinions on PRISMA and proposed changes\nValuable\n\n\nPrady et al., 2007 [11]\n40 authors\nNot reported\nStandards for Reporting Interventions in Controlled Trials of Acupuncture\nMixed methods survey\nExperiences using PRISMA\nFairly valuable\n\n\nPrager et al., 2021 [12]\n5 of 18 survey respondents that answered the free text question\nNot reported\nSTARD\nMixed methods survey\nBarriers to STARD 2015 adherence\nFairly valuable\n\n\nRader et al., 2014 [13]\n263 systematic reviewers\nNot reported\nPRISMA\nMixed methods survey\nBarriers or difficulties in meeting more detailed reporting standards in PRISMA\nFairly valuable\n\n\ndu Sert et al., 2020 [14] 11 authors\nUK, USA, Belgium, Br\nazil ARRIVE\nInterview and writin\ng task Authors’ opinions, i\nnterpretation, and experiences of updated ARRIVE guidelines Fairly valuable\n\n\n\nSharp et al., 2020 [15]\n203 of 1015 researchers that answered free text questions\nThe full survey was answered by participants in Africa, Asia, Europe, North and South America, Middle East, and Pacific Region. It is unclear who answered the free text question.\nSTROBE\nMixed methods survey\nExperiences and attitudes towards STROBE\nValuable\n\n\nStruthers et al., 2021 [16]\n623 authors, 274 of whom answered free the text question\nNot reported\nReporting guidelines in general\nMixed methods survey\nThe question asked, “What could I do to improve the guideline?”\nFairly valuable\n\n\nSvensøy et al., 2021 [17]\n10 authors\nNot reported\nNot specified\nSemi structured interviews\nExperiences using guidelines or templates\nValuable\n\n\nTam et al., 2019 [18]\n230 authors, 62 of whom answered the open-ended questions\nNot reported\nPRISMA\nMixed methods survey\nOpinions on PRISMA\nFairly valuable\n\n\n\n\n\n\n\n\n1. Burford BJ, Welch V, Waters E, Tugwell P, Moher D, O’Neill J, Koehlmoos T, Petticrew M (2013) Testing the PRISMA-Equity 2012 reporting guideline: The perspectives of systematic review authors. PloS one 8:e75122\n\n\n2. Davies L, Batalden P, Davidoff F, Stevens D, Ogrinc G (2015) The SQUIRE Guidelines: An evaluation from the field, 5years post release. BMJ quality & safety 24:769–775\n\n\n3. Davies L, Donnelly KZ, Goodman DJ, Ogrinc G (2016) Findings from a novel approach to publication guideline revision: User road testing of a draft version of SQUIRE 2.0. BMJ quality & safety 25:265–272\n\n\n4. de Vries RBM, Hooijmans CR, Langendam MW, van Luijk J, Leenaars M, Ritskes-Hoitinga M, Wever KE (2015) A protocol format for the preparation, registration and publication of systematic reviews of animal intervention studies. Evidence-based Preclinical Medicine 2:e00007\n\n\n5. Dewey M, Levine D, Bossuyt PM, Kressel HY (2019) Impact and perceived value of journal reporting guidelines among Radiology authors and reviewers. European Radiology 29:3986–3995\n\n\n6. Eysenbach G. (2013) CONSORT-EHEALTH: Implementation of a checklist for authors and editors to improve reporting of web-based and mobile randomized controlled trials. Studies in health technology and informatics 192:657–661\n\n\n7. Fuller T, Pearson M, Peters J, Anderson R (2015) What affects authors’ and editors’ use of reporting guidelines? Findings from an online survey and qualitative interviews. PLoS ONE 10:e0121585\n\n\n8. Korevaar DA, Cohen JF, Reitsma JB, et al (2016) Updating standards for reporting diagnostic accuracy: The development of STARD 2015. Research integrity and peer review 1:7\n\n\n9. Macleod M, Collings AM, Graf C, Kiermer V, Mellor D, Swaminathan S, Sweet D, Vinson V (2021) The MDAR (Materials Design Analysis Reporting) Framework for transparent reporting in the life sciences. Proceedings of the National Academy of Sciences 118:e2103238118\n\n\n10. Page MJ, McKenzie JE, Bossuyt PM, Boutron I, Hoffmann TC, Mulrow CD, Shamseer L, Tetzlaff JM, Moher D (2021) Updating guidance for reporting systematic reviews: Development of the PRISMA 2020 statement. Journal of Clinical Epidemiology 134:103–112\n\n\n11. Prady SL, MacPherson H (2007) Assessing the utility of the standards for reporting trials of acupuncture (STRICTA): A survey of authors. The Journal of Alternative and Complementary Medicine 13:939–943\n\n\n12. Prager R, Gagnon L, Bowdridge J, Unni RR, McGrath TA, Cobey K, Bossuyt PM, McInnes MDF (2021) Barriers to reporting guideline adherence in point-of-care ultrasound research: A cross-sectional survey of authors and journal editors. BMJ Evidence-Based Medicine bmjebm-2020-111604\n\n\n13. Rader T., Mann M., Stansfield C., Cooper C., Sampson M. (2014) Methods for documenting systematic review searches: A discussion of common issues. Research synthesis methods 5:98–115\n\n\n14. Sert NP du, Hurst V, Ahluwalia A, et al (2020) The ARRIVE guidelines 2.0: Updated guidelines for reporting animal research. PLOS Biology 18:e3000410\n\n\n15. Sharp MK, Glonti K, Hren D (2020) Online survey about the STROBE statement highlighted diverging views about its content, purpose, and value. Journal of clinical epidemiology 123:100–106\n\n\n16. Struthers C, Harwood J, de Beyer JA, Dhiman P, Logullo P, Schlüssel M (2021) GoodReports: Developing a website to help health researchers find and use reporting guidelines. BMC medical research methodology 21:217\n\n\n17. Svensøy JN, Nilsson H, Rimstad R (2021) A qualitative study on researchers’ experiences after publishing scientific reports on major incidents, mass-casualty incidents, and disasters. Prehospital and Disaster Medicine 36:536–542\n\n\n18. Tam WWS, Tang A, Woo B, Goh SYS (2019) Perception of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement of authors publishing reviews in nursing journals: A cross-sectional online survey. BMJ Open 9:e026271"
  },
  {
    "objectID": "chapters/3_synthesis/tbl_rgs.html",
    "href": "chapters/3_synthesis/tbl_rgs.html",
    "title": "Thesis",
    "section": "",
    "text": "Table 1: Reporting guidelines featured on the EQUATOR Network’s home page\n\n\n\n\n\n\nName\nFull name\n\n\n\n\nCONSORT\nConsolidated Standards of Reporting Trials\n\n\nSTROBE\nStrengthening the Reporting of Observational Studies in Epidemiology\n\n\nPRISMA\nPreferred Reporting Items for Systematic Reviews and Meta-Analyses\n\n\nPRISMA-P\nPRISMA for systematic review protocols\n\n\nSPIRIT\nStandard Protocol Items: Recommendations for Interventional Trials\n\n\nSTARD\nSTAndards for Reporting Diagnostic accuracy studies\n\n\nTRIPOD\nTransparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis\n\n\nCARE\nguidelines for CAse REports\n\n\nAGREE\nAppraisal of Guidelines, REsearch and Evaluation\n\n\nRIGHT\nReporting Items for practice Guidelines in HealThcare\n\n\nSRQR\nStandards for Reporting Qualitative Research\n\n\nCOREQ\nCOnsolidated criteria for REporting Qualitative research\n\n\nARRIVE\nAnimal Research: Reporting of In Vivo Experiments\n\n\nSQUIRE\nStandards for QUality Improvement Reporting Excellence\n\n\nCHEERS\nConsolidated Health Economic Evaluation Reporting Standards"
  },
  {
    "objectID": "chapters/2_reflexivity/index.html",
    "href": "chapters/2_reflexivity/index.html",
    "title": "Reflections on starting my DPhil",
    "section": "",
    "text": "I use qualitative methods throughout my thesis. Reflection is important in qualitative research as “A researcher’s background and position will affect what they choose to investigate, the angle of investigation, the methods judged most adequate for this purpose, the findings considered most appropriate, and the framing and communication of conclusions” [1]. More succinctly, “Despite the sterility of the instruments, we never come innocent to a research task” [2]. In this chapter, I examine my own lack of innocence. I reflect on my experiences with reporting guidelines before starting this project, and the experiences of my supervisors. The remaining chapters end with shorter reflections where I consider how my beliefs may have shaped my research and how doing the research had, in turn, shaped my beliefs.\nThis thesis marks the ten-year anniversary of my attempts to improve research reporting. My early interest came from reading about psychology’s reproducibility crisis #REF, the cancer reproducibility project #REF, Ben Goldacre’s Bad Pharma #REF, and Ioannidis’ essay “Why Most Published Research Findings Are False” #REF. I was working in a psychology laboratory at the time, having recently finished a MSc in Neuroscience. The compelling Lancet series on research waste [3] drilled home a magnitude of inefficiencies in medical research. I was most stuck by the article on incomplete or unusable research reports [4], where a key takeaway was that “although reporting guidelines are important, major improvements need active enforcement” by editors or reviewers. I took Paul Glasziou’s words as a call to action, and I set about creating tools to help editors enforce good reporting.\nI began by writing software to check whether a manuscript cited a reporting guideline. Next I wrote code to check the reporting of statistical analyses. These were both pet projects. My first “proper” software was a manuscript checker that evaluates whether a manuscript adheres to journal guidelines [5]. I began working with academic journals, and came to appreciate that reporting guidelines are one of many priorities for editors, and often fall behind more pressing issues like ethics, consent, image duplication, and paper mills.\nI later worked with EQUATOR to create GoodReports.org [6], a website where authors can find and complete reporting checklists. EQUATOR and I collaborated with BMJ Open in 2018 to see whether authors improved their manuscripts after completing a checklist as part of manuscript submission [7]. The results were disappointing. Few authors made any changes.\nI was awfully demoralized. It was tempting to blame the failure on lazy authors paying lip service to the checklists because they know that equally-lazy editors will not check. Indeed, this is a refrain I have heard bandied around by frustrated guideline developers. However, a tenet of software development is that when it comes to how a user experiences a product (or service), they cannot be wrong. A software developer that blames poor design on “lazy users” will quickly find themselves out of a job. If users do not use your product, or do not use it how you would expect them to, then your product may lack usability (users cannot understand or use it), product market fit (users do not need or want it), or awareness (users do not know about it). It is the creator’s responsibility to make their product known, foolproof, and useful.\nUnderstanding users’ experiences and needs is therefore central to making a successful product. It is also very difficult. Like many developers, I am happier behind a screen and talking to users never came naturally to me. Instead, I often latched on to an idea before understanding the people I was trying to help, or the problem I was trying to solve. This was a good strategy for building things nobody wanted.\n\nI almost fell into the same trap when starting my DPhil. In my interview I adamantly pitched a tool to create personalised checklists by combining reporting guidelines with journal, funder, and institutional requirements. I envisioned something akin to WebCONSORT on steroids. The old me would have happily spent months falling down that rabbit hole, building something (probably) useless; the custom checklists would have been incredibly long and probably confusing. The new me, the me writing this paragraph, is glad I paused and spent a year trying to better understand authors experiences and needs.\nOn starting my DPhil, I had never used a reporting guideline myself, despite recommending others to do so. Yet I held them in high regard, as did my three initial supervisors. Jen, Michael, and Gary are affiliated with the EQUATOR Network. Gary is Director of the UK EQUATOR Centre and an author of many reporting guidelines and a working-group member for many others. Michael studies reporting completeness, the robustness of reporting guidelines development methods, and the consolidation of different reporting guidelines for the reporting of studies of nutritional interventions. Jen is involved in many studies investigating reporting and reporting guidelines, and runs training on writing and reporting guidelines.\nCharlotte joined my supervision team in my second year, once we realised my thesis was fixed on a qualitative course. Qualitative behaviour change research was new territory for Gary, Jen, Michael, and I, and we needed an expert. Charlotte came to my rescue. She has a lot of experience in qualitative research and behaviour change theory, both from her own research and from leading the Oxford course on qualitative methods. However, she had never studied reporting guidelines before, nor any other meta-research phenomenon. Charlotte has published a suggested amendment to a reporting guideline she uses in her own work [8]. Whereas Jen, Michael, and Gary can be described as reporting guideline advocates, Charlotte was a little more cool-headed. Although she considered reporting guidelines useful for writing up quantitative work, she found guidelines for qualitative research frustrating because they were developed from a positivist perspective, and so did not fit all qualitative work.\nIn summary, I came to this DPhil with an existing passion for improving research, a deep respect for reporting guidelines and EQUATOR, and with determination to make something helpful after rebounding from the disappointing BMJ Open study. Sitting atop some rusty, decade-old research experience, I had a software developer’s vocabulary and mindset. Core to this mindset was a belief that if people do not use what you have made in the way you want them to, it is not their responsibility to change their behaviour. It is your responsibility to change what you have made. The challenge is figuring out what changes you need to make.\nHaving set the scene, I’ll begin my thesis where I began my research: by trying to understand why authors do not adhere to reporting guidelines. In chapter 1 I described the evidence that reporting guidelines have had little effect on reporting quality. I wanted to know why. Chapters 3 - 5 describe my mixed method approach to finding possible answers to this question. In the next chapter, I describe how my first step was to seek and synthesise all available qualitative data exploring this question.\n\n\n\n\n1. Malterud K (2001) Qualitative research: Standards, challenges, and guidelines. Lancet (London, England) 358:483–488\n\n\n2. Clough P (2002) Narratives and Fictions in Educational Research. Open University Press\n\n\n3. Macleod MR, Michie S, Roberts I, Dirnagl U, Chalmers I, Ioannidis JPA, Salman RA-S, Chan A-W, Glasziou P (2014) Biomedical research: Increasing value, reducing waste. The Lancet 383:101–104\n\n\n4. Glasziou P, Altman DG, Bossuyt P, Boutron I, Clarke M, Julious S, Michie S, Moher D, Wager E (2014) Reducing waste from incomplete or unusable reports of biomedical research. Lancet (London, England) 383:267–276\n\n\n5. Penelope.ai. Penelope.ai \n\n\n6. GoodReports.org. \n\n\n7. Struthers C, Harwood J, de Beyer JA, Dhiman P, Logullo P, Schlüssel M (2021) GoodReports: Developing a website to help health researchers find and use reporting guidelines. BMC medical research methodology 21:217\n\n\n8. Albury C, Pope C, Shaw S, et al (2021) Gender in the consolidated criteria for reporting qualitative research (COREQ) checklist. International Journal for Quality in Health Care 33:mzab123"
  },
  {
    "objectID": "chapters/abbreviations.html",
    "href": "chapters/abbreviations.html",
    "title": "List of abbreviations",
    "section": "",
    "text": "Name\nFull name\n\n\n\n\nAGREE\nAppraisal of Guidelines, REsearch and Evaluation\n\n\nARRIVE\nAnimal Research: Reporting of In Vivo Experiments\n\n\nCARE\nguidelines for CAse REports\n\n\nCHEERS\nConsolidated Health Economic Evaluation Reporting Standards\n\n\nCOGS\nConference on Guideline Standardization\n\n\nCONSORT\nConsolidated Standards of Reporting Trials\n\n\nCOREQ\nCOnsolidated criteria for REporting Qualitative research\n\n\nENTREQ\nEnhancing Transparency in Reporting the Synthesis of Qualitative Research statement\n\n\nGREET\nGuideline for Reporting Evidence-based practice Educational interventions and Teaching\n\n\nGRRAS\nGuidelines for Reporting Reliability and Agreement Studies\n\n\nMDAR\nMaterials Design Analysis Reporting\n\n\nORION\nOutbreak Reports and Intervention Studies Of Nonsocomial infection\n\n\nPRISMA\nPreferred Reporting Items for Systematic Reviews and Meta-Analyses\n\n\nPRISMA Equity\nPreferred Reporting Items for Systematic Reviews and Meta-Analyses with a Focus on Health Equity\n\n\nPRISMA-P\nPreferred Reporting Items for Systematic Reviews and Meta-Analyses Protocols\n\n\nPRISMA-S\nPreferred Reporting Items for Systematic Reviews and Meta-Analyses Searches\n\n\nRIGHT\nReporting Items for practice Guidelines in HealThcare\n\n\nSAMPL\nStatistical Analyses and Methods in the Published Literature\n\n\nSPIRIT\nStandard Protocol Items: Recommendations for Interventional Trials\n\n\nSQUIRE\nStandards for QUality Improvement Reporting Excellence\n\n\nSRQR\nStandards for Reporting Qualitative Research\n\n\nSTARD\nSTAndards for Reporting Diagnostic accuracy studies\n\n\nSTREGA\nSTrengthening the REporting of Genetic Association Studies\n\n\nSTRICTA\nSTandards for Reporting Interventions in Clinical Trials of Acupuncture\n\n\nSTROBE\nStrengthening the Reporting of Observational Studies in Epidemiology\n\n\nTREND\nTransparent Reporting of Evaluations with Nonrandomized Designs\n\n\nTRIPOD\nTransparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis\n\n\n\n\n\n\n\n\n\nCASP\nCritical Appraisal Skills Programme\n\n\nEQUATOR\nEnhancing the QUAlity and Transparency Of health Research\n\n\nGIM\nGlobal Index Medicus\n\n\nOSF\nOpen Science Framework\n\n\nSciELO\nScientific Electronic Library"
  },
  {
    "objectID": "chapters/abbreviations.html#sec-rg-abbreviations",
    "href": "chapters/abbreviations.html#sec-rg-abbreviations",
    "title": "List of abbreviations",
    "section": "",
    "text": "Name\nFull name\n\n\n\n\nAGREE\nAppraisal of Guidelines, REsearch and Evaluation\n\n\nARRIVE\nAnimal Research: Reporting of In Vivo Experiments\n\n\nCARE\nguidelines for CAse REports\n\n\nCHEERS\nConsolidated Health Economic Evaluation Reporting Standards\n\n\nCOGS\nConference on Guideline Standardization\n\n\nCONSORT\nConsolidated Standards of Reporting Trials\n\n\nCOREQ\nCOnsolidated criteria for REporting Qualitative research\n\n\nENTREQ\nEnhancing Transparency in Reporting the Synthesis of Qualitative Research statement\n\n\nGREET\nGuideline for Reporting Evidence-based practice Educational interventions and Teaching\n\n\nGRRAS\nGuidelines for Reporting Reliability and Agreement Studies\n\n\nMDAR\nMaterials Design Analysis Reporting\n\n\nORION\nOutbreak Reports and Intervention Studies Of Nonsocomial infection\n\n\nPRISMA\nPreferred Reporting Items for Systematic Reviews and Meta-Analyses\n\n\nPRISMA Equity\nPreferred Reporting Items for Systematic Reviews and Meta-Analyses with a Focus on Health Equity\n\n\nPRISMA-P\nPreferred Reporting Items for Systematic Reviews and Meta-Analyses Protocols\n\n\nPRISMA-S\nPreferred Reporting Items for Systematic Reviews and Meta-Analyses Searches\n\n\nRIGHT\nReporting Items for practice Guidelines in HealThcare\n\n\nSAMPL\nStatistical Analyses and Methods in the Published Literature\n\n\nSPIRIT\nStandard Protocol Items: Recommendations for Interventional Trials\n\n\nSQUIRE\nStandards for QUality Improvement Reporting Excellence\n\n\nSRQR\nStandards for Reporting Qualitative Research\n\n\nSTARD\nSTAndards for Reporting Diagnostic accuracy studies\n\n\nSTREGA\nSTrengthening the REporting of Genetic Association Studies\n\n\nSTRICTA\nSTandards for Reporting Interventions in Clinical Trials of Acupuncture\n\n\nSTROBE\nStrengthening the Reporting of Observational Studies in Epidemiology\n\n\nTREND\nTransparent Reporting of Evaluations with Nonrandomized Designs\n\n\nTRIPOD\nTransparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis"
  },
  {
    "objectID": "chapters/abbreviations.html#sec-abbreviations",
    "href": "chapters/abbreviations.html#sec-abbreviations",
    "title": "List of abbreviations",
    "section": "",
    "text": "CASP\nCritical Appraisal Skills Programme\n\n\nEQUATOR\nEnhancing the QUAlity and Transparency Of health Research\n\n\nGIM\nGlobal Index Medicus\n\n\nOSF\nOpen Science Framework\n\n\nSciELO\nScientific Electronic Library"
  },
  {
    "objectID": "chapters/6_bcw/index.html",
    "href": "chapters/6_bcw/index.html",
    "title": "Selecting the Behaviour Change Wheel framework",
    "section": "",
    "text": "In chapters 3 I explored possible barriers authors may encounter when using reporting guidelines. Some barriers were inherent to the guidelines to themselves, whereas others originated from other parts of the system through which reporting guidelines are disseminated. In this chapter, I explain how I chose a framework to help me understand and improve this complex system.\nI considered the MRC guidance for developing and evaluating complex interventions[1] and the Person Based Approach (PBA)[2]. Both emphasise the importance of understanding context, evidencing needs, and iterative evaluation and refinement. Both recommend involving stakeholders and the intervention recipients. I have adhered to these principles in my thesis. In chapters 3 - 5 I collected evidence for needs and context. In chapters 7, 8, and 11 I explain how I involved stakeholders and users, and in chapters 11 and 12 I explain how I refined the intervention in response to author feedback and possible next steps for iterative development.\nHowever, at this stage in my PhD my aim was to find solutions to the barriers I had identified. Neither MRC guidelines nor PBA advise you how to do this directly. Instead, they encourage intervention designers to theorise how the intervention is working. To do this, designers need to select a theoretical framework. First I considered the Theoretical Domains Framework (TDF, [3]; [4]). The TDF was developed to help implementation researchers identify what influences the behaviour of health professionals and patients. It was made by synthesising 33 theories of behaviour and behaviour change into 14 domains that cover the cognitive, affective, social, and environmental influences on behaviour. The TDF and its accompanying guide [5] are useful, but they require considerable understanding of psychological constructs that might not be familiar to non-behavioural experts, and although the TDF helps understand what influences behaviour, there is little guidance on how to then go about changing behaviour.\nThe Behaviour Change Wheel (BCW, [6]) addresses these limitations. It is designed for “policy makers, intervention designers, researchers and practitioners…from a wide range of disciplines with varying levels of expertise”. It was made by synthesizing 19 existing frameworks. In doing so, Michie et al. ensured the BCW is comprehensive (it covers all intervention options), conceptually coherent (category members are of same type), and linked to an overarching model of behaviour. This was one of their core objectives, as prevailing frameworks at the time lacked these qualities. For example, they criticise MINDSPACE [7] (popular amongst UK policy makers at the time) for not including all intervention types and for conflating psychological constructs with mechanisms of action and modes or delivery.\nThe BCW’s core is the COM-B model of behaviour, which stands for Capability, Opportunity, Motivation and Behaviour and is simpler and more intuitive than the TDF. When making COM-B, Michie et al began with motivation, defined as: “brain processes that energize and direct behaviour” [8]. Their next step was “to consider the minimum number of additional factors needed to account for whether change in the behavioural target would occur, given sufficient motivation” [6]. They used a US consensus meeting of behavioural theorists [9] and a centuries-old principle of criminal law: that to prove guilt one must show means or capability, opportunity, and motive. Michie et al. argued that the “common conclusion from these two separate strands of thought lends confidence to this model of behaviour”[6], and they added further confidence by mapping COM-B to the TDF. When describing COM-B, Michie et al. explain how the components capability, opportunity, and motivation interact with each other to generate behaviour which, in turn, may influence these components. (See Figure 1)\nThe COM-B model holds that behaviour arises from a system that involves three components: Capability, Opportunity, and Motivation. These are defined in Table 1 but, briefly; capability includes having the required knowledge; opportunity includes the necessary resources such as time, materials, or reminders; and motivation means having the required intentions, beliefs, wants and needs to perform the behaviour.\nChanging behaviour requires altering one or more of these components and the BCW goes beyond the TDF by helping intervention designers select options to achieve this. The wheel’s middle ring contains intervention functions, defined as “broad categories of means by which an intervention can change behaviour” [10]. An intervention may have multiple functions. For example, a television advert about drink driving may educate its audience with information about alcohol limits and persuade using emotive imagery. In this example, education is being used as an intervention function to target psychological capability (teaching people how much they can safely drink before driving) and persuasion is being used to target motivation (targeting people’s emotions to make them less likely to want to engage in drink driving). The functions you choose depend on the behavioural drivers you are trying to target. Intervention functions are described in Table 2.\nHaving decided what functions your intervention needs, the guide helps you decide how to implement them with policy categories, defined as “types of decisions made by authorities that help to support and enact the interventions” [10]. Categories include communication/marketing, guidelines, regulation, and service provision, as described in Table 3.\nRecognizing that intervention designers come from various disciplines and places of work, and that not all will have experience in applying behaviour change theory, Michie et al. published a book to help designers apply the BCW [10]. This book contains suggested steps, exercises, and worksheets. In the next chapter, I describe how I guided members of the UK EQUATOR Centre through this process."
  },
  {
    "objectID": "chapters/6_bcw/index.html#reflections-on-this-chapter",
    "href": "chapters/6_bcw/index.html#reflections-on-this-chapter",
    "title": "Selecting the Behaviour Change Wheel framework",
    "section": "Reflections on this chapter",
    "text": "Reflections on this chapter\nAt this point in my thesis I had my transfer of status. It was a pivotal moment where I began to view reporting guidelines differently, Charlotte Albury joined my supervision team, and the direction of my thesis took a qualitative turn.\nIn my introductory chapter I presented why I view reporting guidelines to be part of a complex behaviour change intervention, but I have not always viewed them this way. When I first discovered reporting guidelines in 2014, I would have described them simply as recommendations for authors to adhere to when writing-up research. I would have talked about checklists, full guidance, and journal policies. But I would not have zoomed in to the details of those parts: I would not have scrutinized the contents of resources, nor the intricacies of editorial policy. I also would not have zoomed out to view the broader picture of how guidelines are created and disseminated, or the connections between these parts.\nBut after reading accounts of authors struggling with the guidance content, access, formats, workflows, confidence, and the behaviour of others, I began to see things differently. Instead of revering guidelines as perfect publications set in stone, I began to see them as resources whose content, structure,and layout could be optimized. I began to see details; how a single word could lead to confusion, the nuances between standards and recommendations, between drafting, revising, and checking manuscripts. Beyond the guidelines themselves, I began to see the broader system that creates and disseminates them. I began to recognise the complexity arising from the number of different stakeholders involved, differences between guidelines, the skills and prior knowledge an author must have to act on the guidance, and variation in how, when, and why guidelines are used.\nThis simultaneous zoom-in and zoom-out felt confusing, and I struggled to communicate my thoughts to my initial supervision team - Gary, Jen and Michael - who worked for EQUATOR and shared a different view of reporting guidelines.\nDiscovering the MRC guidance for complex interventions was my first turning point. Its description of complexity resonated and helped me understand how an intervention could be a system, not just a single thing. From there, I discovered the world of behaviour change and took a few tentative steps. My transfer assessors foresaw my thesis becoming increasingly qualitative and felt I would benefit from a qualitative supervisor. My assessors recommended Charlotte Albury. Charlotte’s expertise emboldened my exploration of methods less familiar to EQUATOR and I, and under her guidance my tentative steps into qualitative behaviour change territory became more certain.\n\nhttps://tabletomarkdown.com/convert-spreadsheet-to-markdown/\n\n\nTable 1: COM-B model components and examples, reproduced with permission from the Behaviour Change Wheel guide [10]\n\n\n\n\n\n\nCOM-B model component: Definition\nExample\n\n\n\n\nPhysical capability: Physical skill, strength, or stamina\nHaving the skill to take a blood sample\n\n\nPsychological capability: Knowledge or psychological skills, strength, or stamina to engage in the necessary mental processes\nUnderstanding the impact of carbon dioxide on the environment\n\n\nPhysical opportunity: Opportunity offered by the environment involving time, resources, locations, cues, physical ‘affordance’\nBeing able to go running because one owns appropriate shoes\n\n\nSocial opportunity: Opportunity afforded by interpersonal influences, social cues and cultural norms that influence the way that we think about things, e.g., the words and concepts that make up our language\nBeing able to smoke in the house of someone who smokes but not in the middle of a board meeting\n\n\nReflective motivation: Reflective processes involving plans (self-conscious intentions) and evaluations (beliefs about what is good and bad)\nIntending to stop smoking\n\n\nAutomatic motivation: Automatic process involving emotional reactions, desires (wants and needs), impulses, inhibitions, drive states and reflex responses\nFeeling anticipated pleasure at the prospect of eating a piece of chocolate cake\n\n\n\n\n\n\nTable 2: Intervention function definitions and examples, reproduced with permission from the Behaviour Change Wheel guide [10]\n\n\n\n\n\n\n\nIntervention Function\nDefinition\nExample of intervention function\n\n\n\n\nEducation\nIncreasing knowledge or understanding\nProviding information to promote healthy eating\n\n\nPersuasion\nUsing information to induce positive or negative feelings or stimulate action\nUsing imagery to motivate increases in physical activity\n\n\nIncentivisation\nCreating an expectation of reward\nUsing prize draws to induce attempts to stop smoking\n\n\nCoercion\nCreating an expectation of punishment or cost\nRaising the financial cost to reduce excessive alcohol consumption\n\n\nTraining\nImparting skills\nAdvanced driver training to increase safe driving\n\n\nRestriction\nUsing rules to reduce the opportunity to engage in the target behaviour (or to increase the target behaviour by reducing the opportunity to engage in competing behaviours)\nProhibiting sales of solvents to people under 18 to reduce use for intoxication\n\n\nEnvironmental restructuring\nChanging the physical or social context\nProviding on-screen prompts for GPs to ask about smoking behaviour\n\n\nModelling\nProviding an example for people to aspire to or imitate\nUsing TV drama scenes involving safe-sex practices to increase condom use\n\n\nEnablement\nIncreasing means/reducing barriers to increase capability (beyond education and training) or opportunity (beyond environmental restructuring)\nBehavioural support for smoking cessation, medication for cognitive deficits, surgery to reduce obesity, prosthesis to promote physical activity\n\n\n\n\n\n\nTable 3: Policy categories, reproduced with permission from the Behaviour Change Wheel guide [10]\n\n\n\n\n\n\n\nPolicy Category\nDefinition\nExample\n\n\n\n\nCommunication/marketing\nUsing print, electronic, telephonic or broadcast media\nConducting mass media campaigns\n\n\nGuidelines\nCreating documents that recommend or mandate practice. This includes changes to service provision\nProducing and disseminating treatment protocols.\n\n\nFiscal measures\nUsing the tax system to reduce or increase the financial cost\nIncreasing duty or increasing anti-smuggling activities\n\n\nLegislation\nMaking or changing laws\nProhibiting sale or use\n\n\nEnvironmental/social planning\nDesigning and/or controlling the physical or social environment\nEstablishing support services in workplaces, communities etc.\n\n\n\n\n\n\n\n\n\n\n\n1. Skivington K, Matthews L, Simpson SA, et al (2021) A new framework for developing and evaluating complex interventions: Update of Medical Research Council guidance. BMJ n2061\n\n\n2. Yardley L, Morrison L, Bradbury K, Muller I (2015) The Person-Based Approach to Intervention Development: Application to Digital Health-Related Behavior Change Interventions. Journal of Medical Internet Research 17:e4055\n\n\n3. Cane J, O’Connor D, Michie S (2012) Validation of the theoretical domains framework for use in behaviour change and implementation research. Implementation Science 7:37\n\n\n4. Michie S, Johnston M, Abraham C, Lawton R, Parker D, Walker A (2005) Making psychological theory useful for implementing evidence based practice: A consensus approach. BMJ Quality & Safety 14:26–33\n\n\n5. Atkins L, Francis J, Islam R, et al (2017) A guide to using the Theoretical Domains Framework of behaviour change to investigate implementation problems. Implementation Science 12:77\n\n\n6. Michie S, van Stralen MM, West R (2011) The behaviour change wheel: A new method for characterising and designing behaviour change interventions. Implementation Science 6:42\n\n\n7. Dolan P, Hallsworth M, Halpern D, King D, Vlaev I (2010) MINDSPACE: Influencing behaviour for public policy. \n\n\n8. Mook DG Motivation : The organization of action. (No Title) \n\n\n9. Fishbein M, Triandis HC, Kanfer FH, Becker M, Middlestadt SE, Eichler A, et al (2001) Factors influencing behavior and behavior change. Handbook of health psychology 3:3–17\n\n\n10. Michie S, Atkins L, West R (2014) The Behaviour Change Wheel: A Guide to Designing Interventions. Silverback Publishing, London"
  },
  {
    "objectID": "data/iv_changes.html",
    "href": "data/iv_changes.html",
    "title": "Describe each reporting guideline fully",
    "section": "",
    "text": "When authors first encounter reporting guidelines they may need to know:\n\nwhat reporting guidelines are\nhow and when to use them\nwhy authors should use them, including:\n\nwhat personal benefits to expect\nthe importance to others.\n\n\nDescriptions could be succinct (e.g. on journal instruction pages) or long (e.g. in publications) JH\nA generalised description can go where authors first encounter reporting guidelines e.g., journal author guidelines, EQUATOR’s home page.\nA reporting guideline-specific description could go at the top of guidance documents, checklists, and templates.\n\nConsider specifying whether the reporting guideline is also a design guideline.\nSpecify whether the reporting guidelines are just guidelines, or whether they are intended to be requirements. Name the resource appropriately - words like guideline, standards, criteria, recommended, preferred, and templates, have different meanings.\n\n\n\nid: ready-to-use title: Make resources ready-to-use barrier_ids: [need-usable-formats, need-enough-time] intervention_fn_ids: [enablement] stakeholder_ids: [developers, equator] stage: creation idea_count: 1\n\nEnsure resources are ready-to-use e.g., checklists as Word files, not as tables within published articles.\n\nid: accessible title: Make resources accessible barrier_ids: [need-accessible] intervention_fn_ids: [enablement] stakeholder_ids: [developers, equator] stage: dissemination idea_count: 1\n\nEnsure resources are open access. This allows access to authors without journal subscriptions and allows others to build upon the guidance.\n\nid: citation title: Show and encourage citations barrier_ids: [what-rgs-exist, believed-benefits] intervention_fn_ids: [education, persuasion] stakeholder_ids: [developers, equator, publishers, conferences, preprints] stage: dissemination idea_count: 2\n\n\nDisplay usage data (like citations or downloads) alongside the guidelines as a form of social proof.\nEncourage authors to cite the reporting guideline so readers discover it.\n\n\nid: avoid-prescribing-structure title: Avoid prescribing structure barrier_ids: [need-to-reconcile, need-concise-writing] intervention_fn_ids: [enablement] stakeholder_ids: [developers] stage: development idea_count: 3\n\n\nAvoid prescribing structure of a journal article as it may clash with journal requirements or other reporting guidelines.\nInstead, give options for where items can be reported.\nInclude options beyond the article body where authors can report information, like tables, figures, or appendices be.\n\n\n\nid: create-early-guidance title: Create reporting guidance for early stages of research barrier_ids: [need-tools, need-right-time] intervention_fn_ids: [education] stakeholder_ids: [developers] stage: planning idea_count: 1\n\nConsider creating reporting guidance to help authors write protocols, funding applications, and ethics applications.\n\nid: avoid-proliferation title: Avoid confusing authors with too many reporting guidelines barrier_ids: [what-rgs-exist, need-to-reconcile, best-fit] intervention_fn_ids: [enablement] relation_ids: [information-architecture, easy-understand] stakeholder_ids: [developers, equator, publishers] stage: planning idea_count: 6\n\n\nTo avoid duplicating resources, before commencing a new reporting guideline:\n\nconsult EQUATOR’s register of reporting guidelines under development;\n\nEQUATOR could make this register easier to find and search;\n\ncontact the developers of related reporting guidelines;\njournals could ask reporting guideline developers to prove that they have registered their guideline with EQUATOR (like they do for clinical trials).\n\nWhen a new reporting guideline is justified, build upon existing reporting guidelines instead of starting from scratch. This could mean extending or replacing subsets of items instead of publishing a totally new reporting guideline.\nConsider making modular guidance. For instance, the Journal Article Reporting Standards (JARS) are a set of reporting guidelines for psychology. JARS has a main guideline for quantitative studies. The Design item can be extended by one of three modules depending on whether the study involved experimental manipulation or was conducted on a single individual. the experimental manipulation module can be further extended by modules for random assignment, non-random assignment, and clinical trials. Other extension modules exist for longitudinal and replication studies.\n\n\nid: budget-and-fund-reporting title: Budget for reporting barrier_ids: [need-enough-time, feel-not-a-job] intervention_fn_ids: [enablement] stakeholder_ids: [funders, institutions] stage: ongoing idea_count: 1\n\nFunders and research supervisors could encourage researchers to allocate sufficient time and money for documenting and reporting results of their research.\n\nid: easy-understand title: Make reporting guidelines easy to understand barrier_ids: [understanding, need-translations] intervention_fn_ids: [education, enablement] stakeholder_ids: [developers] stage: creation idea_count: 5\n\n\nUse plain language.\nDefine key terms.\nUse consistent terms across related resources.\nProvide translations.\nUpdate guidance in response to user feedback.\n\n\nid: create-rewards title: Create rewards barrier_ids: [care-about-benefits] intervention_fn_ids: [incentivization] stakeholder_ids: [developers, equator, publishers, funders, institutions, preprints, registries] stage: ongoing idea_count: 6\n\nStakeholders could create new rewards:\n\njournals could fast-track submissions or review for papers that followed a reporting guideline,\njournals could offer discounts on article processing charges for papers that followed a reporting guideline,\njournals, preprint servers, or peer review platforms could badge well reported articles,\nEQUATOR could offer a certification service,\nfunders could reward good reporting financially,\ninstitutions could offer prizes for good reporting.\n\n\nid: create-spaces title: Create discussion spaces barrier_ids: [understanding, feel-patronized, believed-benefits, how-to-report, how-to-do] intervention_fn_ids: [enablement, persuasion] stakeholder_ids: [equator, developers, institutions] stage: ongoing idea_count: 7\n\nCreate spaces for authors to discuss reporting and reporting guidelines. These could be:\n\nonline (forums, social media, email),\nor offline (meet-ups, clubs).\n\nTry to make spaces accessible to researchers from all nationalities, professional disciplines and other demographics. Spaces will allow authors to:\n\nsolicit help,\nshare experiences,\nprovide feedback to guideline developers,\nand cultivate a feeling of inclusivity and community ownership.\n\n\nid: early-acquisition title: Create ways to catch authors earlier barrier_ids: [need-prompts, need-right-time, need-enough-time, when-to-use] intervention_fn_ids: [education, enablement, environmental-restructuring] relation_ids: [create-tools, create-early-guidance] stakeholder_ids: [equator, developers, publishers, funders, ethics, institutions, conferences, preprints, registries] stage: ongoing idea_count: 4\n\n\nConsider creating email campaigns to prompt researchers at early stages.\nThe EQUATOR website could encourage visitors to use reporting guidelines for planning and drafting research.\nWebsites could be optimised for search terms like “how to write [study type]”, “protocol”, “research plan” or “funding application”. For example, reporting guideline pages on EQUATOR’s website rank highly in Google searches for “STROBE checklist” but not “How to write an observational epidemiology study”.JH\nWriting clubs and writing training could flag reporting guidelines.\n\n\nid: design-agnostic title: Keep reporting guidelines agnostic to design choices barrier_ids: [feel-restricted, feel-transparent, believed-costs, what-are-rgs] intervention_fn_ids: [enablement, persuasion] relation_ids: [keep-short, value-statement] stakeholder_ids: [developers] stage: development idea_count: 3\n\n\nAsk authors to describe methods transparently without making assumptions about, or prescribing, methods or design choices. For example, an instruction to “describe how you determined your sample size” may be more helpful than “report your sample size calculation” for authors who encounter checklists at submission and did not perform a sample size calculation before collecting data.\nAvoid recommending or admonishing design choices within the reporting guidance because:\n\ndoing so may make authors feel nervous or ashamed, and therefore less likely to report transparently;\ndesign advice elongates reporting guidelines;\nincluding design advice may give the impression that the reporting guideline is for designing or appraising design.\n\nConsider linking to external design or appraisal tools instead.\n\nid: persuade title: Use persuasive language and design barrier_ids: [feel-transparent, feel-patronized, believed-benefits, feel-not-my-job] intervention_fn_ids: [persuasion] stakeholder_ids: [developers, equator, publishers, funders, ethics, institutions, conferences, registries, preprints] stage: creation idea_count: 6\n\n\nUse language and design to communicate confidence and simplicity as opposed to judgement and complexity.\nEncourage explanation even when choices were unusual or sub-optimal.\nReassure authors that most research has limitations that can be addressed in Discussion sections.\nReassure authors that reporting guidelines are just guidelines.\nAvoid patronizing authors.\nConsider wording instructions directly at the intended user.JH\n\n\nid: create-tools title: Create additional tools barrier_ids: [need-tools, need-right-time] intervention_fn_ids: [enablement] stakeholder_ids: [developers, equator, funders, ethics, publishers] stage: creation idea_count: 12\n\nCreate tools for different tasks:\n\ndiscussion points for planning research in the order decisions are made\nto-do lists for conducting research in the order data is collected\ntemplates for drafting\nwriting assistance tools (e.g., COBWEB)\nchecklists for checking manuscripts that are easy to fill out, update, and cross-check\ntools for co-researchers to check each other’s work\ntools for generating tables and figures\nresources for peer reviewers who wish to review reporting quality including:\n\nguidance specifically for peer reviewers.\ncommonly-used words that reviewers can search for to quickly find relevant text.JH\nsuggested text that peer reviewers can copy to request information\ntools to generate feedback reports\n\njournal articles where reporting guideline items are annotated/highlighted\n\n\nid: findable-resources title: Make resources easy to discover and find barrier_ids: [what-rgs-exist, what-resources-exist, need-findable, best-fit] intervention_fn_ids: [environmental-restructuring] stakeholder_ids: [developers, equator] stage: creation idea_count: 11\n\nLink resources:\n\nEnsure all resources link to each other. For example, checklists should link to example and elaboration documents and vice versa.\nRelated reporting guidelines should link to each other.\nReporting guidelines and resources should link to translations\nLinks should be permanent (e.g. DOIs) where possible and old links should be maintained or redirected. Broken links should be replaced.\n\nMake searching easy:\n\nHost resources somewhere consistent, like the EQUATOR Network website and database.\nProvide easy-to-use website search functions\nWeb pages should be optimized for search engines JH\nCreated curated collections for study types\nCreate decision tools for identifying reporting guidelines\n\nNames reporting guidelines to make them easy to discover and find:\n\n\nReporting guideline names could be descriptive, as acronyms may be meaningless to novice users.\nRelated reporting guidelines should use consistent names to show relationships (e.g. PRISMA and PRISMA-P appear more related than CONSORT and SPRIT).\n\n\nid: endorse-enforce title: Endorse and enforce reporting guidelines barrier_ids: [what-rgs-exist, believed-costs] intervention_fn_ids: [education, restriction, persuasion] stakeholder_ids: [publishers, institutions, ethics, funders, registries, conferences, preprints] stage: ongoing idea_count: 3\n\nStakeholders could:\n\nendorse reporting guidelines\nenforce their use by mandating checklists or (preferably) checking adherence to items.\nFunders could ask about reporting guidelines or checklists when collecting updates from grant recipients.\n\n\nid: evidence-benefits title: Evidence the benefits barrier_ids: [believed-benefits] intervention_fn_ids: [persuasion, education] relation_ids: [testimonials, value-statement] stakeholder_ids: [developers, equator, publishers] stage: ongoing idea_count: 2\n\nEvidence any stated benefits:\n\nQuantifiable benefits could be evidenced with data (e.g., acceptance rates, publishing speed, writing speed).\nExperiential benefits could be evidenced by collecting case studies from authors who find that reporting guidelines help them feel confident and write more easily, and from readers who value well-reported research.\n\n\nid: information-architecture title: Make information digestible barrier_ids: [need-enough-time, believed-costs, need-findable] intervention_fn_ids: [enablement] relation_ids: [item-content] stakeholder_ids: [developers, equator] stage: creation idea_count: 7\n\nOrganise information so it is easy to navigate and not overwhelming.\n\nCater to users that read from start to finish, and those that dip in and out.\nStructure text with headings.\nUse section URLs to send authors directly to relevant parts of guidance.\nConsider hyperlinking related resources\nConsider embedding reporting guidelines that “fit together”, like PRISMA and PRISMA-Abstracts\nFor information presented online, consider showing/hiding information as required. For example, if PRISMA-Abstracts were embedded into PRISMA, users could choose to expand or collapse it. Or you could show/hide guidance depending on whether the author is writing a funding application, protocol, manuscript.\n\n\nid: item-content title: Describe reporting items fully barrier_ids: [scope, how-to-report, how-to-do, how-to-report-not-done, need-concise-writing, importance, care-about-benefits, need-to-remove, need-enough-time, need-concise-writing] intervention_fn_ids: [education, persuasion] stakeholder_ids: [developers] stage: development idea_count: 12\n\nFor each item, authors may need to know the following:\n\nWhat needs to be reported – a brief description could go in all resources (checklists, templates etc) with a longer description in the full guideline document.\nWhy the information is important, and to whom\nAny circumstances where the item is not applicable and what to write\nIndicate priority, and any circumstances that modify importance\nWhere the item can be reported, including beyond the main article body (e.g., section, table, figure, appendix)\nWhat to write if an item wasn’t, or couldn’t be done\nWhat to write if an item cannot be reported for external reasons. For example, if items cannot be reported because of intellectual property restrictions.\nExamples, which could be real or generated, including:\n\nexamples of good and bad reporting with explanations.\nexamples of concise or word-count-friendly reporting, perhaps in alternative formats like tables and figures.JH\nexamples of well reported “imperfect” items (items that were not done)\nexamples from different research contexts\n\nLinks to external design or appraisal advice\n\n\nid: apparent-priority title: Make reporting guidelines appear as a priority barrier_ids: [believed-benefits, care-about-benefits] intervention_fn_ids: [persuasion] stakeholder_ids: [publishers, funders, ethics, institutions, preprints, conferences, registries] stage: ongoing idea_count: 3\n\nJournals, funders and ethics committees could make reporting guidelines appear as a priority:\n\nMake them prominent in author instructions.\nPlacing checklists earlier in the PDFs that are automatically created by journal submission systems.\nPublicize when reporting guidelines are used by reviewers.\n\n\nid: promote title: Promote reporting guidelines barrier_ids: [what-are-rgs, what-rgs-exist] intervention_fn_ids: [education] relation_ids: [endorse-enforce, reporting-champions] stakeholder_ids: [institutions, publishers, developers, equator, ethics, funders, societies, registries, conferences, preprints] stage: ongoing idea_count: 4\n\n\nPromote reporting guidelines on and offline.\n\nOnline may include websites, email campaigns, social media, and blogs.\nOffline may include appearing at conferences, seminars, and workshops.\n\nInstitutions could promote reporting guidelines in their curricula, learning materials, or through reporting champions. Reporting guideline developers or EQUATOR could push for reporting guidelines to be included in text books.\nPromotion can begin before a reporting guideline has been published so that researchers know about guidelines being developed.\n\nNB. Promotion is different to endorsement; a journal could run an email campaign to promote reporting guidelines without having an endorsement policy.\n\nid: reporting-champions title: Install reporting champions barrier_ids: [what-are-rgs, benefits, understanding, when-to-use, importance] intervention_fn_ids: [education, enablement] stakeholder_ids: [equator, developers, institutions, funders, ethics, publishers, conferences, preprints, registries] stage: ongoing idea_count: 2\n\nAll stakeholders could have members to promote and facilitate the usage of reporting guidelines.\n\nThis could follow a local network model with EQUATOR as the central organiser.\nCould make use of existing networks, like regional reproducibility networks.\n\nFor each reporting guideline, authors may need the following information:\n\nA clear definition of the reporting guideline’s intended scope in plain language.\nIf-then rules to direct authors to other, more appropriate reporting guidelines. For example, CONSORT could point authors writing protocols to SPIRIT.\nIf no better guidance exists then indicate which items do/do not apply. For example, no guideline exists for authors writing protocols for observational epidemiology. Their best option currently is to use STROBE, but only some items will be required in a protocol.\nWhat tasks the reporting guideline can and cannot be used for\nHow long the resource will take to use\nWhy the guidance should be trusted and link to how it was developed\n\n\nid: keep-short title: Keep guidance short barrier_ids: [need-enough-time, believed-costs] intervention_fn_ids: [enablement] relation_ids: [information-architecture, design-agnostic] stakeholder_ids: [developers] stage: development idea_count: 5\n\nKeep guidance as a short as possible:\n\nBe concise but clear.\nBe realistic about what to expect from authors as each additional item increases the chances an author will be put off\nLink to other guidance elsewhere if desired.\nConsider splitting broad guidance that tries to cater for different options into shorter, modular guidance (modularity avoids duplication).\n\n\nid: testimonials title: Provide testimonials barrier_ids: [benefits, believed-benefits, believed-costs, care-about-benefits, feel-transparent] intervention_fn_ids: [persuasion, modelling] stakeholder_ids: [developers, equator] stage: dissemination idea_count: 5\n\nTestimonials can be short quotes or longer case studies. They could come from:\n\nresearchers who have had positive experiences using reporting guidelines, including researchers that were nervous about transparency,\ndecision makers (e.g., editors/grant managers) that value good reporting and/or check for reporting as part of their evaluation,\npeer reviewers that use reporting guidelines to check for good reporting,\npatients who are affected by research waste,\nand researchers who need to understand, synthesise, or apply research articles.\n\n\nid: support title: Provide additional teaching barrier_ids: [feel-not-a-job, understanding, importance, how-to-report, need-right-time, care-about-benefits] intervention_fn_ids: [education, training] stakeholder_ids: [developers, equator, publishers, institutions, funders, ethics] stage: ongoing idea_count: 5\n\nProvide education or training (e.g., courses, videos) specific to particular reporting guidelines.\nMore generally, students could:\n\nlearn about writing as a process and workflows for documenting and communicating research,\nlearn about research waste from poor reporting,JH\nattempt a replication to learn about the importance of complete reporting,\nand use a reporting guideline as part of their studies.\n\n\nid: updating title: Make updating guidelines easier barrier_ids: [need-up-to-date-guidance, understanding] intervention_fn_ids: [enablement] stakeholder_ids: [equator, funders] stage: ongoing idea_count: 4\n\nUpdate guidance in response to user feedback or changes in the field. This would be easier if:\n\nreporting guideline developers could easily collect feedback from authors.\nsmall updates or refinements could be made without publishing a new article.\nreporting guideline developers had funding to evaluate, refine, and update their resources.JH"
  },
  {
    "objectID": "data/barriers.html",
    "href": "data/barriers.html",
    "title": "Thesis",
    "section": "",
    "text": "id: what-are-rgs title: Researchers may not know what reporting guidelines are driver_id: capability —\nResearchers may have never heard the term “reporting guideline” or may misunderstand it. Researchers may more commonly use terms like “writing” or “writing up” and the word “reporting” may get interpreted as a formal task (such as reporting progress to a funder). The word “guideline” may be interpreted by some as rules (as per journal “author guidelines”) and others as recommendations. Some researchers may perceive reporting guidelines as a set of design requirements, especially if they only use checklists, which typically lack the instructions and nuances included in the full guidance.\n\nid: what-rgs-exist title: Researchers may not know what reporting guidelines exist driver_id: capability\n\nResearchers may not be aware of which reporting guidelines exist. Most guidelines on the EQUATOR site are hardly ever accessed\n\nid: scope title: Researchers may not know whether a reporting guideline applies to them driver_id: capability\n\nIf the scope of a reporting guideline is undefined or unclear, then researchers won’t know whether the guidance applies to them. Researchers may not understand study designs, making it difficult for them to identify which guidance applies.\n\nid: best-fit title: Researchers may not know what reporting guideline is their best fit driver_id: capability relation_ids: [what-rgs-exist, scope]\n\nResearchers may not know when more specific guidance exists. An author’s “perfect fit” guideline may not exist, in which case they may not know know when to stop searching, and they may try to use an “imperfect fit” guideline without understanding which items are applicable.\n\nid: what-resources-exist title: Researchers may not know what resources exist for a reporting guideline driver_id: capability\n\nResources include the guidance itself, checklists, E&E files, templates, and web tools (e.g. PRISMA flow chart maker). Not all resources exist for each reporting guideline and researchers may be unaware of the ones that do. Many researchers may only use the checklist. Sometimes this is purposeful, but other times it may be because researchers don’t know that full guidance and examples exist.\n\nid: when-to-use title: Researchers may not know when reporting guidelines should be used driver_id: capability\n\nResearchers may not know when they should use reporting guidelines in their research workflow. Guideline developers may want researchers to use guidance as early as possible, but this is may not be obvious to researchers who may only ever receive instruction to complete a checklist as part of journal submission and may never discover the full guidance. Consequently, researchers may assume that reporting guidelines are supposed to be used by single authors as pre-submission checklists to demonstrate adherence. It may not occur to them that reporting guidelines can be used earlier, or by teams. Some researchers, having come to this realisation themselves, report wanting to be told to use reporting guidelines earlier in their research.\n\nid: understanding title: Researchers may misunderstand driver_id: capability\n\nResearchers may not understand concepts, terms or words within the guidance, or they may understand them differently to how the developers intended. Some items (or entire guidelines) might be new concepts. E.g. SQUIRE guidelines written at a time where Quality Improvement was still a new concept to many people, and some items (e.g. Context, Study of the intervention) were less familiar than others. Researchers may have nowhere to turn for help should they not understand something.\n\nid: benefits title: Researchers may not know what benefits to expect driver_id: capability relation_ids: [believed-benefits]\n\nResearchers may not know what benefits to expect from using a reporting guideline. These benefits may include:\n\nimproved completeness of reporting which helps readers use research and reduces research waste.\nimproved flow and less “waffle” in writing\nfacilitated discussions between collaborators, especially at the design or protocol stage\npublishing and passing peer review more efficiently\nincreased publisher acceptance rates\nefficient, confident writing\nincreased impact of manuscript, as the article is easier to search for and information within the article is easier to find.\n\n\nid: importance title: Researchers may not know why items are important driver_id: capability relation_ids: [care-about-benefits]\n\nResearchers may not know why an item is important, or who it is important to.\n\nid: how-to-do title: Researchers may not know how to do an item driver_id: capability\n\nResearchers might not know how to do something (e.g., a sample size calculation)\n\nid: how-to-report title: Researchers may not know how to report an item in practice driver_id: capability\n\nResearchers may not understand how to report a particular item in practice\n\nid: how-to-report-not-done title: Researchers may not know what to write when they cannot report an item driver_id: capability\n\nResearchers may not know how to report an item that they did not do (deliberately or as an oversight), or an item that they are unable to report for external reasons (e.g., IP, or data was missing from primary studies).\n\nid: need-enough-time title: Researchers have limited time driver_id: opportunity\n\nGuidelines take time to find, read, understand, and apply. Sometimes they may require time and work from multiple co-authors. Researchers & guideline developers may underestimate the time required for writing, and time is often most limited at the point of submission as grant funding may have run out.\nChecklists take time to complete, and completing them with page numbers or pasted content can be annoying if future edits necessitate updating the checklist too. Checklists also generate work for editors and peer-reviewers who must cross check page numbers or pasted content with manuscript content.\n\nid: need-right-time title: Researchers may not encounter reporting guidelines early enough to act on them driver_id: opportunity relation_ids: [need-enough-time]\n\nSome reporting guideline items require work that has to be done within a certain time windows such as:\n\nduring planning or designing\nbefore or during data collection\nwhen other colleagues are available\nduring the duration of a grant\n\n\nid: need-translations title: Researchers may not understand the language driver_id: opportunity relation_ids: [understanding]\n\nResearchers may not understand the language guidance is written in. A lot of research comes from countries where English is not the first language, as do a lot of EQUATOR website visitors. Even if a researcher speaks English as a second language, language may be an additional barrier.\n\nid: need-concise-writing title: Researchers may struggle to keep writing concise driver_id: opportunity\n\n\nFollowing a guideline can result in lengthy, bloated reports which are unpleasant to read and breach journals’ word limits. Researchers may not know how to keep writing fluid and concise or where they can report an item (e.g., what section, in the text or in a table or figure, in the manuscript or in supplementary material).\n\nid: need-tools title: Researchers may not have tools for the job at hand driver_id: opportunity\n\nResearchers use reporting guidelines for different tasks and want tools to make that job easier. Researchers report using reporting guidelines for:\n\nPlanning research\nDesigning research\n\nResearchers report wanting items presented in the order in which decisions need to be made\nResearchers report wanting links to resources\n\nWhilst collecting data\n\nResearchers report wanting items ordered in the order they are done\nResaerchers report wanting items embedded into data collection tools\n\nDrafting manuscript\n\nResearchers report wanting templates\n\nChecking manuscripts\nDemonstrating compliance\n\nResearchers report wanting checklists embedded into submission workflows\n\nReviewing the reporting of other people’s manuscripts\nAppraising the quality of other people’s manuscripts\n\n\nid: need-up-to-date-guidance title: reporting guidelines can become outdated driver_id: opportunity\n\nGuidelines can become out of date compared to other guidance or compared to current research standards.\n\nid: need-to-reconcile title: Researchers may struggle to reconcile multiple sets of guidance driver_id: opportunity\n\nResearchers must adhere to journal guidelines, multiple reporting guidelines (e.g., PRISMA + PRISMA-Abstracts + PRISMA-S) and other best practice guidelines (like NIH principles). Using multiple guidelines increases complexity and costs, and guidelines can contradict each other.\n\nid: need-to-remove title: Researchers may be asked to remove reporting guideline content driver_id: opportunity\n\nResearchers may be asked to remove guideline content by co-researchers, editors or reviewers.\n\nid: need-prompts title: Researchers may forget to use reporting guidelines at earlier research stages driver_id: opportunity relation_ids: [need-enough-time, need-right-time]\n\nHaving been told to complete a checklist upon journal submission, researchers may forget to use a reporting guideline earlier next time.\nNB forgetting is different to not realising that reporting guidelines can be used early.\n\nid: need-findable title: Guidance may be difficult to find driver_id: opportunity relation_ids: [what-rgs-exist]\n\nResearcher should be able to easily find guidance and resources that they believe to exist. However:\n\nsearch functions can be hard to find or use,\nresearchers may not know which search terms to use,\nwebsites may be hard to navigate,\nguidance can be buried within articles,\nresources may not be optimised for search engines,\nand resources may not be in the same place.\n\n\nid: need-accessible title: reporting guidelines may be difficult to access driver_id: opportunity\n\nResearchers may be unable to access guidance published in subscription journals. Journal websites can feature broken links.\n\nid: need-usable-formats title: reporting guideline resources may not be in usable formats driver_id: opportunity\n\nResources differ in how easy or readily usable they are. For example, some checklists are published as PDF tables that cannot be filled or copied. Some guidance can be dense, unstructured text that is hard to digest or navigate; whereas some researchers will read the guidance sequentially, others may dip in and out whilst writing, and unstructured text can make information harder to find.\n\nid: feel-transparent title: Researchers may feel afraid to report transparently driver_id: motivation relation_ids: [how-to-report-not-done]\n\nResearchers may feel afraid or uncertain when trying to report something that they didn’t (or couldn’t) do.\n\nid: feel-restricted title: Researchers may feel restricted if reporting guidelines prescribe design driver_id: motivation\n\nAdvice or assumptions about design choices narrow the scope of the guidance and can make checklists appear prescriptive. Sometimes design assumptions can be implicit. For example, in requiring authors to report the method used to assess risk of bias, PRISMA is implying that authors should have designed their review to assess risk of bias.\n\nid: feel-patronized title: Researchers may feel patronized driver_id: motivation\n\nResearchers can feel patronized by checklists.\n\nid: believed-benefits title: Researchers may not believe stated benefits driver_id: motivation\n\nResearchers may not believe that using a reporting guideline will affect their acceptance rate or publication speed, that using a reporting guideline will help them write, or improve the quality of their manuscript.\n\nid: care-about-benefits title: Researchers may not care about the benefits of using a reporting guideline driver_id: motivation\n\nResearchers may understand that reporting guidelines aim to reduce poor reporting, but may not feel that poor reporting matters. Instead of hypothetical benefits or benefits to others, researchers report caring more about personal, immediate benefits like feeling confident, efficiency, and job performance.\n\nid: believed-costs title: Researchers may expect the costs to outweigh benefits driver_id: motivation\n\nResearchers may feel that the costs of using a reporting guideline - the time and work required and the added manuscript length - outweigh the benefits.\n\nid: feel-not-my-job title: Researchers may feel that checking reporting is someone else’s job. driver_id: motivation\n\nResearchers report feeling that completing a reporting checklist should be the job of the editor or peer reviewer, not the author. Editors and reviewers may also disagree about whose role it is.\n(NB. researchers, editors and reviewers could all check for reporting quality, but this research focusses only on researchers).\n\nid: feel-not-a-job title: Researchers may not consider writing as reporting driver_id: motivation\n\nResearchers may need to change their approach to writing or what they consider writing to be.Researchers differ in their writing process. Authors that follow a structured approach to writing may find it easier to incorporate reporting guidelines into their workflow. Some experienced researchers may be used to a way of working and reluctant to change, and some inexperienced researchers may be unaware of alternative writing processes."
  }
]