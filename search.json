[
  {
    "objectID": "data/eq_changes.html",
    "href": "data/eq_changes.html",
    "title": "Thesis",
    "section": "",
    "text": "website website-home website-guideline service guidance"
  },
  {
    "objectID": "data/intervention_components.html",
    "href": "data/intervention_components.html",
    "title": "Intervention Components",
    "section": "",
    "text": "barrier_ids: [what-are-rgs, when-to-use, benefits, importance, feel-not-my-job] intervention_fn_ids: [education]\nA clear, obvious description of:\n\nwhat reporting guidelines are\nhow and when to use them\nwhy authors should use them, including:\n\nwhat personal benefits to expect\nthe importance to others.\n\n\n\n\n\nDocuments in editable formats.\n\n\n\n\n\n\n\n\n\nRemove any specification of structure.\nBe explicit about flexibility where items can go.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbarrier_ids: [understanding, feel-patronized, believed-benefits, how-to-report, how-to-do] intervention_fn_ids: [enablement, persuasion]\n\nsolicit help,\nshare experiences,\nprovide feedback to guideline developers,\nand cultivate a feeling of inclusivity and community ownership.\n\n\n\n\n\n\n\nbarrier_ids: [feel-restricted, feel-transparent, believed-costs, what-are-rgs] intervention_fn_ids: [education]\n\nremove design assumptions\nadd null case\nbe explicit about agnosticism\n\n\n\n\nbarrier_ids: [feel-transparent, feel-patronized, believed-benefits, feel-not-my-job] intervention_fn_ids: persuasion\n\nUse language and design to communicate confidence and simplicity as opposed to judgement and complexity.\nEncourage explanation even when choices were unusual or sub-optimal.\nReassure authors that most research has limitations that can be addressed in Discussion sections.\nReassure authors that reporting guidelines are just guidelines.\nAvoid patronizing authors.\nConsider wording instructions directly at the intended user.JH\n\n\n\n\nbarrier_ids: [need-tools, need-right-time] intervention_fn_ids: [enablement]\n\ndiscussion points for planning research in the order decisions are made\nto-do lists for conducting research in the order data is collected\ntemplates for drafting\nchecklists for checking manuscripts that are easy to fill out, update, and cross-check\nresources for peer reviewers who wish to review reporting quality including:\n\nguidance specifically for peer reviewers.\ncommonly-used words that reviewers can search for to quickly find relevant text.JH\nsuggested text that peer reviewers can copy to request information\ntools to generate feedback reports"
  },
  {
    "objectID": "data/intervention_components.html#value-statement",
    "href": "data/intervention_components.html#value-statement",
    "title": "Intervention Components",
    "section": "",
    "text": "barrier_ids: [what-are-rgs, when-to-use, benefits, importance, feel-not-my-job] intervention_fn_ids: [education]\nA clear, obvious description of:\n\nwhat reporting guidelines are\nhow and when to use them\nwhy authors should use them, including:\n\nwhat personal benefits to expect\nthe importance to others."
  },
  {
    "objectID": "data/intervention_components.html#ready-to-use-tools",
    "href": "data/intervention_components.html#ready-to-use-tools",
    "title": "Intervention Components",
    "section": "",
    "text": "Documents in editable formats."
  },
  {
    "objectID": "data/intervention_components.html#structure-agnostic",
    "href": "data/intervention_components.html#structure-agnostic",
    "title": "Intervention Components",
    "section": "",
    "text": "Remove any specification of structure.\nBe explicit about flexibility where items can go."
  },
  {
    "objectID": "data/intervention_components.html#feedback-channels",
    "href": "data/intervention_components.html#feedback-channels",
    "title": "Intervention Components",
    "section": "",
    "text": "barrier_ids: [understanding, feel-patronized, believed-benefits, how-to-report, how-to-do] intervention_fn_ids: [enablement, persuasion]\n\nsolicit help,\nshare experiences,\nprovide feedback to guideline developers,\nand cultivate a feeling of inclusivity and community ownership."
  },
  {
    "objectID": "data/intervention_components.html#design-agnostic",
    "href": "data/intervention_components.html#design-agnostic",
    "title": "Intervention Components",
    "section": "",
    "text": "barrier_ids: [feel-restricted, feel-transparent, believed-costs, what-are-rgs] intervention_fn_ids: [education]\n\nremove design assumptions\nadd null case\nbe explicit about agnosticism"
  },
  {
    "objectID": "data/intervention_components.html#persuasion",
    "href": "data/intervention_components.html#persuasion",
    "title": "Intervention Components",
    "section": "",
    "text": "barrier_ids: [feel-transparent, feel-patronized, believed-benefits, feel-not-my-job] intervention_fn_ids: persuasion\n\nUse language and design to communicate confidence and simplicity as opposed to judgement and complexity.\nEncourage explanation even when choices were unusual or sub-optimal.\nReassure authors that most research has limitations that can be addressed in Discussion sections.\nReassure authors that reporting guidelines are just guidelines.\nAvoid patronizing authors.\nConsider wording instructions directly at the intended user.JH"
  },
  {
    "objectID": "data/intervention_components.html#tools-for-the-job",
    "href": "data/intervention_components.html#tools-for-the-job",
    "title": "Intervention Components",
    "section": "",
    "text": "barrier_ids: [need-tools, need-right-time] intervention_fn_ids: [enablement]\n\ndiscussion points for planning research in the order decisions are made\nto-do lists for conducting research in the order data is collected\ntemplates for drafting\nchecklists for checking manuscripts that are easy to fill out, update, and cross-check\nresources for peer reviewers who wish to review reporting quality including:\n\nguidance specifically for peer reviewers.\ncommonly-used words that reviewers can search for to quickly find relevant text.JH\nsuggested text that peer reviewers can copy to request information\ntools to generate feedback reports"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Transforming reporting guidelines into a behaviour change intervention with a view to increasing their impact",
    "section": "",
    "text": "Here is the working version of my thesis.\nYou’ll find chapters on the left, and can download Word files for each chapter on the right.\nChapters that I’ve not done (including this one) are this colour. Chapters in this colour are a works-in-progress.\nIf you see anything like “@something”, “?var:something”, or “#TODO” then please ignore.\nSome of the tables and figures aren’t rendering nicely yet, especially in Word format. You can ignore that too."
  },
  {
    "objectID": "chapters/gantt.html",
    "href": "chapters/gantt.html",
    "title": "Project Timeline",
    "section": "",
    "text": "gantt\n    dateFormat  YYYY-MM-DD\n    axisFormat %-m/%-y\n    title       \n    %% excludes    weekends\n    todayMarker on\n    %% (`excludes` accepts specific dates in YYYY-MM-DD format, days of the week (\"sunday\") or \"weekends\", but not the word \"weekdays\".)\n    section 3. Thematic synthesis\n    Article written    :milestone, done, 2022-12-01, 0d\n    Revise                       :active, ThemSyn, after BCWDraft, 2w\n\n    section 4. Survey review\n    Article written    :milestone, done, 2022-12-01, 0d\n    Revise                       :active, SurveyRev, after BCWDraft, 2w\n\n    section 5. Website review\n    Data analysed & described           :milestone, done, 2022-12-01, 0d\n    Revise                       :active, WebsiteRev, after BCWDraft, 2w\n\n    section 6. Audit\n    Data analysed                       :active, crit, AuditAnalysis, 2022-12-01, 2w\n    Write chapter                       :active, crit, AuditDraft, after AuditAnalysis, 3w\n\n    section 7. BCW \n    Draft   :done, BCWDraft, 2023-07-01, 17d\n\n    section 8. Workshops\n    Draft   :done, Workshops, 2023-06-14, 4w\n\n    section 9. Focus groups\n    Draft   :done, FocusGroups, 2023-01-04, 6w\n    Revision 1 :done, FocusGroupsRevision, 2023-05-14, 8w\n\n    section 4. Design intervention\n    Beta-version                        :milestone, done, DesignBeta, 2023-02-28, 0d\n    Draft                       :done, DesignDraft, 2023-03-25, 16w\n\n    section 5. Refining intervention\n    Collect data                        :active, PilotData, 2023-04-03, 2023-08-01\n    Analyse data                        :active, PilotAnalysis, after PilotData, 30d\n    Draft                       :active, PilotDraft, after PilotAnalysis, 4w     \n\n    section Introduction\n    Draft                      :active, IntroDraft, after ThemSyn, 30d\n\n    section Discussion\n    Draft                      :active, DiscussionDraft, after ThemSyn, 30d\n\n    section Reflexivity & Context\n    Draft                      :active, ReflexivityDraft, after ThemSyn, 30d\n\n    section Abstract, Appendices, etc., \n    Draft :active, OtherDraft, after DiscussionDraft, 2w\n\n    section Milestones\n    Complete draft     :milestone, active, CompleteDraft, after OtherDraft, \n    Revisions   :active, Revisions, after CompleteDraft, 16w\n    Submit thesis          :milestone, active, Submit, after Revisions, 0d"
  },
  {
    "objectID": "chapters/gantt.html#july-16th",
    "href": "chapters/gantt.html#july-16th",
    "title": "Project Timeline",
    "section": "",
    "text": "gantt\n    dateFormat  YYYY-MM-DD\n    axisFormat %-m/%-y\n    title       \n    %% excludes    weekends\n    todayMarker on\n    %% (`excludes` accepts specific dates in YYYY-MM-DD format, days of the week (\"sunday\") or \"weekends\", but not the word \"weekdays\".)\n    section 3. Thematic synthesis\n    Article written    :milestone, done, 2022-12-01, 0d\n    Revise                       :active, ThemSyn, after BCWDraft, 2w\n\n    section 4. Survey review\n    Article written    :milestone, done, 2022-12-01, 0d\n    Revise                       :active, SurveyRev, after BCWDraft, 2w\n\n    section 5. Website review\n    Data analysed & described           :milestone, done, 2022-12-01, 0d\n    Revise                       :active, WebsiteRev, after BCWDraft, 2w\n\n    section 6. Audit\n    Data analysed                       :active, crit, AuditAnalysis, 2022-12-01, 2w\n    Write chapter                       :active, crit, AuditDraft, after AuditAnalysis, 3w\n\n    section 7. BCW \n    Draft   :done, BCWDraft, 2023-07-01, 17d\n\n    section 8. Workshops\n    Draft   :done, Workshops, 2023-06-14, 4w\n\n    section 9. Focus groups\n    Draft   :done, FocusGroups, 2023-01-04, 6w\n    Revision 1 :done, FocusGroupsRevision, 2023-05-14, 8w\n\n    section 4. Design intervention\n    Beta-version                        :milestone, done, DesignBeta, 2023-02-28, 0d\n    Draft                       :done, DesignDraft, 2023-03-25, 16w\n\n    section 5. Refining intervention\n    Collect data                        :active, PilotData, 2023-04-03, 2023-08-01\n    Analyse data                        :active, PilotAnalysis, after PilotData, 30d\n    Draft                       :active, PilotDraft, after PilotAnalysis, 4w     \n\n    section Introduction\n    Draft                      :active, IntroDraft, after ThemSyn, 30d\n\n    section Discussion\n    Draft                      :active, DiscussionDraft, after ThemSyn, 30d\n\n    section Reflexivity & Context\n    Draft                      :active, ReflexivityDraft, after ThemSyn, 30d\n\n    section Abstract, Appendices, etc., \n    Draft :active, OtherDraft, after DiscussionDraft, 2w\n\n    section Milestones\n    Complete draft     :milestone, active, CompleteDraft, after OtherDraft, \n    Revisions   :active, Revisions, after CompleteDraft, 16w\n    Submit thesis          :milestone, active, Submit, after Revisions, 0d"
  },
  {
    "objectID": "chapters/gantt.html#feb-1st",
    "href": "chapters/gantt.html#feb-1st",
    "title": "Project Timeline",
    "section": "Feb 1st",
    "text": "Feb 1st\n\n\n\n\ngantt\n    dateFormat  YYYY-MM-DD\n    axisFormat %-m/%-y\n    title       \n    %% excludes    weekends\n    todayMarker off\n    %% (`excludes` accepts specific dates in YYYY-MM-DD format, days of the week (\"sunday\") or \"weekends\", but not the word \"weekdays\".)\n    section 2a. Thematic synthesis\n    Data analysed, article submitted    :milestone, done, 2022-12-01, 0d\n    Write chapter                       :active, ThemSyn, after DesignDraft, 3w\n\n    section 2b. Survey review\n    Data analysed, article submitted    :milestone, done, 2022-12-01, 0d\n    Write chapter                       :active, SurveyRev, after ThemSyn, 3w\n\n    section 2c. Website review\n    Data analysed & described           :milestone, done, 2022-12-01, 0d\n    Write chapter                       :active, WebsiteRev, after SurveyRev, 3w\n\n    section 2d. Audit\n    Data analysed                       :active, crit, AuditAnalysis, 2022-12-01, 2w\n    Write chapter                       :active, crit, AuditDraft, after AuditAnalysis, 3w\n\n    section 3. Focus groups\n    Methods & results written           :milestone, done, 2022-12-01, 0d\n    Write Chapter                       :active, crit, FocusGroups, 2023-01-05, 4w\n\n    section 4. Design intervention\n    Beta-version                        :active, crit, DesignBeta, 2022-12-01, 13w\n    Write chapter                       :active, DesignDraft, after FocusGroups, 4w\n\n    section 5. Refining intervention\n    Protocol                            :milestone, done, 2022-12-01, 0d\n    Collect data                        :active, PilotData, after DesignBeta, 12w\n    Analyse data                        :active, PilotAnalysis, after PilotData, 21d\n    Write chapter                       :active, PilotDraft, after PilotAnalysis, 4w     \n\n    section Intro & Discussion\n    Write chapters                      :active, after PilotDraft, 42d\n\n    section Milestones\n    Outline                             :milestone, done,   2022-11-01, 0d\n    Confirmation                        :milestone, crit, 2023-01-23, 0d\n    Submit thesis                       :milestone, active, 2023-09-01, 0d"
  },
  {
    "objectID": "chapters/gantt.html#dec-1st",
    "href": "chapters/gantt.html#dec-1st",
    "title": "Project Timeline",
    "section": "Dec 1st",
    "text": "Dec 1st\n\n\n\n\ngantt\n    dateFormat  YYYY-MM-DD\n    axisFormat %-m/%-y\n    title       \n    %% excludes    weekends\n    todayMarker off\n    %% (`excludes` accepts specific dates in YYYY-MM-DD format, days of the week (\"sunday\") or \"weekends\", but not the word \"weekdays\".)\n    section 2a. Thematic synthesis\n    Data analysed, article submitted    :milestone, done, 2022-12-01, 0d\n    Write chapter                       :active, ThemSyn, after DesignDraft, 3w\n\n    section 2b. Survey review\n    Data analysed, article submitted    :milestone, done, 2022-12-01, 0d\n    Write chapter                       :active, SurveyRev, after ThemSyn, 3w\n\n    section 2c. Website review\n    Data analysed & described           :milestone, done, 2022-12-01, 0d\n    Write chapter                       :active, WebsiteRev, after SurveyRev, 3w\n\n    section 2d. Audit\n    Data analysed                       :active, AuditAnalysis, 2022-12-01, 2w\n    Write chapter                       :active, AuditDraft, after AuditAnalysis, 3w\n\n    section 3. Focus groups\n    Methods & results written           :milestone, done, 2022-12-01, 0d\n    Write Chapter                       :active, FocusGroups, 2023-01-05, 3w\n\n    section 4. Design intervention\n    Beta-version developed              :active, DesignBeta, 2022-12-01, 5w\n    Write chapter                       :active, DesignDraft, after FocusGroups, 4w\n\n    section 5. Refining intervention\n    Protocol                            :milestone, done, 2022-12-01, 0d\n    Collect data                        :active, PilotData, 2023-01-05, 12w\n    Analyse data                        :active, PilotAnalysis, after PilotData, 21d\n    Write chapter                       :active, PilotDraft, after PilotAnalysis, 4w     \n\n    section Intro & Discussion\n    Write chapters                      :active, after PilotDraft, 42d\n\n    section Milestones\n    Outline                             :milestone, done,   2022-11-01, 0d\n    Confirmation                        :milestone, crit, 2023-01-23, 0d\n    Submit thesis                       :milestone, active, 2023-09-01, 0d"
  },
  {
    "objectID": "chapters/gantt.html#original",
    "href": "chapters/gantt.html#original",
    "title": "Project Timeline",
    "section": "Original",
    "text": "Original\n\n\n\n\ngantt\n    dateFormat  YYYY-MM-DD\n    axisFormat %-m/%-y\n    title       \n    %% excludes    weekends\n    todayMarker off\n    %% (`excludes` accepts specific dates in YYYY-MM-DD format, days of the week (\"sunday\") or \"weekends\", but not the word \"weekdays\".)\n    section 2a. Thematic synthesis\n    Data analysed, article submitted    :milestone, done, 2022-12-01, 0d\n    Write chapter                       :active, ThemSyn, after s4, 3w\n\n    section 2b. Survey review\n    Data analysed, article submitted    :milestone, done, 2022-12-01, 0d\n    Write chapter                       :active, SurveyRev, after ThemSyn, 3w\n\n    section 2c. Website review\n    Data analysed & described           :milestone, done, 2022-12-01, 0d\n    Write chapter                       :active, WebsiteRev, after SurveyRev, 3w\n\n    section 2d. Audit\n    Data analysed                       :milestone, done, 2022-12-01, 0d\n    Write chapter                       :active, s2d, 2022-12-01, 3w\n\n    section 3. Focus groups\n    Methods & results written           :milestone, done, 2022-12-01, 0d\n    Write Chapter                       :active, FocusGroups, 2023-01-05, 3w\n\n    section 4. Design intervention\n    Beta-version developed              :milestone, done, 2022-12-01, 0d\n    Write chapter                       :active, s4, after FocusGroups, 4w\n\n    section 5. Refining intervention\n    Protocol                            :milestone, done, 2022-12-01, 0d\n    Collect data                        :active, PilotData, 2023-01-05, 12w\n    Analyse data                        :active, PilotAnalysis, after PilotData, 21d\n    Write chapter                       :active, PilotDraft, after PilotAnalysis, 4w     \n\n    section Intro & Discussion\n    Write chapters                      :active, after PilotDraft, 42d\n\n    section Milestones\n    Outline                             :milestone, done,   2022-11-01, 0d\n    Confirmation                        :milestone, crit, 2023-01-23, 0d\n    Submit thesis                       :milestone, active, 2023-09-01, 0d"
  },
  {
    "objectID": "chapters/discussion/index.html",
    "href": "chapters/discussion/index.html",
    "title": "Thesis",
    "section": "",
    "text": "Have I succeeded?\n\nRehash why I wanted to transform RGs from a “it seemed like a good idea at the time” to a thought through intervention.\n\nMain motivator was to increase efficacy and raise standard of reporting\nBut I haven’t done any kind of comparison or evaluation. Could be done in future\nProcess did raise some interesting points though.\n\nNorman door\nIntention of guideline developers is for authors to consider nuanced guidance when writing.\nBut existing distribution relied heavily on checklist enforcement by journals. Checklists lack nuance, don’t link to full guidance, often much easier to find than supplementary E&Es. So guidelines were often perceived as the checklists, and as an administrative task to rather than help, and as design restrictions rather than description recommendations."
  },
  {
    "objectID": "chapters/1_introduction/index.html",
    "href": "chapters/1_introduction/index.html",
    "title": "Thesis",
    "section": "",
    "text": "Describe how RG system evolved organically.\n\nThis is fairly common in efforts to change behaviour, and is sometimes referred to as the “It seemed like a good idea at the time” method, or “ISLAGIATT” which Michie et al. say was coined by professor Martin Eccles, or “SLAGIATT” which my my 10 year old niece would recognize from a Horrible Histories chapter on dodgy interventions.\n\nJustify why it is worth turning RGs into a behaviour change intervention\n\nIncrease efficacy, acceptability, reduce side effects\nTheorise how it is working & therefore how a process evaluation could be designed"
  },
  {
    "objectID": "chapters/todo.html",
    "href": "chapters/todo.html",
    "title": "Thesis",
    "section": "",
    "text": "Todo…"
  },
  {
    "objectID": "chapters/10_redesign/index.html",
    "href": "chapters/10_redesign/index.html",
    "title": "Turning reporting guidelines into a behaviour change intervention: Behavioural analysis and development process",
    "section": "",
    "text": "In chapter 8 I describe how I followed the Behaviour Change Wheel guide through running workshops with staff members from the UK EQUATOR Centre. By the end of these workshops we had defined our target behaviour, identified what needed to change for this behaviour to occur, prioritized intervention functions and policy categories that we wanted to pursue, identified possible behaviour change techniques that we might use, and decided that we could deliver this new intervention by improving and extending the EQUATOR website.\nPrioritizing abstract intervention options was useful but I still had to decide how our chosen options could be actualized. To facilitate this ideation step (and to seek input from a broader range of stakeholders) I ran focus groups, as described in the previous chapter. These focus groups produced ideas. These ideas were relevant to a range of stakeholders and often conflated intervention function with policy categories.\nMy next step was to turn these ideas into intervention components and decide which ideas to act on.\nWhat remained was to select which ideas I wanted to act on, and to turn them into intervention components. By intervention component, I mean:\n\na designed element that uses one or more behaviour change technique,\nwhich is theorized to work through one or more intervention functions\nto target one or more behavioural drivers.\n\nIn this chapter I describe how I turned ideas into components by coding them against the barriers they address and the functions and behaviour change techniques they are employing. Defining intervention content in this way is useful because it helps intervention developers to understand why the component has been added (or removed), how it is theorised to be working and, therefore, how its effectiveness may be tested.\nI then describe how I designed and built these components into a prototype that could be piloted amongst authors."
  },
  {
    "objectID": "chapters/10_redesign/index.html#introduction",
    "href": "chapters/10_redesign/index.html#introduction",
    "title": "Turning reporting guidelines into a behaviour change intervention: Behavioural analysis and development process",
    "section": "",
    "text": "In chapter 8 I describe how I followed the Behaviour Change Wheel guide through running workshops with staff members from the UK EQUATOR Centre. By the end of these workshops we had defined our target behaviour, identified what needed to change for this behaviour to occur, prioritized intervention functions and policy categories that we wanted to pursue, identified possible behaviour change techniques that we might use, and decided that we could deliver this new intervention by improving and extending the EQUATOR website.\nPrioritizing abstract intervention options was useful but I still had to decide how our chosen options could be actualized. To facilitate this ideation step (and to seek input from a broader range of stakeholders) I ran focus groups, as described in the previous chapter. These focus groups produced ideas. These ideas were relevant to a range of stakeholders and often conflated intervention function with policy categories.\nMy next step was to turn these ideas into intervention components and decide which ideas to act on.\nWhat remained was to select which ideas I wanted to act on, and to turn them into intervention components. By intervention component, I mean:\n\na designed element that uses one or more behaviour change technique,\nwhich is theorized to work through one or more intervention functions\nto target one or more behavioural drivers.\n\nIn this chapter I describe how I turned ideas into components by coding them against the barriers they address and the functions and behaviour change techniques they are employing. Defining intervention content in this way is useful because it helps intervention developers to understand why the component has been added (or removed), how it is theorised to be working and, therefore, how its effectiveness may be tested.\nI then describe how I designed and built these components into a prototype that could be piloted amongst authors."
  },
  {
    "objectID": "chapters/10_redesign/index.html#behavioural-analysis",
    "href": "chapters/10_redesign/index.html#behavioural-analysis",
    "title": "Turning reporting guidelines into a behaviour change intervention: Behavioural analysis and development process",
    "section": "Behavioural Analysis",
    "text": "Behavioural Analysis\n\nMethods\nFor every idea generated from the workshops and focus groups, I labelled which barriers it was addressing, which behavioural drivers it was targeting, and which intervention functions it was employing to do so. This list was data driven, in that it was based upon the ideas and barriers generated from previous research (see chapters 3, 4, 5, 6, 8 and 9). To give structure and context to this list, I grouped ideas according to the sub-behaviours they targeted: 1) engaging with guidance and 2) applying it (see section on identifying the target behaviour in chapter 8).\nOnce all ideas were coded, I selected ideas to implement by considering a) whether they could be incorporated into a web-based intervention, b) the priority of the intervention function (determined in objective 2), and c) whether I could feasibly deliver the idea within the time constraints of my DPhil.\n\nTODO: my results table currently includes all ideas at the moment. But some of these ideas aren’t actually intervention components (some are modes of delivery e.g. “provide training” or “networks of champions”). And I’ve not included all of the ideas in my intervention. Given that I already report all “ideas” in the appendix (and results section of the focus group chapter), I think it might be more useful to just report the intervention components that I’ve taken forward into the pilot or could include in the website going forward.\n\n\nResults\nSee table Table 1 for all 28 ideas, labelled with the barriers they address, the drivers they target, the intervention functions they use, and whether I could implement them.\n\n\nBuilding the intervention\n\nPurpose\nThe result of my behavioural analysis was a list of components that were abstract. “Reassuring language” or “design that communicates simplicity” could be realised in many different ways. To build a working prototype that could be piloted I had to make these real.\n\n\nMethods and Results\n\nDesigning the intervention\nI began by describing how each intervention component could be realised and how this compared to the existing system (see #tab-int-plan). In doing these comparisons, I looked at how the EQUATOR website is currently, and I made generalisation about how popular reporting guidelines are disseminated, and the content of their Example and Elaboration documents and checklists.\nDesigning was iterative and collaborative. I included the same members of EQUATOR UK that had participated in the workshops. We met 3 times between November 2022 and January 2023 to discuss intervention design. In our first meeting, we decided which webpages required redesigning, and how webpages should be navigated. On the existing website, authors starting on the home page must visit up to 5 webpages to reach the full reporting guidance. Many authors leave at each step and so few reach the guidance. We redesigned this workflow to reduce this journey to 2 webpages - the EQUATOR home page, and a reporting guideline page containing the full guidance. These different website layouts are visualised in Figure 1 and Figure 2.\nWorkshop participants then sketched ideas for how the home page and guidance pages could be laid out and where intervention components could be placed. Once participants had agreed on a layout, I created an alpha version of the new website and invited members to comment on it. These were webpages that could be viewed in a browser, but used dummy text and images. After another round of feedback I refined the alpha version, populated it with real text and images, and participants gave feedback again. The new pages can be viewed in Figure 9, Figure 10, and Figure 12, and can be compared with the old pages in Figure 3 and Figure 4.\nMy intention was to create guideline pages for a sample of the most frequently accessed guidelines so that the website felt real for pilot participants. However, many intervention components involved changing the wording and layout of the guidance itself. Editing multiple guidelines was neither feasible not necessary, as we only needed one edited guideline to pilot the new website.\nI selected SRQR as my test guideline to edit because I was familiar with it, having used it when writing up my own research, and because I felt it would make a good guideline to test with (see next chapter for why). I got written permission from Bridget O’Brien, the lead developer or SRQR, and from the publisher. I kept Bridget up to date with my work and invited her feedback.\nI began editing SRQR by pasting the text into Microsoft Word and rearranging content into categories: what to write, how/where to write it, what to write if the item wasn’t/couldn’t be done, why the item is important and to whom, examples. I edited sentences to speak directly to authors. E.g. “Describe X” instead of “Authors should describe X”, and to use active voice. This shortened the text and made it clearer that the primary audience is authors.\nFor composite items I split the sub-items into bulleted lists. E.g.\n\nFor each X, describe:\n\nX\nY\nZ\n\n\nI rearranged conditional sub-items so that they read “If X, then describe Y”, rather than “Describe Y if X”. I moved definitions into the glossary and contextual information into notes. I edited the resulting text to join it back together. I edited the tone of voice to add reassuring language. An example of the redesigned guidance can be viewed in #sec-box-item.\nAfter development, I double checked the intervention against the initial list of intervention components to ensure I had covered all of them. I consulted with EQUATOR members to verify that the components were realised as expected and invited another round of feedback.\n\n\nSystem architecture\nWhen considering architecture options I prioritized technology that could feasibly be maintained by EQUATOR staff or a future PhD student. I looked for tools that would be familiar to early career researchers. I considered DIY website builders (like Wix or Squarespace) but these services can be expensive. Most offer a ‘drag and drop’ building experience which, although easy to use, is a laborious way of uploading and formatting large amounts of content. Should EQUATOR want to change how an item is presented, they would have to manually edit each item for each reporting guideline. Additionally, our intended intervention changes required custom functionality that wasn’t offered by these services (e.g. integration with a DOI minting service, glossary definitions, discussion boards).\nAlthough coding languages like html or javascript are used by many software developers to create websites, few early career researchers are familiar with them. I decided on markdown, an incredibly simple language that can be learnt in a few minutes. It uses asterisks, underscores, and carets to make text **bold**, _italic_, or ^superscript^. Headings, URLS, and references are similarly easy, and can be simplified further by using one of many readily available editors that make writing markdown feel like writing a Microsoft Word document.\nMany researchers write reproducible manuscripts in markdown using tools like RStudio or Quarto. Quarto can turn markdown into many different file formats including docx, pdf, and html (a website). Quarto documents can be further customised using programming languages commonly used in research, like Ruby or Python. Quarto requires no technical knowledge, is easy to learn, has great documentation, and is open source.\nThe website is served using Github Pages which is free, beginner friendly, configurable, and integrates (almost) seamlessly with Github’s version control system which will already be familiar to many researchers. I wanted EQUATOR to have ultimate control over the website. I also wanted guideline developers to have selective access to edit their own guideline content but not to other guidelines. I have built the website such that each reporting guideline is stored in its own repository on Github, accessible only to its developers. These guideline repositories are then “pulled in” to the main EQUATOR repository, so EQUATOR can double check changes that developers make before allowing them to go live on the site.\nThe website source code can be viewed at https://github.com/jamesrharwood/equator-guidelines-website and the website can be viewed at https://jamesrharwood.github.io/equator-guidelines-website/.\n\n\n\n\n\nTable 1: Intervention Planning Table. Intervention ingredients labelled with the intervention function (INT. FN) and behaviour change technique(s) they employ, examples of how they are (or are not) used before this redesign (Before) and, when included, within the redesigned intervention (Now). Components are grouped according to the key behaviours, barriers, and behavioural drivers that they are designed to target.\n\n\n\n\n\n\n\n\n\nINTERVENTION INGREDIENT\nINT. FN\nBEFORE\nNOW\nBEHAVIOUR CHANGE TECHNIQUE\n\n\n\n\nKey Behaviour: Engage with (read) appropriate reporting guidance as early as possible\n\n\n\n\n\n\nTargeted barrier: Researchers may not know what RGs are\nBehavioural driver: Capability\n\n\n\n\n\n\nDescribe what RGs are where they are first encountered\nEducation\nNo prominent description of what reporting guidelines are on EQUATOR home page or in RGs resources.\nExample: See Figure 3, Figure 5, Figure 7, Figure 8\nProminent definition on home page and guideline page.\nExample: See Figure 9, Figure 10\nInstruction on how to perform the behavior\n\n\nDifferentiate resources and advice aimed at different tasks (writing vs designing vs appraising)\nEducation\nNo clear instruction on what tasks RGs can and cannot be used for.\nExample: See Figure 3, Figure 4, Figure 5\nClear instruction and differentiation of resources\nExample: See Figure 9, Figure 10\nInstruction on how to perform the behavior\n\n\nTargeted barrier: Researchers may not know what RGs exist\nBehavioural driver: Capability\n\n\n\n\n\n\nInstruct authors to cite reporting guidelines\nEducation\nNo consistent instruction to cite reporting guidelines\nConsistent instruction to cite reporting guidelines\nInstruction on how to perform the behavior\n\n\nDecision tools for discovering resources\nEnablement\nWe previously made a “reporting guideline wizard” but it was difficult to find.\nExample: see Figure 3\nNot included yet\nInstruction on how to perform the behaviour\n\n\nCollections of related reporting guidelines\nEnvironmental Restructuring\nCollections exist on EQUATOR site but are difficult to find\nExample: See Figure 3\nHome page includes a small table of collections\nExample: See Figure 9\nInstruction on how to perform the behavior\n\n\nLinks between related guidelines\nEnvironmental Restructuring\nE&E articles may reference guidelines published previously, but these references can be buried in text, and E&Es are not updated to reference guidelines published subsequently. EQUATOR website guideline pages feature links to extensions, but these are hard to find. Checklists do not link to related resources.\nExample: See Figure 4, Figure 5\nGuidelines prominently link to other relevant guidelines and explain when they should be used.\nExample: See Figure 10\nRestructuring the physical environment\n\n\nembed reporting guidelines that “fit together”\nEnablement\nChecklists and their extensions are published separately. The best example of modular guidance is perhaps the JARS guidelines, but even these are published as separate documents.\nNo change\nInstruction on how to perform the behavior\n\n\nTargeted barrier: Guidance may be difficult to find\nBehavioural driver: Opportunity\n\n\n\n\n\n\nCentralised hosting\nEnablement\nEQUATOR maintained a database of reporting guideline meta-data, but the guidance and checklists were published and hosted in different locations and in different ways.\nA core set of frequently accessed guidelines are now presented on a single website in a consistent structure and format, alongside all of their checklists, templates, and other tools.\nRestructuring the physical environment\n\n\nSearch function on website\nEnablement\nEQUATOR’s search function was difficult to find.\nSearch function is easier to find. A recognizable icon appears in the navigation bar of every page, and the home page includes additional ways to access search functionality.\nRestructuring the physical environment\n\n\nSearch Engine Optimization\nEnablement\nEQUATOR’s website did not make use of some commonly used search optimization heuristics. It ranked well for guideline acronyms (like STROBE) but not for general terms that naive authors may use, like “observational epidemiology”.\nExample: (Not visible)\nSite has meta data required by search engines\nExample: (Not visible)\nAdding objects to the environment\n\n\nPermanent links (DOIs)\nEnablement\nAlthough guideline publications have DOIs, tools (commonly hosted on guideline developer’s websites) do not. EQUATOR’s website does not use document object identifiers. If resources move (e.g. a website is reorganised or depreciated) then links can “die”.\nNo change\nRestructuring the physical environment\n\n\nTargeted barrier: Researchers may not know whether a RG applies to them\nBehavioural driver: Capability\n\n\n\n\n\n\nDescribe the scope of a reporting guideline at the top of every resource\nEducation\nSome reporting guidelines may describe their scope within their E&E. Others might not, or may only describe their scope broadly without fully explaining the design assumptions within the guidance. Guidelines would not explain circumstances where the guidance should not be used. Checklists rarely define intended scope beyond the title of the guideline.\nThe intended scope of a guideline is clearly & prominently described. This definition includes contexts in which the guidance should not be used.\nInstruction on how to perform the behavior\n\n\nTargeted barrier: Researchers may not know what RG is their best fit\nBehavioural driver: Capability\n\n\n\n\n\n\nUse if-then rules to direct authors to more appropriate guidance when available\nEducation\nReporting guidelines do not consistently point authors towards related resources that might be better fits. If a reporting guideline E&E does do this, this advice becomes out of date as and when new guidelines are published.\nReporting guidelines clearly and consistently point authors to more appropriate guidance when appropriate, using if-then rules.\nInstruction on how to perform the behavior\n\n\nExplicitly state when no better guidance exists for a particular use case\nEducation\nGuideline E&Es do not consistently explain what to do when no better guidance exists for their use case\nExample: See Figure 5\nGuidelines warn authors when no better guidance exists for a use case, and how the current guidance can be adapted instead\nExample: See Figure 10\nInstruction on how to perform the behavior\n\n\nTargeted barrier: Researchers may not understand the language\nBehavioural driver: Opportunity\n\n\n\n\n\n\nprovide translations of resources\nEnablement\nTranslations were listed on EQUATOR database pages, but not on the guidance or checklists\nExample: See Figure 4\nTranslations are prominently listed above the guidance\nExample: See Figure 10\nInstruction on how to perform the behavior\n\n\nTargeted barrier: Researchers may expect the costs to outweigh benefits\nBehavioural driver: Motivation\n\n\n\n\n\n\nMake guidance appear shorter by removing superfluous parts, hiding optional content, splitting long guidelines, using concise language, and separating design advice\nEnvironmental restructuring\nRG publications may include lengthy explanation of development, verbose language, and can be bloated by design advice\nExample: See Figure 6\nThe only information presented immediately is what to describe. Additional information is hidden at first and can be expanded. Text is shortened through editing and by using active voice. Overall, this reduces the text length by 60%.\nExample: See Figure 11\nRestructuring the physical environment\n\n\nCater to different kinds of user (readers vs dippers) by structuring guidance with headings, itemisation, hyperlinking to particular sections, and with dynamic content\nEnvironmental restructuring\nBesides being split into items, reporting guidance is largely unstructured, and different items can be structured in different ways. Checklist items do not link to items within the E&E.\nExample: See Figure 6\nSRQR items are structured consistently, making information easier to find.\nExample: See Figure 11\nRestructuring the physical environment\n\n\nInclude testimonials from researchers who were nervous about being punished for reporting transparently\nPersuasion\nNo such testimonials exist\nQuotes included alongside guideline\nExample: see Figure 11\nSocial support (practical & emotional)\n\n\nDecrease fear of judgement by making reporting guidelines design agnostic\nCoercion (Removal of)\nReporting guidelines may conflate reporting advice with design advice or design assumptions. The justification for why an item is important to describe is frequently presented in terms of good and bad design.\nSRQR explicitly states that it makes no assumptions about design. Where design advice was given, this has been moved.\nRemove aversive stimulus\n\n\nRemove branding and messaging that may invoke feelings of judgement, complexity, or administrative red-tape\nCoercion (Removal of)\nEQUATOR’s website looked cluttered and visually unappealing. Guidance published in articles can look unappealing, depending on the publisher. Reporting guidance made frequent reference to waste, (lack of) transparency, bias, and poor design.\nExample: See Figure 3\nA clean, simple interface for the home page and guidance pages. Text makes less use of to judgemental phrases.\nExample: See Figure 9\nRemove aversive stimulus\n\n\nReassure that all research has limitations and encourage explanation over perfection\nPersuasion\nFew guidelines would include this kind of reassurance\nExample: See Figure 5\nThis reassurance appears on the home page and all guidance pages\nExample: See Figure 10\nSocial support (unspecified)\n\n\nTargeted barrier: Researchers may feel that checking reporting is someone else’s job.\nBehavioural driver: Motivation\n\n\n\n\n\n\nAddress communications to authors\nPersuasion\nIt wasn’t immediately clear whether the EQUATOR guideline website was aimed at authors, editors, reviewers, or all. Guidance was written in third person and would generally be worded as “authors should describe how…”, but it wouldn’t always be clear who was expected to use a checklist.\nAll resources and website copy are directed predominantly at authors.\nInstruction on how to perform a behaviour\n\n\nCommunicate why reporting is the responsibility of the author and not an editor or reviewer\nSee above\nBecause it wasn’t clear how reporting guidelines and checklists should be used, they (especially checklists) could appear as administrative tasks that should be the responsibility of the editor or reviewer.\nClearer description of how guidelines and tools should be used and who should use them.\nExample: See Figure 10\nInstruction on how to perform a behaviour\n\n\nTargeted barrier: Researchers may not consider writing as reporting\nBehavioural driver: Motivation\n\n\n\n\n\n\nEducate about writing as a process\nEducation\nEQUATOR provided education about how to write\nGuidelines link to this education\nExample: See Figure 11\nInstruction on how to perform a behaviour\n\n\nProvide training and education about writing as a process\nTraining\nEQUATOR provided training about how to write\nGuidelines link to this training\nExample: See Figure 11\nInstruction on how to perform a behaviour\n\n\nKey Behaviour: Apply reporting guidance to writing\n\n\n\n\n\n\nTargeted barrier: Researchers may not know what resources exist for a RG\nBehavioural driver: Capability\n\n\n\n\n\n\nlink all resources to each other\nEnvironmental restructuring\nDevelopment articles, E&E articles, and checklist files may not link to each other.\nExample: See Figure 7\nGuidance links to all tools and development article\nExample: See Figure 10\nRestructuring the physical environment\n\n\nTargeted barrier: Researchers may not know what benefits to expect\nBehavioural driver: Capability\n\n\n\n\n\n\nDescribe personal benefits and benefits to others where reporting guidelines are introduced (home page, on resources, in communications)\nEducation\nBenefits are not prominently described on EQUATOR’s home page, nor within the guideline publications. Benefits that are described may by hard to find, and often focus on hypothetical benefits to the research community, but not personal benefits to the author.\nExample: See Figure 3\nBenefits are prominently and consistently displayed across the home page and guidance pages. Descriptions prioritize personal benefits to the authors above hypothetical benefits to others.\nExample: See Figure 10\nInformation about emotional consequences, Information about others’ approval, Information about social and environmental consequences\n\n\nTargeted barrier: Researchers may not believe stated benefits\nBehavioural driver: Motivation\n\n\n\n\n\n\nGather and communicate evidence for benefits\nPersuasion\nBenefits often presented without evidence (if at all)\nDummy quotes provides evidence for experienced benefits.\nExample: See Figure 11\nInformation about emotional consequences, Information about others’ approval, Information about social and environmental consequences\n\n\nDisplay citation metrics\nPersuasion\nCitation metrics are available for guideline publications, but are not displayed on the EQUATOR website or within the guidance or checklists themselves.\nExample: See Figure 4\nCitation metrics are presented at the top of each reporting guideline.\nExample: See Figure 10\nInformation about emotional consequences, Information about others’ approval, Information about social and environmental consequences\n\n\nCreate spaces for authors to see other authors using reporting guidelines\nPersuasion\nThere were no official on or offline spaces for authors to witness other users discussing guidelines.\nEach reporting item has a discussion board.\nExample: See Figure 12\nInformation about emotional consequences, Information about others’ approval, Information about social and environmental consequences\n\n\nUse language to communicate personal benefits; confidence and simplicity\nPersuasion\nGuidance text made little use of a reassuring tone or words. The EQUATOR website and guideline articles looked dense and complex.\nExample: See Figure 3 and Figure 5\nA clean, simple interface for the home page and guidance pages. Text uses phrases like “confidence”, “quick”, “maximum impact”.\nExample: See Figure 9 and Figure 10\nRemove aversive stimulus, Information about others’ approval\n\n\nTargeted barrier: Researchers may not care about the benefits of using a RG\nBehavioural driver: Motivation\n\n\n\n\n\n\nInclude testimonials from research users who benefit from complete reporting\nPersuasion\nTestimonials not included in reporting guidelines.\nSRQR includes dummy testimonials and quotes from research users\nExample: See Figure 11\nSalience of consequence\n\n\nTargeted barrier: Researchers may misunderstand\nBehavioural driver: Capability\n\n\n\n\n\n\nuse plain language\nEnablement\nAlthough developers aspire to write clearly, authors may misinterpret guidance or fail to understand it completely.\nSRQR is edited to use plainer language.\nInstruction on how to perform the behavior\n\n\ndefine key terms\nEducation\nFew guidelines came with a glossary. Some key terms may be defined within the guideline text. Including definitions this way makes them hard to find and elongates the guidance.\nSRQR now has a glossary, and text is marked-up with definitions that appear upon click.\nExample: See Figure 11\nInstruction on how to perform the behavior\n\n\nuse consistent terms\nEnablement\nGuidelines may use different terms to refer to the same thing. A single guideline may do this too.\nSRQR uses consistent terms across items.\nInstruction on how to perform the behavior\n\n\nprovide translations\nEnablement\nSome guidelines have been translated, but many haven’t. The EQUATOR website uses an automatic translation tool, but this doesn’t cover the guidance itself. Translations aren’t always easy to find.\nExample: SRQR was available in French but the translation wasn’t advertised on the main guidance.\nTranslations are linked at the top of the guidance.\nExample: See Figure 10\nInstruction on how to perform the behavior\n\n\nprovide channels for feedback and support (see above)\nNone\nSee above\nSee above\nSee above\n\n\nTargeted barrier: Researchers may not know why items are important\nBehavioural driver: Capability\n\n\n\n\n\n\nFor each item, explain why the information is important and to whom (regardless of whether a particular design choice is important)\nEducation\nSometimes there was no explanation as to why an item should be reported. Other times the justification would be about why a particular design choice was important.\nExample: See Figure 6\nInformation added\nExample: See Figure 11\nInformation about social and environmental consequences\n\n\nExplain importance of complete reporting to the scientific community\nEducation\nEQUATOR and most RGs do this already\nContinue to do this\nInformation about social and environmental consequences\n\n\nTargeted barrier: Researchers may not know how to do an item\nBehavioural driver: Capability\n\n\n\n\n\n\nProvide links to other resources that explain how an item can be done\nSee above\nSome RG E&Es include instruction in text but many don’t. SRQR did not.\nLinks included when relevant.\nExample: See Figure 11\nInstruction on how to perform the behavior\n\n\nTargeted barrier: Researchers may not know how to report an item in practice\nBehavioural driver: Capability\n\n\n\n\n\n\nFor each item, provide clear instruction of what needs to be described\nEducation\nWriting instructions are often mixed in with other explanation and context.\nWriting instruction occurs first for each item.\nExample: See Figure 11\nInstruction on how to perform the behavior\n\n\nFor each item, provide examples of reporting in different contexts\nmodelling\nNot all RGs provide examples. Examples may not cover different contexts.\nSRQR already had some examples. No examples added\nExample: See Figure 11\nDemonstration of the behavior\n\n\nDiscussion spaces (see above)\nEnablement\nNo formal discussion spaces existed. Authors may use platforms like Twitter.\nEach item is linked to its own discussion board.\nExample: See Figure 12\nInstruction on how to perform the behavior\n\n\nTargeted barrier: Researchers may not know what to write when they cannot report an item\nBehavioural driver: Capability\n\n\n\n\n\n\nProvide clear instruction of what needs to be described when an item was not done, could not be done, or does not apply\nEducation\nRarely instructed\nExample: See Figure 6\nInstructed where relevant\nExample: See Figure 11\nInstruction on how to perform the behavior\n\n\nProvide examples of reporting “imperfect” items\nModelling\nExamples not provided\nNo changes made\nDemonstration of the behavior\n\n\nTargeted barrier: Researchers have limited time\nBehavioural driver: Opportunity\n\n\n\n\n\n\nEnsure resources (checklists and templates) are in ready-to-use formats\nEnablement\nSome checklists not in immediately usable formats e.g., PDFs\nNo changes made\nAdding objects to the environment\n\n\nStructure guidance to make it quicker to digest (see above)\nEnablement\nE&E documents not structured below the item level\nExample: See Figure 6\nItems have consistent structure\nExample: See Figure 11\nRestructuring the physical environment\n\n\nTell authors how long the guidance will take to read\nEducation\nEstimated reading time not given\nEstimated reading time given\nExample: See Figure 10\nInstruction on how to perform the behavior\n\n\nTell authors how long guidance will take to apply\nEducation\nNo advice given\nNo changes made\nExample: No changes made\nInstruction on how to perform the behavior\n\n\nTargeted barrier: Researchers may not know when RGs should be used\nBehavioural driver: Capability\n\n\n\n\n\n\nTell authors when to use RGs, or that RGs are best used as early as possible\nEducation\nRarely stated prominently\nStated prominently\nExample: See Figure 10\nInstruction on how to perform the behavior\n\n\nDifferentiate resources by task on website\nEnablement\nResources not explicitly linked to tasks\nResources presented alongside the tasks they are intended for\nExample: See Figure 10\nRestructuring the physical environment\n\n\nTargeted barrier: Researchers may not encounter RGs early enough to act on them\nBehavioural driver: Opportunity\n\n\n\n\n\n\nOptimize websites for search terms aimed at early use like “how to write”, or “funding application”.\nEnablement\nNot done\nNo changes made\nRestructuring the physical environment\n\n\nCreate prompts / communication campaigns to target authors early in their research\nEnablement\nEQUATOR has no way to do this\nNo changes made\nPrompts/cues\n\n\nCreate tools to be used for early writing tasks\nEnablement\nMost RGs come with a checklist and some kind of E&E. Few guidelines cater to protocols.\nNo changes made\nAdding objects to the environment\n\n\nTargeted barrier: Researchers may struggle to keep writing concise\nBehavioural driver: Opportunity\n\n\n\n\n\n\nProvide instruction as to how information can be reported without breaching word count limits or making articles bloated.\nEducation\nFew RGs or RG items include this information\nInstruction at top of RG and in some items where most useful\nExample: See Figure 10\nInstruction on how to perform the behavior\n\n\nProvide examples of concise reporting\nModelling\nNo examples specifically to display concise reporting\nNo changes made\nDemonstration of the behavior\n\n\nTargeted barrier: Researchers may not have tools for the job at hand\nBehavioural driver: Opportunity\n\n\n\n\n\n\nCreate planning-points; discussion points for planning research in the order decisions are made (see above)\nSee above\nSee above\nSee above\nSee above\n\n\nCreate to-do lists in the order research is conducted, to help authors collect information they will need to report (see above)\nSee above\nSee above\nSee above\nSee above\n\n\nCreate templates for drafting (see above)\nSee above\nSee above\nSee above\nSee above\n\n\nCreate tools to facilitate checklist completion\nEnablement\nUpdating page numbers in a checklist is time consuming. It takes editors time to double check page numbers and content. Checklists may not include instructions of how to complete them.\nExample: See Figure 7\nNo changes made\nAdding objects to the environment\n\n\nCreate tools to facilitate particular reporting items\nEnablement\nSome tools exist (e.g., PRISMA flow chart diagram maker, COBWEB)\nNo changes made\nAdding objects to the environment\n\n\nCreate tools to help collaborators check each other’s work\nEnablement\nChecklists exist but aren’t specifically designed for collaborators\nNo changes made\nAdding objects to the environment\n\n\nCreate tools to help peer reviewers check reporting and request missing information\nEnablement\nChecklists are RGs are not specifically aimed at peer reviewers\nNo changes made\nAdding objects to the environment\n\n\nTargeted barrier: RGs can become outdated\nBehavioural driver: Opportunity\n\n\n\n\n\n\nProvide feedback channels to help developers keep guidance updated (see Discussion spaces above)\nNone\nSee above\nSee above\nSee above\n\n\nCreate a system so that guideline developers can make small edits without publishing new articles\nEnablement\nDevelopers would have to publish a new article\nDevelopers can make small updates any time\nRestructuring the physical environment\n\n\nTargeted barrier: Researchers may struggle to reconcile multiple sets of guidance\nBehavioural driver: Opportunity\n\n\n\n\n\n\nExplain when RGs do not intended to prescribe structure\nEducation\nNot always stated. Not always prominent\nExample: see Figure 5\nExplained at top of guidance\nExample: see Figure 10\nInstruction on how to perform the behavior\n\n\nProvide options for where items can be reported (see above)\nNone\nSee above\nSee above\nSee above\n\n\nCreate a system whereby guidelines can be embedded into each other (see above)\nNone\nSee above\nSee above\nSee above\n\n\nTargeted barrier: Researchers may be asked to remove RG content\nBehavioural driver: Opportunity\n\n\n\n\n\n\nProvide advice regarding how to respond if asked to remove reporting guideline content by a colleague, editor, or reviewer\nEducation\nNo advice given\nAdvice given in FAQ beneath each RG\nExample: See Figure 10\nProblem solving\n\n\nTargeted barrier: RG resources may not be in usable formats\nBehavioural driver: Opportunity\n\n\n\n\n\n\nEnsure resources (checklists and templates) are in ready-to-use formats (see above)\nNone\nSee above\nSee above\nSee above\n\n\nTargeted barrier: Researchers may feel afraid to report transparently\nBehavioural driver: Motivation\n\n\n\n\n\n\nPresent design advice separately to reporting advice\nCoercion (removal of)\nSome RG E&Es include design advice\nNo changes made\nRestructuring the physical environment\n\n\nMake reporting guidelines agnostic to design choices\nCoercion (removal of)\nSome RGs make implicit assumptions about design choices\nNo changes made to SRQR\nRemove aversive stimulus\n\n\nEncourage explanation even when choices are unusual or not optimal\nEducation\nNot always present\nExample: See Figure 6\nAdded to items\nExample: See Figure 11\nInstruction on how to perform the behavior\n\n\nReassure authors that all research has limitations\nPersuasion\nNot always present\nExample: See Figure 3, Figure 5\nPresent on home page and RG page\nExample: See Figure 9, Figure 10\nSocial support\n\n\nInclude testimonials from authors who were nervous about reporting transparently\nPersuasion\nNot present\nQuotes added\nSocial support\n\n\nTargeted barrier: Researchers may feel restricted if RGs prescribe design\nBehavioural driver: Motivation\n\n\n\n\n\n\nPresent design advice separately and remain design agnostic (see above)\nNone\nSee above\nSee above\nSee above\n\n\nReassure when guidelines are just guidelines\nPersuasion\nNot always present or prominent\nExample: See Figure 5\nProminently displayed at top of RG\nExample: See Figure 10\nSocial support\n\n\nTargeted barrier: Researchers may feel patronized\nBehavioural driver: Motivation\n\n\n\n\n\n\nCreate discussion spaces for authors to voice disagreements and contribute to revisions (see above)\nPersuasion\nSee above\nSee above\nSee above\n\n\nAvoid patronizing language\nPersuasion\nAlthough authors may feel patronized when asked to adhere to a RG, RGs themselves rarely use patronizing language\nContinue to avoid using patronizing language\nRemove aversive stimulus\n\n\nExplain how the guidance was developed and why it can be trusted\nEducation\nMost RGs explain this in a published article. Checklists do not\nExample: See Figure 5\nBrief description included on home page and at top of RG, links to full to development information\nExample: See Figure 10\nCredible source\n\n\nKey Behaviour: Repeat engagement with reporting guidelines for subsequent studies\n\n\n\n\n\n\nTargeted barrier: Researchers may forget to use RGs at earlier research stages\nBehavioural driver: Opportunity\n\n\n\n\n\n\nCreate prompts / communication campaigns (see above)\nNone\nSee above\nSee above\nSee above"
  },
  {
    "objectID": "chapters/10_redesign/index.html#figures",
    "href": "chapters/10_redesign/index.html#figures",
    "title": "Turning reporting guidelines into a behaviour change intervention: Behavioural analysis and development process",
    "section": "Figures",
    "text": "Figures\n\n\n\n\n\nFigure 1: The layout of the existing EQUATOR Network website, as of the 5th of April, 2023. Users must navigate through up to 5 different web pages to reach reporting guidance. The proportion of users navigating between each step is shown in the width of the links. Links in grey are estimated proportions.\n\n\n\n\n\n\nFigure 2: The layout of the new web-intervention. Users must now only need to navigate to 2 pages to access reporting guidance. The proportion of users navigating between each step is shown in the width of the links. All links in are estimated proportions, based on a realistic aim to reduce the exit rate from the home page and database page by 50%.\n\n\n\n\n\n\n\nFigure 3: The existing EQUATOR home page, as captured on 5th of April, 2023. Limitations include: 1) No prominent description of what RGs are 2) No clear instruction on what tasks RGs can and cannot be used for 3) Search function hard to find (area A) 4) Decision tool for identifying which RG to use was hard to find (area B) 5) The page looked cluttered and unappealing 6) The tone of voice was functional. It was not particularly judgemental but not reassuring either. 7) There was little description of benefits of using a reporting guideline besides the mention of ‘quality’ and ‘transparency’ in the definition of EQUATOR, reference to ‘high-impact research’, ‘improve your writing’, and ‘enhance your peer review’ in the header. 8) No reassurance that most research has limitations 9) Frequently accessed guidelines are fairly easy to find (area C).\n\n\n\n\n\n\nFigure 4: The existing EQUATOR page for SRQR, as captured on 5th April, 2023. Limitations include:\n1) The actual guidance is hard to find. Area A includes 3 links. The first two send users to an article describing how SRQR was developed. The actual guidance appears in a supplement of that article, which is the third link in area A. The label “relevant URLs” is vague. 2) Little instruction regarding what the RG is or can be used for other than “Qualitative research” 3) Links to related guidelines that are hard to find or, for SRQR, absent 4) No metrics around how many authors use this RG (e.g. citation counts) 5) The French translation of the guidance is well labelled and fairly easy to find (area B), but to the right of it is a box prominently labelled “Translations”, and the link in here would actually take the user further away from the translated guidance.\n\n\n\n\n\n\nFigure 5: The SRQR publication, captured on the 5th April, 2023. Limitations of reporting guideline publications may include:\n1) RG publications often focus on how the guidance was developed. The actual guidance (see area C) or checklist (area B) may be relegated to a box, table, or a linked supplement. 2) Not all RGs describe what RGs are or what they can be used for, and these descriptions can be hard to find (areas A). 3) RG publications may not reassure authors that most research has limitations, and that transparency is OK 4) Publications may not be written with a reassuring tone of voice. Instead, guideline developers may justify their work by emphasising the negative impact of research waste. This may be how developers justify their work to themselves, editors, reviewers, or readers. As a result, to a naive author considering using the guidance, the tone of voice may come across as judgemental. 5) Benefits to the user may be hard to find or (as with this RG) not described at all. Benefits to others are more likely to be described, including a focus on how transparent, complete reporting benefits the research community or, conversely, how poor reporting is wasteful. 6) Instruction on when RGs do/do not intend to prescribe structure, or instruction may be hard to find (see area D) or missing. 7) Instructions on whether a RG intends to be a strict standard vs. ‘just’ a guideline may be hard to find (see area D) or missing. 8) Links to related resources only include those that were created before the RG was published. Some guidelines don’t include any links.\n9) No clear instruction on whether to use the guideline in a situation that it wasn’t designed for, but when no better guidance exists.\n\n\n\n\n\n\nFigure 6: An example item from the SRQR guideline. Limitations may include:\n1) Text is unstructured, so it is difficult to immediately identify what needs to be written. 2) Text uses verbose, passive language 3) The text appears long and difficult to digest 4) Terms aren’t always defined 5) Not all reporting items are justified 6) Not all items include instruction of what to write if the item could not/was not done.\n\n\n\n\n\n\nFigure 7: The SRQR checklist. Limitations of RG checklists may include:\n1) Checklists may not define what RGs are, what they can be used for, or their benefits. 2) Checklists may not be in a usable format (e.g. a PDF that cannot be filled in, or a table that cannot be copied) 3) Checklists may not include instruction of how to complete them. 4) Checklists may not link to the underlying guidance, or other related resources. 5) Content may lack nuance of full guidance and may appear dictatorial and administrative\n\n\n\n\n\n\nFigure 8: Author instructions for BMJ Open, a typical journal, captured on 5th April, 2023. Limitations of Journal instruction to authors may include:\n1) Instructions advise authors to use RGs, but don’t define what RGs are, what they can be used for, or the benefits or using them. 2) Advice regarding reporting guidelines may be hard to find amongst lengthy instruction pages (see area A)\n\n\n\n\n\n\n\nFigure 9: Intervention home page. Intervention changes made to the homepage include the following:\n1) RGs are now clearly defined (areas A) 2) The site looks simple and has plenty of white space\n3) Personal benefits are described explicitly and communicated through reassuring language and quotes (see areas B)\n4) Search and browse buttons are easy to find (area C) 5) Frequently accessed guidelines are still easy to find (area D) 6) The site describes what tasks RGs can be used for, and differentiates tools by task (area E)\n\n\n\n\n\n\nFigure 10: Intervention reporting guideline page. Intervention changes made to RG introductions include:\n1) Clear description of what the RG is, what it can and cannot be used for, the benefits to the author and to society, and how and when it can be used. (area A) 2) Description of whether the RG is intended to be a standard or ‘just’ a guideline (area A) 3) Tools are clearly differentiated by task (area B) 4) Related guidelines and other resources are linked. These links can be updated as and when newer guidelines are published (area C) 5) Clear instruction on whether a RG can be used in a situation that it wasn’t designed for, but where no better guidance exists (area D) 6) Links to translations (area E) 7) Reassuring language throughout, and reassuring quotes from editors, readers, and authors (e.g., area F) 8) Citation metrics (area G) 9) An estimation of how long guidance will take to read (area H) 10) Advice on how or where to report items so as not to breach word count limits and when RGs do or do not intend to prescribe structure (area I) 11) Full guidance (area J, see Figure 11) 12) Citation information (area K) 13) Information on how the guidance was developed and why it can be trusted (area L)\n\n\n\n\n\n\nFigure 11: A re-designed item from the SRQR reporting guideline. Intervention changes include:\n1) Content is separated into what to write (area A), why information is important (area B), examples (area C), and any additional background information (not shown). 2) Areas B and C are presented as expandable content, so the only instruction immediately visible is what to write (area A). This means that the guidance is easier to digest and less intimidating. 3) Definitions are presented as pop-ups for technical terms (area D) 4) Quotes provide reassurance and persuasion 5) Language is direct and edited for clarity and brevity 6) Each item has its own discussion page (linked to from the top right of area A)\n\n\n\n\n\n\nFigure 12: Intervention discussion page. Every reporting item now has its own discussion page where authors can ask and answer questions, and provide feedback to guideline developers."
  },
  {
    "objectID": "chapters/10_redesign/index.html#discussion",
    "href": "chapters/10_redesign/index.html#discussion",
    "title": "Turning reporting guidelines into a behaviour change intervention: Behavioural analysis and development process",
    "section": "Discussion",
    "text": "Discussion\nI have demonstrated how I have used a data-driven approach, guided by behaviour theory, to re-design how reporting guidance is disseminated. I have proposed TODO intervention components, addressing TODO barriers and employing TODO intervention functions. By linking components with barriers and functions, I have justified my suggestions using evidence and described how they are theorized to work. I have then created a prototype website to demonstrate how these components could be realised.\nTogether, these changes amount to a complete redesign of two key parts of the existing system through which reporting guidelines are currently disseminated; the guidelines themselves, and the EQUATOR Network website which is visited by almost 1 million authors each year.\n\nWhen comparing current intervention\nThis reassessment required participants to take a step back and look at the current set-up with fresh eyes. We did this informally. Some participants shared long-standing frustrations with the website or guidelines. One participant shared designs she had created years ago for a redesigned EQUATOR website. Other times, after discussing a barrier or idea, we would go to the guidelines to see how things are done currently.\nSo this comparison was ad-hoc, and I have included pieces of it in this chapter purely to provide context to the proposed changes. I sought out examples of a behaviour change technique being implemented, not being implemented, or being implemented poorly. I made generalisations about RGs using words like “some” or “few” to give an impression of how frequently RGs currently use a given BCT. These frequency descriptions are based on my own observation, and not on a formal audit.\nI considered systematically auditing the content of EQUATOR Network website and popular guidelines to see which behaviour change techniques they employ and which of our ideas were already present. I decided against this for two reasons. Firstly, with so many ideas and so many guidelines, this would have taken time and I decided instead to prioritize building and testing a prototype. Secondly, this audit wouldn’t have dramatically influenced the intervention components we designed, but would merely quantify how different my proposed intervention is to the current set-up. Who would be interested in quantifying this difference? Perhaps my thesis examiners, and perhaps the guideline development community. But quantifying differences wouldn’t bring me any further towards helping authors or impacting reporting quality, like building a prototype would. Should the guideline development community need that evidence then this audit could be done in the future once the redesigned intervention has been refined (see next chapter) and finalised.\n\n\nLimitations & reflections on process\nUsing a framework and a systematic method helped participants (and I) to check our biases. Instead of relying on personal preference, we tried to ensure choices reflected the function we were trying to employ. For example, when choosing a background image, instead of asking “do you like this one?”, the questions became “what feelings do you think this image conveys? Does it communicate simplicity?”. Working as a group helped mitigate individual preferences and peculiarities.\nHowever, there is no avoiding the fact that many decisions required a degree of subjectivity and, as lead researcher, designer, and developer, often these decisions landed on my shoulders. I tried to mitigate this by involving EQUATOR members in the workshops and development process, prioritizing their ideas over my own, and providing many opportunities for feedback. But the result definitely has my “stamp”. If someone else had built it using the same table of intervention components then some things might be the same (like simplifying the user journey from 5 steps to 2, or the conventional layout of the home page) but other things would look very different (like the choice of wording and images).\nUsing a framework also helped participants to consider options that may not have otherwise come to mind. However, our imagination may have been constrained by what already exists. Although I encouraged blue-sky thinking, participants often focussed on tweaking what already exists instead of starting from a blank slate. If reporting guidelines didn’t exist, how else might we have tackled poor reporting? If EQUATOR didn’t exist, would we have a similar organisation to fill its place? How might that organisation be structured, governed, and what kind of legal entity might it be? If the publishing industry didn’t exist, might we have imagined different ways of describing research that were more formulaic than free-form articles?\nThese imagination constraints may be a weakness, but they are also al practical. EQUATOR is in a privileged position that in that it is known and trusted by publishers, guideline developers, and many authors. Thousands of journals and authors already use reporting checklists. So whilst the changes proposed in this chapter (and the ideas proposed in the chapter before) may be criticised for not being radical enough, for an organisation (and a PhD student) with limited time and resources, it makes sense to improve a system that already has significant buy-in from the academic community, over and above destroying that system or trying to create a new one from scratch.\nOur horizons may have also been limited by group-think. If I were to repeat the work, I would have included a small, diverse group of authors to take part in the design process. I would have invited representatives from the publishing industry, funding community, and people more familiar with designing digital behavioural interventions. Including these diverse, informed voices in the design process could have lead to more radical design choices.\nI did, however, include guideline developers in the design process. When editing SRQR I made sure to include Bridget O’Reilly, SRQR’s lead author, in every step. I explained my process and invited her feedback during and after editing. The experience was very positive. Bridget was supportive of what I was doing and liked the end result. But I acknowledge that other guideline developers may feel protective over their writing, and that many may not have the time nor funding to revise their guidance. I discuss these limitations further in my discussion chapter.\nEditing SRQR revealed another limitation: gaps in item description. There was often no guidance of what to write if an item wasn’t or couldn’t be done. For instance, the target sample size item had no instruction of what to write if you didn’t ever have a target in mind. Some items were missing any kind of justification of why the item was important and to whom. Bridget and I both felt that filling these blanks would require time and input from SRQR’s development team, and so I left these gaps unfilled for now.\n\n\nFuture work\nI anticipate similar gaps for other reporting guidelines, and would seek to work alongside guideline developers to fill them and I upload other popular reporting guidelines and edit items into a consistent structure using the same process as for SRQR. Further development work will be required before the new website can be made live. Some of this work are technical tasks that, although necessary, do not have behavioural impact. For example, I will need to integrate the new website as a subdomain of EQUATOR’s existing one, and I will create automated tests that run before each deployment. However, other tasks do appear on the list of intervention ideas, and will affect behaviour. For example, I intend to optimise each guideline page so that it ranks highly in search engines.\nBeyond the intervention components presented here, the prioritization exercise identified other ideas that EQUATOR would like to implement, but that I chose not to act upon. For example, participants favoured developing training resources specific to individual reporting items, and creation of network of “reporting champions”, akin to the UKRN model. EQUATOR participants liked the idea of lobbying funders to require reporting guidelines be used for applications. The work in this chapter could be used to support funding applications to support these endeavours.\nI mentioned earlier that one limitation of this study was that authors weren’t included in the design process. In the next chapter, I explain how I have addressed this by piloting the website amongst authors."
  },
  {
    "objectID": "chapters/appendix/ideas.html",
    "href": "chapters/appendix/ideas.html",
    "title": "Ideas generated from workshops and focus groups",
    "section": "",
    "text": "Consider creating reporting guidance to help authors write protocols, funding applications, and ethics applications.\n\nWho could do this: Guideline developers\n\n\nBarriers addressed:\n\nResearchers may not have tools for the job at hand\n\n\nResearchers may not encounter RGs early enough to act on them\n\n\n\n\n\n\nTo avoid duplicating resources, before commencing a new reporting guideline:\n\nconsult EQUATOR’s register of reporting guidelines under development;\n\nEQUATOR could make this register easier to find and search;\n\ncontact the developers of related reporting guidelines;\njournals could ask reporting guideline developers to prove that they have registered their guideline with EQUATOR (like they do for clinical trials).\n\nWhen a new reporting guideline is justified, build upon existing reporting guidelines instead of starting from scratch. This could mean extending or replacing subsets of items instead of publishing a totally new reporting guideline.\nConsider making modular guidance. For instance, the Journal Article Reporting Standards (JARS) are a set of reporting guidelines for psychology. JARS has a main guideline for quantitative studies. The Design item can be extended by one of three modules depending on whether the study involved experimental manipulation or was conducted on a single individual. the experimental manipulation module can be further extended by modules for random assignment, non-random assignment, and clinical trials. Other extension modules exist for longitudinal and replication studies.\n\n\nWho could do this: Guideline developers, EQUATOR Network, Publishers\n\n\nBarriers addressed:\n\nResearchers may not know what RGs exist\n\n\nResearchers may struggle to reconcile multiple sets of guidance\n\n\nResearchers may not know what RG is their best fit"
  },
  {
    "objectID": "chapters/appendix/ideas.html#before-developing-guidance",
    "href": "chapters/appendix/ideas.html#before-developing-guidance",
    "title": "Ideas generated from workshops and focus groups",
    "section": "",
    "text": "Consider creating reporting guidance to help authors write protocols, funding applications, and ethics applications.\n\nWho could do this: Guideline developers\n\n\nBarriers addressed:\n\nResearchers may not have tools for the job at hand\n\n\nResearchers may not encounter RGs early enough to act on them\n\n\n\n\n\n\nTo avoid duplicating resources, before commencing a new reporting guideline:\n\nconsult EQUATOR’s register of reporting guidelines under development;\n\nEQUATOR could make this register easier to find and search;\n\ncontact the developers of related reporting guidelines;\njournals could ask reporting guideline developers to prove that they have registered their guideline with EQUATOR (like they do for clinical trials).\n\nWhen a new reporting guideline is justified, build upon existing reporting guidelines instead of starting from scratch. This could mean extending or replacing subsets of items instead of publishing a totally new reporting guideline.\nConsider making modular guidance. For instance, the Journal Article Reporting Standards (JARS) are a set of reporting guidelines for psychology. JARS has a main guideline for quantitative studies. The Design item can be extended by one of three modules depending on whether the study involved experimental manipulation or was conducted on a single individual. the experimental manipulation module can be further extended by modules for random assignment, non-random assignment, and clinical trials. Other extension modules exist for longitudinal and replication studies.\n\n\nWho could do this: Guideline developers, EQUATOR Network, Publishers\n\n\nBarriers addressed:\n\nResearchers may not know what RGs exist\n\n\nResearchers may struggle to reconcile multiple sets of guidance\n\n\nResearchers may not know what RG is their best fit"
  },
  {
    "objectID": "chapters/appendix/ideas.html#when-developing-guidance",
    "href": "chapters/appendix/ideas.html#when-developing-guidance",
    "title": "Ideas generated from workshops and focus groups",
    "section": "When developing guidance",
    "text": "When developing guidance\n\n1: Avoid prescribing structure\n\nAvoid prescribing structure of a journal article as it may clash with journal requirements or other reporting guidelines.\nInstead, give options for where items can be reported.\nInclude options beyond the article body where authors can report information, like tables, figures, or appendices be.\n\n\n\nWho could do this: Guideline developers\n\n\nBarriers addressed:\n\nResearchers may struggle to reconcile multiple sets of guidance\n\n\nResearchers may struggle to keep writing concise\n\n\n\n\n2: Keep reporting guidelines agnostic to design choices\n\nAsk authors to describe methods transparently without making assumptions about, or prescribing, methods or design choices. For example, an instruction to “describe how you determined your sample size” may be more helpful than “report your sample size calculation” for authors who encounter checklists at submission and did not perform a sample size calculation before collecting data.\nAvoid recommending or admonishing design choices within the reporting guidance because:\n\ndoing so may make authors feel nervous or ashamed, and therefore less likely to report transparently;\ndesign advice elongates reporting guidelines;\nincluding design advice may give the impression that the reporting guideline is for designing or appraising design.\n\nConsider linking to external design or appraisal tools instead.\n\nWho could do this: Guideline developers\n\n\nBarriers addressed:\n\nResearchers may feel restricted if RGs prescribe design\n\n\nResearchers may feel afraid to report transparently\n\n\nResearchers may expect the costs to outweigh benefits\n\n\nResearchers may not know what RGs are\n\n\n\n\n3: Describe reporting items fully\nFor each item, authors may need to know the following:\n\nWhat needs to be reported – a brief description could go in all resources (checklists, templates etc) with a longer description in the full guideline document.\nWhy the information is important, and to whom\nAny circumstances where the item is not applicable and what to write\nIndicate priority, and any circumstances that modify importance\nWhere the item can be reported, including beyond the main article body (e.g., section, table, figure, appendix)\nWhat to write if an item wasn’t, or couldn’t be done\nWhat to write if an item cannot be reported for external reasons. For example, if items cannot be reported because of intellectual property restrictions.\nExamples, which could be real or generated, including:\n\nexamples of good and bad reporting with explanations.\nexamples of concise or word-count-friendly reporting, perhaps in alternative formats like tables and figures.JH\nexamples of well reported “imperfect” items (items that were not done)\nexamples from different research contexts\n\nLinks to external design or appraisal advice\n\n\nWho could do this: Guideline developers\n\n\nBarriers addressed:\n\nResearchers may not know whether a RG applies to them\n\n\nResearchers may not know how to report an item in practice\n\n\nResearchers may not know how to do an item\n\n\nResearchers may not know what to write when they cannot report an item\n\n\nResearchers may struggle to keep writing concise\n\n\nResearchers may not know why items are important\n\n\nResearchers may not care about the benefits of using a RG\n\n\nResearchers may be asked to remove RG content\n\n\nResearchers have limited time\n\n\nResearchers may struggle to keep writing concise\n\n\n\n\n4: Describe each reporting guideline fully\nFor each reporting guideline, authors may need the following information:\n\nA clear definition of the reporting guideline’s intended scope in plain language.\nIf-then rules to direct authors to other, more appropriate reporting guidelines. For example, CONSORT could point authors writing protocols to SPIRIT.\nIf no better guidance exists then indicate which items do/do not apply. For example, no guideline exists for authors writing protocols for observational epidemiology. Their best option currently is to use STROBE, but only some items will be required in a protocol.\nWhat tasks the reporting guideline can and cannot be used for\nHow long the resource will take to use\nWhy the guidance should be trusted and link to how it was developed\n\n\nWho could do this: Guideline developers, EQUATOR Network\n\n\nBarriers addressed:\n\nResearchers may not know whether a RG applies to them\n\n\nResearchers may not know what RGs exist\n\n\nResearchers may not know what RG is their best fit\n\n\nResearchers may not know when RGs should be used\n\n\nResearchers may feel that checking reporting is someone else’s job.\n\n\nResearchers have limited time\n\n\nResearchers may feel patronized\n\n\n\n\n5: Keep guidance short\nKeep guidance as a short as possible:\n\nBe concise but clear.\nBe realistic about what to expect from authors as each additional item increases the chances an author will be put off\nLink to other guidance elsewhere if desired.\nConsider splitting broad guidance that tries to cater for different options into shorter, modular guidance (modularity avoids duplication).\n\n\nWho could do this: Guideline developers\n\n\nBarriers addressed:\n\nResearchers have limited time\n\n\nResearchers may expect the costs to outweigh benefits"
  },
  {
    "objectID": "chapters/appendix/ideas.html#when-writing-guidance-down-and-creating-resources",
    "href": "chapters/appendix/ideas.html#when-writing-guidance-down-and-creating-resources",
    "title": "Ideas generated from workshops and focus groups",
    "section": "When writing guidance down and creating resources",
    "text": "When writing guidance down and creating resources\n\n1: Make resources ready-to-use\nEnsure resources are ready-to-use e.g., checklists as Word files, not as tables within published articles.\n\nWho could do this: Guideline developers, EQUATOR Network\n\n\nBarriers addressed:\n\nRG resources may not be in usable formats\n\n\nResearchers have limited time\n\n\n\n\n2: Make reporting guidelines easy to understand\n\nUse plain language.\nDefine key terms.\nUse consistent terms across related resources.\nProvide translations.\nUpdate guidance in response to user feedback.\n\n\nWho could do this: Guideline developers\n\n\nBarriers addressed:\n\nResearchers may misunderstand\n\n\nResearchers may not understand the language\n\n\n\n\n3: Use persuasive language and design\n\nUse language and design to communicate confidence and simplicity as opposed to judgement and complexity.\nEncourage explanation even when choices were unusual or sub-optimal.\nReassure authors that most research has limitations that can be addressed in Discussion sections.\nReassure authors that reporting guidelines are just guidelines.\nAvoid patronizing authors.\nConsider wording instructions directly at the intended user.JH\n\n\nWho could do this: Guideline developers, EQUATOR Network, Publishers, Funders, Ethics committees, Institutions, Conference organisers, Registries, Preprint servers\n\n\nBarriers addressed:\n\nResearchers may feel afraid to report transparently\n\n\nResearchers may feel patronized\n\n\nResearchers may not believe stated benefits\n\n\nResearchers may feel that checking reporting is someone else’s job.\n\n\n\n\n4: Create additional tools\nCreate tools for different tasks:\n\ndiscussion points for planning research in the order decisions are made\nto-do lists for conducting research in the order data is collected\ntemplates for drafting\nwriting assistance tools (e.g., COBWEB)\nchecklists for checking manuscripts that are easy to fill out, update, and cross-check\ntools for co-researchers to check each other’s work\ntools for generating tables and figures\nresources for peer reviewers who wish to review reporting quality including:\n\nguidance specifically for peer reviewers.\ncommonly-used words that reviewers can search for to quickly find relevant text.JH\nsuggested text that peer reviewers can copy to request information\ntools to generate feedback reports\n\njournal articles where reporting guideline items are annotated/highlighted\n\n\nWho could do this: Guideline developers, EQUATOR Network, Funders, Ethics committees, Publishers\n\n\nBarriers addressed:\n\nResearchers may not have tools for the job at hand\n\n\nResearchers may not encounter RGs early enough to act on them\n\n\n\n\n5: Make resources easy to discover and find\nLink resources:\n\nEnsure all resources link to each other. For example, checklists should link to example and elaboration documents and vice versa.\nRelated reporting guidelines should link to each other.\nReporting guidelines and resources should link to translations\nLinks should be permanent (e.g. DOIs) where possible and old links should be maintained or redirected. Broken links should be replaced.\n\nMake searching easy:\n\nHost resources somewhere consistent, like the EQUATOR Network website and database.\nProvide easy-to-use website search functions\nWeb pages should be optimized for search engines JH\nCreated curated collections for study types\nCreate decision tools for identifying reporting guidelines\n\nNames reporting guidelines to make them easy to discover and find:\n\n\nReporting guideline names could be descriptive, as acronyms may be meaningless to novice users.\nRelated reporting guidelines should use consistent names to show relationships (e.g. PRISMA and PRISMA-P appear more related than CONSORT and SPRIT).\n\n\nWho could do this: Guideline developers, EQUATOR Network\n\n\nBarriers addressed:\n\nResearchers may not know what RGs exist\n\n\nResearchers may not know what resources exist for a RG\n\n\nGuidance may be difficult to find\n\n\nResearchers may not know what RG is their best fit\n\n\n\n\n6: Make information digestible\nOrganise information so it is easy to navigate and not overwhelming.\n\nCater to users that read from start to finish, and those that dip in and out.\nStructure text with headings.\nUse section URLs to send authors directly to relevant parts of guidance.\nConsider hyperlinking related resources\nConsider embedding reporting guidelines that “fit together”, like PRISMA and PRISMA-Abstracts\nFor information presented online, consider showing/hiding information as required. For example, if PRISMA-Abstracts were embedded into PRISMA, users could choose to expand or collapse it. Or you could show/hide guidance depending on whether the author is writing a funding application, protocol, manuscript.\n\n\nWho could do this: Guideline developers, EQUATOR Network\n\n\nBarriers addressed:\n\nResearchers have limited time\n\n\nResearchers may expect the costs to outweigh benefits\n\n\nGuidance may be difficult to find"
  },
  {
    "objectID": "chapters/appendix/ideas.html#when-disseminating-resources",
    "href": "chapters/appendix/ideas.html#when-disseminating-resources",
    "title": "Ideas generated from workshops and focus groups",
    "section": "When disseminating resources",
    "text": "When disseminating resources\n\n1: Describe reporting guidelines where they are encountered\n\nWhen authors first encounter reporting guidelines they may need to know:\n\nwhat reporting guidelines are\nhow and when to use them\nwhy authors should use them, including:\n\nwhat personal benefits to expect\nthe importance to others.\n\n\nDescriptions could be succinct (e.g. on journal instruction pages) or long (e.g. in publications) JH\nA generalised description can go where authors first encounter reporting guidelines e.g., journal author guidelines, EQUATOR’s home page.\nA reporting guideline-specific description could go at the top of guidance documents, checklists, and templates.\n\nConsider specifying whether the reporting guideline is also a design guideline.\nSpecify whether the reporting guidelines are just guidelines, or whether they are intended to be requirements. Name the resource appropriately - words like guideline, standards, criteria, recommended, preferred, and templates, have different meanings.\n\n\n\nWho could do this: Publishers, EQUATOR Network, Guideline developers, Funders, Ethics committees, Institutions, Registries, Preprint servers, Conference organisers\n\n\nBarriers addressed:\n\nResearchers may not know what RGs are\n\n\nResearchers may not know when RGs should be used\n\n\nResearchers may not know what benefits to expect\n\n\nResearchers may not know why items are important\n\n\nResearchers may feel that checking reporting is someone else’s job.\n\n\n\n\n2: Make resources accessible\nEnsure resources are open access. This allows access to authors without journal subscriptions and allows others to build upon the guidance.\n\nWho could do this: Guideline developers, EQUATOR Network\n\n\nBarriers addressed:\n\nRGs may be difficult to access\n\n\n\n\n3: Show and encourage citations\n\nDisplay usage data (like citations or downloads) alongside the guidelines as a form of social proof.\nEncourage authors to cite the reporting guideline so readers discover it.\n\n\nWho could do this: Guideline developers, EQUATOR Network, Publishers, Conference organisers, Preprint servers\n\n\nBarriers addressed:\n\nResearchers may not know what RGs exist\n\n\nResearchers may not believe stated benefits\n\n\n\n\n4: Provide testimonials\nTestimonials can be short quotes or longer case studies. They could come from:\n\nresearchers who have had positive experiences using reporting guidelines, including researchers that were nervous about transparency,\ndecision makers (e.g., editors/grant managers) that value good reporting and/or check for reporting as part of their evaluation,\npeer reviewers that use reporting guidelines to check for good reporting,\npatients who are affected by research waste,\nand researchers who need to understand, synthesise, or apply research articles.\n\n\nWho could do this: Guideline developers, EQUATOR Network\n\n\nBarriers addressed:\n\nResearchers may not know what benefits to expect\n\n\nResearchers may not believe stated benefits\n\n\nResearchers may expect the costs to outweigh benefits\n\n\nResearchers may not care about the benefits of using a RG\n\n\nResearchers may feel afraid to report transparently"
  },
  {
    "objectID": "chapters/appendix/ideas.html#on-an-ongoing-basis",
    "href": "chapters/appendix/ideas.html#on-an-ongoing-basis",
    "title": "Ideas generated from workshops and focus groups",
    "section": "On an ongoing basis",
    "text": "On an ongoing basis\n\n1: Budget for reporting\nFunders and research supervisors could encourage researchers to allocate sufficient time and money for documenting and reporting results of their research.\n\nWho could do this: Funders, Institutions\n\n\nBarriers addressed:\n\nResearchers have limited time\n\n\nResearchers may not consider writing as reporting\n\n\n\n\n2: Create rewards\nStakeholders could create new rewards:\n\njournals could fast-track submissions or review for papers that followed a reporting guideline,\njournals could offer discounts on article processing charges for papers that followed a reporting guideline,\njournals, preprint servers, or peer review platforms could badge well reported articles,\nEQUATOR could offer a certification service,\nfunders could reward good reporting financially,\ninstitutions could offer prizes for good reporting.\n\n\nWho could do this: Guideline developers, EQUATOR Network, Publishers, Funders, Institutions, Preprint servers, Registries\n\n\nBarriers addressed:\n\nResearchers may not care about the benefits of using a RG\n\n\n\n\n3: Create discussion spaces\nCreate spaces for authors to discuss reporting and reporting guidelines. These could be:\n\nonline (forums, social media, email),\nor offline (meet-ups, clubs).\n\nTry to make spaces accessible to researchers from all nationalities, professional disciplines and other demographics. Spaces will allow authors to:\n\nsolicit help,\nshare experiences,\nprovide feedback to guideline developers,\nand cultivate a feeling of inclusivity and community ownership.\n\n\nWho could do this: EQUATOR Network, Guideline developers, Institutions\n\n\nBarriers addressed:\n\nResearchers may misunderstand\n\n\nResearchers may feel patronized\n\n\nResearchers may not believe stated benefits\n\n\nResearchers may not know how to report an item in practice\n\n\nResearchers may not know how to do an item\n\n\n\n\n4: Create ways to catch authors earlier\n\nConsider creating email campaigns to prompt researchers at early stages.\nThe EQUATOR website could encourage visitors to use reporting guidelines for planning and drafting research.\nWebsites could be optimised for search terms like “how to write [study type]”, “protocol”, “research plan” or “funding application”. For example, reporting guideline pages on EQUATOR’s website rank highly in Google searches for “STROBE checklist” but not “How to write an observational epidemiology study”.JH\nWriting clubs and writing training could flag reporting guidelines.\n\n\nWho could do this: EQUATOR Network, Guideline developers, Publishers, Funders, Ethics committees, Institutions, Conference organisers, Preprint servers, Registries\n\n\nBarriers addressed:\n\nResearchers may forget to use RGs at earlier research stages\n\n\nResearchers may not encounter RGs early enough to act on them\n\n\nResearchers have limited time\n\n\nResearchers may not know when RGs should be used\n\n\n\n\n5: Endorse and enforce reporting guidelines\nStakeholders could:\n\nendorse reporting guidelines\nenforce their use by mandating checklists or (preferably) checking adherence to items.\nFunders could ask about reporting guidelines or checklists when collecting updates from grant recipients.\n\n\nWho could do this: Publishers, Institutions, Ethics committees, Funders, Registries, Conference organisers, Preprint servers\n\n\nBarriers addressed:\n\nResearchers may not know what RGs exist\n\n\nResearchers may expect the costs to outweigh benefits\n\n\n\n\n6: Evidence the benefits\nEvidence any stated benefits:\n\nQuantifiable benefits could be evidenced with data (e.g., acceptance rates, publishing speed, writing speed).\nExperiential benefits could be evidenced by collecting case studies from authors who find that reporting guidelines help them feel confident and write more easily, and from readers who value well-reported research.\n\n\nWho could do this: Guideline developers, EQUATOR Network, Publishers\n\n\nBarriers addressed:\n\nResearchers may not believe stated benefits\n\n\n\n\n7: Make reporting guidelines appear as a priority\nJournals, funders and ethics committees could make reporting guidelines appear as a priority:\n\nMake them prominent in author instructions.\nPlacing checklists earlier in the PDFs that are automatically created by journal submission systems.\nPublicize when reporting guidelines are used by reviewers.\n\n\nWho could do this: Publishers, Funders, Ethics committees, Institutions, Preprint servers, Conference organisers, Registries\n\n\nBarriers addressed:\n\nResearchers may not believe stated benefits\n\n\nResearchers may not care about the benefits of using a RG\n\n\n\n\n8: Promote reporting guidelines\n\nPromote reporting guidelines on and offline.\n\nOnline may include websites, email campaigns, social media, and blogs.\nOffline may include appearing at conferences, seminars, and workshops.\n\nInstitutions could promote reporting guidelines in their curricula, learning materials, or through reporting champions. Reporting guideline developers or EQUATOR could push for reporting guidelines to be included in text books.\nPromotion can begin before a reporting guideline has been published so that researchers know about guidelines being developed.\n\nNB. Promotion is different to endorsement; a journal could run an email campaign to promote reporting guidelines without having an endorsement policy.\n\nWho could do this: Institutions, Publishers, Guideline developers, EQUATOR Network, Ethics committees, Funders, Societies, Registries, Conference organisers, Preprint servers\n\n\nBarriers addressed:\n\nResearchers may not know what RGs are\n\n\nResearchers may not know what RGs exist\n\n\n\n\n9: Install reporting champions\nAll stakeholders could have members to promote and facilitate the usage of reporting guidelines.\n\nThis could follow a local network model with EQUATOR as the central organiser.\nCould make use of existing networks, like regional reproducibility networks.\n\n\nWho could do this: EQUATOR Network, Guideline developers, Institutions, Funders, Ethics committees, Publishers, Conference organisers, Preprint servers, Registries\n\n\nBarriers addressed:\n\nResearchers may not know what RGs are\n\n\nResearchers may not know what benefits to expect\n\n\nResearchers may misunderstand\n\n\nResearchers may not know when RGs should be used\n\n\nResearchers may not know why items are important\n\n\n\n\n10: Provide additional teaching\nProvide education or training (e.g., courses, videos) specific to particular reporting guidelines.\nMore generally, students could:\n\nlearn about writing as a process and workflows for documenting and communicating research,\nlearn about research waste from poor reporting,JH\nattempt a replication to learn about the importance of complete reporting,\nand use a reporting guideline as part of their studies.\n\n\nWho could do this: Guideline developers, EQUATOR Network, Publishers, Institutions, Funders, Ethics committees\n\n\nBarriers addressed:\n\nResearchers may not consider writing as reporting\n\n\nResearchers may misunderstand\n\n\nResearchers may not know why items are important\n\n\nResearchers may not know how to report an item in practice\n\n\nResearchers may not encounter RGs early enough to act on them\n\n\nResearchers may not care about the benefits of using a RG\n\n\n\n\n11: Make updating guidelines easier\nUpdate guidance in response to user feedback or changes in the field. This would be easier if:\n\nreporting guideline developers could easily collect feedback from authors.\nsmall updates or refinements could be made without publishing a new article.\nreporting guideline developers had funding to evaluate, refine, and update their resources.JH\n\n\nWho could do this: EQUATOR Network, Funders\n\n\nBarriers addressed:\n\nRGs can become outdated\n\n\nResearchers may misunderstand"
  },
  {
    "objectID": "chapters/5_website_audit/index.html",
    "href": "chapters/5_website_audit/index.html",
    "title": "How could equator-network.org be improved? Inferences from Google Analytics data.",
    "section": "",
    "text": "This is the same as the article I drafted a year or so ago. Gary had suggestions about how to reframe it but I’ve not edited it at all yet."
  },
  {
    "objectID": "chapters/5_website_audit/index.html#abstract",
    "href": "chapters/5_website_audit/index.html#abstract",
    "title": "How could equator-network.org be improved? Inferences from Google Analytics data.",
    "section": "Abstract",
    "text": "Abstract\nThe UK EQUATOR Centre maintains a website to help authors find and access reporting guidelines. Web analytics shine a light on how well the website is performing and what could be done to improve it. Here we describe some key insights from web analytics. 1) Many users leave the site very quickly without interacting with it at all. 2) Guideline views are very heavily skewed towards the few guidelines that are featured on EQUATOR’s home page and on journal instructions to authors. 3) Users access checklists more often than the full guidance, perhaps because the full guidance is harder to find. 4) EQUATOR could optimize their site for search engines. 5) EQUATOR could improve acquisition data by updating their links on third party websites and offline documents. 6) Some countries with high publication volumes are under-represented amongst website visitors. 7) EQUATOR could consider translating parts of their website to cater for a global audience."
  },
  {
    "objectID": "chapters/5_website_audit/index.html#introduction",
    "href": "chapters/5_website_audit/index.html#introduction",
    "title": "How could equator-network.org be improved? Inferences from Google Analytics data.",
    "section": "Introduction",
    "text": "Introduction\nIn chapter ?var:chapters.survey_content I found two studies that exploring authors’ awareness of EQUATOR as an organisation ([1]; [2]). Although some authors will know EQUATOR from their training programmes or publications, most will know EQUATOR from its website. Despite running on a shoestring budget and without any in-house expertise in software, design, or user experience, the website’s traffic has increased from 100,000 users to almost 1 million over the last 10 years, attracting visitors from all around the world. For context, around 4.3 million authors published 2.4 million academic articles in 2014 [3], and the STM association estimated that 11 million people working in research and development in 2018 [4]. Whilst these numbers will have grown since, they suggest that EQUATOR’s website traffic numbers are within a single order of magnitude of its potential audience size.\nSTRETCH These numbers are for research in general. I haven’t found numbers specific to medicine yet.\nHence what started as a research-group’s attempt to catalogue reporting guidelines has become a significant part of the academic eco-system. And yet, EQUATOR has never formally considered how successfully their website is performing, or how it could be improved.\nIn this chapter I describe how I worked with EQUATOR to identify key metrics of success and how I used web analytics to describe those metrics. In the discussion section, I infer how the website may need improving.\nTODO: dashboard no longer works"
  },
  {
    "objectID": "chapters/5_website_audit/index.html#methods",
    "href": "chapters/5_website_audit/index.html#methods",
    "title": "How could equator-network.org be improved? Inferences from Google Analytics data.",
    "section": "Methods",
    "text": "Methods\nGoogle Analytics [5] is a web analytics service that helps website owners track and understand users’ activity. Used by 85% of websites globally, it is the most popular web analytics service by far [6]. EQUATOR has used Google Analytics to collect data since creating their website. Mostly they use the data to report high level impact metrics to funders (such as number of visitors) but they have never used it to evaluate their site in depth.\nWhen evaluating a website, there are a huge number of metrics you can consider; Google Analytics collects over 50 by default [7]; [8], and Google Tag Manager [WebMobileTag?] allows you to collect additional custom metrics (see ?@tbl-ga for definitions of Google Analytics terms and metrics). With so many options, the first step of evaluation is to decide what metrics are most important for your website’s objectives.\nI met with three members of the UK EQUATOR Centre in October 2021. When I asked what the main purposes of their website [9] was, I received many answers. EQUATOR has content aimed at authors, editors, peer reviewers, educators, librarians, and guideline developers. There are web pages promoting training courses, toolkits for writing and reviewing research, newsletters, and blogs highlighting work done by EQUATOR and guideline developers.\nHowever, at its core, the EQUATOR staff I spoke to agreed that the purpose of the website is to help the global research community learn about and access reporting guidelines. They want website visitors to access the guidance that is right for them and come back to the website whenever they need guidance.\nTo explore how far reality meets this vision, I used Google Analytics to answer the following questions:\n\nHow many people visit the website each year?\nWhere (in the world) are visitors from?\nHow often do visitors come back?\nHow do visitors get to the website?\nHow many visitors access guidance?\nWhat guidelines do visitors access?\nHow many visitors access publications or checklists?\n\nThe last two questions could not be answered by Google Analytics’ default configuration; it recorded which database records visitors looked at, but not how many people went on to view guidance on third party websites, or whether visitors were viewing checklists or full guidance. Therefore I used Google Tag Manager to create two custom metrics: one which counted when visitors downloaded a reporting checklist file, and another which counted when visitors accessed a third party website.\nSTRETCH could elaborate on these but my gut is to keep chapter short.\nEQUATOR staff felt it was important that website users were able to access the right resource. Although Google Analytics could tell us what pages visitors access, it can’t tell us what they needed, or why. To answer this, I used PopupSmart [PopupBuilderThat?] to create a single-question exit survey that asked “What were you looking for today?”. I used a single, open ended question to maximize my response rate.\nTODO name and reference technique"
  },
  {
    "objectID": "chapters/5_website_audit/index.html#results",
    "href": "chapters/5_website_audit/index.html#results",
    "title": "How could equator-network.org be improved? Inferences from Google Analytics data.",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "chapters/5_website_audit/index.html#keep-visitors-engaged",
    "href": "chapters/5_website_audit/index.html#keep-visitors-engaged",
    "title": "How could equator-network.org be improved? Inferences from Google Analytics data.",
    "section": "Keep visitors engaged",
    "text": "Keep visitors engaged\nGoogle Analytics classified almost all users as new, which means they had not visited the site within the previous 2 years (the default expiry limit for Google Analytics cookies). Most users only visited the site once within 2021. Two thirds of sessions lasted less than 10 seconds, and over half of sessions ended without the user interacting with the site at all. EQUATOR’s website holds users’ attention for a very short amount of time and, currently, a large proportion of users are leaving the site without accessing any reporting advice.\nA third of sessions begin with users arriving on the home page, where almost half of visitors then immediately leave without interacting with the website. Google Analytics calls this behaviour bouncing. Many users arrive directly on a guideline database page. Some of these pages have even higher bounce rates, such as 56% for STROBE and 73% for COREQ.\nThere are many reasons why a user may abandon a website quickly. In autumn 2021, the UK EQUATOR Centre added a popup survey to ask users why they were leaving the website. Whilst some answered that they “got what they were looking for,” others wrote that “the site is very complex,” “too big a mess” or that they “could not find what they needed.” One visitor wrote that they were looking for the formatting instructions for their target journal, suggesting that they did not understand what EQUATOR was or that they were no longer on the journal’s website. EQUATOR should continue to research why users leave these pages so quickly, why they don’t return and how the website could be improved, with special attention paid to the home page and guideline pages."
  },
  {
    "objectID": "chapters/5_website_audit/index.html#help-users-discover-reporting-guidelines-besides-the-few-that-are-featured",
    "href": "chapters/5_website_audit/index.html#help-users-discover-reporting-guidelines-besides-the-few-that-are-featured",
    "title": "How could equator-network.org be improved? Inferences from Google Analytics data.",
    "section": "Help users discover reporting guidelines besides the few that are featured",
    "text": "Help users discover reporting guidelines besides the few that are featured\nEQUATOR maintains a database of reporting guidelines. Each database entry is viewable as a webpage that displays the name of the guideline, bibliographic data, meta data, and links to the associated publications and files. Users must navigate this page before accessing the guideline or checklist.\nThere are over 500 reporting guidelines indexed in EQUATOR’s database, but hardly any are viewed regularly. Only 13 guideline database records were viewed in more than 1% of visits, all of which appear in the list of “reporting guidelines for main study types” featured on EQUATOR’s home page and in a side bar on all reporting guideline sub-pages. STROBE was the most viewed reporting guideline, with 231,207 unique page views in 2021. These numbers dropped rapidly: SQUIRE, the tenth most viewed reporting guideline record, was viewed ten times less frequently than STROBE. Only 65 reporting guideline pages were viewed in more than 0.1% of sessions.\nViewership is thus heavily skewed towards the few guidelines that are featured on EQUATOR’s home page and journal instruction to author pages. We expect some guidelines to be viewed more than others, but this distribution seems extreme and suggests that authors are not discovering newer or more specific guidelines that might be more applicable to their work.\nPossible solutions to this would be to make the search function more prominent and easier to use, and to better highlight related guidelines at the top guideline databases pages, checklists, and guidance articles, along with clear instructions of when each reporting guideline should or should not be used. Guideline developers should be aware that inclusion in EQUATOR’s database does not guarantee authors will find and use their resources, and so should consider complementary dissemination strategies."
  },
  {
    "objectID": "chapters/5_website_audit/index.html#do-users-access-checklists-articles-or-neither",
    "href": "chapters/5_website_audit/index.html#do-users-access-checklists-articles-or-neither",
    "title": "How could equator-network.org be improved? Inferences from Google Analytics data.",
    "section": "Do users access checklists, articles, or neither?",
    "text": "Do users access checklists, articles, or neither?\nChecklists are accessed more frequently than the full guidance when they are available (Table 3). This ratio is 15:1 for STROBE, 6:1 for CONSORT, and 8:1 for STARD. This is probably because EQUATOR links to checklists at the very top of the guideline page and links to the full guidance appear lower in the page and are harder to find. This has implications for guideline developers who should ensure that important information about the guideline such as its aim, scope, or how to use it, should be placed in the checklist, as should a link to the full guidance. Guideline developers should not assume that users will discover the E&E document before the checklist or at all.\nIt is worth noting that many users leave the guideline reporting guideline database page without accessing any resources. Even if a visitor clicks an outbound link to the full guidance, these links generally take visitors to a PubMed record where they must press another button to access the full text. There will therefore be additional drop-off before visitors reach the full guidance. EQUATOR should improve their reporting guideline database pages to make the checklists and full guidance easier to discover."
  },
  {
    "objectID": "chapters/5_website_audit/index.html#search-engine-optimization-to-catch-naïve-authors-early-in-their-research",
    "href": "chapters/5_website_audit/index.html#search-engine-optimization-to-catch-naïve-authors-early-in-their-research",
    "title": "How could equator-network.org be improved? Inferences from Google Analytics data.",
    "section": "Search engine optimization to catch naïve authors early in their research",
    "text": "Search engine optimization to catch naïve authors early in their research\nA third of users arrive at the site via a search engine. There are many ways EQUATOR could optimize their site for search engines, including adding metadata, mobile optimization, and taking advantage of Google Search’s featured snippets and description features. Keyword rankings could also help to drive potential users to the site. For instance, Google Search Console is a Google product that allows website owners to view how their site performs in Google searches. It shows that when users search for “STROBE guidelines,” EQUATOR’s site appears at the top of search results and has a click through rate of 36%. However, if a user searches for “how to write an epidemiological report,” EQUATOR drops to 29th place with a click through rate of 0%. EQUATOR should ensure the site is optimized for naïve users at an early stage of writing who may not know guideline acronyms."
  },
  {
    "objectID": "chapters/5_website_audit/index.html#update-links-on-other-websites-to-understand-where-users-are-coming-from",
    "href": "chapters/5_website_audit/index.html#update-links-on-other-websites-to-understand-where-users-are-coming-from",
    "title": "How could equator-network.org be improved? Inferences from Google Analytics data.",
    "section": "Update links on other websites to understand where users are coming from",
    "text": "Update links on other websites to understand where users are coming from\nAn eighth of traffic was explicitly labelled as referrals from other websites, most commonly Wiley and Elsevier journals and Manuscript Central. However, it is likely that much of the traffic from unknown source is also referrals from other websites. These unknown sources currently account for half of EQUATOR’s traffic. Some of this unknown traffic may come from links within offline files. It is likely that a lot is from journal websites and submission systems that link to EQUATOR using links that start with http instead of https. When a secure website with an https address links to a less secure site with an http address, no referral data gets sent. EQUATOR upgraded its website to use https years ago, but other websites have continued to link using http. A campaign to have journals and submission platforms use https links would result in more correct referral data. This would tell EQUATOR which journals are successfully recommending reporting guidelines and would allow EQUATOR to infer visitors’ intentions. For example, traffic from submission systems may signify users who have been asked to complete a checklist from."
  },
  {
    "objectID": "chapters/5_website_audit/index.html#target-authors-from-under-represented-countries",
    "href": "chapters/5_website_audit/index.html#target-authors-from-under-represented-countries",
    "title": "How could equator-network.org be improved? Inferences from Google Analytics data.",
    "section": "Target authors from under-represented countries",
    "text": "Target authors from under-represented countries\nOver 800,000 users visited the EQUATOR website in 2021 (Table 4), 150,000 more than the previous year, thus continuing growth seen since attracting 20,000 visitors in 2008. A third of users were from the United States or United Kingdom. Brazil accounted for the same proportion of users as the UK (7%) despite producing far fewer citable, medical documents [10]. Conversely, China produces twice as many citable documents as the UK but accounts for only 5% of users. These numbers suggest that awareness of EQUATOR is greater in some counties than in others. EQUATOR should continue to run awareness campaigns and should prioritize countries like China which have high publication output but low visitor numbers."
  },
  {
    "objectID": "chapters/5_website_audit/index.html#consider-non-native-english-speakers",
    "href": "chapters/5_website_audit/index.html#consider-non-native-english-speakers",
    "title": "How could equator-network.org be improved? Inferences from Google Analytics data.",
    "section": "Consider non-native English speakers",
    "text": "Consider non-native English speakers\nTwo fifths of users had their browsers set to a language other than English (Table 4), most frequently Portuguese, Chinese, and Spanish. Manually translating popular guidelines or website sections may be appropriate for frequent languages, but manual translations are expensive, difficult to update, and cannot be scaled. Automatic machine translation is more scalable, easy to update, and inaccuracies can be refined with custom glossaries and language models. EQUATOR and guideline developers should also use plain language to help users and automatic translators alike.\n\nSurvey data suggest some visitors may not understand what the website is about, and may not find it useful\nI let the survey run for 2 weeks before deciding to take it down.\nThe response rate was poor -\nThe responses that we received were rarely insightful. Only three visitors answered the question. One wanted “to get reporting guidelines” another wanted “a tool called standard for reporting qualitative research”, and a third “could not find any reporting guidelines for reporting guidelines (specifically, abstracts for reporting guidelines)”. Two others hinted at what they wanted but weren’t explicit, “qualitative”, or “word format would be more easy to fulfil” (presumably referring to reporting checklists). A handful of users didn’t specify what they were looking for, but stated whether they had found it (e.g. “got what I was looking for!”, “found what I needed!”, “Could not find what I needed”, “I cannot find the guidance that I seek”, “I don’t see what I want”).\nBut the majority of responses did not help answer our question. Some visitors seemed to be in the wrong place: one was looking for a “quality of life questionnaire”, another for “scientific research”. Two authors seemed to be looking for requirements for specific journals: one wrote “format for paper submission to Hindawi”, and another wrote “awful site. I just want to know the requirements in terms of number of words and format for a submission and cannot seem to find this anywhere”. Other visitors also seemed frustrated with the website (“i did not under stand any thing”, “The site is very complex”, “Too big a mess”) and the popup itself (“You might want to do something about your annoying popup!”, “don’t ask that”).\nSome user’s responses made no sense at all. One wrote “ALGERIA”, another “Germany”."
  },
  {
    "objectID": "chapters/5_website_audit/index.html#limitations",
    "href": "chapters/5_website_audit/index.html#limitations",
    "title": "How could equator-network.org be improved? Inferences from Google Analytics data.",
    "section": "Limitations",
    "text": "Limitations\nGoogle Analytics uses cookies to track users over time. If a user clears their cookies between visits or uses multiple devices or browsers, the user will appear as multiple users. Cookies expire after 2 years by default. The proportion of new vs. returning users is thus an overestimation but, nevertheless, is still high.\nAlthough we discuss bounce rates here it is difficult to know what a “good” bounce rate would be. E-Commerce websites may hope for a bounce rate of around 40%, but newspapers may consider 70% to be good. What is important is that EQUATOR now has a baseline against which to measure improvement. Ultimately, numbers can only tell you so much. Counting bounces is useful, but only qualitative research will explain why users bounce from the EQUATOR website with the frequency that they do."
  },
  {
    "objectID": "chapters/5_website_audit/index.html#todo-definitions",
    "href": "chapters/5_website_audit/index.html#todo-definitions",
    "title": "How could equator-network.org be improved? Inferences from Google Analytics data.",
    "section": "TODO Definitions",
    "text": "TODO Definitions\nUser\nVisitor\nSession\nBounce\nBounce Rate\nSTROBE\nPRISMA\nCONSORT\nSTARD\nSQUIRE\nE&E\nGoogle Analytics\nGoogle Search Console\n\nTable 1 Session information for EQUATOR-Network.org for the year 2021. A session is defined as group of user interactions that take place within a given time frame. If a user visited the site twice within a year, then that would appear as two sessions. A session ends after 30 minutes of inactivity.\n\n\n\n\n\n\n\n\n\nNumber\nPercent\n\n\n\n\nNumber of sessions\n1,209,420\n\n\n\nMean number of sessions per user\n1.5\n\n\n\nNumber of sessions originating from…\n\n\n\n\na referral from another website\n142,158\n12%\n\n\na search engine\n417,671\n35%\n\n\nunknown sources\n590,659\n49%\n\n\nNumber of sessions lasting…\n\n\n\n\n&lt; 10s\n760,967\n63%\n\n\n&lt; 1 min\n908,516\n75%\n\n\nNumber of pages viewed within a session\n\n\n\n\n0\n1,451\n0%\n\n\n1\n676,670\n56%\n\n\n2\n223,864\n19%\n\n\n3\n81,878\n7%\n\n\n4 or more\n225,557\n19%\n\n\nNumber of sessions ending without interaction\n648,370\n54%\n\n\nNumber of sessions including views of…\n\n\n\n\nreporting guidelines\n605,570\n50%\n\n\nlanding page\n411,429\n34%\n\n\nlibrary\n83,238\n7%\n\n\ntoolkits\n15,574\n1%\n\n\nstudy design info\n15,268\n1%\n\n\n(All other categories of content were viewed in less than 1% of sessions)\n\n\n\n\n\n\n\nTable 2 Pages where visitors began their session and their bounce rates between 1st of January and 1st of July 2022. Bounces are sessions where a visitor leaves without interacting with the website at all.\n\n\n\n\n\n\n\n\n\nPage\nSessions\n% Sessions\nBounce Rate\n\n\n\n\nAll pages\n602,921\n100%\n53%\n\n\nHome page\n218,260\n36%\n45%\n\n\nSTROBE\n68,274\n11%\n56%\n\n\nPRISMA\n29,398\n5%\n60%\n\n\nReporting guidelines\n27,122\n4%\n40%\n\n\nCONSORT\n24,615\n4%\n55%\n\n\nCOREQ\n19,962\n3%\n73%\n\n\nSTARD\n19,316\n3%\n57%\n\n\nSRQR\n12,039\n2%\n65%\n\n\nCARE\n10,759\n2%\n56%\n\n\nTRIPOD\n10,342\n2%\n58%\n\n\n\n\n\nTable 3 How frequently reporting guideline pages were viewed and resources accessed between the 1st of January 2022 – 1st of July 2022. Outbound links were mainly links to publications and files were mainly checklists, but a few guidelines also have flow diagrams which were rarely downloaded. The ten most accessed guidelines are shown.\n\n\n\n\n\n\n\n\n\n\nReporting Guideline\nSessions where database page was viewed\n\nUsers that accessed an outbound link\nUsers that accessed a checklist\n\n\n\n\nSTROBE\n\n110,910\n2,502\n38,780\n\n\nPRISMA\n\n50,146\n17,240\n15,522\n\n\nCONSORT\n\n45,832\n2,465\n13,862\n\n\nCOREQ\n\n30,487\n11,152\n-\n\n\nSTARD\n\n27,301\n987\n7,803\n\n\nSRQR\n\n26,132\n10,035\n-\n\n\nCARE\n\n21,892\n8,761\n8,350\n\n\nTRIPOD\n\n15,240\n4,678\n3,960\n\n\nSPIRIT\n\n11,108\n3,985\n3,244\n\n\nSQUIRE\n\n11,061\n1,022\n3,061\n\n\n\n\n\nTable 4 Google Analytics data for equator-network.org for the year 2021.\n\n\n\n\n\n\n\n\n\nNumber\nPercent\n\n\n\n\nNumber of users\n830,134\n\n\n\nNumber of users who had not visited before\n823,087\n99%\n\n\nCountry (top 10)\n\n\n\n\nUnited States\n195,217\n24%\n\n\nUnited Kingdom\n61,292\n7%\n\n\nBrazil\n55,894\n7%\n\n\nChina\n41,628\n5%\n\n\nIndia\n41,196\n5%\n\n\nAustralia\n30,647\n4%\n\n\nCanada\n29,620\n4%\n\n\nNetherlands\n25,426\n3%\n\n\nGermany\n24,561\n3%\n\n\nSpain\n23,020\n3%\n\n\nLanguage (top 10)\n\n\n\n\nEnglish (United States)\n371,331\n45%\n\n\nEnglish (Great Britain)\n86,580\n10%\n\n\nPortuguese (Brazil)\n50,172\n6%\n\n\nChinese\n42,951\n5%\n\n\nSpanish\n36,204\n4%\n\n\nFrench\n14,708\n2%\n\n\nDutch\n14,331\n2%\n\n\nSpanish (Latin America and Caribbean)\n14,259\n2%\n\n\nJapanese\n13,381\n2%\n\n\nGerman\n12,460\n2%\n\n\nUsers whose browsers are set to English*\n498,286\n60%\n\n\nNumber of sessions that users made within the time window\n\n\n\n\n1 or more sessions\n818,512\n99%\n\n\n2 or more sessions\n176,570\n21%\n\n\n&gt; 3 sessions\n72,886\n9%\n\n\n\n* Includes 105 English locales like United States, India, Ireland etc."
  },
  {
    "objectID": "chapters/appendix/barriers.html",
    "href": "chapters/appendix/barriers.html",
    "title": "Barriers",
    "section": "",
    "text": "1: Researchers may not know what RGs are\nResearchers may have never heard the term “reporting guideline” or may misunderstand it. Researchers may more commonly use terms like “writing” or “writing up” and the word “reporting” may get interpreted as a formal task (such as reporting progress to a funder). The word “guideline” may be interpreted by some as rules (as per journal “author guidelines”) and others as recommendations. Some researchers may perceive RGs as a set of design requirements, especially if they only use checklists, which typically lack the instructions and nuances included in the full guidance.\n\nIdeas to address this barrier:\n\nDescribe reporting guidelines where they are encountered\n\n\nKeep reporting guidelines agnostic to design choices\n\n\nPromote reporting guidelines\n\n\nInstall reporting champions\n\n\n\n\n2: Researchers may not know what RGs exist\nResearchers may not be aware of which reporting guidelines exist. Most guidelines on the EQUATOR site are hardly ever accessed\n\nIdeas to address this barrier:\n\nShow and encourage citations\n\n\nAvoid confusing authors with too many reporting guidelines\n\n\nMake resources easy to discover and find\n\n\nEndorse and enforce reporting guidelines\n\n\nPromote reporting guidelines\n\n\nDescribe each reporting guideline fully\n\n\n\n\n3: Researchers may not know whether a RG applies to them\nIf the scope of a RG is undefined or unclear, then researchers won’t know whether the guidance applies to them. Researchers may not understand study designs, making it difficult for them to identify which guidance applies.\n\nIdeas to address this barrier:\n\nDescribe reporting items fully\n\n\nDescribe each reporting guideline fully\n\n\n\n\n4: Researchers may not know what RG is their best fit\nResearchers may not know when more specific guidance exists. An author’s “perfect fit” guideline may not exist, in which case they may not know know when to stop searching, and they may try to use an “imperfect fit” guideline without understanding which items are applicable.\n\nIdeas to address this barrier:\n\nAvoid confusing authors with too many reporting guidelines\n\n\nMake resources easy to discover and find\n\n\nDescribe each reporting guideline fully\n\n\n\n\n5: Researchers may not know what resources exist for a RG\nResources include the guidance itself, checklists, E&E files, templates, and web tools (e.g. PRISMA flow chart maker). Not all resources exist for each RG and researchers may be unaware of the ones that do. Many researchers may only use the checklist. Sometimes this is purposeful, but other times it may be because researchers don’t know that full guidance and examples exist.\n\nIdeas to address this barrier:\n\nMake resources easy to discover and find\n\n\n\n\n6: Researchers may not know when RGs should be used\nResearchers may not know when they should use RGs in their research workflow. Guideline developers may want researchers to use guidance as early as possible, but this is may not be obvious to researchers who may only ever receive instruction to complete a checklist as part of journal submission and may never discover the full guidance. Consequently, researchers may assume that RGs are supposed to be used by single authors as pre-submission checklists to demonstrate adherence. It may not occur to them that RGs can be used earlier, or by teams. Some researchers, having come to this realisation themselves, report wanting to be told to use reporting guidelines earlier in their research.\n\nIdeas to address this barrier:\n\nDescribe reporting guidelines where they are encountered\n\n\nCreate ways to catch authors earlier\n\n\nInstall reporting champions\n\n\nDescribe each reporting guideline fully\n\n\n\n\n7: Researchers may misunderstand\nResearchers may not understand concepts, terms or words within the guidance, or they may understand them differently to how the developers intended. Some items (or entire guidelines) might be new concepts. E.g. SQUIRE guidelines written at a time where Quality Improvement was still a new concept to many people, and some items (e.g. Context, Study of the intervention) were less familiar than others. Researchers may have nowhere to turn for help should they not understand something.\n\nIdeas to address this barrier:\n\nMake reporting guidelines easy to understand\n\n\nCreate discussion spaces\n\n\nInstall reporting champions\n\n\nProvide additional teaching\n\n\nMake updating guidelines easier\n\n\n\n\n8: Researchers may not know what benefits to expect\nResearchers may not know what benefits to expect from using a reporting guideline. These benefits may include:\n\nimproved completeness of reporting which helps readers use research and reduces research waste.\nimproved flow and less “waffle” in writing\nfacilitated discussions between collaborators, especially at the design or protocol stage\npublishing and passing peer review more efficiently\nincreased publisher acceptance rates\nefficient, confident writing\nincreased impact of manuscript, as the article is easier to search for and information within the article is easier to find.\n\n\nIdeas to address this barrier:\n\nDescribe reporting guidelines where they are encountered\n\n\nInstall reporting champions\n\n\nProvide testimonials\n\n\n\n\n9: Researchers may not know why items are important\nResearchers may not know why an item is important, or who it is important to.\n\nIdeas to address this barrier:\n\nDescribe reporting guidelines where they are encountered\n\n\nDescribe reporting items fully\n\n\nInstall reporting champions\n\n\nProvide additional teaching\n\n\n\n\n10: Researchers may not know how to do an item\nResearchers might not know how to do something (e.g., a sample size calculation)\n\nIdeas to address this barrier:\n\nCreate discussion spaces\n\n\nDescribe reporting items fully\n\n\n\n\n11: Researchers may not know how to report an item in practice\nResearchers may not understand how to report a particular item in practice\n\nIdeas to address this barrier:\n\nCreate discussion spaces\n\n\nDescribe reporting items fully\n\n\nProvide additional teaching\n\n\n\n\n12: Researchers may not know what to write when they cannot report an item\nResearchers may not know how to report an item that they did not do (deliberately or as an oversight), or an item that they are unable to report for external reasons (e.g., IP, or data was missing from primary studies).\n\nIdeas to address this barrier:\n\nDescribe reporting items fully\n\n\n\n\n13: Researchers have limited time\nGuidelines take time to find, read, understand, and apply. Sometimes they may require time and work from multiple co-authors. Researchers & guideline developers may underestimate the time required for writing, and time is often most limited at the point of submission as grant funding may have run out.\nChecklists take time to complete, and completing them with page numbers or pasted content can be annoying if future edits necessitate updating the checklist too. Checklists also generate work for editors and peer-reviewers who must cross check page numbers or pasted content with manuscript content.\n\nIdeas to address this barrier:\n\nMake resources ready-to-use\n\n\nBudget for reporting\n\n\nCreate ways to catch authors earlier\n\n\nMake information digestible\n\n\nDescribe reporting items fully\n\n\nDescribe each reporting guideline fully\n\n\nKeep guidance short\n\n\n\n\n14: Researchers may not encounter RGs early enough to act on them\nSome RG items require work that has to be done within a certain time windows such as:\n\nduring planning or designing\nbefore or during data collection\nwhen other colleagues are available\nduring the duration of a grant\n\n\nIdeas to address this barrier:\n\nCreate reporting guidance for early stages of research\n\n\nCreate ways to catch authors earlier\n\n\nCreate additional tools\n\n\nProvide additional teaching\n\n\n\n\n15: Researchers may not understand the language\nResearchers may not understand the language guidance is written in. A lot of research comes from countries where English is not the first language, as do a lot of EQUATOR website visitors. Even if a researcher speaks English as a second language, language may be an additional barrier.\n\nIdeas to address this barrier:\n\nMake reporting guidelines easy to understand\n\n\n\n\n16: Researchers may struggle to keep writing concise\n\nFollowing a guideline can result in lengthy, bloated reports which are unpleasant to read and breach journals’ word limits. Researchers may not know how to keep writing fluid and concise or where they can report an item (e.g., what section, in the text or in a table or figure, in the manuscript or in supplementary material).\n\nIdeas to address this barrier:\n\nAvoid prescribing structure\n\n\nDescribe reporting items fully\n\n\nDescribe reporting items fully\n\n\n\n\n17: Researchers may not have tools for the job at hand\nResearchers use reporting guidelines for different tasks and want tools to make that job easier. Researchers report using reporting guidelines for:\n\nPlanning research\nDesigning research\n\nResearchers report wanting items presented in the order in which decisions need to be made\nResearchers report wanting links to resources\n\nWhilst collecting data\n\nResearchers report wanting items ordered in the order they are done\nResaerchers report wanting items embedded into data collection tools\n\nDrafting manuscript\n\nResearchers report wanting templates\n\nChecking manuscripts\nDemonstrating compliance\n\nResearchers report wanting checklists embedded into submission workflows\n\nReviewing the reporting of other people’s manuscripts\nAppraising the quality of other people’s manuscripts\n\n\nIdeas to address this barrier:\n\nCreate reporting guidance for early stages of research\n\n\nCreate additional tools\n\n\n\n\n18: RGs can become outdated\nGuidelines can become out of date compared to other guidance or compared to current research standards.\n\nIdeas to address this barrier:\n\nMake updating guidelines easier\n\n\n\n\n19: Researchers may struggle to reconcile multiple sets of guidance\nResearchers must adhere to journal guidelines, multiple reporting guidelines (e.g., PRISMA + PRISMA-Abstracts + PRISMA-S) and other best practice guidelines (like NIH principles). Using multiple guidelines increases complexity and costs, and guidelines can contradict each other.\n\nIdeas to address this barrier:\n\nAvoid prescribing structure\n\n\nAvoid confusing authors with too many reporting guidelines\n\n\n\n\n20: Researchers may be asked to remove RG content\nResearchers may be asked to remove guideline content by co-researchers, editors or reviewers.\n\nIdeas to address this barrier:\n\nDescribe reporting items fully\n\n\n\n\n21: Researchers may forget to use RGs at earlier research stages\nHaving been told to complete a checklist upon journal submission, researchers may forget to use a RG earlier next time.\nNB forgetting is different to not realising that RGs can be used early.\n\nIdeas to address this barrier:\n\nCreate ways to catch authors earlier\n\n\n\n\n22: Guidance may be difficult to find\nResearcher should be able to easily find guidance and resources that they believe to exist. However:\n\nsearch functions can be hard to find or use,\nresearchers may not know which search terms to use,\nwebsites may be hard to navigate,\nguidance can be buried within articles,\nresources may not be optimised for search engines,\nand resources may not be in the same place.\n\n\nIdeas to address this barrier:\n\nMake resources easy to discover and find\n\n\nMake information digestible\n\n\n\n\n23: RGs may be difficult to access\nResearchers may be unable to access guidance published in subscription journals. Journal websites can feature broken links.\n\nIdeas to address this barrier:\n\nMake resources accessible\n\n\n\n\n24: RG resources may not be in usable formats\nResources differ in how easy or readily usable they are. For example, some checklists are published as PDF tables that cannot be filled or copied. Some guidance can be dense, unstructured text that is hard to digest or navigate; whereas some researchers will read the guidance sequentially, others may dip in and out whilst writing, and unstructured text can make information harder to find.\n\nIdeas to address this barrier:\n\nMake resources ready-to-use\n\n\n\n\n25: Researchers may feel afraid to report transparently\nResearchers may feel afraid or uncertain when trying to report something that they didn’t (or couldn’t) do.\n\nIdeas to address this barrier:\n\nKeep reporting guidelines agnostic to design choices\n\n\nUse persuasive language and design\n\n\nProvide testimonials\n\n\n\n\n26: Researchers may feel restricted if RGs prescribe design\nAdvice or assumptions about design choices narrow the scope of the guidance and can make checklists appear prescriptive. Sometimes design assumptions can be implicit. For example, in requiring authors to report the method used to assess risk of bias, PRISMA is implying that authors should have designed their review to assess risk of bias.\n\nIdeas to address this barrier:\n\nKeep reporting guidelines agnostic to design choices\n\n\n\n\n27: Researchers may feel patronized\nResearchers can feel patronized by checklists.\n\nIdeas to address this barrier:\n\nCreate discussion spaces\n\n\nUse persuasive language and design\n\n\nDescribe each reporting guideline fully\n\n\n\n\n28: Researchers may not believe stated benefits\nResearchers may not believe that using a RG will affect their acceptance rate or publication speed, that using a RG will help them write, or improve the quality of their manuscript.\n\nIdeas to address this barrier:\n\nShow and encourage citations\n\n\nCreate discussion spaces\n\n\nUse persuasive language and design\n\n\nEvidence the benefits\n\n\nMake reporting guidelines appear as a priority\n\n\nProvide testimonials\n\n\n\n\n29: Researchers may not care about the benefits of using a RG\nResearchers may understand that RGs aim to reduce poor reporting, but may not feel that poor reporting matters. Instead of hypothetical benefits or benefits to others, researchers report caring more about personal, immediate benefits like feeling confident, efficiency, and job performance.\n\nIdeas to address this barrier:\n\nCreate rewards\n\n\nDescribe reporting items fully\n\n\nMake reporting guidelines appear as a priority\n\n\nProvide testimonials\n\n\nProvide additional teaching\n\n\n\n\n30: Researchers may expect the costs to outweigh benefits\nResearchers may feel that the costs of using a RG - the time and work required and the added manuscript length - outweigh the benefits.\n\nIdeas to address this barrier:\n\nKeep reporting guidelines agnostic to design choices\n\n\nEndorse and enforce reporting guidelines\n\n\nMake information digestible\n\n\nKeep guidance short\n\n\nProvide testimonials\n\n\n\n\n31: Researchers may feel that checking reporting is someone else’s job.\nResearchers report feeling that completing a reporting checklist should be the job of the editor or peer reviewer, not the author. Editors and reviewers may also disagree about whose role it is.\n(NB. researchers, editors and reviewers could all check for reporting quality, but this research focusses only on researchers).\n\nIdeas to address this barrier:\n\nDescribe reporting guidelines where they are encountered\n\n\nUse persuasive language and design\n\n\nDescribe each reporting guideline fully\n\n\n\n\n32: Researchers may not consider writing as reporting\nResearchers may need to change their approach to writing or what they consider writing to be.Researchers differ in their writing process. Authors that follow a structured approach to writing may find it easier to incorporate RGs into their workflow. Some experienced researchers may be used to a way of working and reluctant to change, and some inexperienced researchers may be unaware of alternative writing processes.\n\nIdeas to address this barrier:\n\nBudget for reporting\n\n\nProvide additional teaching"
  },
  {
    "objectID": "chapters/11_pilot/index.html",
    "href": "chapters/11_pilot/index.html",
    "title": "Refining the intervention: qualitative pilot with authors",
    "section": "",
    "text": "Having defined intervention components and built a prototype (chapter 10) I wanted to refine the website by getting feedback from authors. Although I had included evidence from authors in my earlier work, this evidence came from secondary sources many of which had limitations. Many studies were free-form text surveys which resulted in thin description, and samples generally lacked diversity; almost all participants came from western academic institutions. To refine my website, I wanted to capture rich descriptions of experiences from authors from around the world, with different levels of experience, and different places of work. This was especially important as I hadn’t involved authors earlier in the design or development process."
  },
  {
    "objectID": "chapters/11_pilot/index.html#sampling-strategy",
    "href": "chapters/11_pilot/index.html#sampling-strategy",
    "title": "Refining the intervention: qualitative pilot with authors",
    "section": "Sampling strategy",
    "text": "Sampling strategy\n\nTo be eligible, participants had to be actively engaged in qualitative research and be able to attend an online interview conducted in English. I used purposive sampling, seeking participants that were all doing qualitative research but varied in their experience (years doing research), workplace (academia, industry, clinician etc.), geographic location, and stage of writing (drafting vs editing). I chose these dimensions as my previous work identified that different groups of people encounter different barriers. I was prepared to do additional sampling should I discover other dimensions of importance during data collection. Participants were offered £50 as remuneration for their time as a bank transfer (or Amazon voucher for UK participants).\nI used information power to guide my initial sample size (see chapter 9 for an introduction to information power). I considered my aim to be narrow and sample to be specific but with adequate variation. My intervention and analysis was based on a behaviour change framework, and I expected my methods to provide ample opportunities for deep discussion. For these reasons, I felt confident that 10 participants would provide adequate information power whilst also being manageable within the time limit of my DPhil.\nI recruited participants by a) posting adverts on Twitter which were retweeted by the EQUATOR Network and AuthorAid [1] (a network for researchers from low and middle income countries); b) emails forwarded by AuthorAid, the African Research Integrity Network [2] and 101 Health Research [3] (a publication service provider based in the Philippines); and c) a notification to authors using Penelope.ai [4], a free manuscript checking tool used by medical journals."
  },
  {
    "objectID": "chapters/11_pilot/index.html#data-collection-methods",
    "href": "chapters/11_pilot/index.html#data-collection-methods",
    "title": "Refining the intervention: qualitative pilot with authors",
    "section": "Data collection methods",
    "text": "Data collection methods\n\nDefining intervention components by their target barriers and intervention functions lends itself nicely to designing a pilot study to gather feedback. Because I knew what each component was supposed to be doing I could design an interview schedule with questions and tasks to specifically explore intervention functions.\nMy interview schedule had 4 parts:\n\n5 second test\nThink Aloud + Interview\nWriting task (at home)\nWriting review + interview"
  },
  {
    "objectID": "chapters/11_pilot/index.html#second-test",
    "href": "chapters/11_pilot/index.html#second-test",
    "title": "Refining the intervention: qualitative pilot with authors",
    "section": "5 Second Test",
    "text": "5 Second Test\nGOT TO HERE\n~For example, the purpose of the top of the home page is to communicate what reporting guidelines are and how they benefit authors. Instead of asking general questions (like “what do you think of…?”), I decided to ask more specific questions (“What do you think the website is about? How do you think it might influence your work?”). Furthermore, because the purpose was to communicate these things quickly, I decided to only give pilot participants 5s before asking these questions.~ Similarly, defining components in this way will facilitate future quantitative work to assess efficacy.\n\nDemographics questionnaire\nAfter giving informed consent, participants provided their experience, place of work, english proficiency, and country of residence using an online form.\n\n\n5 second test\nParticipants were invited to an initial interview conducted over Zoom. The lead researcher shared their screen and displayed the website homepage. Without warning, the page was closed after 5 seconds after which we used semi structured interviews to assess participants immediate understanding of and feeling towards the home page. Questions included:\n“What do you think the website is about?”\nIf the participant mentioned the term “reporting guidelines”, we asked “What do you think reporting guidelines are?” and “What tasks do you think you can use reporting guidelines for?”\n“How would you describe the design of the site?”\n“What would make you want to learn more about the website?”\n“What impact do you expect the website to have on your job as a researcher?”\n\n\nUser protocols / think aloud tasks\n\n1. Finding guidance and resources\nWe gave participants descriptions of research and asked them to identify the most relevant reporting guideline from the website. We did not tell them how to go about this - participants could use direct links on the homepage, the search bar, or could follow the “wizard” questionnaire. We did this task 3 times. The first time, participants were asked to find a guideline for animal research. This was the easiest guideline to find as the description was displayed prominently on the home page. Second, we asked participants to find guidance for reporting cohort studies. This was a little harder as it required participants to read and understand descriptions of study designs to distinguish between different, related, epidemiology guidelines. Finally, we asked participants to find what guidance they would to report (# DECIDE: what impossible task?). This was a difficult question as there is not perfect reporting guideline for this kind of article. Instead, participants had to (# FIXME: complete task).\nWe also presented tasks that involved finding tools. We asked participants to imagine they had been asked to submit a “completed checklist” by a journal, requiring them to find the guidance, then find and use the associated checklist. We asked participants what they expected “to do lists” and “templates” to be, and when they might be used.\n\n\n2. Finding information within a guideline\nWe then asked participants to find information within the #DECIDE: guideline. We asked participants how they would report #FIXME, why it is important to describe #FIXME, and what they should write if they hadn’t done #FIXME. These questions required participants to find items #FIXME respectively, and to locate content within collapsible boxes.\n\n\n\nPlus - minus test\nAt the end of the first interview we gave participants a task to complete in their own time over the coming weeks. We provided them with the methods items of the #FIXME guideline in a #DECIDE Word file. Participants were to read the methods items of the #FIXME guidelines in their own time and highlight sentences that elicited positive or negative responses and to mark them with a “+” or “-” symbol. They could add notes if they wanted to.\n\n\nWriting evaluation (Performance test)\nIf participants were actively writing an article we asked them to use the guidelines to write their methods section. If they had something already written, we asked them to complete a reporting checklist. We asked them to do this in their own time, within two weeks. Once complete, participants sent their work to JH via email who then checked their reporting against the the #FIXME guidelines, noting which items had been reported fully and which hadn’t.\n\n\nRetrospective interview\nParticipants then attended a follow up interview two weeks later where JH asked open questions to explore the reasons behind participants + and - marks, and reasons for neglecting any items in their writing sample. The +/- test aimed to pick up non-specific responses, which would include parts of the text where the participant found difficult to understand. The writing check, however, would also reveal parts of the guidance that the participant had unknowingly _mis_understood.\nFinally, we asked the participants again for their opinions on the website and guidance and how it could be improved."
  },
  {
    "objectID": "chapters/11_pilot/index.html#data-collection-instruments-and-technologies",
    "href": "chapters/11_pilot/index.html#data-collection-instruments-and-technologies",
    "title": "Refining the intervention: qualitative pilot with authors",
    "section": "Data collection instruments and technologies",
    "text": "Data collection instruments and technologies\n\nInterviews were conducted over Microsoft Teams, using its in-built video and audio recording. We created interview schedules (#REF) and piloted them amongst students in the department. The version of the website tested can be viewed at (#REF)."
  },
  {
    "objectID": "chapters/11_pilot/index.html#units-of-study",
    "href": "chapters/11_pilot/index.html#units-of-study",
    "title": "Refining the intervention: qualitative pilot with authors",
    "section": "Units of study",
    "text": "Units of study\n\nFIXME move to Results?"
  },
  {
    "objectID": "chapters/11_pilot/index.html#data-processing",
    "href": "chapters/11_pilot/index.html#data-processing",
    "title": "Refining the intervention: qualitative pilot with authors",
    "section": "Data processing",
    "text": "Data processing\n\nWe used Zoom to automatically transcribe audio recordings, and then manually double checked the transcripts and added context from the videos, interview notes, +/- annotations and writing sample. We imported transcripts into NVivo (#REF), creating cases for participants and intervention components."
  },
  {
    "objectID": "chapters/11_pilot/index.html#data-analysis",
    "href": "chapters/11_pilot/index.html#data-analysis",
    "title": "Refining the intervention: qualitative pilot with authors",
    "section": "Data analysis",
    "text": "Data analysis\n\nWe coded positive and negative experiences and grouped them by intervention component. We did this because we expected its output - negative and positive experiences grouped by intervention component - to be easier to act upon than if we grouped experiences by method."
  },
  {
    "objectID": "chapters/11_pilot/index.html#techniques-to-enhance-trustworthiness",
    "href": "chapters/11_pilot/index.html#techniques-to-enhance-trustworthiness",
    "title": "Refining the intervention: qualitative pilot with authors",
    "section": "Techniques to enhance trustworthiness",
    "text": "Techniques to enhance trustworthiness\n\n\nDouble checking of coding\nMember checking\nTriangulation? Read the paper Charlotte recommended. Perhaps it is about mixed method studies? Perhaps that means I could count errors / number of people that did a task successfully? [https://www.bmj.com/bmj/section-pdf/186156?path=/bmj/341/7783/Research_Methods_Reporting.full.pdf]"
  },
  {
    "objectID": "chapters/11_pilot/index.html#ethical-issues-pertaining-to-human-subjects",
    "href": "chapters/11_pilot/index.html#ethical-issues-pertaining-to-human-subjects",
    "title": "Refining the intervention: qualitative pilot with authors",
    "section": "Ethical issues pertaining to human subjects",
    "text": "Ethical issues pertaining to human subjects\n\nThe study was approved by the Medical Sciences Interdivisional Research Ethics Committee at Oxford University. Participants gave informed consent via a web-form.\nThe context in which users encounter the website will effect how it is perceived.\nMost visitors come from journal websites, having been instructed by an editor or JAI page to use a particular reporting guideline. Others may come to the site because a a guideline has been recommended by a friend. Very few arrive completely cold turkey.\nIn contrast to this study, where all participants were totally cold (unless they were already familiar with EQUATOR).\nSo when one participant suggested that “What are RGs” should come above the Frequently Accessed Guidelines (Or at least that the writing, checking, planning should) this may appear sensible on the surface. But given that most authors will come already with a guideline’s name in mind (even if they don’t really know what it is), this ight not be necessary.\nTherefore, need a real world evaluation.\nAll could speak English = bias\n\n\n\n\n\n1. AuthorAID - Home. \n\n\n2. ARIN home page. ARIN \n\n\n3. 101 Health Research & Statistics Quality. Efficiency. Ethics. Teamwork. \n\n\n4. Penelope.ai. Penelope.ai"
  },
  {
    "objectID": "chapters/4_survey_content/index.html",
    "href": "chapters/4_survey_content/index.html",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 2: Describing the questions asked in quantitative surveys.",
    "section": "",
    "text": "I am reworking the survey review article into a chapter.\nKnown todos:\n\nFix table 4 formatting\nMove definitions and appendicies to thesis appendix"
  },
  {
    "objectID": "chapters/4_survey_content/index.html#background",
    "href": "chapters/4_survey_content/index.html#background",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 2: Describing the questions asked in quantitative surveys.",
    "section": "Background",
    "text": "Background\nIn the previous chapter I describe my systematic search and thematic synthesis of qualitative research exploring authors’ experiences of using reporting guidelines. I identified potential influences that may affect whether an author adheres to reporting guidelines. During my search, I observed that many studies also included quantitative survey questions, and despite not considering them in my qualitative synthesis, I felt that these questions were important to investigate for two reasons. Firstly, as many of the researchers designing these surveys were themselves users or developers of reporting guidelines the questions may reflect real barriers or facilitators that they have experienced, witnessed, or are trying to avoid or achieve. Secondly, in mixed-method surveys, quantitative questions may bias responses to subsequent qualitative questions and, therefore, the findings of my thematic synthesis. For instance, qualitative questions like “Anything else?” or “Please elaborate” may lead respondents to neglect or repeat topics covered by the previous quantitative questions.\nIn this study, I describe the landscape and content of quantitative surveys that solicit information on author experience of using reporting guidelines. My aim was to identify additional possible influences that were absent from the qualitative evidence synthesis."
  },
  {
    "objectID": "chapters/4_survey_content/index.html#methods",
    "href": "chapters/4_survey_content/index.html#methods",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 2: Describing the questions asked in quantitative surveys.",
    "section": "Methods",
    "text": "Methods\nMy qualitative synthesis included a systematic search that sought to capture all survey studies investigating reporting guidelines (see (in press) for full search details). I found 22 studies that included quantitative survey questions, 14 of which also included one or more qualitative questions (see Table 2). YD translated two studies from Chinese into English[1, 2]. I imported files into NVivo, including the full surveys where available, labelled all questions with descriptive codes, creating new codes when necessary, and then inductively grouped related codes into broad categories (see Table 3)."
  },
  {
    "objectID": "chapters/4_survey_content/index.html#results",
    "href": "chapters/4_survey_content/index.html#results",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 2: Describing the questions asked in quantitative surveys.",
    "section": "Results",
    "text": "Results\n\nWhat reporting guidelines were studied?\nBetween the 22 studies 25 reporting guidelines were mentioned, most frequently PRISMA (n=6), STARD (n=6), CONSORT (n=6) and ARRIVE (n=5) (see Table 1 for abbreviation definitions). Thus, only a small proportion of the reporting guidelines indexed in EQUATOR’s database [3] have been evaluated with quantitative questions. Fourteen studies focussed on a single guideline (n=14), with others asking questions about multiple guidelines (e.g. “which reporting guidelines [participants] had known”[4]) or guidelines in general (e.g., “whether they had used reporting guidelines in their publications”[5]). Most studies included participants from the USA, Europe, and Canada and only a few studies conducted elsewhere (e.g., China and Turkey) (see Table 2).\nIn comparison, my thematic synthesis (chapter 3) identified 18 studies that collected qualitative data. These studies covered only 12 reporting guidelines and were all conducted in western countries, hence were slightly less diverse than the quantitative survey studies.\n\n\nThe focus of quantitative questions\nSurvey studies asked participants:\n\nwhether they were aware or familiar with certain reporting guidelines,\nhow often they used them and what for,\nwhether reporting guidelines had influenced their behaviour,\nwhether guidance was usable and useful,\ntheir opinions on guidance content,\ntheir reasons for using a reporting guideline,\ntheir opinions on reporting quality in the literature,\nwhether reporting guidelines were easy to find and access,\nwhose role it was to check for compliance,\nwhether the aim of the guidance was clear, and\nopinions on things explicitly named as a barrier including the length of the guidance, the language it is written in, and the time needed to use it.\nopinions on things explicitly named as a facilitator or motivator including endorsements, evidence, explanatory information, training, the behaviour of peers, and the development process of the guidance.\n\n\n\nComparing the focus of quantitative questions with themes derived from qualitative data\nThe quantitative questions included some novel influences not contained in the qualitative data (shown in bold in Table 3), such as training as a possible facilitator [6], whether authors had heard of the EQUATOR Network [4, 6], and whether transparency in guideline development is important [6]. One study asked whether language may be a barrier to using reporting guidelines for some[7]. This concern may have been missing from the qualitative studies because they were conducted in English. Both quantitative questions and the qualitative data mentioned journals enforcing reporting guidelines, but only quantitative questions asked whether funders and employers should also enforce them.\nMost ideas captured in the quantitative questions also appeared in the qualitative data. This may indicate that the quantitative questions asked were pertinent, or perhaps that they influenced participants’ responses to subsequent, qualitative questions, as mixed method surveys were included in this commentary and the qualitative synthesis.\nOverall, although the qualitative questions contained some novel themes, I found that the qualitative data contained many more ideas that were not addressed by the quantitative questions (see bold items in Table 4). These included what authors understand reporting guidelines to be, the pros and cons of itemization, ideas of how guidance could be improved, negative feelings when an item cannot be reported as desired, the pros and cons of including design advice in reporting guidance, whether optional items were understood as being optional, and frustration when the scope of a reporting guideline is too broad, narrow, or unclear.\nThe qualitative data sometimes provided context to or explanation for quantitative answers (see italicised items in Table 4). For example, many of the quantitative surveys asked participants whether they could understand the guidance. However, a quantitative answer to this question does not reveal what the participant understands, how they understand it, or whether they understand it as intended. The qualitative data contained reports of people failing to understand the wording of an item, how to report that item in practice, whether an item applies to them, whether a reporting guideline applies to them, what the intended scope of a reporting guideline is, or even what a reporting guideline is at all. One study found that although authors reported understanding an item, their writing showed that they had interpreted it differently to how the reporting guideline developers had intended[8]."
  },
  {
    "objectID": "chapters/4_survey_content/index.html#advice-for-future-studies",
    "href": "chapters/4_survey_content/index.html#advice-for-future-studies",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 2: Describing the questions asked in quantitative surveys.",
    "section": "Advice for future studies",
    "text": "Advice for future studies\nAs very few reporting guidelines have undergone any kind of user testing, I urge guideline developers to evaluate their resources to ensure researchers understand their content, aim, and applicability criteria. Advice on how to go about this could be included in an update of guidance for guideline developers, the current version of which contains little advice on how to evaluate reporting guidelines[9].\nBecause quantitative surveys can miss or mask important findings, developers seeking actionable feedback should collect qualitative data when assessing how researchers understand or feel about reporting guidelines, or what could be done to improve the guidance. As survey studies are subject to recall bias when participants are describing past behaviour or opinions, future studies could consider methods that allow researchers to document experiences in real time, like observation or think aloud tasks.\nStudies should ensure participants represent expected users in terms of academic writing experience, discipline, profession, experience (or naivety) with reporting guidelines, and language, or even focus on differential experiences of specific target groups. For example, the EQUATOR Network website gets similar levels of traffic from Asia and Europe, yet very little research into usability or barriers of reporting guidelines has included authors from Asian countries (see chapter 5). The website also sees many new visitors who abandon the site quickly, without accessing any reporting guidance. These visitors may be authors who are naïve to reporting guidelines and decide not to use one. Most of the included studies used snowball sampling or required authors to read the guidance as part of the study itself, and so don’t capture perspectives of these less-engaged authors.\nSurveys should avoid leading questions. For example, the Likert rated statements “The STARD 2015 guidelines are easy to follow” [7] and “The time required to adhere to the STARD 2015 guidelines is a barrier to using the guidelines” [7] are both subject to acquiescence bias; the tendency for participants to agree with research statements [10]. Future studies should consider using neutral questions, such as “Do you think the STARD 2015 guidelines are easy to follow?”.\nStudies used lots of different words to describe reporting guidelines, including guidelines, standards, requirements, checklist, example and elaboration, or just an acronym, e.g., CONSORT. This became a problem in studies where participants were not supplied with guidance documents as part of the study, as it was not always clear which document a participant was considering. For instance, asking participants whether PRISMA is easy to understand will not tell you whether they are talking about the PRISMA checklist, statement, or explanation and elaboration document. Future studies should be specific when asking questions and reporting results."
  },
  {
    "objectID": "chapters/4_survey_content/index.html#discussion",
    "href": "chapters/4_survey_content/index.html#discussion",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 2: Describing the questions asked in quantitative surveys.",
    "section": "Discussion",
    "text": "Discussion\nVery few reporting guidelines have been evaluated using either quantitative or qualitative methods. Reviewing the content of quantitative surveys revealed some novel influences which were absent from the qualitative data synthesised in chapter 3. Quantitative surveys often asked about awareness, usage, usability, usefulness, importance, barriers, facilitators, content, and whether reporting guidelines had led to a change in behaviour but did not address many other themes identified in my qualitative synthesis. Reporting guideline developers who want to make sure their resources are easy to use should consider using qualitative methods, which may produce richer, actionable insights.\nTwo studies asked participants whether they had heard of the EQUATOR Network, noting that it is a “a valuable resource for users and potential users of reporting guidelines” that 44% (19/43) of editors[6] and 38% of authors (38/100) [4] are aware of. Although these studies asked participants whether they were familiar with EQUATOR, authors’ experiences of using EQUATOR’s website has never been explored.\nIn the next chapter I describe EQUATOR’s website and key characteristics of its web traffic, before discussing how well it is helping authors find reporting guidance and what may be limiting its success."
  },
  {
    "objectID": "chapters/3_synthesis/index.html",
    "href": "chapters/3_synthesis/index.html",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 1: A thematic synthesis",
    "section": "",
    "text": "I’ve reworked my thematic synthesis article into a chapter.\nKnown todos:\n\nTable 4 needs formatting\nRe-add PRISMA flow diagram\nEnd of discussion section needs to be reworked so it links to next chapter.\nAppendix will come out and become a thesis appendix\nCan move acronyms and definitions into a thesis appendix"
  },
  {
    "objectID": "chapters/3_synthesis/index.html#approach-to-searching-and-data-sources",
    "href": "chapters/3_synthesis/index.html#approach-to-searching-and-data-sources",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 1: A thematic synthesis",
    "section": "Approach to searching and data sources",
    "text": "Approach to searching and data sources\nMy search strategy sought all research articles that collected qualitative data exploring researchers’ experiences of using reporting guidelines. I wanted to capture the experiences of researchers from around the world, so I included international databases in my information sources. All data sources are listed in Table 1."
  },
  {
    "objectID": "chapters/3_synthesis/index.html#inclusion-and-exclusion-criteria",
    "href": "chapters/3_synthesis/index.html#inclusion-and-exclusion-criteria",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 1: A thematic synthesis",
    "section": "Inclusion and exclusion criteria",
    "text": "Inclusion and exclusion criteria\nI included studies that reported researchers’ experiences of using reporting guidelines derived through qualitative methods. I excluded articles written before 1996, the year that the CONSORT statement was first published. Any articles not written in English, Chinese, Spanish, or Portuguese were excluded during screening.\nI included records reporting feedback from researchers as part of guideline development. I decided not to include reporting guideline development studies where this feedback came exclusively from the development group members, as I considered this context to be too different to how ordinary researchers experience reporting guidelines.\nI found many studies that used a mix of quantitative and qualitative questions. I did not consider categorical survey questions with a free text option for “other” to be qualitative, but I did include findings from free text questions that invited participants to provide context to a previous (not qualitative) question. I describe the quantitative studies and the kinds of questions they asked in an accompanying commentary (in press)."
  },
  {
    "objectID": "chapters/3_synthesis/index.html#electronic-search-strategy",
    "href": "chapters/3_synthesis/index.html#electronic-search-strategy",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 1: A thematic synthesis",
    "section": "Electronic search strategy",
    "text": "Electronic search strategy\nASK: How do I refer to people that weren’t part of my supervisory team? In an article they would be authors, but in a thesis?\nA search specialist (SK) helped develop the comprehensive search strategies, which had a component for reporting guidelines and another for qualitative methods. I constructed my reporting guidelines component from the acronyms of frequently accessed guidelines (Table 2) with generic terms for reporting guidelines to capture guidelines not named explicitly. My qualitative component came from a review of search filters[3], which recommended a sensitive qualitative filter for systematic reviews[4]. I extended the filter to include descriptive methods because I knew some of my target records were mixed method surveys. I conducted scoping searches, but my search strategies were not peer reviewed before execution. My search strategies are reported fully in the Appendix."
  },
  {
    "objectID": "chapters/3_synthesis/index.html#screening",
    "href": "chapters/3_synthesis/index.html#screening",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 1: A thematic synthesis",
    "section": "Screening",
    "text": "Screening\nYD screened Chinese records and asked LZ for second opinions when necessary. I screened all other records, of which MS double-screened a random 10% sample and differences were resolved through discussion. YD and I screened titles and abstracts to identify records that explored researchers’ experiences and then screened full texts to identify whether those articles used qualitative methods."
  },
  {
    "objectID": "chapters/3_synthesis/index.html#describing-and-appraising-records",
    "href": "chapters/3_synthesis/index.html#describing-and-appraising-records",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 1: A thematic synthesis",
    "section": "Describing and appraising records",
    "text": "Describing and appraising records\nI extracted study characteristics and used the Critical Appraisal Skills Programme Qualitative (CASP-Qual) checklist [5] to critically appraise included studies, which helped him and JdB consider the strengths and weaknesses of each study when synthesising them."
  },
  {
    "objectID": "chapters/3_synthesis/index.html#synthesis-methodology",
    "href": "chapters/3_synthesis/index.html#synthesis-methodology",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 1: A thematic synthesis",
    "section": "Synthesis methodology",
    "text": "Synthesis methodology\nI used thematic synthesis as defined by Thomas and Harden[6] because it can handle studies with “thin” descriptions, it allowed us to infer facilitators and barriers from research that may not have addressed my concern directly, and because I expected its output, grouped by themes, to be useful to guideline developers.\nI imported files into NVivo 12.0 for Mac and coded all sentences from the results section and relevant supplementary materials that reported qualitative findings. I assigned each sentence one or more descriptive codes that sought to distil the essence of what was written, creating new codes when necessary and without using a framework. I then used mind-mapping software[7] to visualise similarities and differences between codes and aggregate them inductively into descriptive themes that captured the meaning of the codes they contained. I then used my research question to infer facilitators and barriers from these descriptive themes and to understand the context in which these occur, thereby producing analytic themes. I discussed all steps with JdB, resolving conflicts through discussion when necessary."
  },
  {
    "objectID": "chapters/3_synthesis/index.html#search",
    "href": "chapters/3_synthesis/index.html#search",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 1: A thematic synthesis",
    "section": "Search",
    "text": "Search\nMy search yielded 18 articles (see Figure 1 for full search results). I and MS double-screened 10% of the non-Chinese titles and abstracts and agreed on 98.3% of them (170/173). The remaining three were resolved after discussion and consensus. All eligible records were written in English and included surveys, semi-structured interviews, focus groups, and writing tasks.\nOnly 7 of the 18 records reported where participants came from. Three mixed method survey studies [8–10] included participants from a wide range of countries but it was not possible to tell which participants completed the optional qualitative questions. Four interview studies [11–13] included participants who were almost exclusively from North America, Europe, and Australia, with one participant from Brazil.\nCritical appraisal of the studies using CASP-Qual rated the studies ranging from valuable to not very valuable; the less valuable studies had few qualitative components or minimal reporting of qualitative analysis or findings. Study characteristics are reported in Table 3."
  },
  {
    "objectID": "chapters/3_synthesis/index.html#synthesis-findings",
    "href": "chapters/3_synthesis/index.html#synthesis-findings",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 1: A thematic synthesis",
    "section": "Synthesis findings",
    "text": "Synthesis findings\nThe relationships between my codes, descriptive themes, and analytic themes are reported in Table 4. I identified the following analytic themes: 1) Researchers may not understand guidance as intended or what reporting guidelines are, even if they think they do; 2) Researchers report a variety of reasons for using reporting guidelines, and that some are more important than others; 3) Researchers report using reporting guidelines for different tasks and wanting guidance to be delivered in ways that better fit their needs; 4) Using reporting guidelines has costs which researchers may feel outweigh benefits; 5) Reporting guidelines may need to be revised and updated; 6) Researchers may not be able to report all items, which can leave them feeling uncertain or worried; 7) Awareness and accessibility may limit reporting guideline usage; 8) Reporting guidelines may be more useful to less experienced researchers, but these researchers may find them harder to use; 9) Researchers want or need design advice, but reporting guidelines may not be the right place to find it; 10) Reporting guidelines can be harder to use if their scope is too broad, too narrow, or poorly defined; 11) Researchers may have to use multiple sets of reporting guidelines, multiplying complexity and costs; 12) Researchers may use checklists but never read the full guidance.\nI identified that ‘barriers’ and ‘facilitators’ were not consistent experiences. What may be a barrier for one person might be a facilitator to others or when occurring in different context, and so I refrained from labelling my analytic themes as one or the other.\n\n1) Researchers may not understand guidance as intended or what reporting guidelines are, even if they think they do\nResearchers commonly stated that they need more information to fully understand the intention of the guideline developer. When asked about the clarity of guidance, researchers across many studies reported difficulty in understanding certain terms, concepts, or checklist items:\n\n‘outcome’: “Does it mean the domain, or does it mean the domain measure, metric, method of aggregation, and time?”[14].\n“Primary and secondary improvement related question is confusing, what does that mean?… I had a hard time with the [difference between the] improvement question and the study question.”[11]\n\nA few researchers reported ignoring an item if they could not understand it:\n\n“Only one item was identified as hard to understand by more than one respondent: ‘methods employed to ensure completeness of data’, which two participants said they left out because of difficulty in comprehending the item”[15]\n\nSome researchers reported feeling that reporting guidelines were “simply not comprehensible”[11]. Others reported that they had understood, but further investigation revealed that their interpretations could be “different from that intended by the developers”[15]. For example, Davies et al. [15] found that one SQUIRE item “was reported as used fully [in the quantitative survey], but the qualitative analysis revealed that its usage was frequently inconsistent with the intention of the developers”. One reason for this may be because different researchers may interpret the guidance in different ways depending on their prior experience, the research context, or if the guidance is ambiguous. For example, the SQUIRE developers found that the word ‘theory’ “meant different things to different people. For some, the word ‘theory’ meant ‘mechanism by which an intervention was expected to work’, for others it meant ‘lean or six sigma for example’, and for still others it meant ‘logic model’”[11].\nEven when researchers reported understanding what an item meant, they may not have understood why it is important or who it is important to, leading them to remark that an item “seems unnecessary“[16]. Few researchers referenced the needs of evidence synthesizers or patients as consumers of research, but more reported considering whether an item would be useful to other researchers, editors, and reviewers:\n\n“the information provided does not matter as the reviewers do not know what to do with it’’[10]\n\nIn addition to not understanding guidance or who it is important to, many researchers expressed difficulty understanding whether an item was applicable to their work. Some reporting guidelines specify that not all items are compulsory or that some items may only apply to a subset of research articles. Researchers highlighted that this may not be obvious, especially if this nuance is buried in a long elaboration document. Some researchers therefore reported uncertainty over which items applied to them:\n\n“Authors asked for clarification of which items were always required and which were nonessential”[17]\n“Not always clear what was relevant to their study”[16]\n“He had realised with experience and re-reading the Guidelines that SQUIRE did not require him to include every item in the manuscript”.[11]\n\nThis uncertainty may extend to the entire reporting guideline if researchers don’t know when to use one over another. One researcher declared that “PRISMA guidelines can also be used rather than the MOOSE”[18], when the two are primarily for reviews of intervention studies and observational studies respectively. Sometimes there may not be a perfect reporting guideline for a given study, as one researcher commented after using ARRIVE (which focusses on experimental research involving laboratory animals):\n\n“Our report was an animal based cadaveric study looking at accuracy of drill guides. I were unsure which category it should fall under.”[16]\n\nEven if a researcher understands the guidance, why it is important, and why it applies to them, they may not understand how to report it or “how much detail to report”[16]. Some researchers “used examples [included in the guidance] to understand what should be reported” because they “demonstrate what is meant in practice”[16].\nAt a more fundamental level, researchers varied in their understanding of what reporting guidelines are. Often researchers would talk about reporting guidelines as if they were design guidelines, e.g., describing STROBE as “woefully deﬁcient in encouraging…use of appropriate data analytic approaches”[10]. This suggested that the researcher had not noticed the stipulation that “these recommendations are not prescriptions for designing or conducting studies” included in STROBE’s explanation and elaboration document[19]. Other researchers wrote about STARD as is if the guidance was to be used when collecting imaging data:\n\n“Two comments suggested that reporting quality may be impacted by the physical environment in which […] data are collected. These comments may indicate an incomplete understanding of reporting guidelines which pertain to reporting results during manuscript writing, not the process of imaging acquisition itself.”[20]\n\n\n\n2) Researchers report a variety of reasons for using reporting guidelines, and that some are more important than others\nSome researchers listed personal benefits to using reporting guidelines. Some described reporting guidelines as a “training tool”[21] for personal development, noting that guidance helps “develop a strong foundation and habits”[10]. Some talked about how guidance made them feel: “As a junior scientist it gives me conﬁdence to request the reporting of a certain piece of information”[10]. Others said that reporting guidelines are “a helpful reminder”[21] and that going through the checklist “improved their manuscripts”[22]. Some saw value in fostering a “transparent reporting process”[8] and for making sure your “project is written up as rigorously”[11].\nA couple of researchers noted altruistic benefits, reporting that widespread reporting guideline adherence “helps in standardizing how research is reported”[10] and calling “for more scientific reports to be published, preferably using a template or guideline to make them comparable”[23].\nIn the absence of anticipated benefits, some researchers said that they use reporting guidelines simply because “it was what was implicitly expected of them to do”[12], and that these expectations came from journals and their peers. Some used “tools promoted by journals, which often promised to ease the publishing process”[23] but others wrote that they found this to be an empty promise:\n\n‘’I have never had (nor have I heard of) an editor or reviewer pushing back on a claim that all STROBE criteria were met. Therefore, when a STROBE checklist is required for manuscript submission, it seems to turn into a[n] exercise in additional administrative busywork without really improving the research.’’[10]\n\nA few researchers reported being more likely to comply with journal requirements if they thought the journal was likely to enforce them: “Does the journal only suggest or actually require submission of a reporting guideline checklist?”[12]. Some said they were more likely to comply if “it was a high impact factor journal and I thought that I would only get one crack at it”[12].\nA few researchers compared different motivations for using reporting guidance, noting that personal, guaranteed, and immediate benefits were more motivating than hypothetical benefits or benefits to others:\n\n“I suppose you are looking for short-term gain, short-term benefits as a writer of a report”[23]\n“it can be difﬁcult to put the energy into using STROBE (or any other) one a priori since ultimately, it depends on the journal submitted to and accepted to”[10]\n“All the researchers wanted more homogenous reporting but emphasized that:”As an individual reporter, one is prone to choose the easiest and most accessible one.”“[23]\n\n\n\n3) Researchers report using reporting guidelines for different tasks and wanting guidance to be delivered in ways that better fit their needs\nAlthough reporting guidelines were designed to help researchers draft and check their manuscripts, many researchers mentioned using reporting guidelines for other tasks including designing research, planning, peer-reviewing, and educating. A few researchers suggested different ways that the guidance could be delivered to make their task easier. For example, some “thought [the] order of items should reflect [the] order considered when designing the experiment”[16]. Others wanted “a manuscript template”[8] to make writing easier. Some suggested that “online form[s]”[9] or software to “mark in the text what corresponds to each item in the list”[18] would make it easier to complete a reporting checklist as part of journal submission.\n\n\n4) Using reporting guidelines has costs which researchers may feel outweigh benefits\nResearchers noted that some items require extra work, either to collect the necessary information or just to think about and report, and that sometimes this workload felt overly burdensome:\n\n“If I put the onus on everybody out there who’s trying to improve care to deal with that sophisticated question […], I just think I are putting a barrier in place that is going to be a mountain”[15]\n\nThis work requires time, and “the length of time it would take to consider the items”[21] was cited by many researchers as a cost, with some asking themselves whether “sufﬁcient time [was] available to comply with [the] reporting guideline”[12].\nResearchers noted that a reporting guideline’s “length and content is a key factor inﬂuencing the time needed to complete it.”[10]. Some found checklists to be “very complete, but to follow every single point is overwhelming”[22]. As a solution, many wanted to “simplify” or “shorten the checklists”[18]. A few researchers wanted a “hierarchy” to know which items were most “important to include”[15]. Another suggested that checklists presented as online forms could include “logic for irrelevant [items]”[9] so that the users are presented with only items that apply to them.\nComplexity was sometimes mentioned alongside time: “As the research often was performed out of work hours, the required time and complexity of the guidelines or templates may have played a crucial role [in deciding whether to use a reporting guideline]”[23]. Researchers had conflicting opinions about whether itemization reduces perceived complexity. Proponents noted that the “Checklist is a very helpful summary of sometimes confusing guidelines”[8] and that itemization made guidance “easier to follow” and “more approachable”[14]. But a few said that presenting guidance in small pieces made it difficult to “get the whole picture of what you are supposed to be doing”[11] and that itemization makes “the checklist appear more daunting for users:”If you make the checklist too long people will see it as too complicated and then won’t use it”“[14].\nAnother concern cited by many researchers was that following a reporting guideline can result in long reports:\n\n“I use SQUIRE a lot for planning—I complete the sections up through the methods at the time I design the study…[but] SQUIRE creates sort of long reports if followed exactly.”[11]\n“the document you create if you use SQUIRE exactly as written is unintelligible”[11]\n“this [item] would require another paper”[22]\n\nThis problem was exacerbated by journal word limits:\n\n“I believe it is a useful instrument but it is unrealistic to assume that every single suggestion can be detailed in a 6000-words manuscript.”[22]\n“two remarked that word limitations has necessitated removal of many items”[17]\n\nAlthough a handful of researchers noted that “the relaxation of word limits”[12] would help, many researchers objected to long articles because they were bloated, harder to read, or simply “unintelligible”[11] and requested strategies to “enhance readability” regardless of journal policies. Some wondered where they could place this information besides the article body, such as “in an appendix”, an “online supplement or repository”, or a figure[14]. Some researchers preferred to report information in the checklist instead of the article body because of “space restrictions, because [it was] a minor component of the study, because they considered the information to be obvious, or because they were unsure of how to incorporate it in the manuscript.”[16]. Some used this strategy to report items that had “not been used or observed during the study, for example that no inclusion or exclusion criteria had been set, no data had been excluded, randomisation and blinding had not been used…”[16] although it was not clear whether this was motivated by a desire for a concise article or a concern about highlighting potential weaknesses.\nFaced with the costs of time, work and article length, some researchers explicitly weighed perceived benefits against costs and disagreed about the balance:\n\n“The manuscript has improved. However, I felt that the amount of effort was considerably greater than the degree of improvement.”[22]\n“it also adds to the time required to put together a manuscript, and I am not sure how much it improves the chances of a manuscript being published”[10]\n“it does increase the quality of the articles, it is clearly worth the time”[10]\n\nThe balance of costs versus benefits may be most favourable when guidance is used early in the research workflow. Researchers who used reporting guidelines earlier in their workflow (e.g., for planning research or drafting) used language that implied it was something they did regularly (e.g., “I use SQUIRE a lot for planning”[11]). Some reported that they had come to this habit by their own initiative and that reporting guideline developers should “encourage people to use the criteria early in the writing process (I have, which probably is why I only changed one thing [at the point of submission])”[18]. One researcher suggested that “policy that focuses on a front end approach would be helpful”[10], noting that “To fully apply the criteria, I would need to systematically apply the STROBE criteria on the front end design of a project, grant, etc. rather than at the time of writing a project”[10].\nConversely, many authors who completed a checklist during manuscript submission, very late in their in workflow, emphasised the costs, using words like “arduous”[10] and expressing negative opinions of this process (see Researchers may use the checklist but never read the full guidance). This may be because researchers lack the motivation, time, or ability to edit their manuscripts at this point.\n\n\n5) Reporting guidelines may need to be revised and updated for different reasons\nResearchers in most studies had opinions on how guidance could be improved through clarifying, reorganising, splitting, merging, adding, or deleting items, and sometimes these views fed into the revision of reporting guidelines[14, 15]. This feedback may be useful for reporting guideline developers. Even if a reporting guideline was considered perfect at one point in time, researchers noted that guidance must be kept up to date in response to changes in the field and broader scientific ecosystem:\n\n“The evolution of the healthcare improvement scholarly literature in the intervening years since the publication of the SQUIRE Guidelines has led to the development of concepts that were not fully anticipated at the time of initial release”[11].\n\nUpdates to one reporting guideline may necessitate the update of another. For instance, as PRISMA was being updated, a few researchers “supported referring to PRISMA for Abstracts, but suggested it also needs updating” to reflect updates being made to PRISMA[14].\n\n\n6) Researchers may not be able to report all items, which can leave them feeling uncertain or worried\nSome researchers described being unable to report items because of external factors, including intellectual property or data rules, disagreement between co-authors, or because “peer reviewers or editors had suggested editing out much of their [reporting guideline]-specific text”[17]. Others reported feeling unable to report an item because they did not do it, whether on purpose, due to an oversight, or because requirements had changed since the study began:\n\n“[This item was] not part of the study objectives”[22]\n“This [item] is a good idea, but I did not do this.”[22]\n“The RCT was initiated before trial registration became customary in Norway, and therefore does not have a Trial ID number.”[22]\n\nThis left some researchers fearing that “an ‘’incomplete’’ checklist [gave] the impression that their study is ‘’less than ’perfect’.’’[10]. Some expressed concern that strict wording that assumed something was done may”force people to lie/mislead by asking a question they cannot answer”[14] and suggested that guidance should instead use more agnostic language and specify what to do if an item were not addressed, such as “If no publicly accessible protocol is available, please state this”[14].\n\n\n7) Awareness and accessibility may limit reporting guideline usage\nResearchers may not know what guidance exists and may be more likely to use whatever is most accessible and discoverable:\n\n“Several of the researchers did not have extensive knowledge about the different reporting tools, so the accessibility of the guideline or template was often a decisive factor.”[23]\n\nOne researcher wrote that “poor dissemination strategy by authors of reporting guidelines had inhibited uptake”[12], and others recognised that reporting guidelines could be “better highlighted”[8] by journals or advertised on “social media platforms”[18].\n\n\n8) Reporting guidelines may be more useful to less experienced researchers, but these researchers may find them harder to use\nSome researchers reported that they didn’t need the guidance as they were experienced enough to know what they were doing:\n\n“One of the most prevalent themes was the expression of self-assuredness. ’‘[I] follow the STROBE guidelines in my reporting reasonably well without actually referring to them or using a checklist’’ (group 3, ID1) and ’‘[I] already apply the STROBE recommendations despite not having heard of it until today’’”[10]\n\nSometimes this was accompanied by an acknowledgement that reporting guidance may be more beneficial to less experienced researchers:\n\n“Despite experienced researchers generally not seeing a beneﬁt to personally using STROBE, there were strong feelings that it is valuable to early-career researchers”[10]\n“Helpful at beginning of career, but not at later stage”[8]\n“this exercise might be good for college students but is insulting for professionals”[22]\n\nHowever, less experienced researchers often reported finding “reporting guidelines being difficult to use initially”[12], or that reporting guidelines became easier with experience in medical writing in general, and with experience in using other reporting guidelines. For instance, “Participants with less experience in scholarly medical writing found the SQUIRE Guidelines harder”[11].\n\n\n9) Researchers want or need design advice, but reporting guidelines may not be the right place\nMany researchers reported wanting advice on design choices but disagreed on where that design guidance should go. Some researchers suggested referring researchers to other design resources through hyperlinks or citations. Others explicitly wanted design guidance to be written into reporting guidelines so that others would read it. Some went as far as calling for reporting guidelines to express an opinion and encourage one technique over another. One researcher objected to a “neutral tone”[14] in a reporting guideline that may give the impression that a design choice (that they disapproved of) was reasonable practice.\nHowever, other researchers objected to reporting guidelines that were opinionated about design choices. One user described STROBE as a “procedural straightjacket”[10], suggesting that it dictates how studies should be conducted. Users who encounter the guidance late in writing may be unable to act on any design recommendations and consequently may feel fearful of reporting transparently if their design choices deviate from what the guideline recommends as best practice (see Researchers may not be able to report all items, which can leave them feeling uncertain or worried).\nPerhaps with these concerns in mind, one wrote that “I need to make sure that the language around this elaboration gives [researchers] some flexibility”[14], with another noting that “I am OK with the idea of emphasizing the value of [this design choice], but I cannot mandate it”[14].\n\n\n10) Reporting guidelines can be harder to use if their scope is too broad, too narrow, or poorly defined\nReporting guideline developers may narrow the scope of their guidance by limiting it to certain design choices or research contexts. This frustrated some researchers, who noted that narrow “checklists cannot fit all types of research”[8] and “cautioned that ‘’that balance between freedom and structure is important to consider’’ […] and that it is ‘’important to recognise that each study/analysis is unique and doesn’t always ﬁt with the recommendations’’.[10]\nThis narrowing of scope may have been a conscious decision, as requested by this researcher giving feedback on a proposed update of the PRISMA guideline: “I need an agreement on whether PRISMA is to be only for intervention studies (as implied by the proposed modification above) or more general.”[14] However, scope may not always be clearly communicated: another PRISMA user “opined that”the assessment of risk of bias, statement of risk ratio and explaining additional analyses depend on the study design … [For] a systematic review of cross-sectional surveys or a meta-synthesis I do not need this information”“[24], suggesting they were unaware of PRISMA’s focus on interventional studies or that MOOSE and ENTREQ would be more appropriate for these kinds of studies (see previous themes for further discussion of awareness and understanding the applicability of reporting guidelines).\nResearchers noted that scope could be made broader by removing items or, more commonly, by extending items with more options and examples:\n\n“omit”(benefits or harms)” from the checklist item to be more inclusive of reviews that do not examine effects of interventions”[14]\n“If the new PRISMA will more explicitly embrace topics other than interventions (which I think it should), then some additional examples could be added to the parenthesis (e.g. sensitivity and specificity, disease prevalence, regression coefficient)”[14]\n\nHowever, extending guidance with options can make the guidance appear longer and means researchers must work out which parts apply to them.\n\n\n11) Researchers may have to use multiple sets of reporting guidelines, multiplying complexity and costs\nThere are now over 500 reporting guidelines indexed on the EQUATOR Network website, with more added each year as reporting guideline developers seek to cover more and more use cases. Researchers may be expected to use one reporting guideline instead of another. Other times they may be expected to use a second or third reporting guideline in addition to the original one. Some researchers “pointed out that these extensions have created needless complexity and ‘’additional confusion in reporting of observational studies’’ […] and that the ‘’number of extensions has become excessive, especially given that multiple extensions may apply to a single study’’”[10].\nOne researcher wrote: “it would be good to have better connection between different checklists (perhaps using digital linking, decision-trees, etc.)”[14]. Some showed concern that hyperlinks will go unused and so developers should “incorporate all relevant details in the […] checklist and elaboration (in case authors don’t read the extension)”[14]. When writing about PRISMA, one researcher noted that “it would be wise to limit the number of additional documents to look up. This is only item 7, and I have already been referred to PRISMA for Abstracts and PRISMA for Searches. As a systematic review author, reviewer, or editor, I would be unlikely to go to several sources for reporting guidance”[14].\nA few researchers wrote that related reporting guidelines should be mutually updated to keep in sync with each other before linking or embedding them. Researchers wanted the instruction, terminology, and structure of different sets of reporting guidelines to be coherent, suggesting, for example, that the updated PRISMA should be structured to be “in line with PRISMA-P”[14].\nIn addition to reconciling multiple reporting guidelines, researchers must also comply with journal, funder, and other scientific guidelines and expressed frustration when instructions contradicted each other. For example, some reporting guidelines specify subheadings for abstracts and one researcher pointed out that a “major issue is that journals wildly differ in requirements/what is allowed in abstracts”[14].\n\n\n12) Researchers may use checklists but never read the full guidance\nReporting guidelines typically consist of the guidance itself and a checklist that serves as a summary of the guidance and a tool to demonstrate compliance. Sometimes the document containing the full guidance is called the Explanation and Elaboration (or E&E for short). When talking about a reporting guideline, it was often unclear whether the researcher was talking about the checklist or the E&E.\nSome researchers implied that their only experience with reporting guidelines was completing a checklist as part of submission. I noticed that many negative statements were directed specifically at this process, describing checklists as “painful”[8], “pedantic”, “annoying”[10], or a “stupid exercise”[22].\nOne study explored researchers’ use of checklists and E&E documents, noting that “Participants used the guidelines and the E&E in different ways. Some did not read the E&E and used only the checklist, others read the E&E first and then used the checklist and a further group used the checklist and referred to the E&E for help with specific items.”[16]. One researcher even went as far as to say that the “E&E appeared to be redundant”[16].\nIf some researchers only use checklists, which typically lack any nuance included in the E&E, this may explain why some described reporting guidance as inflexible and prescriptive, warning that “Blind checklists are not relevant to most work”[18] or that “Authors may ‘’fear the ’Checklist Manifesto’ becoming a rigid bureaucracy, and also becoming contrived’’”[10]."
  },
  {
    "objectID": "chapters/3_synthesis/index.html#limitations",
    "href": "chapters/3_synthesis/index.html#limitations",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 1: A thematic synthesis",
    "section": "Limitations",
    "text": "Limitations\nI were limited by the availability of literature and the relative thinness of some studies’ qualitative analysis. Most studies relied on participants recalling what they had done or thought in the past, and so may be subject to recall bias. Future studies could consider using ‘in the moment’ methods like think aloud tasks.\nSurveys could be subject to question order bias. Often the qualitative question appeared at the end of a survey, and so participants’ responses could have been influenced by the proceeding quantitative questions. In my accompanying commentary (in press) I describe how most of the concepts covered by the quantitative survey questions also appeared in the qualitative data and some of these quantitative questions were phrased leadingly. However, the qualitative data contained many additional themes that did not appear in the quantitative questions.\nI tried to capture the experience of a diverse range of researchers, but most participants of the reviewed studies were from western countries. My Chinese database searches yielded no relevant studies. Likewise, I found no studies on this topic published in Spanish or Portuguese. Around a quarter of visitors to the EQUATOR Network’s website have their browser set to a language other than English (in press), and it is likely that researchers who are not native English speakers face additional challenges not covered here. As all of the qualitative research I found was conducted in English, it is unsurprising that language barriers did not appear as a theme, despite being identified as a potential issue in quantitative surveys[20].\nI considered including grey literature, commentaries, and opinion pieces. These may have contributed themes to my analysis but finding these pieces (many of which may have been on private blog posts not indexed by search tools), and the extra work of synthesising primary and secondary order constructs was not feasible. I also considered synthesising quantitative survey data, most of which collected ordinal or categorical data. I decided that synthesising these quantitatively would not add value to my analysis. However, I have categorised the kinds of quantitative questions asked and compared them with the themes identified here in an accompanying commentary (in press).\nI did not distinguish between different guidelines and expect that the themes I found may apply to reporting guidelines to different degrees. I also expected code frequency to be biased by the questions asked in each study. I therefore decided not to prioritise themes by importance or frequency.\nTODO: rework the end of discussion to link to next chapter"
  },
  {
    "objectID": "chapters/3_synthesis/index.html#implications-for-future-research",
    "href": "chapters/3_synthesis/index.html#implications-for-future-research",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 1: A thematic synthesis",
    "section": "Implications for future research",
    "text": "Implications for future research\nAddressing the issues identified here may make reporting guidelines easier to use and increase their impact, thereby improving the quality of published research. Although a lot of consideration has been given to how reporting guidelines should be developed, the way in which that guidance is evaluated and disseminated also deserves to be studied and optimised. The results of this study will inform my future work to improve the dissemination of reporting guidelines."
  },
  {
    "objectID": "chapters/3_synthesis/index.html#conclusions",
    "href": "chapters/3_synthesis/index.html#conclusions",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 1: A thematic synthesis",
    "section": "Conclusions",
    "text": "Conclusions\nResearchers encounter many barriers and facilitators when using reporting guidelines. Future research could use my findings to improve how reporting guidelines are developed, evaluated, and disseminated. Reporting guideline developers should consider using qualitative methods when piloting and refining their resources."
  },
  {
    "objectID": "chapters/3_synthesis/index.html#app-search-strategies",
    "href": "chapters/3_synthesis/index.html#app-search-strategies",
    "title": "What facilitators and barriers might researchers encounter when using reporting guidelines? Part 1: A thematic synthesis",
    "section": "Search strategies",
    "text": "Search strategies\nI did not seek any external peer review of my search. I did not record the database versions at the time of searching due to an oversight. I performed forward and backwards citation searching but found no additional records. I did not set up email alerts.\n\nOvid search strategy\nDatabases: Medline, Embase, AMED, PsycINFO.\nSearch date: 08/12/2021\nI used a federated search. The kw field does not exist in PsycINFO or AMED and so was ignored by these databases. The tw field does not exist in AMED either and was mapped to af instead.\n\n((reporting or writ$ or author$) adj2 (checklist$ or statement$ or guid$ or template$ or standard$ or recommendation$)).ti,kw.\n((consort$ or strobe$ or stard$ or prisma$ or moose$ or squire$ or arrive$ or remark$ or tripod$ or cheers$ or spirit$ or srqr$ or coreq$) adj3 (guid$ or statement$ or checklist$)).ti,kw.\n(experience$ or interview$ or survey$ or questionnaire$ or \"focus group$\" or facilitat$ or barrier$).af.\nqualitative.tw.\n1 or 2\n3 or 4\n5 and 6\n\nPlatform-specific filter applied: 1996 – current year\n\n\nGlobal Index Medicus & SciELO search strategy\nDatabases: Latin American and Caribbean Health Sciences Literature, African Index Medicus, Western Pacific Region Index Medicus, Index Medicus for South-East Asia Region, and Index Medicus for the Eastern Mediterranean Region, searched using Global Index Medicus (https://www.globalindexmedicus.net/); Scientific Electronic Library Online (https://scielo.org/en/).\nSearch date: 08/12/2021\nti:(((reporting OR writ* OR author*) AND (checklist* OR statement* OR guid* OR template* OR standard* OR recommendation* OR experience* OR interview* OR survey* OR questionnaire* OR \"focus group*\" OR facilitat* OR barrier* OR qualitative*)))\nPlatform-specific filter applied: 1996 – current year\n\n\nChinese Biomedical Literature Database\nDatabase URL: https://www.imicams.ac.cn/\nSearch Date: 25/10/2021\n1. 报告 OR 撰写 OR 作者\n2. 清单 OR 声明 OR 指导 OR 规范 OR 指南 OR 共识 OR 模板 OR 标准 OR 推荐意见\n3. CONSORT OR PRISMA OR STROBE OR SPIRIT OR STARD OR SRQR OR ARRIVE OR SQUIRE OR CHEERS OR TRIPOD OR COREQ\n4. 经历 OR 体验 OR 访谈OR 调查 OR 问卷调查 OR 焦点小组 OR 焦点群众\n5. 促进 OR 阻碍\n6. 质性研究 OR 定性研究\n7. (#2) AND (#1)\n8. (#7) OR (#3)\n9. (#6) OR (#5) OR (#4)\n10. (#9) AND (#8)\n11. ((#9) AND (#8)) AND (\"循证文献\"[文献类型] OR \"临床试验\"[文献类型])\n\n\nChina National Knowledge Infrastructure\nDatabase URL: https://www.cnki.net/\nSearch Date: 25/10/2021\n( ( ( TI = '报告' OR TI = '撰写' OR TI = '作者') AND (TI = '清单' OR TI = '声明' OR TI = '指导' OR TI = '规范' OR TI = '指南' OR TI = '共识' OR TI = '模板' OR TI = '标准' OR TI = '推荐意见' ) ) OR ( TI = 'CONSORT' OR TI = 'STROBE' OR TI = 'PRISMA' OR TI = 'SPIRIT' OR TI = 'STARD' OR TI = 'SRQR' OR TI = 'ARRIVE' OR TI = 'SQUIRE' OR TI = 'CHEERS' OR TI = 'TRIPOD' OR TI = 'COREQ' ) ) AND (TI = '经历' OR TI = '体验' OR TI = '访谈' OR TI = '调查' OR TI = '问卷调查' OR TI = '焦点群众' OR TI = '焦点小组' OR TI = '促进' OR TI = '阻碍' OR TI = '质性研究' OR TI = '定性研究' )\n\n\nWanfang Data\nDatabase URL: http://www.wanfangdata.com/\n(((题名或关键词:(报告 or 撰写 or 作者)) and (题名或关键词:(清单 or 声明 or 指导 or 规范 or 指南 or 共识 or 模板 or 标准 or 推荐意见))) or (题名或关键词:(CONSORT or STROBE or STARD or PRISMA or MOOSE or SQUIRE or ARRIVE or REMARK or TRIPOD or CHEERS or SPIRIT or SRQR or COREQ))) and (题名或关键词:(经历 or 体验 or 访谈 or 访问 or 采访 or 调查 or 问卷调查 or 焦点小组 or 焦点群众 or 促进 or 阻碍 or 质性研究 or 定性研究))\n\n\nVIP Chinese Medical Journal Database\nDatabase URL: http://www.cqvip.com/\n(((M=(报告 OR 撰写 OR 作者)) AND (M=(清单 OR 声明 OR 指导 OR 规范 OR 指南 OR 共识 OR 模板 OR 标准 OR 推荐意见))) OR (M=(CONSORT OR PRISMA OR STROBE OR SPIRIT OR STARD OR SRQR OR ARRIVE OR SQUIRE OR CHEERS OR TRIPOD OR COREQ))) AND (M=(体验 OR 访谈 OR 调查 OR 问卷调查 OR 焦点群众 OR 焦点小组 OR 质性研究 OR 定性研究))\n\n\nOSF\nURL: https://osf.io/\nSearch Date: 15/12/2021\ntitle:(((reporting OR writ* OR author*) AND (checklist* OR statement* OR guid* OR template* OR standard* OR recommendation* OR experience* OR interview* OR survey* OR questionnaire* OR \"focus group*\" OR facilitat* OR barrier* OR qualitative*)))\n\n\nMethods in Research on Research\nURL: http://miror-ejd.eu/publications/\nSearch Date: 14/12/2021\nI manually searched the list of publications.\n\n\n\n\n1. Rethlefsen ML, Kirtley S, Waffenschmidt S, et al (2021) PRISMA-S: An extension to the PRISMA statement for reporting literature searches in systematic reviews. Systematic Reviews 10:39\n\n\n2. Tong A, Flemming K, McInnes E, Oliver S, Craig J (2012) Enhancing transparency in reporting the synthesis of qualitative research: ENTREQ. BMC Medical Research Methodology 12:181\n\n\n3. Rosumeck S, Wagner M, Wallraf S, Euler U (2020) A validation study revealed differences in design and performance of search filters for qualitative research in PsycINFO and CINAHL. J Clin Epidemiol 128:101–108\n\n\n4. Rogers M, Bethel A, Abbott R (2018) Locating qualitative studies in dementia on MEDLINE, EMBASE, CINAHL, and PsycINFO: A comparison of search strategies. Research Synthesis Methods 9:579–586\n\n\n5. Programme CAS (2018) CASP Qualitative Checklist. CASP - Critical Appraisal Skills Programme \n\n\n6. Thomas J, Harden A (2008) Methods for the thematic synthesis of qualitative research in systematic reviews. BMC Medical Research Methodology 8:45\n\n\n7. Bierner M (2022) Markdown Preview Mermaid Support. \n\n\n8. Dewey M, Levine D, Bossuyt PM, Kressel HY (2019) Impact and perceived value of journal reporting guidelines among Radiology authors and reviewers. European Radiology 29:3986–3995\n\n\n9. Macleod M, Collings AM, Graf C, Kiermer V, Mellor D, Swaminathan S, Sweet D, Vinson V (2021) The MDAR (Materials Design Analysis Reporting) Framework for transparent reporting in the life sciences. Proceedings of the National Academy of Sciences 118:e2103238118\n\n\n10. Sharp MK, Glonti K, Hren D (2020) Online survey about the STROBE statement highlighted diverging views about its content, purpose, and value. Journal of clinical epidemiology 123:100–106\n\n\n11. Davies L, Batalden P, Davidoff F, Stevens D, Ogrinc G (2015) The SQUIRE Guidelines: An evaluation from the field, 5years post release. BMJ quality & safety 24:769–775\n\n\n12. Fuller T, Pearson M, Peters J, Anderson R (2015) What affects authors’ and editors’ use of reporting guidelines? Findings from an online survey and qualitative interviews. PLoS ONE 10:e0121585\n\n\n13. Korevaar DA, Cohen JF, Reitsma JB, et al (2016) Updating standards for reporting diagnostic accuracy: The development of STARD 2015. Research integrity and peer review 1:7\n\n\n14. Page MJ, McKenzie JE, Bossuyt PM, Boutron I, Hoffmann TC, Mulrow CD, Shamseer L, Tetzlaff JM, Moher D (2021) Updating guidance for reporting systematic reviews: Development of the PRISMA 2020 statement. Journal of Clinical Epidemiology 134:103–112\n\n\n15. Davies L, Donnelly KZ, Goodman DJ, Ogrinc G (2016) Findings from a novel approach to publication guideline revision: User road testing of a draft version of SQUIRE 2.0. BMJ quality & safety 25:265–272\n\n\n16. Sert NP du, Hurst V, Ahluwalia A, et al (2020) The ARRIVE guidelines 2.0: Updated guidelines for reporting animal research. PLOS Biology 18:e3000410\n\n\n17. Prady SL, MacPherson H (2007) Assessing the utility of the standards for reporting trials of acupuncture (STRICTA): A survey of authors. The Journal of Alternative and Complementary Medicine 13:939–943\n\n\n18. Struthers C, Harwood J, de Beyer JA, Dhiman P, Logullo P, Schlüssel M (2021) GoodReports: Developing a website to help health researchers find and use reporting guidelines. BMC medical research methodology 21:217\n\n\n19. von Elm E, Altman DG, Egger M, Pocock SJ, Gøtzsche PC, Vandenbroucke JP (2007) Strengthening the reporting of observational studies in epidemiology (STROBE) statement: Guidelines for reporting observational studies. BMJ : British Medical Journal 335:806–808\n\n\n20. Prager R, Gagnon L, Bowdridge J, Unni RR, McGrath TA, Cobey K, Bossuyt PM, McInnes MDF (2021) Barriers to reporting guideline adherence in point-of-care ultrasound research: A cross-sectional survey of authors and journal editors. BMJ Evidence-Based Medicine bmjebm-2020-111604\n\n\n21. Burford BJ, Welch V, Waters E, Tugwell P, Moher D, O’Neill J, Koehlmoos T, Petticrew M (2013) Testing the PRISMA-Equity 2012 reporting guideline: The perspectives of systematic review authors. PloS one 8:e75122\n\n\n22. Eysenbach G. (2013) CONSORT-EHEALTH: Implementation of a checklist for authors and editors to improve reporting of web-based and mobile randomized controlled trials. Studies in health technology and informatics 192:657–661\n\n\n23. Svensøy JN, Nilsson H, Rimstad R (2021) A qualitative study on researchers’ experiences after publishing scientific reports on major incidents, mass-casualty incidents, and disasters. Prehospital and Disaster Medicine 36:536–542\n\n\n24. Tam WWS, Tang A, Woo B, Goh SYS (2019) Perception of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement of authors publishing reviews in nursing journals: A cross-sectional online survey. BMJ Open 9:e026271\n\n\n25. Moher D, Weeks L, Ocampo M, et al (2011) Describing reporting guidelines for health research: A systematic review. Journal of Clinical Epidemiology 64:718–742\n\n\n26. Moher D, Schulz KF, Simera I, Altman DG (2010) Guidance for developers of health research reporting guidelines. PLOS Medicine 7:e1000217\n\n\n27. Chinese Biomedical Literature Database. \n\n\n28. SinoMed. \n\n\n29. China National Knowledge Infrastructure. \n\n\n30. Wanfang Data - A Leading Provider of Electronic Resources for China Studies. \n\n\n31. VIP Chinese Medical Journal Database. \n\n\n32. | LILACS. \n\n\n33. Alves B/O/O-M Global Index Medicus World Health Organization. \n\n\n34. Health and Medical Articles Database - African Index Medicus. \n\n\n35. Western Pacific Region Index Medicus. \n\n\n36. IMSEAR at SEARO: Home. \n\n\n37. WHO EMRO | IMEMR | Library. \n\n\n38. About SciELO. \n\n\n39. Projet MiRoR. Projet MiRoR \n\n\n40. de Vries RBM, Hooijmans CR, Langendam MW, van Luijk J, Leenaars M, Ritskes-Hoitinga M, Wever KE (2015) A protocol format for the preparation, registration and publication of systematic reviews of animal intervention studies. Evidence-based Preclinical Medicine 2:e00007\n\n\n41. Rader T., Mann M., Stansfield C., Cooper C., Sampson M. (2014) Methods for documenting systematic review searches: A discussion of common issues. Research synthesis methods 5:98–115\n\n\n42. Page MJ, McKenzie JE, Bossuyt PM, et al (2021) The PRISMA 2020 statement: An updated guideline for reporting systematic reviews. BMJ (Clinical research ed) 372:n71"
  },
  {
    "objectID": "chapters/9_focus_groups/index.html",
    "href": "chapters/9_focus_groups/index.html",
    "title": "Generating ideas to address factors limiting reporting guideline impact: workshops with EQUATOR and focus groups with developers, publishers, and experts",
    "section": "",
    "text": "Known todos:\n\nMulti-panel figure not rendering nicely in docx format\nI need to name and cite bias in methods section"
  },
  {
    "objectID": "chapters/9_focus_groups/index.html#workshopping-ideas-with-equator",
    "href": "chapters/9_focus_groups/index.html#workshopping-ideas-with-equator",
    "title": "Generating ideas to address factors limiting reporting guideline impact: workshops with EQUATOR and focus groups with developers, publishers, and experts",
    "section": "Workshopping Ideas with EQUATOR",
    "text": "Workshopping Ideas with EQUATOR\nI described the workshops I ran with EQUATOR in chapter , including the techniques I used to encourage rich discussion and to navigate my position as a participating researcher, contributing ideas myself whilst facilitating others’ to share their voices. For this particular step, I asked workshop participants to consider each intervention function in turn and suggest ideas that would employ that function. After the workshop, I then labelled each idea with the influence(s) it would be addressing to create an “ideas document” with two columns: influences were described in the left hand column, and ideas in the right hand column. I invited workshop participants to review and comment on this document. The file can be seen in Appendix #TODO."
  },
  {
    "objectID": "chapters/9_focus_groups/index.html#focus-groups-with-external-stakeholders",
    "href": "chapters/9_focus_groups/index.html#focus-groups-with-external-stakeholders",
    "title": "Generating ideas to address factors limiting reporting guideline impact: workshops with EQUATOR and focus groups with developers, publishers, and experts",
    "section": "Focus groups with external stakeholders",
    "text": "Focus groups with external stakeholders\nFocus groups are researcher-led group discussions that use conversation as a form of data collection [2]. A key element of focus groups is interactions between participants as they agree, disagree, challenge, and ‘feed off’ of each other. I chose focus groups because I expected this interaction to lead to more ideas being generated than if I interviewed participants in isolation. Focus groups are also a practical way to collect data from larger groups of people. This is in contrast to in-depth interviews which are more useful in eliciting detail about individuals’ perspectives."
  },
  {
    "objectID": "chapters/9_focus_groups/index.html#sampling",
    "href": "chapters/9_focus_groups/index.html#sampling",
    "title": "Generating ideas to address factors limiting reporting guideline impact: workshops with EQUATOR and focus groups with developers, publishers, and experts",
    "section": "Sampling",
    "text": "Sampling\nTo seek variation and ideas from broad range of stakeholders, I invited a purposive sample including the developers of popular reporting guidelines, publishing professionals, and academics that have studied reporting guidelines. I asked participants to extend the invite to others they felt would be appropriate. Because the BCW requires input from experts with insight into the intervention, I decided to elicit the opinions of authors in a separate study (see chapter 11).\nFollowing best-practice, I used information power [3] to guide my estimated sample size. Malterud et al. posit that the more relevant information a sample holds, the fewer participants are needed. They argue that sample size sufficiency depends on five factors: 1) whether the study’s aim is narrow or broad, 2) whether samples are considered dense (they have a lot of relevant experience or knowledge of the phenomena) or sparse, 3) whether the study is well supported by theory, 4) the quality of dialogue, and 5) whether data will between compared between participants/groups.\nMy aim was narrow and well defined. My sample was dense in that participants knew a lot about how reporting guidance is disseminated but also showed variance in terms of which guidelines they work on and which parts of the academic system they represented. I used the BCW as an applied theory. I used open questioning and to encourage strong dialogue (I elaborate on this later), and I was not planning a cross-case analysis. Therefore, I deemed my information power sufficient to justify initially recruiting 15-20 participants in 4-5 groups.\nI used the dialogue criteria from Information Power to decide when to stop recruiting. By monitoring the number of edits to the co-produced file I could be confident that my information power was good. Once groups began to add fewer and fewer comments, I judged that the benefit of continued recruitment was insufficient given time constraints."
  },
  {
    "objectID": "chapters/9_focus_groups/index.html#materials",
    "href": "chapters/9_focus_groups/index.html#materials",
    "title": "Generating ideas to address factors limiting reporting guideline impact: workshops with EQUATOR and focus groups with developers, publishers, and experts",
    "section": "Materials",
    "text": "Materials\nI used the ideas document generated in the workshops with EQUATOR to prompt discussion in the focus groups. However, because ideas generated by previous participants could bias or limit the creativity of current participants I initially hid them by turning the text in the ideas column white before sharing. I would reveal the text only after participants had exhausted their own imagination (see Figure 1 for an example). All participants could edit this file to record their own ideas or elaborate on other people’s ideas. At the end of each focus group, I would then turn the ideas column white again, ready for the next group to continue the process. In this way, each group built upon the output of the previous groups.\n\n\n\n\n\n\n\nIdeas initially hidden\n\n\n\n\n\n\n\nIdeas showing\n\n\n\n\nFigure 1: An example entry in the Ideas Document that was co-edited by participants. Existing ideas were initially hidden (by turning the text white), and only made visible once participants had discussed their own ideas."
  },
  {
    "objectID": "chapters/9_focus_groups/index.html#focus-groups",
    "href": "chapters/9_focus_groups/index.html#focus-groups",
    "title": "Generating ideas to address factors limiting reporting guideline impact: workshops with EQUATOR and focus groups with developers, publishers, and experts",
    "section": "Focus Groups",
    "text": "Focus Groups\nI conducted focus groups between May-July 2022 online using Zoom. Before each focus group, I asked participants to spend some time thinking about barriers and facilitators. I did this because I wanted participants to get into the frame of mind and come to the focus group “armed” with influences they were ready to discuss. I was also interested to see whether participants would contribute influences that I did not identify in chapters 3.\nASK CA Is this combatting recall bias? https://catalogofbias.org/biases/recall-bias/\nEach focus group lasted 2 hours. Following standard practice, I began by introducing myself and the project in a way that I hoped would help participants relax and to think open-mindedly, not defensively. I explained where the list of influences had come from, and that the influences were in reference to reporting guidelines in general, and not necessarily a comment on their guideline. I encouraged participants to think beyond the guideline documents themselves, and to consider all stakeholders and resources involved. I explained the goal was to brainstorm as many ideas as possible, and not worry about whether ideas were good or bad.\nASK CA I say that I wanted to put participants at ease. Is that addressing apprehension bias and hawthorne effect? https://catalogofbias.org/biases/apprehension-bias/ https://catalogofbias.org/biases/hawthorne-effect/\nIt was not possible for a single focus group to cover all influences within a reasonable amount of time, so I allowed participants to select which items they wanted to discuss. I did this by giving them a few minutes to read through a list of influences, raise any additional influences they felt were missing, and mark those that they wanted to talk about. I occasionally selected influences to discuss myself, either because they had been neglected by previous groups or because I expected participants to have insight into it.\nFor each influence discussed, I would explain it and allow participants to ask questions. I then asked participants to spend a couple of minutes reflecting on the influence and brainstorming solutions on their own before discussing them as a group. I encouraged this solo reflection because I wanted all participants to engage with the problem. (#REF)\nASK CA: What is a reference for this technique?\nTo facilitate discussion, I would ask open ended questions, often drawing on intervention functions from the BCW by asking questions like “how could this be easier to do?” or “how could we change how people feel about this?”. I did this when participants ran out of ideas, or when they got fixated on a particular type of intervention, in which case I would reassure participants that their fixated solution was already documented and that it would be useful to think of alternatives.\nOnce participants had discussed all of their own ideas, I would reveal the ideas identified by previous groups by changing the colour of text from white to black. Participants could then edit and extend the text until it reflected all of their thoughts too. Ideas were never removed from the document, but participants could add concerns or disagreements if they wanted to. Editing the file in this way allowed participants to document their thoughts in their own words.\nAfter each focus groups I made notes on how the session went and reflected on what I could have done differently. I made a copy of the ideas document and then turned the text in the ideas column white again, ready for the next group. Taking copies after each group created a paper trail of how the document had evolved after each session, and counted the number of additions so that I could monitor how many new ideas had been added and, therefore, whether I could stop data collection. \n\nData processing and analysis\nI imported the final ideas document into NVivo and applied descriptive codes to ideas. If a sentence contained multiple ideas, I would code each idea separately. I also coded the barriers and stakeholders that were related to each idea. I then grouped ideas that appeared against multiple influences.\nI used qualitative description for my analysis ([4]; [5]), which involved aggregating and summarising ideas. I did not interpret data as doing so would erase the views captured during co-production. I grouped ideas inductively in ways that felt cohesive and made the results easy for my intended audience (the reporting guideline community) to understand and act upon. For example, I aggregated “ask authors to cite reporting guidelines” and “display citation metrics on reporting guideline resources” into a group about “Citations”, even though they target different barriers (discoverability and perceived trustworthiness) and employ different intervention functions (education and persuasion). \nI discussed and refined my coding, aggregating and summarising with JdB. I sent the aggregated, summarised ideas to focus group participants and EQUATOR members, inviting them to check that it reflected their ideas faithfully. I also invited feedback from guideline developers who had shown an interest in the study but had been unable to attend a focus group."
  },
  {
    "objectID": "chapters/9_focus_groups/index.html#reflexivity-trust",
    "href": "chapters/9_focus_groups/index.html#reflexivity-trust",
    "title": "Generating ideas to address factors limiting reporting guideline impact: workshops with EQUATOR and focus groups with developers, publishers, and experts",
    "section": "Reflexivity & Trust",
    "text": "Reflexivity & Trust\nIn chapter 8 I described my active role within my workshops with EQUATOR. In contrast, I tried to remain objective when running focus groups with external stakeholders, in order to capture the perspectives of participants without influencing them. My research paradigm for the focus groups was post-positivist, in that I considered that ideas were “out there”, but that differences in context, experience, and opinion would affect what I (and participants) observed, understood, and concluded.\nWhereas I argued in chapter 8 that my subjectivity was an asset within the workshops, I still wanted to ensure that my results could be trusted as an account of participants’ views. Lincoln and Guba [6] argue that for a study to be trustworthy, the researcher must show that the findings are credible (‘true’), transferable (applicable to other contexts), dependable (consistent and repeatable), and confirmable (shaped by participants, not by the researcher’s bias or motivation). Lincon and Guba propose a number of techniques to achieve these criteria, and I describe the techniques I used in Table 1.\nASK CA: I haven’t named an approach. I’ve tried to be clear with what I did, but I’ve struggled to find the correct label for it. We talked previously about Active Research but I don’t think this is correct for this chapter.\n\n\nTable 1: Techniques for establishing trustworthiness. Based on Lincoln and Guba’s Evaluative Criteria [6]\n\n\n\n\n\n\nTechnique\nImplementation\n\n\n\n\nTechniques for establishing credibility\n\n\n\nMember-checking\nLincoln and Guba argue that member checking is the most important way to the establish validity of an account [6]. Accordingly, I invited participants to comment on my synthesised results, asking for feedback on the structure of categories, my interpretation of their data, and my findings and conclusions. I also 3 invited participants to comment on the product of my data analysis in the form of itemized information and condensed notes.\n\n\nPeer debriefing\nThroughout the design, data collection, analysis and reporting, CA acted as a disinterested peer. By questioning my reasoning and exploring my assumptions, she helped me become aware of biases, perspectives that I was taking for granted, and assumptions I was making. JdB acted as a disinterested peer during data analysis.\n\n\nTechniques for establishing transferability\n\n\n\nThick description\nI aspired to report my results with context by indicating when ideas were common or rare, and who they originated from when I felt this was particularly relevant. I reported disagreements, provide quotes, and relationships between ideas.\n\n\nTechniques for establishing confirmability\n\n\n\nAudit trail\nI referred to audio recordings of the focus groups whenever he needed to clarify parts of the document. I kept versions of raw data collected from all stages. I made a note of my own ideas before commencing data collection, documented all stages of the workshops I held internally with the EQUATOR Network, and kept copies of the co-produced file after every focus group. I kept a copy of my coding in NVivo, and versions of the unitized information and summaries that I sent to participants before and after member checking. This audit trail meant that I could be certain of which stages of research ideas originated from.\n\n\nReflexivity\nI wrote down my own ideas before commencing the study (see chapter 8), along with my beliefs and experiences of reporting guidelines. I continued to keep personal notes throughout planning, data collection, analysis and reporting, in an attempt to remain aware of my own perspectives and positions, and how they may influence my research."
  },
  {
    "objectID": "chapters/9_focus_groups/index.html#ethics-data-management",
    "href": "chapters/9_focus_groups/index.html#ethics-data-management",
    "title": "Generating ideas to address factors limiting reporting guideline impact: workshops with EQUATOR and focus groups with developers, publishers, and experts",
    "section": "Ethics & Data Management",
    "text": "Ethics & Data Management\nThe study was approved by the Medical Sciences Interdivisional Research Ethics Committee (R80414/RE001). Participants gave informed consent by completing an online form. Participant’s edits to the co-produced file were anonymous. I recorded the audio of focus groups so that I could refer to them during analysis, if necessary. All data and recordings were kept on secure university storage."
  },
  {
    "objectID": "chapters/9_focus_groups/index.html#units-of-study",
    "href": "chapters/9_focus_groups/index.html#units-of-study",
    "title": "Generating ideas to address factors limiting reporting guideline impact: workshops with EQUATOR and focus groups with developers, publishers, and experts",
    "section": "Units of study",
    "text": "Units of study\nI held 7 focus groups involving 16 participants in total. Participants included guideline developers (n=11), publishing professionals (n=3), and academics that study reporting guidelines (n=2).\nOf the 15 guideline groups I invited, 5 took part. Of the remainder, 4 guideline groups wanted to participate but were unable to coordinate a time, 5 groups did not respond, and 1 group refused to participate; they felt that their guideline didn’t need updating because it was highly cited.\nOf the 23 invitations that I sent in total, 7 received no response. But because I invited people to share the invitation (which they did), I have no way of knowing my recruitment rate.\nAlthough I had intended to include 4 or 5 participants per focus group, in practice it was difficult to coordinate participants across time zones, and so sessions only had 2 or 3 participants.\nBefore the focus groups, my workshops with EQUATOR had generated a list of TODO ideas, which formed the initial “ideas document” presented to the first focus group. After the final focus group, participants had extended this list further to include 128 ideas to address 32 barriers. Participants identified 10 stakeholders that could enact these ideas including funders, ethics committees, institutions, publishers, equator network, guideline developers, registries, preprint servers, conference organisers, and societies. I grouped these ideas into 28 broader ideas, which I categorised according to whether they could be considered before developing guidance, when developing guidance, when writing guidance down and creating resources, when disseminating resources, or on an ongoing basis.\nI do not describe which stakeholder ideas came from for three reasons. Firstly, stakeholders were all editing the same file, so some ideas would be revisited multiple times by different stakeholders who would build upon the thoughts of previous contributors, editing and extending ideas. Consequently, it wasn’t always possible to definitively say who any particular idea came from, as it may have been the product of multiple stakeholders. Secondly, just because a stakeholder didn’t edit an idea in the document didn’t mean that the idea hadn’t also occurred to them. Hence allocating an idea to a stakeholder just because they were the one that wrote it down may be misleading. Thirdly, I didn’t consider labelling the origin of an idea to be useful because I didn’t see it as an indication of that idea’s quality. In chapter 8 I describe how I judged options according to their acceptability, practicability, effectiveness, affordability, side-effects, and equity, and how these criteria are subjective and will differ between stakeholders. I had sought input from stakeholders because I expected that doing so would lead to more ideas, and ideas that were more likely to gain traction. But I didn’t expect all of their ideas to be “good”. To the contrary, I had explicitly encouraged participants not to worry about whether or not an ideas was “good”. #TODO: edit the paragraph above."
  },
  {
    "objectID": "chapters/9_focus_groups/index.html#synthesis-and-summary",
    "href": "chapters/9_focus_groups/index.html#synthesis-and-summary",
    "title": "Generating ideas to address factors limiting reporting guideline impact: workshops with EQUATOR and focus groups with developers, publishers, and experts",
    "section": "Synthesis and summary",
    "text": "Synthesis and summary\nHere I describe the aggregated ideas. I use the term “stakeholders” instead of “participants” to clarify that ideas came from the focus group participants and the workshop participants.\n\nBefore developing guidance\n\nCreate reporting guidance for protocols and applications\nStakeholders suggested “developing [reporting guidance] for protocols”, funding, and ethics applications as a way to encourage authors to consult reporting guidance earlier in their work when they are more likely to have the time, motivation, and ability to reflect and act on it.\n\n\nAvoid confusing authors with too many reporting guidelines\n“We need fewer, better, reporting guidelines” wrote one stakeholder, when discussing how authors may struggle to identify which reporting guidelines apply to their work.\nAcknowledging that reporting guideline developers may duplicate each other’s work unwittingly, stakeholders wrote that developers should consult EQUATOR’s register of reporting guidelines under development before creating a new guideline. Stakeholders noted that “EQUATOR cannot prevent guideline developers from creating duplicate guidelines” but could “improve the registration process for reporting guidelines that are under development”, better “highlight [reporting guidelines] that are under development in the main search results”, and could “create options and instructions to encourage developers to extend existing guidance instead of duplicating. This could go in the new guidance for guideline developers”.\nStakeholders had ideas of how this “extend”ing could be done. They suggested that developers could tailor existing guidance to a particular niche by making different “versions” or “extensions” of existing guidance “e.g., STROBE split into STROBE Cohort, Case-control etc”. If only a few items need to be edited or added, stakeholders suggested creating “modular” guidance instead of duplicating an entire guideline. Stakeholders spoke of modules in two different ways. Firstly, new reporting guidelines can be created to substitute for particular items in existing reporting guidelines. Stakeholders named TIDIER as an example of this strategy, which can be substituted for item 5 in CONSORT. Stakeholders identified a second kind of modularity in the JARS guidelines, where a general reporting guideline covering all quantitative psychology research can be mixed-and-matched by modules for specific designs (non-experimental and experimental designs, with or without random assignment, or special modules for longitudinal, n-of-1, or replication studies [7]).\nWhen discussing how modules or extensions could be “harmonized” so that they “speak to another”, participants suggested resources could have compatible structure, and should not “use different wording for what is essentially the same item”. Participant’s recommendation to “use similar terminology [across] related guidelines” extended to the title; STROBE-nut, STROBE-ME and STROBE-RDS are more readily identifiable as STROBE extensions than STREGA, ROSES-I, or STROME-ID are. Naming can also indicate when a guideline has been revised: “ARRIVE 2.0 is recognisable as a replacement of ARRIVE”, whereas “TIDIER Placebo appears to be an extension but should be called TIDIER 2.0”.\n\n\nAvoid prescribing structure\nStakeholders suggested that “[reporting] guidelines should avoid prescribing structure as [it] may clash with journal guidelines”.\n\n\nKeep reporting guidelines agnostic to design choices\nStakeholders discussed how designing research is a separate task to reporting it, and that many authors will only encounter reporting guidelines after the manuscript has been written, at which point design advice is less useful.\nNevertheless, stakeholders cited multiple reasons for including design opinions in reporting guidance. Some suggested it should be included so that authors can “learn for next time”, and others wrote that consequences of design choices justify why an item was important to report, but acknowledged that that justifying items in this way can be problematic if developers do not consider all contexts or study types in which their guidance may be used.\nSome participants argued that design advice could be removed from reporting guidelines entirely. This was proposed as a solution when considering how authors may feel afraid to report transparently if what they did goes against the design recommendation, or may feel restricted if forced to use a reporting guideline that prescribes design choices.\nInstead, developers could encourage authors to “explain reasons for methods choices, [which may] be legitimate”, noting that “the consequence of not choosing one [design] option over another, even if the choice is a rarely used option, may not have major consequence on the results of the study”. One stakeholder wrote that authors may feel reassured if told that “editors and peer-reviewers may not judge as harshly when they understand the rationale for the choice”. Stakeholders wrote that developers should “encourage transparent reporting over and above good design”, and that authors could be encouraged to “describe what they did in plain language to make it clear - if what they did doesn’t quite fit with standard terminology (e.g., if they didn’t really do theoretical sampling or aren’t sure if they did theoretical sampling) then just describe what they did, how they made sampling decisions”.\nDiscussions about removing design advice also arose when considering how including design advice elongates guidance, potentially deterring authors from reading it. To solve this, stakeholders suggested reporting guidelines could link to design resources “elsewhere”. For example, if reporting guidance were presented on a website, authors could be given options to “display or hide” design advice by choice, or depending on whether the author was “designing [research], applying for funding, or drafting” a manuscript.\n\n\nDescribe reporting items fully\nMany barriers prompted stakeholders to suggest that specific content should be included for every reporting item.\nFirstly, stakeholders noted that authors need to know what to write, and that a brief description could go in checklists and a longer description in the full guidance. Stakeholders suggested this description should include what to write if they didn’t or couldn’t do something to make it “easy for researchers to report things they are embarrassed about”. In these instances, developers could suggest authors “explain reasons” for their choices or “consider the item in their discussion section as a possible limitation” if necessary. Stakeholders also wrote that developers could suggest what to write when an item doesn’t apply.\nStakeholders suggested explaining “why [an item] is important and who it is important to” as this isn’t always obvious to authors. To make guidelines faster to use, stakeholders suggested indicating which items are most important, perhaps “prioritise[ing] certain items as essential vs. recommended, like ARRIVE 2.0 essential 10” and indicating conditions that make items less or more important, including circumstances that make the item non-applicable.\nTo help authors who want to keep writing concise or need to reduce word counts, stakeholders suggested advising “when an item can be put into a table, figure, box, supplement, or appendix etc” and to “explain the pros and cons of different options e.g., whether content will be peer reviewed or indexed” by search tools.\nMany stakeholders suggested including examples to help authors understand and apply guidance. These examples could include both “good and bad reporting” with an explanation of “how [bad reporting] could be improved”. To help authors who are afraid to transparently report limitations, stakeholders suggested including examples of imperfect research that is perfectly-reported. Examples could also include “reporting in different contexts”, from different disciplines, “in multiple languages”, and concise reporting e.g. reporting “TIDIER items nicely in a table”.\nBecause examples from the published literature can be “difficult to […] find and list”, stakeholders suggested that examples could be “real or generated” by developers. Examples should be “easy to find” from within the guidance resources. Others suggested a searchable “bank” of examples, or ways to showcase “exemplar papers (e.g., badges)”.\nFinally, stakeholders discussed the pros and cons of including item-specific design advice, procedural instructions, and appraisal advice, and the different ways of doing it (see Keep reporting guidelines agnostic to design choices).\n\n\nDescribe each reporting guideline fully\nStakeholders identified that many of the barriers they discussed could be addressed by editing the text that authors might read before using a guideline or checklist.\nFor example, stakeholders suggested that reporting guidelines and resources should “clearly state what kind of research the guidance applies to” when considering how authors may struggle to identify which reporting guideline they should use. Additionally, stakeholders suggested guidelines could “point researchers to other guidelines if more relevant, perhaps using ‘if, then’ rules e.g., ‘if you did X then use Y instead of this guideline’”. If no better guideline exists for a particular type of study, stakeholders suggested to “warn the user that they can still use this guideline but that they may need to ignore certain questions e.g. ‘This guideline is for studies that did X. You can still use it if you did Y, but you will need to ignore items 4, 6, and 8-10’”.\nTo ensure that a reporting guideline for writing manuscripts can also be used for writing protocols, developers could specify “which items need to be considered at protocol/planning stage, and which at results reporting stage.”\nStakeholders warned that developers should be mindful when using words like standard instead of guideline when introducing a reporting guideline, as these words may influence how prescriptive the user expects the resource to be. In choosing their wording, stakeholders suggested developers should be honest and clear about a guideline’s “aim”. Developers could also “be clear about what reporting guidelines are not”. For instance, “when they are not design guidelines or critical appraisal tools” this could be specified and authors could be directed to other tools instead.\nNoting that it might not be obvious how or when to use guidance, participants suggested being explicit about this. For example, “tell authors that they don’t need to fill out templates [or checklists] sequentially but can use an order that matches their workflow or decision making. Put this instruction on the template, checklists, and other tools. Example from PRISMA – population and subgroup items are separated in the checklist but go together when thinking/making decisions.”. Similarly, authors could benefit from explicit suggestions of how to use reporting guidelines as a team, perhaps by asking “their co-researchers to check their reporting”. Finally, stakeholders suggested telling authors how much time the guideline is expected to take them to read and use, why it can be trusted, and where they can read about its development.\n\n\nKeep guidance short\n“Make guidance shorter” wrote one stakeholder, and “Make checklists shorter”, wrote another, when considering how “guideline length […] may put researchers off” as “long guidelines appear more complex and time consuming” and can challenge word limits.\nOne stakeholder posited that guidelines could be shorter if developers were “realistic about what they ask for”. Another challenged developers to “try using a guideline from start to finish and see how the manuscript ends up”. Others wrote that guidelines could be shortened by linking out to content that wasn’t directly related to reporting, like design or appraisal advice (see Keep reporting guidelines agnostic to design choices). Guidelines that are “very long or [have] lots of optional items” could be split into multiple versions (see STROBE as an example in Avoid confusing authors with too many reporting guidelines).\nOthers suggested presenting guidance online with non-essential content collapsed so that guidance appeared short but authors could choose to “display or hide” content. Stakeholders noted that “itemisation can make guidance more digestible, but can also make it harder to get the bigger picture” and makes text appear longer.\n\n\n\nWhen developing guidance\n\nMake resources ready-to-use\nStakeholders suggested that resources should be in ready-to-use formats. For instance, “checklists should be editable (not PDFs)”.\n\n\nMake reporting guidelines easy to understand\n“Make guidance easier to understand” was written as a solution to help authors who misinterpret, or can’t understand, guidance. Stakeholders suggested developers could “make guidance as ‘plain language’ as possible” os “create plain language versions of existing guidance”, whilst being “mindful of language that may appear patronizing”.\nStakeholders suggested defining “key words or phrases in a glossary or tooltips”, using consistent terms across related resources, translating guidelines and examples, and ensuring these translations are easy to find by making them “searchable”, “linked properly” to each other, and “more evident on [the] EQUATOR site”.\nStakeholders recognised a need to collect and respond to feedback from “international researchers” representative of the user base across disciplines and institutions, and that this could be sought when developing guidance to “test” it, but also on an ongoing basis to “continually revisit the items in the guidelines that may be confusing or difficult to implement”. As an example, one guideline developer contributed that collecting feedback had lead them to remove “the word ‘context’ because users struggled to understand it”.\n\n\nUse persuasive language and design\nWhen discussing how authors feel about reporting guidelines and checklists, stakeholders reflected on how the wording and presentation of resources can influence authors’ perceptions of it.\nStakeholders noted that a poorly formatted checklist, lacking “visual appeal/graphic design”, can “appear outdated” or “larger than it is”. Stakeholders suggested that developers could use design to “foster feelings of simplicity”, and “engage graphic designers” if necessary.\nReassurance seemed especially important to stakeholders when considering how to motivate authors to transparently report items that aren’t perfect. Stakeholders suggested that developers could use language and tone of voice to “foster a feeling of confidence, not judgement”, perhaps reminding “researchers that all research has limitations”. Stakeholders also wrote that reassurance could also be given by including notes aimed at reviewers, and cited JARS as an example “which explains to reviewers why an author may make certain choices”, thereby educating reviewers whilst reassuring authors that they won’t be penalised for transparency. Stakeholders suggested rewording text and tone of voice to make reporting guidelines and checklists “appear less like ‘red tape’”, and to reassure authors that “reporting guidelines are just that: guidelines!”.\n\n\nCreate additional tools\nStakeholders noted that checklists may be easier to use if they are editable (see Make resources ready-to-use), and if authors could complete them with “the relevant text, rather than [the] page/section number” which can be frustrating to keep updated. Although one stakeholder, a publisher, noted that checklists completed with text are difficult to double check, and therefore it would be most useful to include the text and the page number.\nTo aid writing, stakeholders suggested “templates for drafting” manuscripts, interactive forms and “writing tools (e.g., COBWEB)”, “tools for creating figures and tables like PRISMA’s flowchart generator”, and tools for generating text, like TIDIER’s tool for generating intervention description (#REF). However, one stakeholder warned that these kinds of structured writing “provide opportunities for inclusion [of reporting items] but there is always the risk that they exclude more [important items] that are outside the boundaries of the template”.\nTo encourage authors to consider reporting guidance earlier in their research, stakeholders considered to-do lists with items “in the order they are done”, or “embed[ing] items into data collection tools” like software for systematic reviewers.\nTo help reviewers check reporting, stakeholders suggested creating “tools for co-researchers to check each others’ work”, creating extra guidance for peer reviewers, providing text that can be pasted in reviewer feedback forms to “request additional information” for poorly reported items, or even building a “reviewer tool that generates a report”.\nStakeholders noted that tools should presented in ways that “better differentiate” how and when they should be used “e.g., resources for writing vs. checking vs. reviewing”.\n\n\nMake resources easy to discover and find\nStakeholders had ideas for how to help authors discover and find resources. Stakeholders wrote about hyperlinks being an important way for authors to discover related guidelines and tools. These could be “horizontal links between related guidelines” and between guideline “documents and tools” like checklists. All resources should “link to one another”, ideally “item by item” so that checklist items could link directly to the relevant section of full guidance. Stakeholders noted that hyperlinks can become out of date, and so stakeholders should “fix broken links”.\nStakeholders also wrote that resources could be hosted on “a convenient place, such as a unified website”. EQUATOR’s website is one such place, and participants suggested making it “easier to navigate” and its “search tool more prominent and easier to use”. Although some authors will use such search tools, stakeholders recognised that those who browse may benefit from “curated” collections of reporting guidelines or a “manageable list of related, commonly used guidelines”. As an example, an author writing a review article may be helped by a page listing “PRISMA, MOOSE, ENTREQ, PRISMA-SRc etc.” and that “this page could be kept up to date as guidelines are revised”. Another stakeholder (a publisher) warned, however, that journals may not want to link to these pages if the collection has “a much broader scope” and includes guidelines that the journal doesn’t endorse.\nWhen discussing how to help authors who are less familiar with study designs, one stakeholder suggested creating “tools to help researchers identify study designs (e.g., a questionnaire)”, and another, already familiar with such a tool previously developed by EQUATOR suggested it should “use plain language”.\n\n\nMake information digestible\nStakeholders acknowledged that authors’ needs may differ between tasks (e.g. drafting an article vs demonstrating compliance), and that authors may use guidance in different ways; some will read the whole thing from start to finish, whilst others will dip in and out as-and-when they need. Consequently, stakeholders wrote that “having different options available that meet the needs of different users is vital” and that authors should be able to consult guidance in ways that “work for them”.\nOne suggested way of doing this may be to “structure guidance using navigation menus and subheadings” so that “it is easy to find the information you need”, making reporting guidelines faster to use and less overwhelming. Another noted that checklists can also be designed, citing TIDIER as “a nice example” that has “integrated the intervention and placebo into one table” with the active intervention and placebo in adjacent columns.\nDynamically hiding and showing content was floated here again (see Keep guidance short), with one stakeholder suggesting that users could “filter out” irrelevant content, to only see instructions for their task (e.g. planning, writing, reviewing) or specific to their study. This could be done with a “decision tree” or “branching questions” to determine specific features of the study (“e.g., a systematic review with network meta-analysis of individual participant data”). Answers to these questions could then be used to to “modify” items to create “personalised guidelines”, or to generate a “customised reporting checklist” that includes all “main and relevant extension items”.\nDynamic content was also seen as a favourable way to embed guideline extensions, with the aim of making them easier to discover without overwhelming the author. For example, noting that “some guidelines ‘fit together’…e.g., PRISMA and PRISMA-Abstracts”, stakeholders wrote that PRISMA-Abstracts could be “embedded” as collapsed content that interested authors could expand.\n\n\n\nWhen disseminating resources\n\nDescribe reporting guidelines where they are encountered\nWhen first introducing reporting guidelines stakeholders suggested telling authors what reporting guidelines are, “when and how best to use” them, and what benefits to expect. This information could go wherever authors are advised to use reporting guidelines (like like journal instruction pages, registries), EQUATOR’s website, social media campaigns, at the start of the guidelines, and could go at the beginning of checklists too “in case people don’t read the whole [guideline] paper”. This introductory text could be “short, sweet, and to the point”. Benefits could be even more prominent by putting them “in a box, or [by using] font or positioning”.\n\n\n\nMake resources accessible\nStakeholders wrote “Ensure guidance is open access” so that all authors can it. Stakeholders also noted that if guidance is published under a permissive license then others can reuse the content to extend the guidance or build new tools.\n\n\nShow and encourage citations\nDisplaying citation counts on the EQUATOR Network website (or other websites where authors search for reporting guidelines) was described as a way to “provide social proof” and convince authors that guidelines are credible.\nTo generate these citations, stakeholders suggested explicitly asking “researchers to cite the guideline they used”. Stakeholders wrote that if an author cites a guideline they have used, then readers may discover the guideline from that authors’ article.\n\n\nProvide testimonials\nStakeholders suggested providing “testimonials” as a way to tackle a few different barriers using education and persuasion. Stakeholders suggested providing “quotes from authors/researchers who felt that reporting guidelines helped their work and who have had positive experiences” such as making “writing easier” or helping “with co-authorship communications”. Stakeholders proposed that testimonials could bring benefits to life, thereby making them more believable.\nTo make authors care more about research waste caused by poor reporting, stakeholders suggested testimonials from “research consumers for whom an item is important”, or quotes that illustrate “how detrimental poor reporting is for end users”.\nStakeholders wrote that testimonials from decision makers (like editors, reviewers, and grant-givers) could communicate their “preference for transparent reporting” and convince authors that reporting will be checked. If these testimonials conveyed that transparency is valued above perfectionism, participants suggested this could reassure authors. Stakeholders also suggested collecting positive testimonials from such “nervous researchers”.\nFinally, stakeholders suggested collecting testimonials from researchers “with a range of experience”, including “experienced researchers who have benefited by changing their practices”. Diverse case studies would help engage a diverse user base, and challenge assumptions that reporting guidelines are too patronizing for experienced researchers or too complicated for inexperienced ones.\n\n\n\nOn an ongoing basis\n\nBudget for reporting\nStakeholders noted that “researchers need budget to allocate time to writing” and that “funders could encourage proper financial/time budgeting for writing”, as could research supervisors.\n\n\nCreate rewards\nStakeholders suggested “offer[ing] some sort of tangible reward/benefit” to motivate guideline use, creating new rewards when necessary. Ideas included “publishers offering a fast-track review/discount”, “badges on published articles” or platforms “like publons”, or “a certificate after completing training”.\n\n\nCreate discussion spaces\nMultiple barriers lead stakeholders to suggest “create[ing] spaces for researchers to connect with other researchers to celebrate and share experiences”. These spaces could include “forums, meetings, tea clubs, [and] clinics both in real life and virtual”. Such spaces could help authors solicit help and could act as social proof, as seeing “others using and talking about the guidance” may be motivational.\nOnline discussion spaces were also considered a useful way to gather feedback from users directly (by asking for it) and indirectly (by monitoring discussions). Stakeholders wrote that feedback channels “could be useful to guideline developers”, and may also “cultivate a feeling of community ownership” by “communicating an invitational attitude”, thereby making guidelines appear less bureaucratic.\n\n\n\nCreate ways to catch authors earlier\nStakeholders thought of ways to “try and shift the time at which researchers discover or use guidelines”, hypothesising that “it’s more likely that guidelines will save them time” if used earlier or “at the right time” and “not just upon submission”.\nMost simply, stakeholders suggested “telling” or “encouraging” authors to use reporting guidelines for planning or drafting research (and not just for demonstrating compliance upon submission). Building upon this, stakeholders suggested organising the EQUATOR Network website to make it obvious which stages of work resources can be used for.\nStakeholders suggested “including [reporting guidelines] in the university teaching and training curriculum and text books” so that students learn about them before running or writing up their first study.\nStakeholders suggested creating reporting guidance for early research outputs like funding applications and protocols (as previously described in Create reporting guidance for early stages of research), and advertising resources through funders, ethics committees, and writing training programmes.\nWhen considering whether authors may need reminders to use a reporting guideline for their next study, stakeholders suggested publishers and EQUATOR could use “email reminders” or strategies used by e-commerce sites “like when you buy something from an online business…then they work hard to gain your custom again”.\n\n\nEndorse and enforce reporting guidelines\nStakeholders suggested “encouraging more journals to endorse guidelines” and drew a distinction between endorsing reporting guidelines and promoting them on a website, social media, or email (see Promote reporting guidelines). Endorsement was described as a long term commitment to recommend or encourage guideline use, requiring buy-in from organisational leaders, and possibly changes to policies, instructions, infrastructure, and workflows. Promotion, conversely, was described as ephemeral and does not require organisational changes.\nStakeholders drew another distinction between endorsement and enforcement, whereby enforcement meant reporting guidelines are “a requirement” or “condition”. Enforcement was further divided into enforcing checklist completion or, noting that checklists may not always accurately reflect manuscript content, checking text for adherence to guidance.\nWhen considering who could enforce guidelines, stakeholders noted that reporting guidelines could be “a requirement for publication”, “for registration (where applicable) (e.g. clinical trial registries, PROSPERO)”, or when submitting conference abstracts. “Ethics committees and funding organisations” could require that adhere to guidelines, “for example, completion of SPIRIT for clinical trial submissions”, or to declare that they will use a guideline when writing their results. To facilitate enforcement, stakeholders suggested that the software academics use to provide funders with updates could ask for completed checklists. One stakeholder suggested that reporting guideline adherence should be a condition of university employment and that a “digital dashboard [may] help audit[ing] and monitoring”. Noting that enforcement requires resources, stakeholders suggested to “focus on main RGs and being compliant with them”.\n\n\nEvidence the benefits\nStakeholders suggested that benefits may be more believable if there were evidenced. This could be “evidence that [reporting guidelines] improve the completeness and transparency of the output”. For quantifiable benefits, the suggestion was to collect and report data on “acceptance rates, publishing speed, writing speed”. One stakeholder posited that “more transparent reporting / structured reporting may lead to faster editorial processes as it becomes easy for peer reviewers and editors to review papers about their study” and another suggested to “provide statistics about processing times of articles that follow / don’t follow reporting standards” would help evidence this claim and “emphasise to researchers that clear reporting will minimise the number of times others contact them for clarification”. However, some stakeholders were sceptical whether data on acceptance rates would show any benefit at all: “Likelihood of being accepted might not be heavily influenced – bad research well reported would still be rejected”.\nFor experiential benefits that cannot be quantified, stakeholders suggested providing case studies or testimonials (see Provide testimonials).\n\n\nMake reporting guidelines appear as a priority\nWhen a journal endorses or enforces reporting guidelines, a few stakeholders suggested making reporting guidelines more prominent within the journal’s workflow to make them appear more important. Notes included that “reporting guidelines could be more prominent on journal author guideline pages”, or that “if the journal uses any sort of structured peer review (e.g., specific questions related to methodology) to tell authors this explicitly [on author guideline pages and within review feedback] and link it to the reporting guideline content”. A third suggestion was that “when journals ‘stitch’ or ‘build’ together the manuscript pdf (including cover letter, manuscript main text, appendices, etc.), prioritise the reporting guideline or move it earlier in the pdf”.\nHowever, a few stakeholders (publishers) warned that prioritising guidelines on author instruction pages “is complicated as these [pages] already have to do a lot”, although guidelines could be more prominent if the pages “were better organised and/or filterable”.\n\n\nPromote reporting guidelines\nStakeholders noted may bodies that could help spread the word that reporting guidelines exist. For example, “professional societies” could “advertise” reporting guidelines, despite not having a role in the funding, regulation or dissemination of research.\nPromotion could occur online. Most obviously, on stakeholder guidance web pages. “Email campaigns, social media, blogs” could be useful channels to “share and connect with others [and] drive traffic to guideline website[s]”, but “these require time and energy” from the reporting guideline community to set up and manage.\n“Conferences and workshops at institutions” were cited as channels to promote reporting guideline offline, as were ” seminars, webinars, and presentations” especially in “hard to reach countries/fields”. These events were described as useful ways “to assist in the interpretation and use of the guidelines” and could be opportunities for “universities/funders/journals [to speak] together about the importance of reporting guidelines”.\nTo reach students, participants suggested that universities could include reporting guidelines in their curricula, learning materials, or through reporting champions (see Install reporting champions).\nStakeholders wrote that “promotion can begin before a guideline has been published so that researchers know about guidelines being developed” and suggested “the provision of a time buffer/phasing period for updating new reporting guidelines which would allow researchers to have information about these new guidelines”.\n\n\nInstall reporting champions\nStakeholders wrote that publishers, universities, funders, and ethics committees could have members to promote and facilitate the usage of reporting guidelines, centring on the terms “champions” and “EQUATOR ambassadors”. Within publishers, funders, or ethics board, a champion’s responsibility may be to “expand knowledge/awareness of guidelines”. Within institutions, champions could also “help researchers” by “providing feedback on writing” and that ECRs may feel most comfortable talking with a champion from “an accessible level (e.g., post-doc, library staff)”. This could follow a local network model (UKRN was cited as an example) with EQUATOR as the central organiser, and could utilize existing reproducibility networks.\n\n\nProvide additional teaching\nStakeholders proposed additional teaching as a way to promote reporting guidelines, make them easier to use, and a way to communicate the impacts of poor reporting. Education and training could be general (EQUATOR’s publication school was cited as an example) of could be “guideline-specific”, and could be delivered in person or online, as courses, videos, or text.\nIn addition to learning about a particular guideline, stakeholders suggested that students could learn about “writing as a process” and “workflows for documenting and communicating research”. This was considered useful as “researchers don’t necessarily understand that reporting is a stage in the research process”. Curricula could include “methods studies that indicate the research waste” to teach students why reporting matters, or students could learn for themselves by attempting “to replicate a study or do a systematic review to discover how poorly research is currently reported”.\nTo gain experience in using reporting guidelines, stakeholders suggested students could develop “research protocols (as Bachelor or Master Theses) using reporting guidelines” and that these could “be assessed based on the compliance with the appropriate reporting guideline” in addition to “other criteria more related with methodology”. To make this easier, stakeholders suggested structuring courses around reporting guideline items “for example: a course on [randomized controlled trials] covering every single SPIRIT or CONSORT item”.\n\n\n\nMake updating guidelines easier\nStakeholders acknowledged that “researchers have opinions on how the guidance could be improved including how to make it clearer, and whether items should be rearranged, separated, combined, added or removed” and that “this information could be useful to guideline developers”. Some stakeholders went further, expressing that guidance and websites should be updated “in response to user feedback or changes in the field”. Others suggested that “developers could consult different user groups when creating guidance” and “engage as many health professions as possible” so that “professional cultural issues can be usefully accommodated.”\nHowever, stakeholders were not forthcoming on how to go about gathering this feedback. One wrote “provide ways for researchers to give feedback to guideline developers” without suggesting any ways to do this. Another simply asked “how can we enable users to give feedback on guidance?”. Many guideline developers noted that they required access to extra funding to evaluate, refine, and update their resources. One developer suggested that “minor updates could be made without publishing a new article” if the guidance were disseminated on a website."
  },
  {
    "objectID": "chapters/9_focus_groups/index.html#description-of-findings",
    "href": "chapters/9_focus_groups/index.html#description-of-findings",
    "title": "Generating ideas to address factors limiting reporting guideline impact: workshops with EQUATOR and focus groups with developers, publishers, and experts",
    "section": "Description of findings",
    "text": "Description of findings\n\n\nSummary of results\nMy objective was to identify ideas to address factors that may influence adherence to reporting guidelines by running workshops with the EQUATOR Network and focus groups with guideline developers, publishers, and other experts. Stakeholders identified 128 ideas employing all intervention functions. There were ideas to consider before, during, and after creating guidance, ideas to consider when writing the guidance down, ideas about tools to help guidance application, ideas about ongoing activities to support or promote guidance use, and ideas about refining guidance over time in response to feedback. Many of these ideas could be enacted by guideline developers, publishers, and EQUATOR, but participants also saw opportunities for ethics committees, funders, academics, registries, and syllabus writers; stakeholders who are typically less frequently considered.\nI believed that including perspectives from a range of stakeholders would lead to more ideas. This seems to be the case: EQUATOR staff thought of TODO which was then expanded to 128 through the focus groups.\nThis is the first time that guideline developers from different groups have come together with publishers and academics to consider reporting guidance dissemination as a system. I had a fair response rate from guideline groups and of their responses were generally supportive, even if they were unable to take part. Multiple guideline developers volunteered their guideline to be a “guinneapig” and expressed support for my work. All stakeholders were open minded to the barriers I presented except for one developer who expressed scepticism that reporting guidelines were anything but perfect, and requested additional evidence of authors’ negative experiences.\n\n\n\n\n\n\nImplications\nThese results will also be of interest to the reporting guideline community. Publishers, funders, and institutions will find food for thought on how to create effective policies to support reporting guidelines. Many of the ideas could be enacted by the EQUATOR Network. Guideline developers may find inspiration here when writing or revising guidance. At the time of writing, the EQUATOR Network was in the process of updating its advice for guideline developers, which I hope will be informed by these results. The ideas generated may also be of interest to publishers, funders, and Universities.\nTODO: elaborate on what the guidance for guideline developers covers currently, how that could be extended by the ideas here, and perhaps contrast that with what happens in reality.\n\nAlthough only a few reporting guideline development groups took part in this study, most ideas identified were abstract enough to generalise to most reporting guidance, which tend to be developed and distributed in similar ways; (e.g., all development groups will have to consider what guidance to create, its scope, how to communicate it clearly and how to disseminate it). Some ideas may even generalise to other interventions to encourage good research practices (e.g., to communicate personal benefits, not to patronize researchers).\n\n\nLimitations\nThe study may have benefited from more diversity in participants’ backgrounds and expertise. We could have recruited participants from funders, ethics committees, or registries, behaviour change experts or experts familiar with user experience of websites or written documents. All participants were western and proficient in English. Although this is partly a limitation of who writes reporting guideline, we could have sought input from publishing houses that cater to non-western authors. Broadening the participant pool in this way would have led to more ideas.\nMy focus groups were smaller than I had planned. Some would consider these group sizes too small to be called focus groups, and may instead call them paired interviews, dyads, or triads (#REF). A hallmark of focus groups is that they use “group interaction to produce data” [8], and these interactions may include sharing experiences and challenging each other. However, I did not feel the small group sizes to be a limitation in this study for two reasons. Firstly, because participants were co-editing a file and building upon the thoughts of previous groups, participants could react and respond to to participants from previous groups. Secondly, participants had deep understandings of the topic (evidenced by sessions overrunning and participants dwelling on a single topic) which meant that even pairs of participants had plenty to discuss, share, and debate. If I had condensed participants into, say, 3 groups of 5-6 participants, each participants would have had less time to speak and I anticipate that many ideas would have gone un-spoken.\n\n\nFuture work\nI purposefully did not seek input from authors as this study required input from experts familiar with reporting guideline dissemination. Instead, I describe how I sought input from authors in chapter 11. I also purposefully did not ask stakeholders to prioritize or rank ideas. In chapter 8 I explained that prioritization is subjective and, therefore, should be done by stakeholder separately. I also described how EQUATOR began to prioritize intervention options. In the next chapter I describe how I decided which ideas to act on and how I turned them into intervention components.\n\n\n\n\n\n1. Michie S, Atkins L, West R (2014) The Behaviour Change Wheel: A Guide to Designing Interventions. Silverback Publishing, London\n\n\n2. Given L (2008) Focus Groups. In: The SAGE Encyclopedia of Qualitative Research Methods. SAGE Publications, Inc., Thousand Oaks, pp 353–354\n\n\n3. Malterud K, Siersma VD, Guassora AD (2016) Sample Size in Qualitative Interview Studies: Guided by Information Power. Qualitative Health Research 26:1753–1760\n\n\n4. Bradshaw C, Atkinson S, Doody O (2017) Employing a Qualitative Description Approach in Health Care Research. Global Qualitative Nursing Research 4:2333393617742282\n\n\n5. Kim H, Sefcik JS, Bradway C (2017) Characteristics of Qualitative Descriptive Studies: A Systematic Review. Research in nursing & health 40:23–42\n\n\n6. Lincoln YS, Guba EG (1985) Naturalistic Inquiry. SAGE\n\n\n7. Appelbaum M, Cooper H, Kline RB, Mayo-Wilson E, Nezu AM, Rao SM (2018) Journal article reporting standards for quantitative research in psychology: The APA Publications and Communications Board task force report. American Psychologist 73:3–25\n\n\n8. L.Morgan D (1997) Focus Groups as Qualitative Research. https://doi.org/10.4135/9781412984287"
  },
  {
    "objectID": "chapters/8_workshops/index.html",
    "href": "chapters/8_workshops/index.html",
    "title": "Following the BCW Guide: Workshops with EQUATOR",
    "section": "",
    "text": "Having identified factors that may limit the impact of reporting guidelines (chapters 3 - 6) and selected the Behaviour Change Wheel as a framework [1] (chapter 7), my next step was to use the framework to turn reporting guidelines into a behaviour change intervention. In my introduction I describe how the reporting guideline system grew organically and I justified why I wanted to redesign this system and the guidelines themselves using evidence and behaviour change theory.\n\nIn “The Behaviour Change Wheel - A Guide To Designing Interventions”[2], Michie et al. suggest eight steps to help intervention designers understand behaviour and identify intervention options, content, and implementation options (see Figure 1). The guide is aimed at intervention designers who are not behaviour science specialists, and so offered a practical way to include other stakeholders from reporting guideline ecosystem. I wanted to do this because I expected that input from experts with intimate knowledge of reporting guidelines would lead to more ideas, and that these ideas to be more likely to gain traction and have impact.\n\n\n\nFigure 1: The three stages and eight steps recommended in The Behaviour Change Wheel - A Guide To Designing Interventions [2]\n\n\nHowever, I had to be mindful of how much time I could expect stakeholders to give to my project. The eight stages outlined by Michie et al. would take many hours and require background familiarity with the COM-B model, intervention functions and policy categories. This seemed like too much to ask of strangers, and so I decided to begin by involving members of the UK EQUATOR Center, with whom I already had a relationship, and who were already invested in the project. See chapter 2 for an introduction to the EQUATOR Network, and see chapters 9 and 11 for how I sought input from wider stakeholders and authors.\nIn this chapter, I describe how I led members of the UK EQUATOR Centre through the intervention design process outlined in “The Behaviour Change Wheel - A Guide To Designing Interventions”[2]. I go through each stage in turn, and describe the methods and results for each."
  },
  {
    "objectID": "chapters/8_workshops/index.html#introduction",
    "href": "chapters/8_workshops/index.html#introduction",
    "title": "Following the BCW Guide: Workshops with EQUATOR",
    "section": "",
    "text": "Having identified factors that may limit the impact of reporting guidelines (chapters 3 - 6) and selected the Behaviour Change Wheel as a framework [1] (chapter 7), my next step was to use the framework to turn reporting guidelines into a behaviour change intervention. In my introduction I describe how the reporting guideline system grew organically and I justified why I wanted to redesign this system and the guidelines themselves using evidence and behaviour change theory.\n\nIn “The Behaviour Change Wheel - A Guide To Designing Interventions”[2], Michie et al. suggest eight steps to help intervention designers understand behaviour and identify intervention options, content, and implementation options (see Figure 1). The guide is aimed at intervention designers who are not behaviour science specialists, and so offered a practical way to include other stakeholders from reporting guideline ecosystem. I wanted to do this because I expected that input from experts with intimate knowledge of reporting guidelines would lead to more ideas, and that these ideas to be more likely to gain traction and have impact.\n\n\n\nFigure 1: The three stages and eight steps recommended in The Behaviour Change Wheel - A Guide To Designing Interventions [2]\n\n\nHowever, I had to be mindful of how much time I could expect stakeholders to give to my project. The eight stages outlined by Michie et al. would take many hours and require background familiarity with the COM-B model, intervention functions and policy categories. This seemed like too much to ask of strangers, and so I decided to begin by involving members of the UK EQUATOR Center, with whom I already had a relationship, and who were already invested in the project. See chapter 2 for an introduction to the EQUATOR Network, and see chapters 9 and 11 for how I sought input from wider stakeholders and authors.\nIn this chapter, I describe how I led members of the UK EQUATOR Centre through the intervention design process outlined in “The Behaviour Change Wheel - A Guide To Designing Interventions”[2]. I go through each stage in turn, and describe the methods and results for each."
  },
  {
    "objectID": "chapters/8_workshops/index.html#methods",
    "href": "chapters/8_workshops/index.html#methods",
    "title": "Following the BCW Guide: Workshops with EQUATOR",
    "section": "Methods",
    "text": "Methods\nI invited 7 members of the UK EQUATOR Center to take part in the workshops, of whom 6 took part.\nI took an active part in the workshops. I felt justified drawing on my own experience from my earlier PhD work, as an author, and developer of tools to help authors (see chapter 2). Together, we completed exercises through discussion. Hence my paradigm was constructivist in that knowledge was “constructed between inquirer and participant through the inquiry process itself”[3]. Constructivism “rejects the idea that there is objective knowledge in some external reality for the researcher to retrieve mechanistically” and instead, “the researcher’s values and dispositions influence the knowledge that is constructed through interaction with the phenomenon and participants in the inquiry”[3].\nOn one hand I expected that my experience would be an asset, and would contribute to our aim of understanding and addressing the phenomena we were interested in. On the other hand, I wanted to ensure that my opinions didn’t bias the group, that I remained open-minded, and that I captured the thoughts of other workshop participants accurately. And so I used a number of established techniques to enhance trustworthiness and facilitate discussion.\nTo enhance trustworthiness, I used Lincoln and Guba’s criteria for trustworthiness [4], which asserts that for for a study to be trustworthy, the researcher must show that the findings are credible (‘true’), transferable (applicable to other contexts), dependable (consistent and repeatable), and confirmable (shaped by participants, not by the researcher’s bias or motivation). I describe the techniques I used to achieve each criteria in Table 1.\n\n\n\n\n\n\nTable 1: Techniques for establishing trustworthiness. Based on Lincoln and Guba’s Evaluative Criteria [4]\n\n\n\n\n\n\nTechnique\nImplementation\n\n\n\n\nTechniques for establishing credibility\n\n\n\nMember-checking\nLincoln and Guba argue that member checking is the most important way to the establish validity of an account [4]. All workshop participants could edit the worksheets during and after each workshop. At the end of each session, I would invite workshop participants to confirm that their thoughts were reflected in the file, and I invited participants to comment on my written account of the workshops.\n\n\nTechniques for establishing transferability\n\n\n\nThick description\nAlthough perhaps most relevant to ethnographic studies, I nevertheless drew on aspects of ‘thick description’ that were relevant here. I encouraged participants to describe ideas in detail and to document disagreement and context. Our aim was to document ideas in sufficient detail such that they could be transferable across different reporting guidelines and stakeholders.\n\n\nTechniques for establishing confirmability\n\n\n\nReflexivity\nI wrote down my own ideas before each workshop, and made notes at the end of each workshop to reflect on the process. I would invite participants to consider the worksheet on their own or in pairs. I did this so that everybody would engage with the task and have an opportunity to think before being influenced by others. We then discussed ideas as a group, with members agreeing, disagreeing, and bouncing off each other. I withheld my own ideas until the end of a session or task, whereupon I would invite discussion on any ideas that I felt hadn’t been covered already. This allowed me to contribute my experience from previous PhD work, as an author, and as a software developer and give others a chance to discuss my thoughts, without biasing or narrowing discussion. I also actively acknowledge, and reflect on, my own role in knowledge creation throughout this chapter.\n\n\nAudit trail\nAll worksheets were stored on the University One Drive account which performs automatic saves and versioning. This created an audit trail which meant I could look at how a worksheet changed over time as participants added and edited content.\n\n\n\n\n\n\nTo encourage open discussion, I encouraged participants to rise above their own preconceptions and reassured them that there were no wrong answers, that all ideas were valid and should be documented (#TODO: name and cite). To facilitate rich discussion I used open-ended questioning (#TODO: name and cite), left space for participants to talk (#TODO: name and cite), and followed Michie et al.’s worksheets which structure inquiry around frameworks, models, and taxonomies (#TODO: name and cite).\nTODO: name all biases. Perhaps using https://www.ideatovalue.com/crea/nickskillicorn/2021/06/a-list-of-all-of-the-biases-which-affect-creativity-and-innovation-performance/ or https://catalogofbias.org/about/\nWe met 7 times between December 2021 and May 2022, and each online meeting lasted around 2 hours. All participants had access to the Behaviour Change Wheel book. We established some ground rules which were that no idea was a bad idea, we should favour evidence over preconceptions, and that we should aspire to challenge our own assumptions and be open minded as far as possible. We didn’t seek consensus. Instead, we kept note of any disagreements that could not be resolved by discussion.\nI followed the eight steps recommended by Michie et al (Figure 1) faithfully, and I would explain the objectives and any background theory at the start of each step. I will now summarise each step and our discussions. Our co-edited worksheets are included in the Appendix. I purposefully use “we” in this chapter instead of “participants” to reflect that my voice is included."
  },
  {
    "objectID": "chapters/8_workshops/index.html#results",
    "href": "chapters/8_workshops/index.html#results",
    "title": "Following the BCW Guide: Workshops with EQUATOR",
    "section": "Results",
    "text": "Results\n\n1. Defining the problem in behavioural terms\nMichie et al.’s first step is to define the problem in terms of who needs to do what. For example, weight loss is not a behaviour, but increasing physical activity is, and could be specified further as walking 10,000 steps a day. We were all in alignment here: we want researchers to include important details in their articles, in line with the relevant reporting guidelines.\n\n\n2. Select target behaviour\n“Behaviours do not exist in a vacuum but occur within the context of other behaviours of the same or other individuals” write Michie et al. [2] when explaining that the desired behaviour needn’t be the behaviour that you target. For instance, if you want a child to eat more fruit (desired behaviour), you may seek to influence what food their parent buys (target behaviour).\nStep 2 involves generating a longlist of candidate target behaviours that could bring about the desired behaviour before then selecting which behaviour(s) to focus on.\nWe generated a longlist of 18 ideas, many of which targeted authors directly such as “reading the guidance in full” or “studying guidance (in an abstract sense)”. We also considered targeting people other than authors. For instance, “peer reviewers” and “editors” could “check articles against guidelines [and] tell authors what is missing”, and supervisors could “encourage” the use of guidelines.\nThis exercise helped us break down things that we hadn’t questioned previously. For instance, we were forced to define what we actually meant by a “reporting guideline”, noting that “guidance” could be distributed across publications, checklists, or supplements. We decided that we wanted people to “use the full guidance (often reported in an Example & Elaboration document)” and “not just the checklist”.\nSimilarly, where we may previously have thought of “writing” as a single task, we began to consider how authors could “use guidance when planning”, “drafting”, “editing [their own work]”, or “checking”. We discussed how these behaviours required researchers’ to be open-minded to assistance and wondered if we could encourage them to “ask for help when writing”.\nThe next step was to select a behaviour from our longlist by considering:\n\nThe expected impact if the behaviour were to be performed\nHow easy we expected behaviour change to be\nThe centrality of behaviour - how close it was to our desired behaviour\nHow easy the behaviour will be to measure\n\nCriteria 2 and 3 lead us to prioritise the behaviour of authors above that of editors or peer reviewers. When considering criteria 1, we felt that authors were most likely to act on guidance if they used it early when writing. Just how early depended on the guideline and discipline; for example, protocols are common in some disciplines and some guidelines cater for this (e.g. SPIRIT, PRISMA-P). But other disciplines don’t typically have a protocol culture, and some guidelines are harder to use for writing protocols than others. Because we wanted our intervention to be transferable between reporting guidelines, we decided not to specify a stage of work and to use the term “early as possible” instead.\n\n\n3. Specify the target behaviour\nHaving decided that we wanted “authors to use the full guidance as early as possible” it was time to specify this behaviour in more detail. Michie et al. suggest defining who needs to perform the behaviour, what do they need to do, when will they do it, where will they do it, how often will they do it, and with whom they will do it.\nOur final definition of our target behaviour was: Researchers should use reporting guidance as early as possible in their research pipeline. They will do this for every piece of research, at their place of work, on their own but in the context of collaboration\nWe broke this key behaviour into two sub behaviours:\n\nEngage with reporting guidelines as early as possible (ie access and read them) and,\nApply the guidance to their writing as intended by the guideline developer.\n\nBy “research pipeline” we mean the many steps involved in a typical project which may include ideation, obtaining funding and ethics permission, designing, writing a protocol, drafting a manuscript, editing a manuscript, submitting a manuscript. Instead of specifying which stage reporting guidance should be used, we decided to specify that we want authors to use guidance as “early as possible”. We did this for a few reasons. Firstly, guidelines differ in how easily they can be used for writing protocols or applications. Secondly, researchers will naturally come across reporting guidelines at different stages of their work. Should a researcher discover a guideline at the point of journal submission, then we would still want them to apply the guidance then, even though this is a relatively late stage. But by specifying “as early as possible”, we declare our hope that next time that same researcher may decide to use a guideline at an earlier point. And finally, not all research projects will begin with a written funding application, ethics application, or protocol.\nBy “apply”, we refer to using guidance to plan, write or edit a written description of research (e.g. within a manuscript or application). Applying guidance may include the use of tools like templates or checklists. Participants discussed specifying “completing a reporting checklist” as a target behaviour, but decided against it as previous research showed that authors who complete checklists upon submission don’t necessarily edit their manuscript or comply with guidelines. Participants also recognised that focussing on checklists may be problematic because checklists appear administrative, are used after a manuscript has been written, at which point authors are least able or motivated to edit their work. Nevertheless, participants recognised that checklists will continue to be an important part of how reporting guidance are disseminated, and so they are included within the term “apply the guidance”, without being named.\nWe felt it important to specify that guidance should be applied “as intended by the guideline developer” as my previous research showed that sometimes authors who misinterpret guidelines may believe that they are adhering when they are not.\nHence our target behaviour definition specifies the who, what, when, where, how often, and with whom but is broad enough to account for differences between researchers’ working practices and reporting guidelines.\n\n\n4. Identify what needs to change\nThis step involved identifying what needs to change in the person and/or environment to achieve our target behaviour. Michie et al provide a questionnaire to facilitate this step, called the COM-B Questionnaire, which asks you to consider each COM-B domain in turn and to consider 23 items like “To perform the target behaviour, authors would…have to know more about why it was important”, “…have more time to do it”, and “…have more support from others” etc.\nMichie et al. emphasise the importance of evidence in this step, recommending that data should be “collected from as many relevant sources as possible” and “triangulated”, as a consistent picture of behaviour from multiple sources will “increase confidence in the analysis”[2]. Consequently, we included all of the factors that I identified from my previous PhD work (see chapters 3) and used the COM-B questionnaire to label each as being driven by capability, opportunity, or motivation.\nThe result was a set of 32 factors that we felt needed to change for our target behaviour to occur. These can be seen in Appendix A.\n\n\n5 & 6. Identify Intervention Functions and Policy Categories\nHaving defined our target behaviour and identifying what needs to change for that behaviour to occur, the next step was to consider how to achieve those changes. Michie et al suggest that it is important “considering the full range” of possible intervention functions and policy categories available. See chapter 7 for a fuller introduction to these terms but, briefly, an intervention function is a “category of means by which an intervention can change behaviour” and policy categories are options for delivering those functions. For example, the function modelling could be delivered through a communication campaign or a service.\nMichie et al. [2] recommend using the APEASE criteria to prioritze options, which stands for Affordability, Practicability, Efficacy, Acceptability, Side-effects, and Equity. They note that whilst effectiveness is key, the other criteria must also be considered as “behaviour change interventions operate within a social context”.\nWe saw Enablement, Education, Training, Persuasion, Modelling, and Environmental Restructuring as ranking favourably on all APEASE criteria. Of these, Enablement was viewed as a particularly low-hanging fruit that would likely be effective and welcomed by the research community.\nWe found the remaining intervention functions problematic. Incentivization and restriction (e.g. rewarding guideline adherence with funding or reduced article processing, or punishing non-adherence by withholding these benefits) were seen as inequitable because as long as guideline adherence is harder for some researchers than others, less-experienced, poorly resourced researchers would be at a disadvantage, as would those working in disciplines where reporting guidelines are poorly designed or harder to apply. Conversely, we felt that enablement has potential to reduce existing inequalities. In addition to being inequitable, participants voiced that restriction or punishment would be unacceptable to researchers, as would coercion (the threat of punishment). Furthermore, threats without enforcement may become known as pointless administration, lose effectiveness, and erode trust.\nRegarding Policy Categories, we saw environmental planning as the most affordable, effective, acceptable, safe, and equitable. When talking about the environment, we were really talking about the digital environment, within which EQUATOR has control over its own website but not websites or digital services run by others. Improving and extending the website was viewed as an affordable investment, that would be welcomed by authors (thus acceptable), and a practical way to increase the proportion of authors engaging with reporting guidelines that would increase equity without side effects.\nWe recognised communication as a favourable policy category that is already utilised. Communication channels included the website, mailing lists, social media, conferences, and publications. Whilst communication was rated as affordable, practicable, acceptable, safe, and equitable, its effectiveness may be limited. Although communication could promote awareness of reporting guidelines are available, what they are, and how they can be used, EQUATOR members noted that the efficacy of a communications campaign may be undermined if the campaign is directing authors to a website that is difficult to use, or guidelines that are difficult to access or understand. Thus communication on its own might not be sufficient, and members suggested that it should come after improving the digital environment.\nService provision was also favoured, as long as the service was financially sustainable. So too where guidelines, specifically guidance to help reporting guideline developers create and disseminate resources. Participants felt that Legislation, Regulation, and Fiscal Measures would not be acceptable to researchers and were not not practical options for EQUATOR to use.\n\n\n7. Identify behaviour change techniques\nThe aim of step 7 is to identify possible behaviour change techniques by systematically considering items from a taxonomy of 93 techniques [5] for each intervention function chosen in step 5. Michie et al suggest doing it this way because “the process of designing behaviour change interventions usually involves first of all determining the broad approach that will be adopted and then working on the specifics of the intervention design. For example, when attempting to reduce excessive antibiotic prescribing one may decide that an educational intervention is the appropriate approach. Alternatively, one may seek to incentivise appropriate prescribing or in some way penalise inappropriate prescribing. Once one has done this, one would decide on the specific intervention components.”. So Michie et al. suggest that intervention designers will select only one or two intervention functions, and then consider all behaviour change techniques relevant to that function.\nBut in our case, picking a “broad approach” didn’t feel helpful. Given that a system already exists for disseminating reporting guidance, and we found ourselves considering how we could use multiple intervention functions to refine the existing system. Asking EQUATOR to consider the full behaviour change technique taxonomy for multiple intervention functions was impractical.\nAdditionally, although the taxonomy is designed to be applicable to a range of contexts and intervention types, it didn’t always feel like a perfect fit for our needs. Some techniques are explicitly health-focussed (e.g. Body changes, Pharmacological support, and Information about health consequences) whereas for others the link with health interventions came from the examples provided. For example, the technique practical social support lists the example of “Ask the partner of the patient to put their tablet on the breakfast tray so that the patient remembers to take it”. It’s not immediately obvious how this technique could generalise to our target behaviour. So before choosing techniques, we first had to “translate” the taxonomy to work out how it might be applied to reporting guidelines. The taxonomy developers perhaps acknowledge this limitation, suggesting in their discussion that the list can be viewed as a “core” taxonomy that can be modified or extended according to context [5].\nHence, given that we didn’t want to choose a “broad approach”, and given that this step required familiarity with the taxonomy and how it can be adapted to our context, doing this step with EQUATOR staff would have been very time consuming, as others have also found (e.g., [6]).\nInstead, I did this step on my own. I was well placed to do this because I was familiar with the taxonomy and the Behaviour Change Wheel Guide, I had lead all workshops, and I was most familiar with the barriers we were trying to address. I went through each intervention function that the group had favoured in step 5 (Enablement, Education, Training, Persuasion, Modelling, and Environmental Restructuring) and considered all relevant behaviour change techniques.\nTODO: I could better explain that this was iterative. I did it before the focus groups and then again afterwards where I went “backwards” from ideas back to BCTs.\n\n\n\n\n\n\n\nIntervention Function\nPossible Behaviour Change Techniques\n\n\n\n\nEnablement\nSocial support (practical)Adding objects to the environmentRestructuring the physical environment (which I took to include the digital environment)Reduce negative emotionsSelf-monitoring of outcome(s) of behaviourFraming/reframingIdentification of self as role modelSalience of consequencesAnticipated regretVicarious consequences\n\n\nEducation\nInformation about social and environmental consequencesInformation about health consequences (which I took to mean other people’s health)Feedback on behaviour (e.g. feedback on guideline use)Feedback on outcomes of the behaviour (e.g. feedback regarding quality of reporting from editors, reviewers, or colleagues)Self monitoring of outcome(s) of behaviour (e.g. checking one’s own work against guidance)Information about emotional consequences (e.g. telling authors they how they will feel)Information about others’ approval\n\n\nTraining\nInstruction on how to perform the behaviourDemonstration of the behaviourFeedback on the behaviourFeedback on the outcome(s) of the behaviourSelf-monitoring of outcome(s) of the behaviour\n\n\nPersuasion\nCredible sourceInformation about social and environmental consequencesInformation about health consequencesFeedback on behaviourFeedback on outcome(s) of the behaviourMonitoring outcome(s) of behaviour by others without feedbackIdentification as self as role modelInformation about emotional consequencesSalience of consequencesInformation about others’ approvalSocial comparisonFraming/reframingRemove aversive stimulus\n\n\nModelling\nDemonstration of the behaviour\n\n\nEnvironmental Restructuring\nAdding objects to the environmentPrompts/cuesRestructuring physical environment\n\n\n\n\n\n\n\n8. Identifying delivery options\nI then had to decide how to deliver our favoured intervention functions and policy options. Michie et al.’s guidance for this step is more open ended [2]; although they offer delivery options for communication as an example intervention function, there is no framework or systematic approach to this step and instead they recommend that designers consider the delivery options that they have at their disposal. For EQUATOR, that meant developing a new service or training programme, running a communication campaign, developing new guidance, or improving their website. It was important that my choice was doable within my funding window, and I wanted it to have a lasting impact after my DPhil was finished.\nCreating a new service felt unsustainable as it would likely stop once my funding ran out. Developing training or guidance for guideline developers could be useful and acceptable but not achievable within my time constraint. I could have developed a communications campaign, but EQUATOR members felt this was something they could do independently and that it should be done after addressing other barriers.\nInstead, refining and extending the existing EQUATOR website felt like the perfect choice for multiple reasons. Firstly, planning the digital environment was ranked most highly as a policy category. Secondly, the website can be used as a means of delivering many of the highly ranked intervention functions (enablement, education, persuasion, modelling). Thirdly, it spoke to my skills as a software developer and is something that the UK EQUATOR staff could not do on their own. And finally, these changes could be made within the time limit of my PhD and, importantly, the impact would be sustained after I finish."
  },
  {
    "objectID": "chapters/8_workshops/index.html#discussion",
    "href": "chapters/8_workshops/index.html#discussion",
    "title": "Following the BCW Guide: Workshops with EQUATOR",
    "section": "Discussion",
    "text": "Discussion\nFollowing Michie et al.’s guide helped us to specify our target behaviour, understand the behavioural drivers behind 32 barriers to our target behaviour, select intervention functions, policy categories, behaviour change techniques, and delivery options.\nMore fundamentally, the process helped the workshop participants to begin thinking about reporting guidelines as a behaviour change intervention. The process helped us break down the differences between tasks (e.g., writing vs. editing vs. reviewing research articles), users (e.g., inexperienced vs. experienced researchers, editors, reviewers), resources (e.g., example and elaboration documents vs. checklists). It helped workshops participants to look at the current system objectively and it helped us challenge our preconceptions.\nOne of the most interesting parts of this process was witnessing an unexpected change of opinion amongst EQUATOR staff. Before beginning this study, a common refrain heard around the office was that in order for reporting guidelines to be successful editors had to start enforcing them and refuse to publish research that didn’t adhere. So it was fascinating to see that workshop participants unanimously rated restriction and coercion as their least favourite options.\nI think two things happened here. Firstly, having discussed the barriers that authors face when trying to use reporting guidelines, participants felt that forcing authors to use them would be unacceptable to authors, impractical for editors, and inequitable as some authors would face larger hurdles than others. Secondly, participants reassessed things they had taken for granted, and realised that there are many low-hanging fruit that could make guidelines easier to find and use, and that these fruit were growing in their own orchard.\nSometimes it was difficult to get participants to think “outside of the box”. The default was to think about how the existing system could be improved, and it was difficult to imagine a world where we could be starting from scratch. In one sense this was an opportunity, as improving a system that already exists is easier than creating something totally new. But it could also be seen as a limitation, as our imagination may have been limited by what already exists.\nWe may also have been limited by group-think #REF. All workshop participants had worked for the EQUATOR Center for many years and had many shared opinions and experiences. Including other stakeholders in these workshops would have helped address this but would have been impractical to coordinate. To mitigate this, I decided to gather input from guideline developers, publishers, and authors through separate pieces of work which I describe in chapters 9 and 11.\nIn conclusion, by working through Michie et al’s suggested approach to applying the Behaviour Change Wheel [2] in a series of workshops with members of the EQUATOR Network, we defined our target behaviour as “Researchers should use reporting guidance as early as possible in their research pipeline. They will do this for every piece of research, at their place of work, on their own but in the context of collaboration”. We broke this down into two sub behaviours: 1) engage with reporting guidelines as early as possible (ie access and read them) and, 2) apply the guidance to their writing as intended by the guideline developer. We identified 32 factors that could affect this target behaviour. We favoured Enablement, Education, Training, Persuasion, Modelling, and Environmental Restructuring as intervention functions, and we favoured Environmental Planning, Communication, Service Provision, and Guidelines as policy categories. I identified #TODO behaviour change techniques that could be used, and we decided that our focus (and the focus of the rest of my thesis) should be on refining and extending the EQUATOR website.\n\n\n\n\n\n\n\n1. Michie S, van Stralen MM, West R (2011) The behaviour change wheel: A new method for characterising and designing behaviour change interventions. Implementation Science 6:42\n\n\n2. Michie S, Atkins L, West R (2014) The Behaviour Change Wheel: A Guide to Designing Interventions. Silverback Publishing, London\n\n\n3. M.Given L (2008) The SAGE Encyclopedia of Qualitative Research Methods. https://doi.org/10.4135/9781412963909\n\n\n4. Lincoln YS, Guba EG (1985) Naturalistic Inquiry. SAGE\n\n\n5. Michie S, Richardson M, Johnston M, Abraham C, Francis J, Hardeman W, Eccles MP, Cane J, Wood CE (2013) The behavior change technique taxonomy (v1) of 93 hierarchically clustered techniques: Building an international consensus for the reporting of behavior change interventions. Annals of Behavioral Medicine: A Publication of the Society of Behavioral Medicine 46:81–95\n\n\n6. Carvalho F (2020) Participatory Design for Behaviour Change: An Integrative Approach to Improving Healthcare Practice Focused on Staff Participation. PhD thesis"
  },
  {
    "objectID": "chapters/7_bcw/index.html",
    "href": "chapters/7_bcw/index.html",
    "title": "Selecting the Behaviour Change Wheel framework",
    "section": "",
    "text": "In my introductory chapter I presented how I view reporting guidelines to be part of a complex behaviour change intervention. But that isn’t how I have always viewed them. When I began my PhD, I didn’t think about how guidance was communicated, whether through publications, example & elaboration documents, or checklists, and the differences between those resources. I didn’t think about the websites authors used to find those resources. I didn’t think about how or when authors used guidance. I merely thought of reporting guidelines as similar sets of recommendations that authors should adhere to.\nBut after reading accounts of authors struggling with the guidance content, access, formats, workflows, confidence, and the behaviour of others, I began to think beyond the guidance text, and more about the system. I began to recognize the complexity that came from the number of different stakeholders involved, differences between guidelines, the skills and prior knowledge an author must have to act on the guidance, and the broad variation in how, when, and why guidelines are used.\nI began looking for a framework that could help me understand and improve this complex system. I considered the MRC guidance for developing and evaluating complex interventions[1] but its guiding principles were too broad. I also considered Person Based Approach (PBA)[2]. The MRC guidance and person based approach both emphasise the importance of understanding context, evidencing needs, and iterative evaluation and refinement. Both recommend involving stakeholders and the intervention recipients. I have adhered to these principles in my thesis: in chapters 3 I collect evidence for needs and context, in chapters 8, 9, and 11 I explain how I involved stakeholders and users, and in chapters 11 and 12 I explain how I refined the intervention in response to author feedback and possible next steps for iterative development.\nDECIDE: Co-design as a Participatory Action Research https://onlinelibrary.wiley.com/doi/10.1111/medu.14411 emphasises consumer involvement from the get go (which I didn’t really do) but also equal weight to their voice and empathy (which I did try to do).\nHowever, both the MRC and PBA guidance require intervention designers to theorize how the intervention is working. To do this, designers need to select a theoretical framework. First I considered the Theoretical Domains Framework (TDF, [3]; [4]). The TDF was developed to help implementation researchers identify what influences the behaviour of health professionals and patients. It was made by synthesising 33 theories of behaviour and behaviour change into 14 domains that cover the cognitive, affective, social, and environmental influences on behaviour. The TDF and its accompanying guide [5] are useful, but they require considerable understanding of psychological constructs that might not be familiar to non-behavioural experts, and although the TDF helps understand what influences behaviour, there is little guidance on how to then go about changing that behaviour.\nThe Behaviour Change Wheel [6] addresses these limitations. It is designed for “policy makers, intervention designers, researchers and practitioners…from a wide range of disciplines with varying levels of expertise”. At its core is the COM-B model of behaviour; a simpler and more intuitive model which can be mapped to the TDF. The BCW then goes beyond the TDF by helping intervention designers select options for changing behaviour.\nWhen making COM-B, Michie et al began with motivation, defined as: “brain processes that energize and direct behaviour” [7]. Their next step was “to consider the minimum number of additional factors needed to account for whether change in the behavioural target would occur, given sufficient motivation” [6]. They used a US consensus meeting of behavioural theorists [8] and a centuries-old principle of criminal law: that to prove guilt one must show means or capability, opportunity, and motive. Michie et al. argued that the “common conclusion from these two separate strands of thought lends confidence to this model of behaviour”[6], and they added further confidence by mapping COM-B to the TDF. When describing COM-B, Michie et al. explain how the components capability, opportunity, and motivation interact with each other to generate behaviour which, in turn, may influence these components. (See Figure 1)\n\n\n\nFigure 1: The COM-B system for understanding behaviour, reproduced with permission from [6]\n\n\nThe outer rings of the BCW (the intervention functions and policy categories) were made by synthesizing 19 existing frameworks to identify. In doing so, they systematically created a framework which is comprehensive (in that it covers all intervention options), conceptually coherent (categories are all of the same type), and linked to an overarching model of behaviour. This was one of their core objectives, as they had identified that previous frameworks often lacked a model of behaviour, were not comprehensive, and lacked coherence. For example, they criticise MINDSPACE [9] (which was popular amongst UK policy makers at the time) for not including all intervention types and for conflating psychological constructs with mechanisms of action and modes or delivery.\nThe COM-B model at the center of the BCW holds that behaviour arises from a system that involves three components: Capability, Opportunity, and Motivation. These are defined in Table 1 but, briefly; capability includes having the required knowledge; opportunity includes the necessary resources such as time, materials, or reminders; and motivation means having the required intentions, beliefs, wants and needs to perform the behaviour. Changing behaviour requires altering one or more of these components.\nAn intervention may have multiple functions. For example, a television advert about drink driving may educate its audience with information about alcohol limits and persuade using emotive imagery. In this example, education is being used as an intervention function to target psychological capability (teaching people how much they can safely drink before driving) and persuasion is being used to target motivation (targeting people’s emotions to make them less likely to want to engage in drink driving). The functions you choose depend on the behavioural drivers you are trying to target. The different types of intervention function are defined in Table 2.\nHaving decided what functions your intervention needs, the guide helps you decide how to implement them with policy categories including communication/marketing, guidelines, regulation, and service provision. Policy categories are defined in Table 3.\nRecognizing that intervention designers come from various disciplines and places of work, Michie et al. published a book to help designers apply the BCW. This book contains suggested steps, exercises, and worksheets. In the next chapter, I describe how I guided members of the UK EQUATOR Centre through this process.\n\nhttps://tabletomarkdown.com/convert-spreadsheet-to-markdown/\n\n\nTable 1: COM-B model components and examples, reproduced with permission from the Behaviour Change Wheel guide [10]\n\n\n\n\n\n\nCOM-B model component: Definition\nExample\n\n\n\n\nPhysical capability: Physical skill, strength, or stamina\nHaving the skill to take a blood sample\n\n\nPsychological capability: Knowledge or psychological skills, strength, or stamina to engage in the necessary mental processes\nUnderstanding the impact of carbon dioxide on the environment\n\n\nPhysical opportunity: Opportunity offered by the environment involving time, resources, locations, cues, physical ‘affordance’\nBeing able to go running because one owns appropriate shoes\n\n\nSocial opportunity: Opportunity afforded by interpersonal influences, social cues and cultural norms that influence the way that we think about things, e.g., the words and concepts that make up our language\nBeing able to smoke in the house of someone who smokes but not in the middle of a board meeting\n\n\nReflective motivation: Reflective processes involving plans (self-conscious intentions) and evaluations (beliefs about what is good and bad)\nIntending to stop smoking\n\n\nAutomatic motivation: Automatic process involving emotional reactions, desires (wants and needs), impulses, inhibitions, drive states and reflex responses\nFeeling anticipated pleasure at the prospect of eating a piece of chocolate cake\n\n\n\n\n\n\nTable 2: Intervention function definitions and examples, reproduced with permission from the Behaviour Change Wheel guide [10]\n\n\n\n\n\n\n\nIntervention Function\nDefinition\nExample of intervention function\n\n\n\n\nEducation\nIncreasing knowledge or understanding\nProviding information to promote healthy eating\n\n\nPersuasion\nUsing information to induce positive or negative feelings or stimulate action\nUsing imagery to motivate increases in physical activity\n\n\nIncentivisation\nCreating an expectation of reward\nUsing prize draws to induce attempts to stop smoking\n\n\nCoercion\nCreating an expectation of punishment or cost\nRaising the financial cost to reduce excessive alcohol consumption\n\n\nTraining\nImparting skills\nAdvanced driver training to increase safe driving\n\n\nRestriction\nUsing rules to reduce the opportunity to engage in the target behaviour (or to increase the target behaviour by reducing the opportunity to engage in competing behaviours)\nProhibiting sales of solvents to people under 18 to reduce use for intoxication\n\n\nEnvironmental restructuring\nChanging the physical or social context\nProviding on-screen prompts for GPs to ask about smoking behaviour\n\n\nModelling\nProviding an example for people to aspire to or imitate\nUsing TV drama scenes involving safe-sex practices to increase condom use\n\n\nEnablement\nIncreasing means/reducing barriers to increase capability (beyond education and training) or opportunity (beyond environmental restructuring)\nBehavioural support for smoking cessation, medication for cognitive deficits, surgery to reduce obesity, prosthesis to promote physical activity\n\n\n\n\n\n\nTable 3: Policy categories, reproduced with permission from the Behaviour Change Wheel guide [10]\n\n\n\n\n\n\n\nPolicy Category\nDefinition\nExample\n\n\n\n\nCommunication/marketing\nUsing print, electronic, telephonic or broadcast media\nConducting mass media campaigns\n\n\nGuidelines\nCreating documents that recommend or mandate practice. This includes changes to service provision\nProducing and disseminating treatment protocols.\n\n\nFiscal measures\nUsing the tax system to reduce or increase the financial cost\nIncreasing duty or increasing anti-smuggling activities\n\n\nLegislation\nMaking or changing laws\nProhibiting sale or use\n\n\nEnvironmental/social planning\nDesigning and/or controlling the physical or social environment\nEstablishing support services in workplaces, communities etc.\n\n\n\n\n\n\n\n\n\n\n\n\n1. Skivington K, Matthews L, Simpson SA, et al (2021) A new framework for developing and evaluating complex interventions: Update of Medical Research Council guidance. BMJ n2061\n\n\n2. Yardley L, Morrison L, Bradbury K, Muller I (2015) The Person-Based Approach to Intervention Development: Application to Digital Health-Related Behavior Change Interventions. Journal of Medical Internet Research 17:e4055\n\n\n3. Cane J, O’Connor D, Michie S (2012) Validation of the theoretical domains framework for use in behaviour change and implementation research. Implementation Science 7:37\n\n\n4. Michie S, Johnston M, Abraham C, Lawton R, Parker D, Walker A (2005) Making psychological theory useful for implementing evidence based practice: A consensus approach. BMJ Quality & Safety 14:26–33\n\n\n5. Atkins L, Francis J, Islam R, et al (2017) A guide to using the Theoretical Domains Framework of behaviour change to investigate implementation problems. Implementation Science 12:77\n\n\n6. Michie S, van Stralen MM, West R (2011) The behaviour change wheel: A new method for characterising and designing behaviour change interventions. Implementation Science 6:42\n\n\n7. Mook DG Motivation : The organization of action. (No Title) \n\n\n8. Fishbein M, Triandis HC, Kanfer FH, Becker M, Middlestadt SE, Eichler A, et al (2001) Factors influencing behavior and behavior change. Handbook of health psychology 3:3–17\n\n\n9. Dolan P, Hallsworth M, Halpern D, King D, Vlaev I (2010) MINDSPACE: Influencing behaviour for public policy. \n\n\n10. Michie S, Atkins L, West R (2014) The Behaviour Change Wheel: A Guide to Designing Interventions. Silverback Publishing, London"
  },
  {
    "objectID": "data/iv_changes.html",
    "href": "data/iv_changes.html",
    "title": "Describe each reporting guideline fully",
    "section": "",
    "text": "When authors first encounter reporting guidelines they may need to know:\n\nwhat reporting guidelines are\nhow and when to use them\nwhy authors should use them, including:\n\nwhat personal benefits to expect\nthe importance to others.\n\n\nDescriptions could be succinct (e.g. on journal instruction pages) or long (e.g. in publications) JH\nA generalised description can go where authors first encounter reporting guidelines e.g., journal author guidelines, EQUATOR’s home page.\nA reporting guideline-specific description could go at the top of guidance documents, checklists, and templates.\n\nConsider specifying whether the reporting guideline is also a design guideline.\nSpecify whether the reporting guidelines are just guidelines, or whether they are intended to be requirements. Name the resource appropriately - words like guideline, standards, criteria, recommended, preferred, and templates, have different meanings.\n\n\n\nid: ready-to-use title: Make resources ready-to-use barrier_ids: [need-usable-formats, need-enough-time] intervention_fn_ids: [enablement] stakeholder_ids: [developers, equator] stage: creation idea_count: 1\n\nEnsure resources are ready-to-use e.g., checklists as Word files, not as tables within published articles.\n\nid: accessible title: Make resources accessible barrier_ids: [need-accessible] intervention_fn_ids: [enablement] stakeholder_ids: [developers, equator] stage: dissemination idea_count: 1\n\nEnsure resources are open access. This allows access to authors without journal subscriptions and allows others to build upon the guidance.\n\nid: citation title: Show and encourage citations barrier_ids: [what-rgs-exist, believed-benefits] intervention_fn_ids: [education, persuasion] stakeholder_ids: [developers, equator, publishers, conferences, preprints] stage: dissemination idea_count: 2\n\n\nDisplay usage data (like citations or downloads) alongside the guidelines as a form of social proof.\nEncourage authors to cite the reporting guideline so readers discover it.\n\n\nid: avoid-prescribing-structure title: Avoid prescribing structure barrier_ids: [need-to-reconcile, need-concise-writing] intervention_fn_ids: [enablement] stakeholder_ids: [developers] stage: development idea_count: 3\n\n\nAvoid prescribing structure of a journal article as it may clash with journal requirements or other reporting guidelines.\nInstead, give options for where items can be reported.\nInclude options beyond the article body where authors can report information, like tables, figures, or appendices be.\n\n\n\nid: create-early-guidance title: Create reporting guidance for early stages of research barrier_ids: [need-tools, need-right-time] intervention_fn_ids: [education] stakeholder_ids: [developers] stage: planning idea_count: 1\n\nConsider creating reporting guidance to help authors write protocols, funding applications, and ethics applications.\n\nid: avoid-proliferation title: Avoid confusing authors with too many reporting guidelines barrier_ids: [what-rgs-exist, need-to-reconcile, best-fit] intervention_fn_ids: [enablement] relation_ids: [information-architecture, easy-understand] stakeholder_ids: [developers, equator, publishers] stage: planning idea_count: 6\n\n\nTo avoid duplicating resources, before commencing a new reporting guideline:\n\nconsult EQUATOR’s register of reporting guidelines under development;\n\nEQUATOR could make this register easier to find and search;\n\ncontact the developers of related reporting guidelines;\njournals could ask reporting guideline developers to prove that they have registered their guideline with EQUATOR (like they do for clinical trials).\n\nWhen a new reporting guideline is justified, build upon existing reporting guidelines instead of starting from scratch. This could mean extending or replacing subsets of items instead of publishing a totally new reporting guideline.\nConsider making modular guidance. For instance, the Journal Article Reporting Standards (JARS) are a set of reporting guidelines for psychology. JARS has a main guideline for quantitative studies. The Design item can be extended by one of three modules depending on whether the study involved experimental manipulation or was conducted on a single individual. the experimental manipulation module can be further extended by modules for random assignment, non-random assignment, and clinical trials. Other extension modules exist for longitudinal and replication studies.\n\n\nid: budget-and-fund-reporting title: Budget for reporting barrier_ids: [need-enough-time, feel-not-a-job] intervention_fn_ids: [enablement] stakeholder_ids: [funders, institutions] stage: ongoing idea_count: 1\n\nFunders and research supervisors could encourage researchers to allocate sufficient time and money for documenting and reporting results of their research.\n\nid: easy-understand title: Make reporting guidelines easy to understand barrier_ids: [understanding, need-translations] intervention_fn_ids: [education, enablement] stakeholder_ids: [developers] stage: creation idea_count: 5\n\n\nUse plain language.\nDefine key terms.\nUse consistent terms across related resources.\nProvide translations.\nUpdate guidance in response to user feedback.\n\n\nid: create-rewards title: Create rewards barrier_ids: [care-about-benefits] intervention_fn_ids: [incentivization] stakeholder_ids: [developers, equator, publishers, funders, institutions, preprints, registries] stage: ongoing idea_count: 6\n\nStakeholders could create new rewards:\n\njournals could fast-track submissions or review for papers that followed a reporting guideline,\njournals could offer discounts on article processing charges for papers that followed a reporting guideline,\njournals, preprint servers, or peer review platforms could badge well reported articles,\nEQUATOR could offer a certification service,\nfunders could reward good reporting financially,\ninstitutions could offer prizes for good reporting.\n\n\nid: create-spaces title: Create discussion spaces barrier_ids: [understanding, feel-patronized, believed-benefits, how-to-report, how-to-do] intervention_fn_ids: [enablement, persuasion] stakeholder_ids: [equator, developers, institutions] stage: ongoing idea_count: 7\n\nCreate spaces for authors to discuss reporting and reporting guidelines. These could be:\n\nonline (forums, social media, email),\nor offline (meet-ups, clubs).\n\nTry to make spaces accessible to researchers from all nationalities, professional disciplines and other demographics. Spaces will allow authors to:\n\nsolicit help,\nshare experiences,\nprovide feedback to guideline developers,\nand cultivate a feeling of inclusivity and community ownership.\n\n\nid: early-acquisition title: Create ways to catch authors earlier barrier_ids: [need-prompts, need-right-time, need-enough-time, when-to-use] intervention_fn_ids: [education, enablement, environmental-restructuring] relation_ids: [create-tools, create-early-guidance] stakeholder_ids: [equator, developers, publishers, funders, ethics, institutions, conferences, preprints, registries] stage: ongoing idea_count: 4\n\n\nConsider creating email campaigns to prompt researchers at early stages.\nThe EQUATOR website could encourage visitors to use reporting guidelines for planning and drafting research.\nWebsites could be optimised for search terms like “how to write [study type]”, “protocol”, “research plan” or “funding application”. For example, reporting guideline pages on EQUATOR’s website rank highly in Google searches for “STROBE checklist” but not “How to write an observational epidemiology study”.JH\nWriting clubs and writing training could flag reporting guidelines.\n\n\nid: design-agnostic title: Keep reporting guidelines agnostic to design choices barrier_ids: [feel-restricted, feel-transparent, believed-costs, what-are-rgs] intervention_fn_ids: [enablement, persuasion] relation_ids: [keep-short, value-statement] stakeholder_ids: [developers] stage: development idea_count: 3\n\n\nAsk authors to describe methods transparently without making assumptions about, or prescribing, methods or design choices. For example, an instruction to “describe how you determined your sample size” may be more helpful than “report your sample size calculation” for authors who encounter checklists at submission and did not perform a sample size calculation before collecting data.\nAvoid recommending or admonishing design choices within the reporting guidance because:\n\ndoing so may make authors feel nervous or ashamed, and therefore less likely to report transparently;\ndesign advice elongates reporting guidelines;\nincluding design advice may give the impression that the reporting guideline is for designing or appraising design.\n\nConsider linking to external design or appraisal tools instead.\n\nid: persuade title: Use persuasive language and design barrier_ids: [feel-transparent, feel-patronized, believed-benefits, feel-not-my-job] intervention_fn_ids: [persuasion] stakeholder_ids: [developers, equator, publishers, funders, ethics, institutions, conferences, registries, preprints] stage: creation idea_count: 6\n\n\nUse language and design to communicate confidence and simplicity as opposed to judgement and complexity.\nEncourage explanation even when choices were unusual or sub-optimal.\nReassure authors that most research has limitations that can be addressed in Discussion sections.\nReassure authors that reporting guidelines are just guidelines.\nAvoid patronizing authors.\nConsider wording instructions directly at the intended user.JH\n\n\nid: create-tools title: Create additional tools barrier_ids: [need-tools, need-right-time] intervention_fn_ids: [enablement] stakeholder_ids: [developers, equator, funders, ethics, publishers] stage: creation idea_count: 12\n\nCreate tools for different tasks:\n\ndiscussion points for planning research in the order decisions are made\nto-do lists for conducting research in the order data is collected\ntemplates for drafting\nwriting assistance tools (e.g., COBWEB)\nchecklists for checking manuscripts that are easy to fill out, update, and cross-check\ntools for co-researchers to check each other’s work\ntools for generating tables and figures\nresources for peer reviewers who wish to review reporting quality including:\n\nguidance specifically for peer reviewers.\ncommonly-used words that reviewers can search for to quickly find relevant text.JH\nsuggested text that peer reviewers can copy to request information\ntools to generate feedback reports\n\njournal articles where reporting guideline items are annotated/highlighted\n\n\nid: findable-resources title: Make resources easy to discover and find barrier_ids: [what-rgs-exist, what-resources-exist, need-findable, best-fit] intervention_fn_ids: [environmental-restructuring] stakeholder_ids: [developers, equator] stage: creation idea_count: 11\n\nLink resources:\n\nEnsure all resources link to each other. For example, checklists should link to example and elaboration documents and vice versa.\nRelated reporting guidelines should link to each other.\nReporting guidelines and resources should link to translations\nLinks should be permanent (e.g. DOIs) where possible and old links should be maintained or redirected. Broken links should be replaced.\n\nMake searching easy:\n\nHost resources somewhere consistent, like the EQUATOR Network website and database.\nProvide easy-to-use website search functions\nWeb pages should be optimized for search engines JH\nCreated curated collections for study types\nCreate decision tools for identifying reporting guidelines\n\nNames reporting guidelines to make them easy to discover and find:\n\n\nReporting guideline names could be descriptive, as acronyms may be meaningless to novice users.\nRelated reporting guidelines should use consistent names to show relationships (e.g. PRISMA and PRISMA-P appear more related than CONSORT and SPRIT).\n\n\nid: endorse-enforce title: Endorse and enforce reporting guidelines barrier_ids: [what-rgs-exist, believed-costs] intervention_fn_ids: [education, restriction, persuasion] stakeholder_ids: [publishers, institutions, ethics, funders, registries, conferences, preprints] stage: ongoing idea_count: 3\n\nStakeholders could:\n\nendorse reporting guidelines\nenforce their use by mandating checklists or (preferably) checking adherence to items.\nFunders could ask about reporting guidelines or checklists when collecting updates from grant recipients.\n\n\nid: evidence-benefits title: Evidence the benefits barrier_ids: [believed-benefits] intervention_fn_ids: [persuasion, education] relation_ids: [testimonials, value-statement] stakeholder_ids: [developers, equator, publishers] stage: ongoing idea_count: 2\n\nEvidence any stated benefits:\n\nQuantifiable benefits could be evidenced with data (e.g., acceptance rates, publishing speed, writing speed).\nExperiential benefits could be evidenced by collecting case studies from authors who find that reporting guidelines help them feel confident and write more easily, and from readers who value well-reported research.\n\n\nid: information-architecture title: Make information digestible barrier_ids: [need-enough-time, believed-costs, need-findable] intervention_fn_ids: [enablement] relation_ids: [item-content] stakeholder_ids: [developers, equator] stage: creation idea_count: 7\n\nOrganise information so it is easy to navigate and not overwhelming.\n\nCater to users that read from start to finish, and those that dip in and out.\nStructure text with headings.\nUse section URLs to send authors directly to relevant parts of guidance.\nConsider hyperlinking related resources\nConsider embedding reporting guidelines that “fit together”, like PRISMA and PRISMA-Abstracts\nFor information presented online, consider showing/hiding information as required. For example, if PRISMA-Abstracts were embedded into PRISMA, users could choose to expand or collapse it. Or you could show/hide guidance depending on whether the author is writing a funding application, protocol, manuscript.\n\n\nid: item-content title: Describe reporting items fully barrier_ids: [scope, how-to-report, how-to-do, how-to-report-not-done, need-concise-writing, importance, care-about-benefits, need-to-remove, need-enough-time, need-concise-writing] intervention_fn_ids: [education, persuasion] stakeholder_ids: [developers] stage: development idea_count: 12\n\nFor each item, authors may need to know the following:\n\nWhat needs to be reported – a brief description could go in all resources (checklists, templates etc) with a longer description in the full guideline document.\nWhy the information is important, and to whom\nAny circumstances where the item is not applicable and what to write\nIndicate priority, and any circumstances that modify importance\nWhere the item can be reported, including beyond the main article body (e.g., section, table, figure, appendix)\nWhat to write if an item wasn’t, or couldn’t be done\nWhat to write if an item cannot be reported for external reasons. For example, if items cannot be reported because of intellectual property restrictions.\nExamples, which could be real or generated, including:\n\nexamples of good and bad reporting with explanations.\nexamples of concise or word-count-friendly reporting, perhaps in alternative formats like tables and figures.JH\nexamples of well reported “imperfect” items (items that were not done)\nexamples from different research contexts\n\nLinks to external design or appraisal advice\n\n\nid: apparent-priority title: Make reporting guidelines appear as a priority barrier_ids: [believed-benefits, care-about-benefits] intervention_fn_ids: [persuasion] stakeholder_ids: [publishers, funders, ethics, institutions, preprints, conferences, registries] stage: ongoing idea_count: 3\n\nJournals, funders and ethics committees could make reporting guidelines appear as a priority:\n\nMake them prominent in author instructions.\nPlacing checklists earlier in the PDFs that are automatically created by journal submission systems.\nPublicize when reporting guidelines are used by reviewers.\n\n\nid: promote title: Promote reporting guidelines barrier_ids: [what-are-rgs, what-rgs-exist] intervention_fn_ids: [education] relation_ids: [endorse-enforce, reporting-champions] stakeholder_ids: [institutions, publishers, developers, equator, ethics, funders, societies, registries, conferences, preprints] stage: ongoing idea_count: 4\n\n\nPromote reporting guidelines on and offline.\n\nOnline may include websites, email campaigns, social media, and blogs.\nOffline may include appearing at conferences, seminars, and workshops.\n\nInstitutions could promote reporting guidelines in their curricula, learning materials, or through reporting champions. Reporting guideline developers or EQUATOR could push for reporting guidelines to be included in text books.\nPromotion can begin before a reporting guideline has been published so that researchers know about guidelines being developed.\n\nNB. Promotion is different to endorsement; a journal could run an email campaign to promote reporting guidelines without having an endorsement policy.\n\nid: reporting-champions title: Install reporting champions barrier_ids: [what-are-rgs, benefits, understanding, when-to-use, importance] intervention_fn_ids: [education, enablement] stakeholder_ids: [equator, developers, institutions, funders, ethics, publishers, conferences, preprints, registries] stage: ongoing idea_count: 2\n\nAll stakeholders could have members to promote and facilitate the usage of reporting guidelines.\n\nThis could follow a local network model with EQUATOR as the central organiser.\nCould make use of existing networks, like regional reproducibility networks.\n\nFor each reporting guideline, authors may need the following information:\n\nA clear definition of the reporting guideline’s intended scope in plain language.\nIf-then rules to direct authors to other, more appropriate reporting guidelines. For example, CONSORT could point authors writing protocols to SPIRIT.\nIf no better guidance exists then indicate which items do/do not apply. For example, no guideline exists for authors writing protocols for observational epidemiology. Their best option currently is to use STROBE, but only some items will be required in a protocol.\nWhat tasks the reporting guideline can and cannot be used for\nHow long the resource will take to use\nWhy the guidance should be trusted and link to how it was developed\n\n\nid: keep-short title: Keep guidance short barrier_ids: [need-enough-time, believed-costs] intervention_fn_ids: [enablement] relation_ids: [information-architecture, design-agnostic] stakeholder_ids: [developers] stage: development idea_count: 5\n\nKeep guidance as a short as possible:\n\nBe concise but clear.\nBe realistic about what to expect from authors as each additional item increases the chances an author will be put off\nLink to other guidance elsewhere if desired.\nConsider splitting broad guidance that tries to cater for different options into shorter, modular guidance (modularity avoids duplication).\n\n\nid: testimonials title: Provide testimonials barrier_ids: [benefits, believed-benefits, believed-costs, care-about-benefits, feel-transparent] intervention_fn_ids: [persuasion, modelling] stakeholder_ids: [developers, equator] stage: dissemination idea_count: 5\n\nTestimonials can be short quotes or longer case studies. They could come from:\n\nresearchers who have had positive experiences using reporting guidelines, including researchers that were nervous about transparency,\ndecision makers (e.g., editors/grant managers) that value good reporting and/or check for reporting as part of their evaluation,\npeer reviewers that use reporting guidelines to check for good reporting,\npatients who are affected by research waste,\nand researchers who need to understand, synthesise, or apply research articles.\n\n\nid: support title: Provide additional teaching barrier_ids: [feel-not-a-job, understanding, importance, how-to-report, need-right-time, care-about-benefits] intervention_fn_ids: [education, training] stakeholder_ids: [developers, equator, publishers, institutions, funders, ethics] stage: ongoing idea_count: 5\n\nProvide education or training (e.g., courses, videos) specific to particular reporting guidelines.\nMore generally, students could:\n\nlearn about writing as a process and workflows for documenting and communicating research,\nlearn about research waste from poor reporting,JH\nattempt a replication to learn about the importance of complete reporting,\nand use a reporting guideline as part of their studies.\n\n\nid: updating title: Make updating guidelines easier barrier_ids: [need-up-to-date-guidance, understanding] intervention_fn_ids: [enablement] stakeholder_ids: [equator, funders] stage: ongoing idea_count: 4\n\nUpdate guidance in response to user feedback or changes in the field. This would be easier if:\n\nreporting guideline developers could easily collect feedback from authors.\nsmall updates or refinements could be made without publishing a new article.\nreporting guideline developers had funding to evaluate, refine, and update their resources.JH"
  },
  {
    "objectID": "data/barriers.html",
    "href": "data/barriers.html",
    "title": "Thesis",
    "section": "",
    "text": "id: what-are-rgs title: Researchers may not know what RGs are driver_id: capability —\nResearchers may have never heard the term “reporting guideline” or may misunderstand it. Researchers may more commonly use terms like “writing” or “writing up” and the word “reporting” may get interpreted as a formal task (such as reporting progress to a funder). The word “guideline” may be interpreted by some as rules (as per journal “author guidelines”) and others as recommendations. Some researchers may perceive RGs as a set of design requirements, especially if they only use checklists, which typically lack the instructions and nuances included in the full guidance.\n\nid: what-rgs-exist title: Researchers may not know what RGs exist driver_id: capability\n\nResearchers may not be aware of which reporting guidelines exist. Most guidelines on the EQUATOR site are hardly ever accessed\n\nid: scope title: Researchers may not know whether a RG applies to them driver_id: capability\n\nIf the scope of a RG is undefined or unclear, then researchers won’t know whether the guidance applies to them. Researchers may not understand study designs, making it difficult for them to identify which guidance applies.\n\nid: best-fit title: Researchers may not know what RG is their best fit driver_id: capability relation_ids: [what-rgs-exist, scope]\n\nResearchers may not know when more specific guidance exists. An author’s “perfect fit” guideline may not exist, in which case they may not know know when to stop searching, and they may try to use an “imperfect fit” guideline without understanding which items are applicable.\n\nid: what-resources-exist title: Researchers may not know what resources exist for a RG driver_id: capability\n\nResources include the guidance itself, checklists, E&E files, templates, and web tools (e.g. PRISMA flow chart maker). Not all resources exist for each RG and researchers may be unaware of the ones that do. Many researchers may only use the checklist. Sometimes this is purposeful, but other times it may be because researchers don’t know that full guidance and examples exist.\n\nid: when-to-use title: Researchers may not know when RGs should be used driver_id: capability\n\nResearchers may not know when they should use RGs in their research workflow. Guideline developers may want researchers to use guidance as early as possible, but this is may not be obvious to researchers who may only ever receive instruction to complete a checklist as part of journal submission and may never discover the full guidance. Consequently, researchers may assume that RGs are supposed to be used by single authors as pre-submission checklists to demonstrate adherence. It may not occur to them that RGs can be used earlier, or by teams. Some researchers, having come to this realisation themselves, report wanting to be told to use reporting guidelines earlier in their research.\n\nid: understanding title: Researchers may misunderstand driver_id: capability\n\nResearchers may not understand concepts, terms or words within the guidance, or they may understand them differently to how the developers intended. Some items (or entire guidelines) might be new concepts. E.g. SQUIRE guidelines written at a time where Quality Improvement was still a new concept to many people, and some items (e.g. Context, Study of the intervention) were less familiar than others. Researchers may have nowhere to turn for help should they not understand something.\n\nid: benefits title: Researchers may not know what benefits to expect driver_id: capability relation_ids: [believed-benefits]\n\nResearchers may not know what benefits to expect from using a reporting guideline. These benefits may include:\n\nimproved completeness of reporting which helps readers use research and reduces research waste.\nimproved flow and less “waffle” in writing\nfacilitated discussions between collaborators, especially at the design or protocol stage\npublishing and passing peer review more efficiently\nincreased publisher acceptance rates\nefficient, confident writing\nincreased impact of manuscript, as the article is easier to search for and information within the article is easier to find.\n\n\nid: importance title: Researchers may not know why items are important driver_id: capability relation_ids: [care-about-benefits]\n\nResearchers may not know why an item is important, or who it is important to.\n\nid: how-to-do title: Researchers may not know how to do an item driver_id: capability\n\nResearchers might not know how to do something (e.g., a sample size calculation)\n\nid: how-to-report title: Researchers may not know how to report an item in practice driver_id: capability\n\nResearchers may not understand how to report a particular item in practice\n\nid: how-to-report-not-done title: Researchers may not know what to write when they cannot report an item driver_id: capability\n\nResearchers may not know how to report an item that they did not do (deliberately or as an oversight), or an item that they are unable to report for external reasons (e.g., IP, or data was missing from primary studies).\n\nid: need-enough-time title: Researchers have limited time driver_id: opportunity\n\nGuidelines take time to find, read, understand, and apply. Sometimes they may require time and work from multiple co-authors. Researchers & guideline developers may underestimate the time required for writing, and time is often most limited at the point of submission as grant funding may have run out.\nChecklists take time to complete, and completing them with page numbers or pasted content can be annoying if future edits necessitate updating the checklist too. Checklists also generate work for editors and peer-reviewers who must cross check page numbers or pasted content with manuscript content.\n\nid: need-right-time title: Researchers may not encounter RGs early enough to act on them driver_id: opportunity relation_ids: [need-enough-time]\n\nSome RG items require work that has to be done within a certain time windows such as:\n\nduring planning or designing\nbefore or during data collection\nwhen other colleagues are available\nduring the duration of a grant\n\n\nid: need-translations title: Researchers may not understand the language driver_id: opportunity relation_ids: [understanding]\n\nResearchers may not understand the language guidance is written in. A lot of research comes from countries where English is not the first language, as do a lot of EQUATOR website visitors. Even if a researcher speaks English as a second language, language may be an additional barrier.\n\nid: need-concise-writing title: Researchers may struggle to keep writing concise driver_id: opportunity\n\n\nFollowing a guideline can result in lengthy, bloated reports which are unpleasant to read and breach journals’ word limits. Researchers may not know how to keep writing fluid and concise or where they can report an item (e.g., what section, in the text or in a table or figure, in the manuscript or in supplementary material).\n\nid: need-tools title: Researchers may not have tools for the job at hand driver_id: opportunity\n\nResearchers use reporting guidelines for different tasks and want tools to make that job easier. Researchers report using reporting guidelines for:\n\nPlanning research\nDesigning research\n\nResearchers report wanting items presented in the order in which decisions need to be made\nResearchers report wanting links to resources\n\nWhilst collecting data\n\nResearchers report wanting items ordered in the order they are done\nResaerchers report wanting items embedded into data collection tools\n\nDrafting manuscript\n\nResearchers report wanting templates\n\nChecking manuscripts\nDemonstrating compliance\n\nResearchers report wanting checklists embedded into submission workflows\n\nReviewing the reporting of other people’s manuscripts\nAppraising the quality of other people’s manuscripts\n\n\nid: need-up-to-date-guidance title: RGs can become outdated driver_id: opportunity\n\nGuidelines can become out of date compared to other guidance or compared to current research standards.\n\nid: need-to-reconcile title: Researchers may struggle to reconcile multiple sets of guidance driver_id: opportunity\n\nResearchers must adhere to journal guidelines, multiple reporting guidelines (e.g., PRISMA + PRISMA-Abstracts + PRISMA-S) and other best practice guidelines (like NIH principles). Using multiple guidelines increases complexity and costs, and guidelines can contradict each other.\n\nid: need-to-remove title: Researchers may be asked to remove RG content driver_id: opportunity\n\nResearchers may be asked to remove guideline content by co-researchers, editors or reviewers.\n\nid: need-prompts title: Researchers may forget to use RGs at earlier research stages driver_id: opportunity relation_ids: [need-enough-time, need-right-time]\n\nHaving been told to complete a checklist upon journal submission, researchers may forget to use a RG earlier next time.\nNB forgetting is different to not realising that RGs can be used early.\n\nid: need-findable title: Guidance may be difficult to find driver_id: opportunity relation_ids: [what-rgs-exist]\n\nResearcher should be able to easily find guidance and resources that they believe to exist. However:\n\nsearch functions can be hard to find or use,\nresearchers may not know which search terms to use,\nwebsites may be hard to navigate,\nguidance can be buried within articles,\nresources may not be optimised for search engines,\nand resources may not be in the same place.\n\n\nid: need-accessible title: RGs may be difficult to access driver_id: opportunity\n\nResearchers may be unable to access guidance published in subscription journals. Journal websites can feature broken links.\n\nid: need-usable-formats title: RG resources may not be in usable formats driver_id: opportunity\n\nResources differ in how easy or readily usable they are. For example, some checklists are published as PDF tables that cannot be filled or copied. Some guidance can be dense, unstructured text that is hard to digest or navigate; whereas some researchers will read the guidance sequentially, others may dip in and out whilst writing, and unstructured text can make information harder to find.\n\nid: feel-transparent title: Researchers may feel afraid to report transparently driver_id: motivation relation_ids: [how-to-report-not-done]\n\nResearchers may feel afraid or uncertain when trying to report something that they didn’t (or couldn’t) do.\n\nid: feel-restricted title: Researchers may feel restricted if RGs prescribe design driver_id: motivation\n\nAdvice or assumptions about design choices narrow the scope of the guidance and can make checklists appear prescriptive. Sometimes design assumptions can be implicit. For example, in requiring authors to report the method used to assess risk of bias, PRISMA is implying that authors should have designed their review to assess risk of bias.\n\nid: feel-patronized title: Researchers may feel patronized driver_id: motivation\n\nResearchers can feel patronized by checklists.\n\nid: believed-benefits title: Researchers may not believe stated benefits driver_id: motivation\n\nResearchers may not believe that using a RG will affect their acceptance rate or publication speed, that using a RG will help them write, or improve the quality of their manuscript.\n\nid: care-about-benefits title: Researchers may not care about the benefits of using a RG driver_id: motivation\n\nResearchers may understand that RGs aim to reduce poor reporting, but may not feel that poor reporting matters. Instead of hypothetical benefits or benefits to others, researchers report caring more about personal, immediate benefits like feeling confident, efficiency, and job performance.\n\nid: believed-costs title: Researchers may expect the costs to outweigh benefits driver_id: motivation\n\nResearchers may feel that the costs of using a RG - the time and work required and the added manuscript length - outweigh the benefits.\n\nid: feel-not-my-job title: Researchers may feel that checking reporting is someone else’s job. driver_id: motivation\n\nResearchers report feeling that completing a reporting checklist should be the job of the editor or peer reviewer, not the author. Editors and reviewers may also disagree about whose role it is.\n(NB. researchers, editors and reviewers could all check for reporting quality, but this research focusses only on researchers).\n\nid: feel-not-a-job title: Researchers may not consider writing as reporting driver_id: motivation\n\nResearchers may need to change their approach to writing or what they consider writing to be.Researchers differ in their writing process. Authors that follow a structured approach to writing may find it easier to incorporate RGs into their workflow. Some experienced researchers may be used to a way of working and reluctant to change, and some inexperienced researchers may be unaware of alternative writing processes."
  }
]